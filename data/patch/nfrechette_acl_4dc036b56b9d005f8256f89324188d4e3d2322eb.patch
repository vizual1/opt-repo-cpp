diff --git a/external/benchmark b/external/benchmark
--- a/external/benchmark
+++ b/external/benchmark
@@ -1 +1 @@
-Subproject commit 0d98dba29d66e93259db7daa53a9327df767a415
+Subproject commit 0d98dba29d66e93259db7daa53a9327df767a415-dirty
diff --git a/external/catch2 b/external/catch2
--- a/external/catch2
+++ b/external/catch2
@@ -1 +1 @@
-Subproject commit c4e3767e265808590986d5db6ca1b5532a7f3d13
+Subproject commit c4e3767e265808590986d5db6ca1b5532a7f3d13-dirty
diff --git a/external/rtm b/external/rtm
--- a/external/rtm
+++ b/external/rtm
@@ -1 +1 @@
-Subproject commit 2a2ba0f93b5ff034753d068828c6199e19e27686
+Subproject commit 2a2ba0f93b5ff034753d068828c6199e19e27686-dirty
diff --git a/external/sjson-cpp b/external/sjson-cpp
--- a/external/sjson-cpp
+++ b/external/sjson-cpp
@@ -1 +1 @@
-Subproject commit 54b3985e8ccc5efb5f24dabf0b4b802a996366e6
+Subproject commit 54b3985e8ccc5efb5f24dabf0b4b802a996366e6-dirty
diff --git a/includes/acl/math/vector4_packing.h b/includes/acl/math/vector4_packing.h
index ed8b375d..5a975919 100644
--- a/includes/acl/math/vector4_packing.h
+++ b/includes/acl/math/vector4_packing.h
@@ -977,71 +977,74 @@ namespace acl
 		return _mm_mul_ps(value, inv_max_value);
 	}
 #elif defined(RTM_NEON_INTRINSICS)
-	// Assumes the 'vector_data' is in big-endian order and padded in order to load up to 16 bytes from it
-	ACL_IMPL_DEBUG_FORCE_INLINE
-	rtm::vector4f RTM_SIMD_CALL unpack_vector3_uXX_unsafe(
-		uint32_t num_bits,
-		const uint8_t* vector_data,
-		uint32_t bit_offset)
-	{
-		ACL_ASSERT(num_bits <= 23, "This function does not support reading more than 23 bits per component");
-
-		struct PackedTableEntry
-		{
-			explicit constexpr PackedTableEntry(uint8_t num_bits_)
-				: max_value(num_bits_ == 0 ? 1.0F : (1.0F / float((1 << num_bits_) - 1)))
-				, mask((1U << num_bits_) - 1)
-			{}
-
-			float max_value;
-			uint32_t mask;
-		};
-
-		alignas(64) static constexpr PackedTableEntry k_packed_constants[24] =
-		{
-			PackedTableEntry(0), PackedTableEntry(1), PackedTableEntry(2), PackedTableEntry(3),
-			PackedTableEntry(4), PackedTableEntry(5), PackedTableEntry(6), PackedTableEntry(7),
-			PackedTableEntry(8), PackedTableEntry(9), PackedTableEntry(10), PackedTableEntry(11),
-			PackedTableEntry(12), PackedTableEntry(13), PackedTableEntry(14), PackedTableEntry(15),
-			PackedTableEntry(16), PackedTableEntry(17), PackedTableEntry(18), PackedTableEntry(19),
-			PackedTableEntry(20), PackedTableEntry(21), PackedTableEntry(22), PackedTableEntry(23),
-		};
-
-		const uint32_t bit_shift = 32 - num_bits;
+        // Assumes the 'vector_data' is in big-endian order and padded in order to load up to 16 bytes from it
+        ACL_IMPL_DEBUG_FORCE_INLINE
+        rtm::vector4f RTM_SIMD_CALL unpack_vector3_uXX_unsafe(
+                uint32_t num_bits,
+                const uint8_t* vector_data,
+                uint32_t bit_offset)
+        {
+                ACL_ASSERT(num_bits <= 23, "This function does not support reading more than 23 bits per component");
+
+                struct PackedTableEntry
+                {
+                        explicit constexpr PackedTableEntry(uint8_t num_bits_)
+                                : max_value(num_bits_ == 0 ? 1.0F : (1.0F / float((1 << num_bits_) - 1)))
+                                , mask((1U << num_bits_) - 1)
+                        {}
+
+                        float max_value;
+                        uint32_t mask;
+                };
+
+                alignas(64) static constexpr PackedTableEntry k_packed_constants[24] =
+                {
+                        PackedTableEntry(0), PackedTableEntry(1), PackedTableEntry(2), PackedTableEntry(3),
+                        PackedTableEntry(4), PackedTableEntry(5), PackedTableEntry(6), PackedTableEntry(7),
+                        PackedTableEntry(8), PackedTableEntry(9), PackedTableEntry(10), PackedTableEntry(11),
+                        PackedTableEntry(12), PackedTableEntry(13), PackedTableEntry(14), PackedTableEntry(15),
+                        PackedTableEntry(16), PackedTableEntry(17), PackedTableEntry(18), PackedTableEntry(19),
+                        PackedTableEntry(20), PackedTableEntry(21), PackedTableEntry(22), PackedTableEntry(23),
+                };
+
+                const uint32_t bit_shift = 32 - num_bits;
 #if defined(RTM_COMPILER_MSVC)
-		// MSVC uses an alias
-		uint32x4_t mask = vdupq_n_u32(static_cast<int32_t>(k_packed_constants[num_bits].mask));
+                // MSVC uses an alias
+                uint32x4_t mask = vdupq_n_u32(static_cast<int32_t>(k_packed_constants[num_bits].mask));
 #else
-		uint32x4_t mask = vdupq_n_u32(k_packed_constants[num_bits].mask);
+                uint32x4_t mask = vdupq_n_u32(k_packed_constants[num_bits].mask);
 #endif
-		float inv_max_value = k_packed_constants[num_bits].max_value;
-
-		uint32_t byte_offset = bit_offset / 8;
-		uint32_t vector_u32 = unaligned_load<uint32_t>(vector_data + byte_offset);
-		vector_u32 = byte_swap(vector_u32);
-		const uint32_t x32 = (vector_u32 >> (bit_shift - (bit_offset % 8)));
-
-		bit_offset += num_bits;
-
-		byte_offset = bit_offset / 8;
-		vector_u32 = unaligned_load<uint32_t>(vector_data + byte_offset);
-		vector_u32 = byte_swap(vector_u32);
-		const uint32_t y32 = (vector_u32 >> (bit_shift - (bit_offset % 8)));
-
-		bit_offset += num_bits;
-
-		byte_offset = bit_offset / 8;
-		vector_u32 = unaligned_load<uint32_t>(vector_data + byte_offset);
-		vector_u32 = byte_swap(vector_u32);
-		const uint32_t z32 = (vector_u32 >> (bit_shift - (bit_offset % 8)));
-
-		uint32x2_t xy = vcreate_u32(uint64_t(x32) | (uint64_t(y32) << 32));
-		uint32x2_t z = vcreate_u32(uint64_t(z32));
-		uint32x4_t value_u32 = vcombine_u32(xy, z);
-		value_u32 = vandq_u32(value_u32, mask);
-		float32x4_t value_f32 = vcvtq_f32_u32(value_u32);
-		return vmulq_n_f32(value_f32, inv_max_value);
-	}
+                float inv_max_value = k_packed_constants[num_bits].max_value;
+
+                // Optimized NEON: use 64-bit window loads and shifts to reduce memory accesses
+                const uint32_t byte_offset = bit_offset / 8;
+                const uint32_t shift_offset = bit_offset % 8;
+
+                uint64_t vector_u64 = unaligned_load<uint64_t>(vector_data + byte_offset + 0);
+                vector_u64 = byte_swap(vector_u64);
+                vector_u64 <<= shift_offset;
+                vector_u64 >>= 32;
+                const uint32_t x32 = uint32_t(vector_u64);
+
+                vector_u64 = unaligned_load<uint64_t>(vector_data + byte_offset + 4);
+                vector_u64 = byte_swap(vector_u64);
+                vector_u64 <<= shift_offset;
+                vector_u64 >>= 32;
+                const uint32_t y32 = uint32_t(vector_u64);
+
+                vector_u64 = unaligned_load<uint64_t>(vector_data + byte_offset + 8);
+                vector_u64 = byte_swap(vector_u64);
+                vector_u64 <<= shift_offset;
+                vector_u64 >>= 32;
+                const uint32_t z32 = uint32_t(vector_u64);
+
+                uint32x2_t xy = vcreate_u32(uint64_t(x32) | (uint64_t(y32) << 32));
+                uint32x2_t z = vcreate_u32(uint64_t(z32));
+                uint32x4_t value_u32 = vcombine_u32(xy, z);
+                value_u32 = vandq_u32(value_u32, mask);
+                float32x4_t value_f32 = vcvtq_f32_u32(value_u32);
+                return vmulq_n_f32(value_f32, inv_max_value);
+        }
 #else
 	// Assumes the 'vector_data' is in big-endian order and padded in order to load up to 16 bytes from it
 	ACL_IMPL_DEBUG_FORCE_INLINE
