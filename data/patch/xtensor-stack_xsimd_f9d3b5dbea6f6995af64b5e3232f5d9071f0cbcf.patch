diff --git a/include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp b/include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp
index a927ba9..23c26ed 100644
--- a/include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp
@@ -42,44 +42,44 @@ namespace xsimd {
     }
 
     // fma
-    template<class A, class T> batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
       return x * y + z;
     }
 
-    template<class A, class T> batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       auto res_r = fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));
       auto res_i = fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));
       return {res_r, res_i};
     }
 
     // fms
-    template<class A, class T> batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
       return x * y - z;
     }
 
-    template<class A, class T> batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       auto res_r = fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));
       auto res_i = fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));
       return {res_r, res_i};
     }
 
     // fnma
-    template<class A, class T> batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
       return -x * y + z;
     }
 
-    template<class A, class T> batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       auto res_r = - fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));
       auto res_i = - fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));
       return {res_r, res_i};
     }
 
     // fnms
-    template<class A, class T> batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {
       return -x * y - z;
     }
 
-    template<class A, class T> batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       auto res_r = - fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));
       auto res_i = - fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));
       return {res_r, res_i};
diff --git a/include/xsimd/arch/generic/xsimd_generic_complex.hpp b/include/xsimd/arch/generic/xsimd_generic_complex.hpp
index 79fd0f1..939037d 100644
--- a/include/xsimd/arch/generic/xsimd_generic_complex.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_complex.hpp
@@ -23,48 +23,40 @@ namespace xsimd {
     using namespace types;
 
     // real
-    template <class A, class T>
-    batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {
       return self;
     }
 
-    template <class A, class T>
-    batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       return self.real();
     }
 
     // imag
-    template <class A, class T>
-    batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {
       return batch<T, A>(T(0));
     }
 
-    template <class A, class T>
-    batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       return self.imag();
     }
     
     // arg
-    template<class A, class T>
-    real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {
       return atan2(imag(self), real(self));
     }
 
     // conj
-    template<class A, class T>
-    complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {
       return {real(self), - imag(self)};
     }
 
     // norm
-    template<class A, class T>
-    real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {
       return {fma(real(self), real(self), imag(self) * imag(self))};
     }
 
     // proj
-    template<class A, class T>
-    complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = complex_batch_type_t<batch<T, A>>;
       using real_batch = typename batch_type::real_batch;
       using real_value_type = typename real_batch::value_type;
@@ -75,8 +67,7 @@ namespace xsimd {
                     batch_type(self));
     }
 
-    template <class A, class T>
-    batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template <class A, class T> inline batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       return batch_bool<T, A>(isnan(self.real()) || isnan(self.imag()));
     }
   }
diff --git a/include/xsimd/arch/generic/xsimd_generic_details.hpp b/include/xsimd/arch/generic/xsimd_generic_details.hpp
index c14ba3d..6c0204d 100644
--- a/include/xsimd/arch/generic/xsimd_generic_details.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_details.hpp
@@ -21,91 +21,51 @@
 
 namespace xsimd {
   // Forward declaration. Should we put them in a separate file?
-  template<class T, class A>
-  batch<T, A> abs(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> abs(batch<std::complex<T>, A> const& self);
-  template<class T, class A>
-  bool any(batch_bool<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other);
-  template<class T, class A>
-  batch<T, A> bitofsign(batch<T, A> const& self);
-  template<class B, class T, class A>
-  B bitwise_cast(batch<T, A> const& self);
-  template<class A>
-  batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self);
-  template<class A>
-  batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self);
-  template<class A>
-  batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self);
-  template<class A>
-  batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self);
-  template<class T, class A>
-  batch<T, A> cos(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> cosh(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> exp(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z);
-  template<class T, class A>
-  batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z);
-  template<class T, class A>
-  batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e);
-  template<class T, class A>
-  T hadd(batch<T, A> const&);
-  template <class T, class A, uint64_t... Coefs>
-  batch<T, A> horner(const batch<T, A>& self);
-  template <class T, class A>
-  batch<T, A> hypot(const batch<T, A>& self);
-  template<class T, class A>
-  batch_bool<T, A> is_even(batch<T, A> const& self);
-  template<class T, class A>
-  batch_bool<T, A> is_flint(batch<T, A> const& self);
-  template<class T, class A>
-  batch_bool<T, A> is_odd(batch<T, A> const& self);
-  template<class T, class A>
-  batch_bool<T, A> isinf(batch<T, A> const& self);
-  template<class T, class A>
-  typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e);
-  template<class T, class A>
-  batch<T, A> log(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> nearbyint(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const& , batch<T, A> const& );
-  template<class T, class A>
-  batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const& , batch<std::complex<T>, A> const& );
-  template<class T, class A>
-  batch<T, A> sign(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> signnz(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> sin(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> sinh(batch<T, A> const& self);
-  template<class T, class A>
-  std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> sqrt(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> tan(batch<T, A> const& self);
-  template<class T, class A>
-  batch<as_float_t<T>, A> to_float(batch<T, A> const& self);
-  template<class T, class A>
-  batch<as_integer_t<T>, A> to_int(batch<T, A> const& self);
-  template<class T, class A>
-  batch<T, A> trunc(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> abs(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> abs(batch<std::complex<T>, A> const& self);
+  template<class T, class A> inline bool any(batch_bool<T, A> const& self);
+  template<class T, class A> inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other);
+  template<class T, class A> inline batch<T, A> bitofsign(batch<T, A> const& self);
+  template<class B, class T, class A> inline B bitwise_cast(batch<T, A> const& self);
+  template<class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self);
+  template<class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self);
+  template<class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self);
+  template<class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self);
+  template<class T, class A> inline batch<T, A> cos(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> cosh(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> exp(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z);
+  template<class T, class A> inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z);
+  template<class T, class A> inline batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e);
+  template<class T, class A> inline T hadd(batch<T, A> const&);
+  template <class T, class A, uint64_t... Coefs> inline batch<T, A> horner(const batch<T, A>& self);
+  template <class T, class A> inline batch<T, A> hypot(const batch<T, A>& self);
+  template<class T, class A> inline batch_bool<T, A> is_even(batch<T, A> const& self);
+  template<class T, class A> inline batch_bool<T, A> is_flint(batch<T, A> const& self);
+  template<class T, class A> inline batch_bool<T, A> is_odd(batch<T, A> const& self);
+  template<class T, class A> inline batch_bool<T, A> isinf(batch<T, A> const& self);
+  template<class T, class A> inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e);
+  template<class T, class A> inline batch<T, A> log(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> nearbyint(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const& , batch<T, A> const& );
+  template<class T, class A> inline batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const& , batch<std::complex<T>, A> const& );
+  template<class T, class A> inline batch<T, A> sign(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> signnz(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> sin(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> sinh(batch<T, A> const& self);
+  template<class T, class A> inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> sqrt(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> tan(batch<T, A> const& self);
+  template<class T, class A> inline batch<as_float_t<T>, A> to_float(batch<T, A> const& self);
+  template<class T, class A> inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& self);
+  template<class T, class A> inline batch<T, A> trunc(batch<T, A> const& self);
 
 
   namespace kernel {
 
     namespace detail {
-      template<class F, class A, class T, class... Batches>
-      batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {
+      template<class F, class A, class T, class... Batches> inline batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {
         constexpr std::size_t size = batch<T, A>::size;
         alignas(A::alignment()) T self_buffer[size];
         alignas(A::alignment()) T other_buffer[size];
@@ -126,23 +86,20 @@ namespace xsimd {
       struct with_fast_conversion{};
       struct with_slow_conversion{};
 
-      template <class A, class From, class To, class = void>
-      struct conversion_type_impl
+      template <class A, class From, class To, class = void> inline struct conversion_type_impl
       {
           using type = with_slow_conversion;
       };
 
       using xsimd::detail::void_t;
 
-      template <class A, class From, class To>
-      struct conversion_type_impl<A, From, To,
+      template <class A, class From, class To> inline struct conversion_type_impl<A, From, To,
                 void_t<decltype(fast_cast(std::declval<const From&>(), std::declval<const To&>(), std::declval<const A&>()))>>
       {
           using type = with_fast_conversion;
       };
 
-      template <class A, class From, class To>
-      using conversion_type = typename conversion_type_impl<A, From, To>::type;
+      template <class A, class From, class To> inline using conversion_type = typename conversion_type_impl<A, From, To>::type;
     }
 
     namespace detail {
@@ -155,26 +112,22 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-        template <class B, uint64_t c>
-        inline B coef() noexcept
+        template <class B, uint64_t c> inline inline B coef() noexcept
         {
             using value_type = typename B::value_type;
             return B(bit_cast<value_type>(as_unsigned_integer_t<value_type>(c)));
         }
-        template <class B>
-        inline B horner(const B&) noexcept
+        template <class B> inline inline B horner(const B&) noexcept
         {
             return B(typename B::value_type(0.));
         }
 
-        template <class B, uint64_t c0>
-        inline B horner(const B&) noexcept
+        template <class B, uint64_t c0> inline inline B horner(const B&) noexcept
         {
             return coef<B, c0>();
         }
 
-        template <class B, uint64_t c0, uint64_t c1, uint64_t... args>
-        inline B horner(const B& self) noexcept
+        template <class B, uint64_t c0, uint64_t c1, uint64_t... args> inline inline B horner(const B& self) noexcept
         {
             return fma(self, horner<B, c1, args...>(self), coef<B, c0>());
         }
@@ -188,20 +141,17 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-        template <class B>
-        inline B horner1(const B&) noexcept
+        template <class B> inline inline B horner1(const B&) noexcept
         {
             return B(1.);
         }
 
-        template <class B, uint64_t c0>
-        inline B horner1(const B& x) noexcept
+        template <class B, uint64_t c0> inline inline B horner1(const B& x) noexcept
         {
             return x + detail::coef<B, c0>();
         }
 
-        template <class B, uint64_t c0, uint64_t c1, uint64_t... args>
-        inline B horner1(const B& x) noexcept
+        template <class B, uint64_t c0, uint64_t c1, uint64_t... args> inline inline B horner1(const B& x) noexcept
         {
             return fma(x, horner1<B, c1, args...>(x), detail::coef<B, c0>());
         }
diff --git a/include/xsimd/arch/generic/xsimd_generic_logical.hpp b/include/xsimd/arch/generic/xsimd_generic_logical.hpp
index fb8110b..12cfa14 100644
--- a/include/xsimd/arch/generic/xsimd_generic_logical.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_logical.hpp
@@ -22,28 +22,28 @@ namespace xsimd {
     using namespace types;
 
     // ge
-    template<class A, class T> batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return other <= self;
     }
 
     // gt
-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return other < self;
     }
 
     // is_even
-    template<class A, class T> batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {
       return is_flint(self * T(0.5));
     }
 
     // is_flint
-    template<class A, class T> batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {
       auto frac = select(isnan(self - self), constants::nan<batch<T, A>>(), self - trunc(self));
       return frac == T(0.);
     }
 
     // is_odd
-    template<class A, class T> batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {
       return is_even(self - T(1.));
     }
 
@@ -52,10 +52,10 @@ namespace xsimd {
     batch_bool<T, A> isinf(batch<T, A> const& , requires_arch<generic>) {
       return batch_bool<T, A>(false);
     }
-    template<class A> batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {
       return abs(self) == std::numeric_limits<float>::infinity();
     }
-    template<class A> batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {
       return abs(self) == std::numeric_limits<double>::infinity();
     }
 
@@ -64,10 +64,10 @@ namespace xsimd {
     batch_bool<T, A> isfinite(batch<T, A> const& , requires_arch<generic>) {
       return batch_bool<T, A>(true);
     }
-    template<class A> batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {
       return (self - self) == 0;
     }
-    template<class A> batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {
       return (self - self) == 0;
     }
 
@@ -85,19 +85,17 @@ namespace xsimd {
 
 
     // neq
-    template<class A, class T> batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return !(other == self);
     }
 
     // logical_and
-    template <class A, class T>
-    batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return detail::apply([](T x, T y) { return x && y;}, self, other);
     }
 
     // logical_or
-    template <class A, class T>
-    batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return detail::apply([](T x, T y) { return x || y;}, self, other);
     }
   }
diff --git a/include/xsimd/arch/generic/xsimd_generic_math.hpp b/include/xsimd/arch/generic/xsimd_generic_math.hpp
index 2b8562b..289f571 100644
--- a/include/xsimd/arch/generic/xsimd_generic_math.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_math.hpp
@@ -37,23 +37,20 @@ namespace xsimd {
       }
     }
 
-    template<class A, class T>
-    batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       return hypot(z.real(), z.imag());
     }
 
     // batch_cast
-    template<class A, class T> batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {
       return self;
     }
 
     namespace detail {
-    template<class A, class T_out, class T_in>
-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {
+    template<class A, class T_out, class T_in> inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {
       return fast_cast(self, out, A{});
     }
-    template<class A, class T_out, class T_in>
-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {
+    template<class A, class T_out, class T_in> inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {
       static_assert(!std::is_same<T_in, T_out>::value, "there should be no conversion for this type combination");
       using batch_type_in = batch<T_in, A>;
       using batch_type_out = batch<T_out, A>;
@@ -67,13 +64,12 @@ namespace xsimd {
 
     }
 
-    template<class A, class T_out, class T_in>
-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {
+    template<class A, class T_out, class T_in> inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {
       return detail::batch_cast(self, out, A{}, detail::conversion_type<A, T_in, T_out>{});
     }
 
     // bitofsign
-    template<class A, class T> batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {
       static_assert(std::is_integral<T>::value, "int type implementation");
       if(std::is_unsigned<T>::value)
         return batch<T, A>(0);
@@ -81,10 +77,10 @@ namespace xsimd {
         return self >> (T)(8 * sizeof(T) - 1);
     }
 
-    template<class A> batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {
       return self & constants::minuszero<batch<float, A>>();
     }
-    template<class A> batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {
       return self & constants::minuszero<batch<double, A>>();
     }
 
@@ -99,7 +95,7 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-    template<class A> batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 batch_type z = abs(self);
 #ifndef XSIMD_NO_DENORMALS
@@ -143,7 +139,7 @@ namespace xsimd {
                 return select(self == batch_type(0.), self, x);
 #endif
     }
-    template<class A> batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 batch_type z = abs(self);
 #ifndef XSIMD_NO_DENORMALS
@@ -190,13 +186,13 @@ namespace xsimd {
     }
 
     // clip
-    template<class A, class T> batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {
       return min(hi, max(self, lo));
     }
 
 
     // copysign
-    template<class A, class T> batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return abs(self) | bitofsign(other);
     }
 
@@ -213,11 +209,9 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-        template <class B>
-        struct erf_kernel;
+        template <class B> inline struct erf_kernel;
 
-        template <class A>
-        struct erf_kernel<batch<float, A>>
+        template <class A> inline struct erf_kernel<batch<float, A>>
         {
             using batch_type = batch<float, A>;
             // computes erf(a0)/a0
@@ -269,8 +263,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct erf_kernel<batch<double, A>>
+        template <class A> inline struct erf_kernel<batch<double, A>>
         {
             using batch_type = batch<double, A>;
             // computes erf(a0)/a0
@@ -374,8 +367,7 @@ namespace xsimd {
          * ====================================================
          */
 
-    template<class A>
-    batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 batch_type x = abs(self);
                 batch_type r1(0.);
@@ -396,7 +388,7 @@ namespace xsimd {
 #endif
                 return r1;
             }
-    template<class A> batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 batch_type x = abs(self);
                 batch_type xx = x * x;
@@ -430,7 +422,7 @@ namespace xsimd {
     }
 
     // erfc
-    template<class A> batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 batch_type x = abs(self);
                 auto test0 = self < batch_type(0.);
@@ -451,7 +443,7 @@ namespace xsimd {
 #endif
                 return select(test0, batch_type(2.) - r1, r1);
     }
-    template<class A> batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 batch_type x = abs(self);
                 batch_type xx = x * x;
@@ -488,13 +480,11 @@ namespace xsimd {
     namespace detail
     {
 
-        template <class B>
-        struct estrin
+        template <class B> inline struct estrin
         {
             B x;
 
-            template <typename... Ts>
-            inline B operator()(const Ts&... coefs) noexcept
+            template <typename... Ts> inline inline B operator()(const Ts&... coefs) noexcept
             {
                 return eval(coefs...);
             }
@@ -510,45 +500,38 @@ namespace xsimd {
                 return fma(x, c1, c0);
             }
 
-            template <size_t... Is, class Tuple>
-            inline B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)
+            template <size_t... Is, class Tuple> inline inline B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)
             {
                 return estrin{x * x}(std::get<Is>(tuple)...);
             }
 
-            template <class... Args>
-            inline B eval(const std::tuple<Args...>& tuple) noexcept
+            template <class... Args> inline inline B eval(const std::tuple<Args...>& tuple) noexcept
             {
                 return eval(::xsimd::detail::make_index_sequence<sizeof...(Args)>(), tuple);
             }
 
-            template <class... Args>
-            inline B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept
+            template <class... Args> inline inline B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept
             {
                 return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0))));
             }
 
-            template <class... Args>
-            inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept
+            template <class... Args> inline inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept
             {
                 return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))));
             }
 
-            template <class... Args, class... Ts>
-            inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept
+            template <class... Args, class... Ts> inline inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept
             {
                 return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))), coefs...);
             }
 
-            template <class... Ts>
-            inline B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept
+            template <class... Ts> inline inline B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept
             {
                 return eval(std::make_tuple(eval(c0, c1)), coefs...);
             }
         };
     }
-    template <class T, class A, uint64_t... Coefs>
-    batch<T, A> estrin(const batch<T, A>& self) {
+    template <class T, class A, uint64_t... Coefs> inline batch<T, A> estrin(const batch<T, A>& self) {
       using batch_type = batch<T, A>;
       return detail::estrin<batch_type>{self}(detail::coef<batch_type, Coefs>()...);
     }
@@ -567,11 +550,9 @@ namespace xsimd {
     {
         enum exp_reduction_tag { exp_tag, exp2_tag, exp10_tag };
 
-        template <class B, exp_reduction_tag Tag>
-        struct exp_reduction_base;
+        template <class B, exp_reduction_tag Tag> inline struct exp_reduction_base;
 
-        template <class B>
-        struct exp_reduction_base<B, exp_tag>
+        template <class B> inline struct exp_reduction_base<B, exp_tag>
         {
             static constexpr B maxlog() noexcept
             {
@@ -584,8 +565,7 @@ namespace xsimd {
             }
         };
 
-        template <class B>
-        struct exp_reduction_base<B, exp10_tag>
+        template <class B> inline struct exp_reduction_base<B, exp10_tag>
         {
             static constexpr B maxlog() noexcept
             {
@@ -598,8 +578,7 @@ namespace xsimd {
             }
         };
 
-        template <class B>
-        struct exp_reduction_base<B, exp2_tag>
+        template <class B> inline struct exp_reduction_base<B, exp2_tag>
         {
             static constexpr B maxlog() noexcept
             {
@@ -612,11 +591,9 @@ namespace xsimd {
             }
         };
 
-        template <class T, class A, exp_reduction_tag Tag>
-        struct exp_reduction;
+        template <class T, class A, exp_reduction_tag Tag> inline struct exp_reduction;
 
-        template <class A>
-        struct exp_reduction<float, A, exp_tag> : exp_reduction_base<batch<float, A>, exp_tag>
+        template <class A> inline struct exp_reduction<float, A, exp_tag> : exp_reduction_base<batch<float, A>, exp_tag>
         {
           using batch_type = batch<float, A>;
             static inline batch_type approx(const batch_type& x)
@@ -640,8 +617,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct exp_reduction<float, A, exp10_tag> : exp_reduction_base<batch<float, A>, exp10_tag>
+        template <class A> inline struct exp_reduction<float, A, exp10_tag> : exp_reduction_base<batch<float, A>, exp10_tag>
         {
           using batch_type = batch<float, A>;
             static inline batch_type approx(const batch_type& x)
@@ -666,8 +642,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct exp_reduction<float, A, exp2_tag> : exp_reduction_base<batch<float, A>, exp2_tag>
+        template <class A> inline struct exp_reduction<float, A, exp2_tag> : exp_reduction_base<batch<float, A>, exp2_tag>
         {
             using batch_type = batch<float, A>;
             static inline batch_type approx(const batch_type& x)
@@ -690,8 +665,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct exp_reduction<double, A, exp_tag> : exp_reduction_base<batch<double, A>, exp_tag>
+        template <class A> inline struct exp_reduction<double, A, exp_tag> : exp_reduction_base<batch<double, A>, exp_tag>
         {
           using batch_type = batch<double, A>;
             static inline batch_type approx(const batch_type& x)
@@ -722,8 +696,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct exp_reduction<double, A, exp10_tag> : exp_reduction_base<batch<double, A>, exp10_tag>
+        template <class A> inline struct exp_reduction<double, A, exp10_tag> : exp_reduction_base<batch<double, A>, exp10_tag>
         {
           using batch_type = batch<double, A>;
             static inline batch_type approx(const batch_type& x)
@@ -756,8 +729,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct exp_reduction<double, A, exp2_tag> : exp_reduction_base<batch<double, A>, exp2_tag>
+        template <class A> inline struct exp_reduction<double, A, exp2_tag> : exp_reduction_base<batch<double, A>, exp2_tag>
         {
           using batch_type = batch<double, A>;
             static inline batch_type approx(const batch_type& x)
@@ -786,7 +758,7 @@ namespace xsimd {
             }
         };
 
-      template<exp_reduction_tag Tag, class A> batch<float, A> exp(batch<float, A> const& self) {
+      template<exp_reduction_tag Tag, class A> inline batch<float, A> exp(batch<float, A> const& self) {
         using batch_type = batch<float, A>;
         using reducer_t = exp_reduction<float, A, Tag>;
         batch_type x;
@@ -796,7 +768,7 @@ namespace xsimd {
         x = select(self >= reducer_t::maxlog(), constants::infinity<batch_type>(), x);
         return x;
       }
-      template<exp_reduction_tag Tag, class A> batch<double, A> exp(batch<double, A> const& self) {
+      template<exp_reduction_tag Tag, class A> inline batch<double, A> exp(batch<double, A> const& self) {
         using batch_type = batch<double, A>;
         using reducer_t = exp_reduction<double, A, Tag>;
         batch_type hi, lo, x;
@@ -809,23 +781,23 @@ namespace xsimd {
       }
     }
 
-    template<class A, class T> batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {
       return detail::exp<detail::exp_tag>(self);
     }
 
-    template<class A, class T> batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       using batch_type = batch<std::complex<T>, A>;
       auto isincos = sincos(self.imag());
       return exp(self.real()) * batch_type(std::get<1>(isincos), std::get<0>(isincos));
     }
 
     // exp10
-    template<class A, class T> batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {
       return detail::exp<detail::exp10_tag>(self);
     }
 
     // exp2
-    template<class A, class T> batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {
       return detail::exp<detail::exp2_tag>(self);
     }
 
@@ -840,8 +812,7 @@ namespace xsimd {
              * (See copy at http://boost.org/LICENSE_1_0.txt)
              * ====================================================
              */
-          template<class A>
-          static inline batch<float, A> expm1(const batch<float, A>& a)
+          template<class A> inline static inline batch<float, A> expm1(const batch<float, A>& a)
             {
               using batch_type = batch<float, A>;
                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);
@@ -864,8 +835,7 @@ namespace xsimd {
                 return ldexp(y, ik);
             }
 
-            template<class A>
-            static inline batch<double, A> expm1(const batch<double, A>& a)
+            template<class A> inline static inline batch<double, A> expm1(const batch<double, A>& a)
             {
               using batch_type = batch<double, A>;
                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);
@@ -895,7 +865,7 @@ namespace xsimd {
 
     }
 
-    template<class A, class T> batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
       return select(self < constants::logeps<batch_type>(),
                     batch_type(-1.),
@@ -904,8 +874,7 @@ namespace xsimd {
                            detail::expm1(self)));
     }
 
-    template <class A, class T>
-    batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+    template <class A, class T> inline batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)
     {
         using batch_type = batch<std::complex<T>, A>;
         using real_batch = typename batch_type::real_batch;
@@ -917,12 +886,12 @@ namespace xsimd {
     }
 
     // fdim
-    template<class A, class T> batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return fmax(batch<T, A>(0), self - other);
     }
 
     // fmod
-    template<class A, class T> batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return fnma(trunc(self / other), other, self);
     }
 
@@ -936,8 +905,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template <class A, class T>
-    batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {
         using batch_type = batch<T, A>;
         using i_type = batch<as_integer_t<T>, A>;
         i_type m1f = constants::mask1frexp<batch_type>();
@@ -949,29 +917,27 @@ namespace xsimd {
     }
 
     // from bool
-    template<class A, class T>
-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {
       return batch<T, A>(self.data) & batch<T,A>(1);
     }
 
     // hadd
-    template<class A, class T> std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       return {hadd(self.real()), hadd(self.imag())};
     }
 
     // horner
-    template <class T, class A, uint64_t... Coefs>
-    batch<T, A> horner(const batch<T, A>& self) {
+    template <class T, class A, uint64_t... Coefs> inline batch<T, A> horner(const batch<T, A>& self) {
       return detail::horner<batch<T, A>, Coefs...>(self);
     }
 
     // hypot
-    template<class A, class T> batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       return sqrt(fma(self, self, other * other));
     }
 
     // ipow
-    template<class A, class T, class ITy> batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {
+    template<class A, class T, class ITy> inline batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {
       return ::xsimd::detail::ipow(self, other);
     }
 
@@ -986,8 +952,7 @@ namespace xsimd {
       * (See copy at http://boost.org/LICENSE_1_0.txt)
       * ====================================================
       */
-    template <class A, class T>
-    batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {
+    template <class A, class T> inline batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {
         using batch_type = batch<T, A>;
         using itype = as_integer_t<batch_type>;
         itype ik = other + constants::maxexponent<T>();
@@ -996,7 +961,7 @@ namespace xsimd {
     }
 
     // lgamma
-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);
+    template<class A, class T> inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);
 
     namespace detail {
     /* origin: boost/simd/arch/common/detail/generic/gammaln_kernel.hpp */
@@ -1008,8 +973,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A>
-            static inline batch<float, A> gammalnB(const batch<float, A>& x)
+    template<class A> inline static inline batch<float, A> gammalnB(const batch<float, A>& x)
             {
                 return horner<batch<float, A>,
                               0x3ed87730,  //    4.227843421859038E-001
@@ -1023,8 +987,7 @@ namespace xsimd {
                               >(x);
             }
 
-    template<class A>
-            static inline batch<float, A> gammalnC(const batch<float, A>& x)
+    template<class A> inline static inline batch<float, A> gammalnC(const batch<float, A>& x)
             {
                 return horner<batch<float, A>,
                               0xbf13c468,  //   -5.772156501719101E-001
@@ -1038,8 +1001,7 @@ namespace xsimd {
                               >(x);
             }
 
-    template<class A>
-            static inline batch<float, A> gammaln2(const batch<float, A>& x)
+    template<class A> inline static inline batch<float, A> gammaln2(const batch<float, A>& x)
             {
                 return horner<batch<float, A>,
                               0x3daaaa94,  //   8.333316229807355E-002f
@@ -1047,8 +1009,7 @@ namespace xsimd {
                               0x3a31fd69  //   6.789774945028216E-004f
                               >(x);
             }
-    template<class A>
-            static inline batch<double, A> gammaln1(const batch<double, A>& x)
+    template<class A> inline static inline batch<double, A> gammaln1(const batch<double, A>& x)
             {
                 return horner<batch<double, A>,
                               0xc12a0c675418055eull,  //  -8.53555664245765465627E5
@@ -1069,8 +1030,7 @@ namespace xsimd {
                            >(x);
             }
 
-    template<class A>
-            static inline batch<double, A> gammalnA(const batch<double, A>& x)
+    template<class A> inline static inline batch<double, A> gammalnA(const batch<double, A>& x)
             {
                 return horner<batch<double, A>,
                               0x3fb555555555554bull,  //    8.33333333333331927722E-2
@@ -1089,11 +1049,9 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-        template <class B>
-        struct lgamma_impl;
+        template <class B> inline struct lgamma_impl;
 
-        template <class A>
-        struct lgamma_impl<batch<float, A>>
+        template <class A> inline struct lgamma_impl<batch<float, A>>
         {
           using batch_type = batch<float, A>;
             static inline batch_type compute(const batch_type& a)
@@ -1213,8 +1171,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct lgamma_impl<batch<double, A>>
+        template <class A> inline struct lgamma_impl<batch<double, A>>
         {
           using batch_type = batch<double, A>;
 
@@ -1295,7 +1252,7 @@ namespace xsimd {
         };
     }
 
-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {
       return detail::lgamma_impl<batch<T, A>>::compute(self);
     }
 
@@ -1310,7 +1267,7 @@ namespace xsimd {
              * (See copy at http://boost.org/LICENSE_1_0.txt)
              * ====================================================
              */
-    template<class A> batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
       using i_type = as_integer_t<batch_type>;
       batch_type x = self;
@@ -1347,7 +1304,7 @@ namespace xsimd {
       return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
 
-    template<class A> batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {
                using batch_type = batch<double, A>;
                 using i_type = as_integer_t<batch_type>;
 
@@ -1394,14 +1351,13 @@ namespace xsimd {
                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
 
-    template <class A, class T>
-    batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+    template <class A, class T> inline batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)
     {
         return batch<std::complex<T>, A>(log(abs(z)), atan2(z.imag(), z.real()));
     }
 
     // log2
-    template<class A> batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 using i_type = as_integer_t<batch_type>;
                 batch_type x = self;
@@ -1437,7 +1393,7 @@ namespace xsimd {
 #endif
                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
-    template<class A> batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 using i_type = as_integer_t<batch_type>;
                 batch_type x = self;
@@ -1489,8 +1445,7 @@ namespace xsimd {
                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
     namespace detail {
-        template <class T, class A>
-        inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base)
+        template <class T, class A> inline inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base)
         {
             using batch_type = batch<T, A>;
             using rv_type = typename batch_type::value_type;
@@ -1498,7 +1453,7 @@ namespace xsimd {
         }
   }
 
-    template<class A, class T> batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
       return detail::logN_complex_impl(self, std::log(2));
     }
 
@@ -1514,7 +1469,7 @@ namespace xsimd {
              * is preserved.
              * ====================================================
              */
-    template<class A> batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 const batch_type
                     ivln10hi(4.3432617188e-01f),
@@ -1561,7 +1516,7 @@ namespace xsimd {
 #endif
                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
-    template<class A> batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 const batch_type
                     ivln10hi(4.34294481878168880939e-01),
@@ -1619,8 +1574,7 @@ namespace xsimd {
                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
 
-        template <class A, class T>
-            batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)
             {
                 return detail::logN_complex_impl(z, std::log(10));
             }
@@ -1635,7 +1589,7 @@ namespace xsimd {
              * (See copy at http://boost.org/LICENSE_1_0.txt)
              * ====================================================
              */
-    template<class A> batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 using i_type = as_integer_t<batch_type>;
                 const batch_type uf = self + batch_type(1.);
@@ -1663,7 +1617,7 @@ namespace xsimd {
 #endif
                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
-    template<class A> batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 using i_type = as_integer_t<batch_type>;
                 const batch_type uf = self + batch_type(1.);
@@ -1700,7 +1654,7 @@ namespace xsimd {
                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);
     }
 
-    template<class A, class T> batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {
         using batch_type = batch<std::complex<T>, A>;
         using real_batch = typename batch_type::real_batch;
         batch_type u = 1 + self;
@@ -1726,7 +1680,7 @@ namespace xsimd {
       return self;
     }
     namespace detail {
-      template<class A, class T> batch<T, A> nearbyintf(batch<T, A> const& self) {
+      template<class A, class T> inline batch<T, A> nearbyintf(batch<T, A> const& self) {
         using batch_type = batch<T, A>;
         batch_type s = bitofsign(self);
         batch_type v = self ^ s;
@@ -1744,10 +1698,10 @@ namespace xsimd {
         return s ^ select(v < t2n, d, v);
       }
     }
-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {
       return detail::nearbyintf(self);
     }
-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {
       return detail::nearbyintf(self);
     }
 
@@ -1770,23 +1724,19 @@ namespace xsimd {
             }
         };
 
-        template <class T, class A>
-        struct bitwise_cast_batch;
+        template <class T, class A> inline struct bitwise_cast_batch;
 
-        template <class A>
-        struct bitwise_cast_batch<float, A>
+        template <class A> inline struct bitwise_cast_batch<float, A>
         {
             using type = batch<int32_t, A>;
         };
 
-        template <class A>
-        struct bitwise_cast_batch<double, A>
+        template <class A> inline struct bitwise_cast_batch<double, A>
         {
             using type = batch<int64_t, A>;
         };
 
-        template <class T, class A>
-        struct nextafter_kernel<T, A, false>
+        template <class T, class A> inline struct nextafter_kernel<T, A, false>
         {
             using batch_type = batch<T, A>;
             using int_batch = typename bitwise_cast_batch<T, A>::type;
@@ -1805,7 +1755,7 @@ namespace xsimd {
             }
         };
     }
-    template<class A, class T> batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {
       using kernel = detail::nextafter_kernel<T, A>;
       return select(from == to, from,
                     select(to > from, kernel::next(from), kernel::prev(from)));
@@ -1822,7 +1772,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A, class T> batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
         using batch_type = batch<T, A>;
         auto negx = self < batch_type(0.);
         batch_type z = exp(other * log(abs(self)));
@@ -1831,8 +1781,7 @@ namespace xsimd {
         return select(invalid, constants::nan<batch_type>(), z);
     }
 
-        template <class A, class T>
-        inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using cplx_batch = batch<std::complex<T>, A>;
             using real_batch = typename cplx_batch::real_batch;
@@ -1851,12 +1800,10 @@ namespace xsimd {
 
 
     // remainder
-    template<class A>
-    batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {
+    template<class A> inline batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {
       return fnma(nearbyint(self / other), other, self);
     }
-    template<class A>
-    batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {
+    template<class A> inline batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {
       return fnma(nearbyint(self / other), other, self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -1866,8 +1813,7 @@ namespace xsimd {
     }
 
     // select
-    template<class A, class T>
-    batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {
       return {select(cond, true_br.real(), false_br.real()), select(cond, true_br.imag(), false_br.imag())};
     }
 
@@ -1879,7 +1825,7 @@ namespace xsimd {
     }
 
     namespace detail {
-    template<class T, class A> batch<T, A> signf(batch<T, A> const& self) {
+    template<class T, class A> inline batch<T, A> signf(batch<T, A> const& self) {
       using batch_type = batch<T, A>;
       batch_type res = select(self > batch_type(0.f), batch_type(1.f), batch_type(0.f)) - select(self < batch_type(0.f), batch_type(1.f), batch_type(0.f));
 #ifdef XSIMD_NO_NANS
@@ -1890,14 +1836,13 @@ namespace xsimd {
     }
     }
 
-    template<class A> batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {
       return detail::signf(self);
     }
-    template<class A> batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {
       return detail::signf(self);
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
@@ -1915,7 +1860,7 @@ namespace xsimd {
     }
 
     namespace detail {
-    template<class T, class A> batch<T, A> signnzf(batch<T, A> const& self) {
+    template<class T, class A> inline batch<T, A> signnzf(batch<T, A> const& self) {
       using batch_type = batch<T, A>;
 #ifndef XSIMD_NO_NANS
                 return select(isnan(self), constants::nan<batch_type>(), batch_type(1.) | (constants::signmask<batch_type>() & self));
@@ -1925,15 +1870,15 @@ namespace xsimd {
     }
     }
 
-    template<class A> batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {
       return detail::signnzf(self);
     }
-    template<class A> batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {
       return detail::signnzf(self);
     }
 
     // sqrt
-    template<class A, class T> batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
 
                 constexpr T csqrt_scale_factor = std::is_same<T, float>::value?6.7108864e7f:1.8014398509481984e16;
                 constexpr T csqrt_scale = std::is_same<T, float>::value?1.220703125e-4f:7.450580596923828125e-9;
@@ -1980,11 +1925,9 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-        template <class B>
-        struct stirling_kernel;
+        template <class B> inline struct stirling_kernel;
 
-        template <class A>
-        struct stirling_kernel<batch<float, A>>
+        template <class A> inline struct stirling_kernel<batch<float, A>>
         {
           using batch_type = batch<float, A>;
             static inline batch_type compute(const batch_type& x)
@@ -2007,8 +1950,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct stirling_kernel<batch<double, A>>
+        template <class A> inline struct stirling_kernel<batch<double, A>>
         {
           using batch_type = batch<double, A>;
             static inline batch_type compute(const batch_type& x)
@@ -2042,8 +1984,7 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-        template <class T, class A>
-        inline batch<T, A> stirling(const batch<T, A>& a)
+        template <class T, class A> inline inline batch<T, A> stirling(const batch<T, A>& a)
         {
           using batch_type = batch<T, A>;
             const batch_type stirlingsplitlim = stirling_kernel<batch_type>::split_limit();
@@ -2074,11 +2015,9 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-        template <class B>
-        struct tgamma_kernel;
+        template <class B> inline struct tgamma_kernel;
 
-        template <class A>
-        struct tgamma_kernel<batch<float, A>>
+        template <class A> inline struct tgamma_kernel<batch<float, A>>
         {
           using batch_type = batch<float, A>;
             static inline batch_type compute(const batch_type& x)
@@ -2096,8 +2035,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct tgamma_kernel<batch<double, A>>
+        template <class A> inline struct tgamma_kernel<batch<double, A>>
         {
           using batch_type = batch<double, A>;
             static inline batch_type compute(const batch_type& x)
@@ -2133,8 +2071,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-        template <class B>
-        B tgamma_large_negative(const B& a)
+        template <class B> inline B tgamma_large_negative(const B& a)
         {
             B st = stirling(a);
             B p = floor(a);
@@ -2147,8 +2084,7 @@ namespace xsimd {
             return sgngam * constants::pi<B>() / (z * st);
         }
 
-        template <class B, class BB>
-        B tgamma_other(const B& a, const BB& test)
+        template <class B, class BB> inline B tgamma_other(const B& a, const BB& test)
         {
             B x = select(test, B(2.), a);
 #ifndef XSIMD_NO_INFINITIES
@@ -2186,7 +2122,7 @@ namespace xsimd {
         }
     }
 
-    template<class A, class T> batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
             auto nan_result = (self < batch_type(0.) && is_flint(self));
 #ifndef XSIMD_NO_INVALIDS
diff --git a/include/xsimd/arch/generic/xsimd_generic_memory.hpp b/include/xsimd/arch/generic/xsimd_generic_memory.hpp
index 39f4c10..e639c75 100644
--- a/include/xsimd/arch/generic/xsimd_generic_memory.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_memory.hpp
@@ -26,7 +26,7 @@ namespace xsimd {
     using namespace types;
 
     // extract_pair
-    template<class A, class T> batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {
       constexpr std::size_t size = batch<T, A>::size;
       assert(0<= i && i< size && "index in bounds");
 
@@ -51,14 +51,12 @@ namespace xsimd {
 
     // load_aligned
     namespace detail {
-      template<class A, class T_in, class T_out>
-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {
+      template<class A, class T_in, class T_out> inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {
         using batch_type_in = batch<T_in, A>;
         using batch_type_out = batch<T_out, A>;
         return fast_cast(batch_type_in::load_aligned(mem), batch_type_out(), A{});
       }
-      template<class A, class T_in, class T_out>
-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {
+      template<class A, class T_in, class T_out> inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {
         static_assert(!std::is_same<T_in, T_out>::value, "there should be a direct load for this type combination");
         using batch_type_out = batch<T_out, A>;
         alignas(A::alignment()) T_out buffer[batch_type_out::size];
@@ -66,34 +64,29 @@ namespace xsimd {
         return batch_type_out::load_aligned(buffer);
       }
     }
-    template<class A, class T_in, class T_out>
-    batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {
+    template<class A, class T_in, class T_out> inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {
       return detail::load_aligned<A>(mem, cvt, A{}, detail::conversion_type<A, T_in, T_out>{});
     }
 
     // load_unaligned
     namespace detail {
-      template<class A, class T_in, class T_out>
-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {
+      template<class A, class T_in, class T_out> inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {
         using batch_type_in = batch<T_in, A>;
         using batch_type_out = batch<T_out, A>;
         return fast_cast(batch_type_in::load_unaligned(mem), batch_type_out(), A{});
       }
 
-      template<class A, class T_in, class T_out>
-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {
+      template<class A, class T_in, class T_out> inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {
         static_assert(!std::is_same<T_in, T_out>::value, "there should be a direct load for this type combination");
         return load_aligned<A>(mem, cvt, generic{}, with_slow_conversion{});
       }
     }
-    template<class A, class T_in, class T_out>
-    batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {
+    template<class A, class T_in, class T_out> inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {
       return detail::load_unaligned<A>(mem, cvt, generic{}, detail::conversion_type<A, T_in, T_out>{});
     }
 
     // store
-    template<class T, class A>
-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {
+    template<class T, class A> inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {
       using batch_type = batch<T, A>;
       constexpr auto size = batch_bool<T, A>::size;
       alignas(A::alignment()) T buffer[size];
@@ -104,7 +97,7 @@ namespace xsimd {
 
 
     // store_aligned
-    template<class A, class T_in, class T_out> void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {
+    template<class A, class T_in, class T_out> inline void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {
       static_assert(!std::is_same<T_in, T_out>::value, "there should be a direct store for this type combination");
       alignas(A::alignment()) T_in buffer[batch<T_in, A>::size];
       store_aligned(&buffer[0], self);
@@ -112,35 +105,31 @@ namespace xsimd {
     }
 
     // store_unaligned
-    template<class A, class T_in, class T_out> void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {
+    template<class A, class T_in, class T_out> inline void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {
       static_assert(!std::is_same<T_in, T_out>::value, "there should be a direct store for this type combination");
       return store_aligned<A>(mem, self, generic{});
     }
 
     namespace detail
     {
-        template <class A, class T>
-        batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)
+        template <class A, class T> inline batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)
         {
             static_assert(std::is_same<T, void>::value, "load_complex not implemented for the required architecture");
         }
 
-        template <class A, class T>
-        batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)
+        template <class A, class T> inline batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)
         {
             static_assert(std::is_same<T, void>::value, "complex_high not implemented for the required architecture");
         }
 
-        template <class A, class T>
-        batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)
+        template <class A, class T> inline batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)
         {
             static_assert(std::is_same<T, void>::value, "complex_low not implemented for the required architecture");
         }
     }
 
     // load_complex_aligned
-    template <class A, class T_out, class T_in>
-    batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {
+    template <class A, class T_out, class T_in> inline batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {
       using real_batch = batch<T_out, A>;
       T_in const* buffer = reinterpret_cast<T_in const*>(mem);
       real_batch hi = real_batch::load_aligned(buffer),
@@ -149,8 +138,7 @@ namespace xsimd {
     }
 
     // load_complex_unaligned
-    template <class A, class T_out, class T_in>
-    batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {
+    template <class A, class T_out, class T_in> inline batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {
       using real_batch = batch<T_out, A>;
       T_in const* buffer = reinterpret_cast<T_in const*>(mem);
       real_batch hi = real_batch::load_unaligned(buffer),
@@ -159,8 +147,7 @@ namespace xsimd {
     }
 
     // store_complex_aligned
-    template <class A, class T_out, class T_in>
-    void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {
+    template <class A, class T_out, class T_in> inline void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {
         using real_batch = batch<T_in, A>;
         real_batch hi = detail::complex_high(src, A{});
         real_batch lo = detail::complex_low(src, A{});
@@ -170,8 +157,7 @@ namespace xsimd {
     }
 
     // store_compelx_unaligned
-    template <class A, class T_out, class T_in>
-    void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {
+    template <class A, class T_out, class T_in> inline void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {
         using real_batch = batch<T_in, A>;
         real_batch hi = detail::complex_high(src, A{});
         real_batch lo = detail::complex_low(src, A{});
diff --git a/include/xsimd/arch/generic/xsimd_generic_rounding.hpp b/include/xsimd/arch/generic/xsimd_generic_rounding.hpp
index b1c9881..c56c84e 100644
--- a/include/xsimd/arch/generic/xsimd_generic_rounding.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_rounding.hpp
@@ -23,20 +23,20 @@ namespace xsimd {
     using namespace types;
 
     // ceil
-    template<class A, class T> batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {
       batch<T, A> truncated_self = trunc(self);
       return select(truncated_self < self, truncated_self + 1, truncated_self);
     }
 
 
     // floor
-    template<class A, class T> batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {
       batch<T, A> truncated_self = trunc(self);
       return select(truncated_self > self, truncated_self - 1, truncated_self);
     }
 
     // round
-    template<class A, class T> batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {
       auto v = abs(self);
       auto c = ceil(v);
       auto cp = select(c - 0.5 > v, c - 1, c);
@@ -48,10 +48,10 @@ namespace xsimd {
     batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) {
       return self;
     }
-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {
       return select(abs(self) < constants::maxflint<batch<float, A>>(), to_float(to_int(self)), self);
     }
-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {
       return select(abs(self) < constants::maxflint<batch<double, A>>(), to_float(to_int(self)), self);
     }
 
diff --git a/include/xsimd/arch/generic/xsimd_generic_trigo.hpp b/include/xsimd/arch/generic/xsimd_generic_trigo.hpp
index f649c6c..4d8230e 100644
--- a/include/xsimd/arch/generic/xsimd_generic_trigo.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_trigo.hpp
@@ -30,7 +30,7 @@ namespace xsimd {
     using namespace types;
 
     // acos
-    template<class A, class T> batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
         batch_type x = abs(self);
         auto x_larger_05 = x > batch_type(0.5);
@@ -40,8 +40,7 @@ namespace xsimd {
         x = select(self < batch_type(-0.5), constants::pi<batch_type>() - x, x);
         return select(x_larger_05, x, constants::pio2<batch_type>() - x);
     }
-        template <class A, class T>
-        batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
@@ -59,7 +58,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A, class T> batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 batch_type x = self - batch_type(1.);
                 auto test = x > constants::oneotwoeps<batch_type>();
@@ -67,8 +66,7 @@ namespace xsimd {
                 batch_type l1pz = log1p(z);
                 return select(test, l1pz + constants::log_2<batch_type>(), l1pz);
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             batch_type w = acos(z);
@@ -77,7 +75,7 @@ namespace xsimd {
         }
 
     // asin
-    template<class A> batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 batch_type x = abs(self);
                 batch_type sign = bitofsign(self);
@@ -94,7 +92,7 @@ namespace xsimd {
                 z = select(x_larger_05, constants::pio2<batch_type>() - (z1 + z1), z1);
                 return z ^ sign;
     }
-    template<class A> batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 batch_type x = abs(self);
                 auto small_cond = x < constants::sqrteps<batch_type>();
@@ -136,8 +134,7 @@ namespace xsimd {
                                      select(x > ct1, zz1, zz2)) ^
                                   bitofsign(self));
     }
-        template <class A, class T>
-        batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
@@ -174,23 +171,20 @@ namespace xsimd {
                 return (x1 & x2) + ((x1 ^ x2) >> 1);
             }
 
-        template <class A, class T>
-          batch<T, A>
+        template <class A, class T> inline batch<T, A>
             averagef(const batch<T, A>& x1, const batch<T, A>& x2)
             {
               using batch_type = batch<T, A>;
                 return fma(x1, batch_type(0.5), x2 * batch_type(0.5));
             }
-        template<class A>
-          batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {
+        template<class A> inline batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {
             return averagef(x1, x2);
           }
-        template<class A>
-          batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {
+        template<class A> inline batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {
             return averagef(x1, x2);
           }
     }
-    template<class A> batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {
       using batch_type = batch<float, A>;
                 batch_type x = abs(self);
                 auto lthalf = x < batch_type(0.5);
@@ -216,7 +210,7 @@ namespace xsimd {
                 return select(lthalf, z, log(tmp) + constants::log_2<batch_type>()) ^ bts;
 #endif
     }
-    template<class A> batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {
+    template<class A> inline batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {
       using batch_type = batch<double, A>;
                 batch_type x = abs(self);
                 auto test = x > constants::oneosqrteps<batch_type>();
@@ -228,8 +222,7 @@ namespace xsimd {
                 z = select(test, l1pz + constants::log_2<batch_type>(), l1pz);
                 return bitofsign(self) ^ z;
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             batch_type w = asin(batch_type(-z.imag(), z.real()));
@@ -239,8 +232,7 @@ namespace xsimd {
 
     // atan
     namespace detail {
-    template<class A>
-            static inline batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx)
+    template<class A> inline static inline batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx)
             {
               using batch_type = batch<float, A>;
                 const auto flag1 = x < constants::tan3pio8<batch_type>();
@@ -260,8 +252,7 @@ namespace xsimd {
                 z1 = select(!flag1, z1 + constants::pio_2lo<batch_type>(), z1);
                 return yy + z1;
             }
-    template<class A>
-            static inline batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx)
+    template<class A> inline static inline batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx)
             {
               using batch_type = batch<double, A>;
                 const auto flag1 = x < constants::tan3pio8<batch_type>();
@@ -289,14 +280,13 @@ namespace xsimd {
                 return yy + z;
             }
     }
-    template<class A, class T> batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 const batch_type absa = abs(self);
                 const batch_type x = detail::kernel_atan(absa, batch_type(1.) / absa);
                 return x ^ bitofsign(self);
     }
-        template <class A, class T>
-        batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
@@ -326,7 +316,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A, class T> batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 batch_type x = abs(self);
                 batch_type t = x + x;
@@ -335,8 +325,7 @@ namespace xsimd {
                 batch_type tmp = select(test, x, t) / z;
                 return bitofsign(self) ^ (batch_type(0.5) * log1p(select(test, fma(t, tmp, t), tmp)));
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using batch_type = batch<std::complex<T>, A>;
             batch_type w = atan(batch_type(-z.imag(), z.real()));
@@ -345,7 +334,7 @@ namespace xsimd {
         }
 
     // atan2
-    template<class A, class T> batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 const batch_type q = abs(self / other);
                 const batch_type z = detail::kernel_atan(q, batch_type(1.) / q);
@@ -356,18 +345,15 @@ namespace xsimd {
     // cos
     namespace detail
     {
-        template <class T, class A>
-        batch<T, A> quadrant(const batch<T, A>& x) {
+        template <class T, class A> inline batch<T, A> quadrant(const batch<T, A>& x) {
           return x & batch<T, A>(3);
         }
 
-        template <class A>
-        batch<float, A> quadrant(const batch<float, A>& x) {
+        template <class A> inline batch<float, A> quadrant(const batch<float, A>& x) {
           return to_float(quadrant(to_int(x)));
         }
 
-        template <class A>
-        batch<double, A> quadrant(const batch<double, A>& x) {
+        template <class A> inline batch<double, A> quadrant(const batch<double, A>& x) {
           using batch_type = batch<double, A>;
                 batch_type a = x * batch_type(0.25);
                 return (a - floor(a)) * batch_type(4.);
@@ -382,8 +368,7 @@ namespace xsimd {
          * ====================================================
          */
 
-        template<class A>
-        inline batch<float, A> cos_eval(const batch<float, A>& z)
+        template<class A> inline inline batch<float, A> cos_eval(const batch<float, A>& z)
         {
           using batch_type = batch<float, A>;
             batch_type y = detail::horner<batch_type,
@@ -393,8 +378,7 @@ namespace xsimd {
             return batch_type(1.) + fma(z, batch_type(-0.5), y * z * z);
         }
 
-        template<class A>
-        inline batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x)
+        template<class A> inline inline batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x)
             {
           using batch_type = batch<float, A>;
                 batch_type y = detail::horner<batch_type,
@@ -404,8 +388,7 @@ namespace xsimd {
                 return fma(y * z, x, x);
             }
 
-        template<class A>
-            static inline batch<float, A> base_tancot_eval(const batch<float, A>& z)
+        template<class A> inline static inline batch<float, A> base_tancot_eval(const batch<float, A>& z)
             {
           using batch_type = batch<float, A>;
                 batch_type zz = z * z;
@@ -419,16 +402,14 @@ namespace xsimd {
                 return fma(y, zz * z, z);
             }
 
-            template <class A, class BB>
-            static inline batch<float, A> tan_eval(const batch<float, A>& z, const BB& test)
+            template <class A, class BB> inline static inline batch<float, A> tan_eval(const batch<float, A>& z, const BB& test)
             {
           using batch_type = batch<float, A>;
                 batch_type y = base_tancot_eval(z);
                 return select(test, y, -batch_type(1.) / y);
             }
 
-            template <class A, class BB>
-            static inline batch<float, A> cot_eval(const batch<float, A>& z, const BB& test)
+            template <class A, class BB> inline static inline batch<float, A> cot_eval(const batch<float, A>& z, const BB& test)
             {
           using batch_type = batch<float, A>;
                 batch_type y = base_tancot_eval(z);
@@ -444,8 +425,7 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-            template<class A>
-            static inline batch<double, A> cos_eval(const batch<double, A>& z)
+            template<class A> inline static inline batch<double, A> cos_eval(const batch<double, A>& z)
             {
           using batch_type = batch<double, A>;
                 batch_type y = detail::horner<batch_type,
@@ -459,8 +439,7 @@ namespace xsimd {
                 return batch_type(1.) - y * z;
             }
 
-            template<class A>
-            static inline batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x)
+            template<class A> inline static inline batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x)
             {
           using batch_type = batch<double, A>;
                 batch_type y = detail::horner<batch_type,
@@ -473,8 +452,7 @@ namespace xsimd {
                 return fma(y * z, x, x);
             }
 
-            template<class A>
-            static inline batch<double, A> base_tancot_eval(const batch<double, A>& z)
+            template<class A> inline static inline batch<double, A> base_tancot_eval(const batch<double, A>& z)
             {
           using batch_type = batch<double, A>;
                 batch_type zz = z * z;
@@ -490,16 +468,14 @@ namespace xsimd {
                 return fma(z, (zz * (num / den)), z);
             }
 
-            template <class A, class BB>
-            static inline batch<double, A> tan_eval(const batch<double, A>& z, const BB& test)
+            template <class A, class BB> inline static inline batch<double, A> tan_eval(const batch<double, A>& z, const BB& test)
             {
           using batch_type = batch<double, A>;
                 batch_type y = base_tancot_eval(z);
                 return select(test, y, -batch_type(1.) / y);
             }
 
-            template <class A, class BB>
-            static inline batch<double, A> cot_eval(const batch<double, A>& z, const BB& test)
+            template <class A, class BB> inline static inline batch<double, A> cot_eval(const batch<double, A>& z, const BB& test)
             {
           using batch_type = batch<double, A>;
                 batch_type y = base_tancot_eval(z);
@@ -522,8 +498,7 @@ namespace xsimd {
         {
         };
 
-        template <class B, class Tag = trigo_radian_tag>
-        struct trigo_reducer
+        template <class B, class Tag = trigo_radian_tag> inline struct trigo_reducer
         {
             static inline B reduce(const B& x, B& xr)
             {
@@ -597,8 +572,7 @@ namespace xsimd {
             }
         };
 
-        template <class B>
-        struct trigo_reducer<B, trigo_pi_tag>
+        template <class B> inline struct trigo_reducer<B, trigo_pi_tag>
         {
             static inline B reduce(const B& x, B& xr)
             {
@@ -610,7 +584,7 @@ namespace xsimd {
         };
 
     }
-    template<class A, class T> batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 const batch_type x = abs(self);
                 batch_type xr = constants::nan<batch_type>();
@@ -625,7 +599,7 @@ namespace xsimd {
                 return z1 ^ sign_bit;
     }
 
-    template<class A, class T> batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       return {cos(z.real()) * cosh(z.imag()), -sin(z.real()) * sinh(z.imag())};
     }
 
@@ -641,7 +615,7 @@ namespace xsimd {
       * ====================================================
       */
 
-    template<class A, class T> batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 batch_type x = abs(self);
                 auto test1 = x > (constants::maxlog<batch_type>() - constants::log_2<batch_type>());
@@ -650,8 +624,7 @@ namespace xsimd {
                 batch_type tmp1 = batch_type(0.5) * tmp;
                 return select(test1, tmp1 * tmp, detail::average(tmp, batch_type(1.) / tmp));
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             auto x = z.real();
             auto y = z.imag();
@@ -661,7 +634,7 @@ namespace xsimd {
 
     // sin
     namespace detail {
-    template<class A, class T, class Tag=trigo_radian_tag> batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {
+    template<class A, class T, class Tag=trigo_radian_tag> inline batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {
       using batch_type = batch<T, A>;
                 const batch_type x = abs(self);
                 batch_type xr = constants::nan<batch_type>();
@@ -677,16 +650,16 @@ namespace xsimd {
     }
     }
 
-    template<class A, class T> batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {
       return detail::sin(self);
     }
 
-    template<class A, class T> batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
       return {sin(z.real()) * cosh(z.imag()), cos(z.real()) * sinh(z.imag())};
     }
 
     // sincos
-    template<class A, class T> std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 const batch_type x = abs(self);
                 batch_type xr = constants::nan<batch_type>();
@@ -703,8 +676,7 @@ namespace xsimd {
                 return std::make_pair(sin_z1 ^ sin_sign_bit, cos_z1 ^ cos_sign_bit);
     }
 
-    template<class A, class T>
-    std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>
+    template<class A, class T> inline std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>
     sincos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
@@ -726,7 +698,7 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-    template<class A> batch<float, A> sinh_kernel(batch<float, A> const& self) {
+    template<class A> inline batch<float, A> sinh_kernel(batch<float, A> const& self) {
       using batch_type = batch<float, A>;
                 batch_type sqr_self = self * self;
                 return detail::horner<batch_type,
@@ -738,7 +710,7 @@ namespace xsimd {
                     self;
     }
 
-    template<class A> batch<double, A> sinh_kernel(batch<double, A> const& self) {
+    template<class A> inline batch<double, A> sinh_kernel(batch<double, A> const& self) {
       using batch_type = batch<double, A>;
                 batch_type sqrself = self * self;
                 return fma(self, (detail::horner<batch_type,
@@ -765,7 +737,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A, class T> batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {
                 using batch_type = batch<T, A>;
                 batch_type half(0.5);
                 batch_type x = abs(a);
@@ -785,8 +757,7 @@ namespace xsimd {
                 batch_type r = select(test1, tmp1 * tmp, tmp1 - half / tmp);
                 return select(lt1, z, r) ^ bts;
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             auto x = z.real();
             auto y = z.imag();
@@ -794,7 +765,7 @@ namespace xsimd {
         }
 
     // tan
-    template<class A, class T> batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 const batch_type x = abs(self);
                 batch_type xr = constants::nan<batch_type>();
@@ -805,7 +776,7 @@ namespace xsimd {
                 const batch_type y = detail::tan_eval(xr, test);
                 return y ^ bitofsign(self);
     }
-    template<class A, class T> batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
+    template<class A, class T> inline batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {
             using batch_type = batch<std::complex<T>, A>;
             using real_batch = typename batch_type::real_batch;
             real_batch d = cos(2 * z.real()) + cosh(2 * z.imag());
@@ -828,11 +799,9 @@ namespace xsimd {
          * (See copy at http://boost.org/LICENSE_1_0.txt)
          * ====================================================
          */
-        template <class B>
-        struct tanh_kernel;
+        template <class B> inline struct tanh_kernel;
 
-        template <class A>
-        struct tanh_kernel<batch<float, A>>
+        template <class A> inline struct tanh_kernel<batch<float, A>>
         {
           using batch_type = batch<float, A>;
             static inline batch_type tanh(const batch_type& x)
@@ -855,8 +824,7 @@ namespace xsimd {
             }
         };
 
-        template <class A>
-        struct tanh_kernel<batch<double, A>>
+        template <class A> inline struct tanh_kernel<batch<double, A>>
         {
           using batch_type = batch<double, A>;
             static inline batch_type tanh(const batch_type& x)
@@ -901,7 +869,7 @@ namespace xsimd {
      * (See copy at http://boost.org/LICENSE_1_0.txt)
      * ====================================================
      */
-    template<class A, class T> batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {
+    template<class A, class T> inline batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {
       using batch_type = batch<T, A>;
                 batch_type one(1.);
                 batch_type x = abs(self);
@@ -917,8 +885,7 @@ namespace xsimd {
                 batch_type r = fma(batch_type(-2.), one / (one + exp(x + x)), one);
                 return select(test, z, r) ^ bts;
     }
-        template <class A, class T>
-        inline batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
+        template <class A, class T> inline inline batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>)
         {
             using real_batch = typename batch<std::complex<T>, A>::real_batch;
             auto x = z.real();
diff --git a/include/xsimd/arch/xsimd_avx.hpp b/include/xsimd/arch/xsimd_avx.hpp
index 3634cbe..c12d16e 100644
--- a/include/xsimd/arch/xsimd_avx.hpp
+++ b/include/xsimd/arch/xsimd_avx.hpp
@@ -31,16 +31,14 @@ namespace xsimd {
       inline __m256i merge_sse(__m128i low, __m128i high) {
         return _mm256_insertf128_si256(_mm256_castsi128_si256(low), high, 1);
       }
-      template <class F>
-      __m256i fwd_to_sse(F f, __m256i self) {
+      template <class F> inline __m256i fwd_to_sse(F f, __m256i self) {
         __m128i self_low, self_high;
         split_avx(self, self_low, self_high);
         __m128i res_low = f(self_low);
         __m128i res_high = f(self_high);
         return merge_sse(res_low, res_high);
       }
-      template<class F>
-      __m256i fwd_to_sse(F f, __m256i self, __m256i other) {
+      template<class F> inline __m256i fwd_to_sse(F f, __m256i self, __m256i other) {
         __m128i self_low, self_high, other_low, other_high;
         split_avx(self, self_low, self_high);
         split_avx(other, other_low, other_high);
@@ -48,8 +46,7 @@ namespace xsimd {
         __m128i res_high = f(self_high, other_high);
         return merge_sse(res_low, res_high);
       }
-      template<class F>
-      __m256i fwd_to_sse(F f, __m256i self, int32_t other) {
+      template<class F> inline __m256i fwd_to_sse(F f, __m256i self, int32_t other) {
         __m128i self_low, self_high;
         split_avx(self, self_low, self_high);
         __m128i res_low = f(self_low, other);
@@ -59,11 +56,11 @@ namespace xsimd {
     }
 
     // abs
-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {
       __m256 sign_mask = _mm256_set1_ps(-0.f);  // -0.f = 1 << 31
       return _mm256_andnot_ps(sign_mask, self);
     }
-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {
       __m256d sign_mask = _mm256_set1_pd(-0.f);  // -0.f = 1 << 31
       return _mm256_andnot_pd(sign_mask, self);
     }
@@ -73,18 +70,18 @@ namespace xsimd {
     batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {
         return detail::fwd_to_sse([](__m128i s, __m128i o) { return add(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);
     }
-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_add_ps(self, other);
     }
-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_add_pd(self, other);
     }
 
     // all
-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline bool all(batch_bool<float, A> const& self, requires_arch<avx>) {
       return _mm256_testc_ps(self, batch_bool<float, A>(true)) != 0;
     }
-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline bool all(batch_bool<double, A> const& self, requires_arch<avx>) {
       return _mm256_testc_pd(self, batch_bool<double, A>(true)) != 0;
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -93,10 +90,10 @@ namespace xsimd {
     }
 
     // any
-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline bool any(batch_bool<float, A> const& self, requires_arch<avx>) {
       return !_mm256_testz_ps(self, self);
     }
-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline bool any(batch_bool<double, A> const& self, requires_arch<avx>) {
       return !_mm256_testz_pd(self, self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -105,17 +102,17 @@ namespace xsimd {
     }
 
     // bitwise_and
-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_and_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_and_pd(self, other);
     }
 
-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return _mm256_and_ps(self, other);
     }
-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return _mm256_and_pd(self, other);
     }
 
@@ -129,17 +126,17 @@ namespace xsimd {
     }
 
     // bitwise_andnot
-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_andnot_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_andnot_pd(self, other);
     }
 
-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return _mm256_andnot_ps(self, other);
     }
-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return _mm256_andnot_pd(self, other);
     }
 
@@ -169,16 +166,16 @@ namespace xsimd {
     }
 
     // bitwise_or
-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_or_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_or_pd(self, other);
     }
-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return _mm256_or_ps(self, other);
     }
-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return _mm256_or_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -197,16 +194,16 @@ namespace xsimd {
     }
 
     // bitwise_xor
-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_xor_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_xor_pd(self, other);
     }
-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return _mm256_xor_ps(self, other);
     }
-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return _mm256_xor_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -233,16 +230,14 @@ namespace xsimd {
     batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx>) {
       return batch<Tp, A>(self.data);
     }
-    template<class A>
-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {
+    template<class A> inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {
       return _mm256_castps_pd(self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx>) {
       return _mm256_castps_si256(self);
     }
-    template<class A>
-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {
       return _mm256_castpd_ps(self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -251,32 +246,30 @@ namespace xsimd {
     }
 
     // bitwise_not
-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));
     }
-    template <class A>
-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {
+    template <class A> inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {
       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));
     }
-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {
       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));
     }
-    template <class A>
-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {
+    template <class A> inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {
       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));
     }
 
     // bool_cast
-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {
         return _mm256_castps_si256(self);
     }
-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {
         return _mm256_castsi256_ps(self);
     }
-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {
         return _mm256_castpd_si256(self);
     }
-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {
         return _mm256_castsi256_pd(self);
     }
 
@@ -291,18 +284,18 @@ namespace xsimd {
         default: assert(false && "unsupported"); return {};
       }
     }
-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx>) {
+    template<class A> inline batch<float, A> broadcast(float val, requires_arch<avx>) {
       return _mm256_set1_ps(val);
     }
-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx>) {
+    template<class A> inline batch<double, A> broadcast(double val, requires_arch<avx>) {
       return _mm256_set1_pd(val);
     }
 
     // ceil
-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_ceil_ps(self);
     }
-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_ceil_pd(self);
     }
 
@@ -310,8 +303,7 @@ namespace xsimd {
     namespace detail {
         // On clang, _mm256_extractf128_ps is built upon build_shufflevector
         // which require index parameter to be a constant
-        template <int index, class B>
-        inline B get_half_complex_f(const B& real, const B& imag)
+        template <int index, class B> inline inline B get_half_complex_f(const B& real, const B& imag)
         {
             __m128 tmp0 = _mm256_extractf128_ps(real, index);
             __m128 tmp1 = _mm256_extractf128_ps(imag, index);
@@ -322,8 +314,7 @@ namespace xsimd {
             res = _mm256_insertf128_ps(res, tmp2, 1);
             return res;
         }
-        template <int index, class B>
-        inline B get_half_complex_d(const B& real, const B& imag)
+        template <int index, class B> inline inline B get_half_complex_d(const B& real, const B& imag)
         {
             __m128d tmp0 = _mm256_extractf128_pd(real, index);
             __m128d tmp1 = _mm256_extractf128_pd(imag, index);
@@ -336,51 +327,51 @@ namespace xsimd {
         }
 
         // complex_low
-        template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {
+        template<class A> inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {
                 return get_half_complex_f<0>(self.real(), self.imag());
         }
-        template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {
+        template<class A> inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {
                 return get_half_complex_d<0>(self.real(), self.imag());
         }
 
         // complex_high
-        template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {
+        template<class A> inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {
                 return get_half_complex_f<1>(self.real(), self.imag());
         }
-        template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {
+        template<class A> inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {
                 return get_half_complex_d<1>(self.real(), self.imag());
         }
     }
     // convert
     namespace detail {
-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {
+    template<class A> inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {
       return _mm256_cvtepi32_ps(self);
     }
-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {
+    template<class A> inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {
       return _mm256_cvttps_epi32(self);
     }
     }
 
     // div
-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_div_ps(self, other);
     }
-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_div_pd(self, other);
     }
 
     // eq
-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_EQ_OQ);
     }
-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_EQ_OQ);
     }
-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return  _mm256_castsi256_ps(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); },
                                   _mm256_castps_si256(self), _mm256_castps_si256(other)));
     }
-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return  _mm256_castsi256_pd(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); }, _mm256_castpd_si256(self), _mm256_castpd_si256(other)));
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -394,18 +385,18 @@ namespace xsimd {
     }
 
     // floor
-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_floor_ps(self);
     }
-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_floor_pd(self);
     }
 
     // ge
-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_GE_OQ);
     }
-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_GE_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -414,10 +405,10 @@ namespace xsimd {
     }
 
     // gt
-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_GT_OQ);
     }
-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_GT_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -427,7 +418,7 @@ namespace xsimd {
 
 
     // hadd
-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx>) {
+    template<class A> inline float hadd(batch<float, A> const& rhs, requires_arch<avx>) {
                 // Warning about _mm256_hadd_ps:
                 // _mm256_hadd_ps(a,b) gives
                 // (a0+a1,a2+a3,b0+b1,b2+b3,a4+a5,a6+a7,b4+b5,b6+b7). Hence we can't
@@ -444,8 +435,7 @@ namespace xsimd {
                 return _mm_cvtss_f32(_mm256_extractf128_ps(tmp, 0));
 
     }
-    template <class A>
-    double hadd(batch<double, A> const &rhs, requires_arch<avx>) {
+    template <class A> inline double hadd(batch<double, A> const &rhs, requires_arch<avx>) {
                 // rhs = (x0, x1, x2, x3)
                 // tmp = (x2, x3, x0, x1)
                 __m256d tmp = _mm256_permute2f128_pd(rhs, rhs, 1);
@@ -464,7 +454,7 @@ namespace xsimd {
     }
 
     // haddp
-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {
+    template<class A> inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {
                 // row = (a,b,c,d,e,f,g,h)
                 // tmp0 = (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)
                 __m256 tmp0 = _mm256_hadd_ps(row[0], row[1]);
@@ -488,8 +478,7 @@ namespace xsimd {
                 tmp1 = _mm256_permute2f128_ps(tmp1, tmp2, 0x21);
                 return _mm256_add_ps(tmp0, tmp1);
     }
-    template <class A>
-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {
+    template <class A> inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {
                 // row = (a,b,c,d)
                 // tmp0 = (a0+a1, b0+b1, a2+a3, b2+b3)
                 __m256d tmp0 = _mm256_hadd_pd(row[0], row[1]);
@@ -503,18 +492,18 @@ namespace xsimd {
     }
 
     // isnan
-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {
                 return _mm256_cmp_ps(self, self, _CMP_UNORD_Q);
     }
-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {
                 return _mm256_cmp_pd(self, self, _CMP_UNORD_Q);
     }
 
     // le
-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_LE_OQ);
     }
-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_LE_OQ);
     }
 
@@ -523,17 +512,17 @@ namespace xsimd {
     batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) {
       return _mm256_load_si256((__m256i const*)mem);
     }
-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {
+    template<class A> inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {
       return _mm256_load_ps(mem);
     }
-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {
+    template<class A> inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {
       return _mm256_load_pd(mem);
     }
 
     namespace detail
     {
     // load_complex
-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {
+    template<class A> inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {
             using batch_type = batch<float, A>;
             __m128 tmp0 = _mm256_extractf128_ps(hi, 0);
             __m128 tmp1 = _mm256_extractf128_ps(hi, 1);
@@ -551,7 +540,7 @@ namespace xsimd {
             imag = _mm256_insertf128_ps(imag, tmp_imag, 1);
             return {real, imag};
     }
-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {
+    template<class A> inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {
             using batch_type = batch<double, A>;
             __m128d tmp0 = _mm256_extractf128_pd(hi, 0);
             __m128d tmp1 = _mm256_extractf128_pd(hi, 1);
@@ -573,18 +562,18 @@ namespace xsimd {
     batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) {
       return _mm256_loadu_si256((__m256i const*)mem);
     }
-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){
+    template<class A> inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){
       return _mm256_loadu_ps(mem);
     }
-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){
+    template<class A> inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){
       return _mm256_loadu_pd(mem);
     }
 
     // lt
-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_LT_OQ);
     }
-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_LT_OQ);
     }
 
@@ -594,10 +583,10 @@ namespace xsimd {
     }
 
     // max
-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_max_ps(self, other);
     }
-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_max_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -606,10 +595,10 @@ namespace xsimd {
     }
 
     // min
-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_min_ps(self, other);
     }
-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_min_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -618,18 +607,18 @@ namespace xsimd {
     }
 
     // mul
-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_mul_ps(self, other);
     }
-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_mul_pd(self, other);
     }
 
     // nearbyint
-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_round_ps(self, _MM_FROUND_TO_NEAREST_INT);
     }
-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_round_pd(self, _MM_FROUND_TO_NEAREST_INT);
     }
 
@@ -638,19 +627,18 @@ namespace xsimd {
     batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) {
       return 0 - self;
     }
-    template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> neg(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)));
     }
-    template <class A>
-    batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {
+    template <class A> inline batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {
       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)));
     }
 
     // neq
-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_ps(self, other, _CMP_NEQ_OQ);
     }
-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_cmp_pd(self, other, _CMP_NEQ_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -659,10 +647,10 @@ namespace xsimd {
     }
 
 
-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {
       return _mm256_xor_ps(self, other);
     }
-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {
       return _mm256_xor_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -671,10 +659,10 @@ namespace xsimd {
     }
 
     // sadd
-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return add(self, other); // no saturated arithmetic on floating point numbers
     }
-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return add(self, other); // no saturated arithmetic on floating point numbers
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -693,10 +681,10 @@ namespace xsimd {
     }
 
     // select
-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {
+    template<class A> inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {
                 return _mm256_blendv_ps(false_br, true_br, cond);
     }
-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {
+    template<class A> inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {
                 return _mm256_blendv_pd(false_br, true_br, cond);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -721,14 +709,12 @@ namespace xsimd {
 
 
     // set
-    template<class A, class... Values>
-    batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {
+    template<class A, class... Values> inline batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {
       static_assert(sizeof...(Values) == batch<float, A>::size, "consistent init");
       return _mm256_setr_ps(values...);
     }
 
-    template<class A, class... Values>
-    batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {
+    template<class A, class... Values> inline batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {
       static_assert(sizeof...(Values) == batch<double, A>::size, "consistent init");
       return _mm256_setr_pd(values...);
     }
@@ -755,31 +741,29 @@ namespace xsimd {
       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;
     }
 
-    template<class A, class... Values>
-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {
+    template<class A, class... Values> inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {
       static_assert(sizeof...(Values) == batch_bool<float, A>::size, "consistent init");
       return _mm256_castsi256_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);
     }
 
-    template<class A, class... Values>
-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {
+    template<class A, class... Values> inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {
       static_assert(sizeof...(Values) == batch_bool<double, A>::size, "consistent init");
       return _mm256_castsi256_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);
     }
 
     // sqrt
-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {
+    template<class A> inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {
       return _mm256_sqrt_ps(val);
     }
-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {
+    template<class A> inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {
       return _mm256_sqrt_pd(val);
     }
 
     // ssub
-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_sub_ps(self, other); // no saturated arithmetic on floating point numbers
     }
-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_sub_pd(self, other); // no saturated arithmetic on floating point numbers
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -802,10 +786,10 @@ namespace xsimd {
     void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {
       return _mm256_store_si256((__m256i *)mem, self);
     }
-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_store_ps(mem, self);
     }
-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_store_pd(mem, self);
     }
 
@@ -818,10 +802,10 @@ namespace xsimd {
     void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {
       return _mm256_storeu_si256((__m256i *)mem, self);
     }
-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_storeu_ps(mem, self);
     }
-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_storeu_pd(mem, self);
     }
 
@@ -830,20 +814,18 @@ namespace xsimd {
     batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {
         return detail::fwd_to_sse([](__m128i s, __m128i o) { return sub(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);
     }
-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_sub_ps(self, other);
     }
-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_sub_pd(self, other);
     }
 
     // to_float
-    template<class A>
-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {
       return _mm256_cvtepi32_ps(self);
     }
-    template<class A>
-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {
       // FIXME: call _mm_cvtepi64_pd
       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];
       self.store_aligned(&buffer[0]);
@@ -851,13 +833,11 @@ namespace xsimd {
     }
 
     // to_int
-    template<class A>
-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_cvttps_epi32(self);
     }
 
-    template<class A>
-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {
       // FIXME: call _mm_cvttpd_epi64
       alignas(A::alignment()) double buffer[batch<double, A>::size];
       self.store_aligned(&buffer[0]);
@@ -865,10 +845,10 @@ namespace xsimd {
     }
 
     // trunc
-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {
       return _mm256_round_ps(self, _MM_FROUND_TO_ZERO);
     }
-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {
+    template<class A> inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {
       return _mm256_round_pd(self, _MM_FROUND_TO_ZERO);
     }
 
@@ -883,10 +863,10 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_unpackhi_ps(self, other);
     }
-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_unpackhi_pd(self, other);
     }
 
@@ -901,10 +881,10 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {
       return _mm256_unpacklo_ps(self, other);
     }
-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
+    template<class A> inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {
       return _mm256_unpacklo_pd(self, other);
     }
 
diff --git a/include/xsimd/arch/xsimd_avx2.hpp b/include/xsimd/arch/xsimd_avx2.hpp
index 2dacb2e..1095bf3 100644
--- a/include/xsimd/arch/xsimd_avx2.hpp
+++ b/include/xsimd/arch/xsimd_avx2.hpp
@@ -166,14 +166,14 @@ namespace xsimd {
     }
 
     // complex_low
-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {
+    template<class A> inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {
             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 1, 1, 0));
             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(1, 2, 0, 0));
             return _mm256_blend_pd(tmp0, tmp1, 10);
     }
 
     // complex_high
-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {
+    template<class A> inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {
             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 3, 1, 2));
             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(3, 2, 2, 0));
             return _mm256_blend_pd(tmp0, tmp1, 10);
@@ -240,7 +240,7 @@ namespace xsimd {
       }
     }
     // load_complex
-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {
+    template<class A> inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {
             using batch_type = batch<float, A>;
             batch_type real = _mm256_castpd_ps(
                          _mm256_permute4x64_pd(
@@ -252,7 +252,7 @@ namespace xsimd {
                              _MM_SHUFFLE(3, 1, 2, 0)));
             return {real, imag};
     }
-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {
+    template<class A> inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {
             using batch_type = batch<double, A>;
             batch_type real = _mm256_permute4x64_pd(_mm256_unpacklo_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));
             batch_type imag = _mm256_permute4x64_pd(_mm256_unpackhi_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));
diff --git a/include/xsimd/arch/xsimd_avx512bw.hpp b/include/xsimd/arch/xsimd_avx512bw.hpp
index e33b426..32b1321 100644
--- a/include/xsimd/arch/xsimd_avx512bw.hpp
+++ b/include/xsimd/arch/xsimd_avx512bw.hpp
@@ -22,8 +22,7 @@ namespace xsimd {
     using namespace types;
 
     namespace detail {
-    template<class A, class T,  int Cmp>
-    batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {
+    template<class A, class T,  int Cmp> inline batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {
       using register_type = typename batch_bool<T, A>::register_type;
       if(std::is_signed<T>::value) {
         switch(sizeof(T)) {
diff --git a/include/xsimd/arch/xsimd_avx512dq.hpp b/include/xsimd/arch/xsimd_avx512dq.hpp
index 00fe1f5..ba659a8 100644
--- a/include/xsimd/arch/xsimd_avx512dq.hpp
+++ b/include/xsimd/arch/xsimd_avx512dq.hpp
@@ -20,51 +20,49 @@ namespace xsimd {
     using namespace types;
 
     // bitwise_and
-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
       return _mm512_and_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
       return _mm512_and_pd(self, other);
     }
 
     // bitwise_andnot
-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
       return _mm512_andnot_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
       return _mm512_andnot_pd(self, other);
     }
 
     // bitwise_or
-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
       return _mm512_or_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
       return _mm512_or_pd(self, other);
     }
 
-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data | other.data);
     }
 
     // bitwise_xor
-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {
       return _mm512_xor_ps(self, other);
     }
-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
+    template<class A> inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {
       return _mm512_xor_pd(self, other);
     }
 
     // to_float
-    template<class A>
-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {
+    template<class A> inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {
       return _mm512_cvtepi64_pd(self);
     }
 
     // to_int
-    template<class A>
-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {
+    template<class A> inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {
       return _mm512_cvttpd_epi64(self);
     }
 
diff --git a/include/xsimd/arch/xsimd_avx512f.hpp b/include/xsimd/arch/xsimd_avx512f.hpp
index 13cc0da..cc91b6c 100644
--- a/include/xsimd/arch/xsimd_avx512f.hpp
+++ b/include/xsimd/arch/xsimd_avx512f.hpp
@@ -45,16 +45,14 @@ namespace xsimd {
       inline __m512d merge_avx(__m256d low, __m256d high) {
         return _mm512_insertf64x4(_mm512_castpd256_pd512(low), high, 1);
       }
-      template<class F>
-      __m512i fwd_to_avx(F f, __m512i self) {
+      template<class F> inline __m512i fwd_to_avx(F f, __m512i self) {
         __m256i self_low, self_high;
         split_avx512(self, self_low, self_high);
         __m256i res_low = f(self_low);
         __m256i res_high = f(self_high);
         return merge_avx(res_low, res_high);
       }
-      template<class F>
-      __m512i fwd_to_avx(F f, __m512i self, __m512i other) {
+      template<class F> inline __m512i fwd_to_avx(F f, __m512i self, __m512i other) {
         __m256i self_low, self_high, other_low, other_high;
         split_avx512(self, self_low, self_high);
         split_avx512(other, other_low, other_high);
@@ -62,8 +60,7 @@ namespace xsimd {
         __m256i res_high = f(self_high, other_high);
         return merge_avx(res_low, res_high);
       }
-      template<class F>
-      __m512i fwd_to_avx(F f, __m512i self, int32_t other) {
+      template<class F> inline __m512i fwd_to_avx(F f, __m512i self, int32_t other) {
         __m256i self_low, self_high;
         split_avx512(self, self_low, self_high);
         __m256i res_low = f(self_low, other);
@@ -118,8 +115,7 @@ namespace xsimd {
       return z;
     }
 
-    template<class A, class T,  int Cmp>
-    batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {
+    template<class A, class T,  int Cmp> inline batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {
       using register_type = typename batch_bool<T, A>::register_type;
       if(std::is_signed<T>::value) {
         switch(sizeof(T)) {
@@ -189,13 +185,13 @@ namespace xsimd {
     }
 
     // abs
-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {
       __m512 self_asf = (__m512)self;
       __m512i self_asi = *reinterpret_cast<__m512i *>(&self_asf);
       __m512i res_asi = _mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF), self_asi);
       return *reinterpret_cast<__m512*>(&res_asi);
     }
-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {
                 __m512d self_asd = (__m512d)self;
                 __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asd);
                 __m512i res_asi = _mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),
@@ -227,32 +223,30 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_add_ps(self, other);
     }
-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_add_pd(self, other);
     }
 
     // all
-    template<class A, class T>
-    bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {
+    template<class A, class T> inline bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return self.data == register_type(-1);
     }
 
     // any
-    template<class A, class T>
-    bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {
+    template<class A, class T> inline bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return self.data != register_type(0);
     }
 
     // bitwise_and
-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_ps(_mm512_and_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));
     }
-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_pd(_mm512_and_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));
     }
 
@@ -261,17 +255,16 @@ namespace xsimd {
       return _mm512_and_si512(self, other);
     }
 
-    template<class A, class T>
-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data & other.data);
     }
 
     // bitwise_andnot
-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_ps(_mm512_andnot_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));
     }
-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_pd(_mm512_andnot_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));
     }
 
@@ -280,8 +273,7 @@ namespace xsimd {
       return _mm512_andnot_si512(self, other);
     }
 
-    template<class A, class T>
-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data & ~other.data);
     }
@@ -315,29 +307,27 @@ namespace xsimd {
     batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) {
       return _mm512_xor_si512(self, _mm512_set1_epi32(-1));
     }
-    template<class A, class T>
-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(~self.data);
     }
 
-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_xor_ps(self, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));
     }
-    template <class A>
-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {
+    template <class A> inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {
       return _mm512_xor_pd(self, _mm512_castsi512_pd(_mm512_set1_epi32(-1)));
     }
 
     // bitwise_or
-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_ps(_mm512_or_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));
     }
-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_pd(_mm512_or_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));
     }
 
-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data | other.data);
     }
@@ -386,14 +376,14 @@ namespace xsimd {
     }
 
     // bitwise_xor
-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));
     }
-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));
     }
 
-    template<class A, class T> batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data | other.data);
     }
@@ -416,16 +406,14 @@ namespace xsimd {
     batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx512f>) {
       return batch<Tp, A>(self.data);
     }
-    template<class A>
-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {
       return _mm512_castps_pd(self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {
       return _mm512_castps_si512(self);
     }
-    template<class A>
-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {
       return _mm512_castpd_ps(self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -434,16 +422,16 @@ namespace xsimd {
     }
 
     // bool_cast
-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {
         return self.data;
     }
-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {
         return self.data;
     }
-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {
         return self.data;
     }
-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {
         return self.data;
     }
 
@@ -459,18 +447,18 @@ namespace xsimd {
         default: assert(false && "unsupported"); return {};
       }
     }
-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> broadcast(float val, requires_arch<avx512f>) {
       return _mm512_set1_ps(val);
     }
-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> broadcast(double val, requires_arch<avx512f>) {
       return _mm512_set1_pd(val);
     }
 
     // ceil
-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_ps(self, _MM_FROUND_TO_POS_INF);
     }
-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_pd(self, _MM_FROUND_TO_POS_INF);
     }
 
@@ -478,21 +466,21 @@ namespace xsimd {
     namespace detail
     {
     // complex_low
-    template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {
         __m512i idx = _mm512_setr_epi32(0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);
         return _mm512_permutex2var_ps(self.real(), idx, self.imag());
     }
-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {
         __m512i idx = _mm512_setr_epi64(0, 8, 1, 9, 2, 10, 3, 11);
         return _mm512_permutex2var_pd(self.real(), idx, self.imag());
     }
 
     // complex_high
-    template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {
         __m512i idx = _mm512_setr_epi32(8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);
         return _mm512_permutex2var_ps(self.real(), idx, self.imag());
     }
-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {
         __m512i idx = _mm512_setr_epi64(4, 12, 5, 13, 6, 14, 7, 15);
         return _mm512_permutex2var_pd(self.real(), idx, self.imag());
     }
@@ -500,28 +488,28 @@ namespace xsimd {
 
     // convert
     namespace detail {
-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {
       return _mm512_cvtepi32_ps(self);
     }
-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {
+    template<class A> inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {
       return _mm512_cvttps_epi32(self);
     }
     }
 
     // div
-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_div_ps(self, other);
     }
-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_div_pd(self, other);
     }
 
 
     // eq
-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_EQ_OQ);
     }
-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_EQ_OQ);
     }
 
@@ -529,31 +517,29 @@ namespace xsimd {
     batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {
       return detail::compare_int_avx512f<A, T, _MM_CMPINT_EQ>(self, other);
     }
-    template<class A, class T>
-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(~self.data ^ other.data);
     }
 
     // floor
-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_ps(self, _MM_FROUND_TO_NEG_INF);
     }
-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_pd(self, _MM_FROUND_TO_NEG_INF);
     }
 
     // from bool
-    template<class A, class T>
-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {
+    template<class A, class T> inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {
       return select(self, batch<T, A>(1), batch<T, A>(0));
     }
 
     // ge
-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_GE_OQ);
     }
-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_GE_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -562,10 +548,10 @@ namespace xsimd {
     }
 
     // gt
-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_GT_OQ);
     }
-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_GT_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -575,15 +561,14 @@ namespace xsimd {
 
 
     // hadd
-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {
+    template<class A> inline float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {
                 __m256 tmp1 = _mm512_extractf32x8_ps(rhs, 1);
                 __m256 tmp2 = _mm512_extractf32x8_ps(rhs, 0);
                 __m256 res1 = _mm256_add_ps(tmp1, tmp2);
                 return hadd(batch<float, avx2>(res1), avx2{});
 
     }
-    template <class A>
-    double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {
+    template <class A> inline double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {
                 __m256d tmp1 = _mm512_extractf64x4_pd(rhs, 1);
                 __m256d tmp2 = _mm512_extractf64x4_pd(rhs, 0);
                 __m256d res1 = _mm256_add_pd(tmp1, tmp2);
@@ -598,7 +583,7 @@ namespace xsimd {
     }
 
     // haddp
-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {
                 // The following folds over the vector once:
                 // tmp1 = [a0..8, b0..8]
                 // tmp2 = [a8..f, b8..f]
@@ -656,8 +641,7 @@ namespace xsimd {
                 concat = _mm512_insertf32x8(concat, halfx1, 1);
                 return concat;
     }
-    template <class A>
-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {
+    template <class A> inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {
 #define step1(I, a, b)                                                   \
         batch<double, avx512f> res ## I;                                           \
         {                                                                    \
@@ -690,18 +674,18 @@ namespace xsimd {
     }
 
     // isnan
-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {
                 return _mm512_cmp_ps_mask(self, self, _CMP_UNORD_Q);
     }
-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {
                 return _mm512_cmp_pd_mask(self, self, _CMP_UNORD_Q);
     }
 
     // le
-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_LE_OQ);
     }
-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_LE_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -715,24 +699,24 @@ namespace xsimd {
     batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) {
       return _mm512_load_si512((__m512i const*)mem);
     }
-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {
       return _mm512_load_ps(mem);
     }
-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {
       return _mm512_load_pd(mem);
     }
 
     // load_complex
     namespace detail
     {
-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {
+    template<class A> inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {
         __m512i real_idx = _mm512_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);
         __m512i imag_idx = _mm512_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);
         auto real = _mm512_permutex2var_ps(hi, real_idx, lo);
         auto imag = _mm512_permutex2var_ps(hi, imag_idx, lo);
         return {real, imag};
     }
-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {
+    template<class A> inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {
         __m512i real_idx = _mm512_setr_epi64(0, 2, 4, 6, 8, 10, 12, 14);
         __m512i imag_idx = _mm512_setr_epi64(1, 3, 5, 7, 9, 11, 13, 15);
         auto real = _mm512_permutex2var_pd(hi, real_idx, lo);
@@ -746,18 +730,18 @@ namespace xsimd {
     batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) {
       return _mm512_loadu_si512((__m512i const*)mem);
     }
-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){
+    template<class A> inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){
       return _mm512_loadu_ps(mem);
     }
-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){
+    template<class A> inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){
       return _mm512_loadu_pd(mem);
     }
 
     // lt
-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_LT_OQ);
     }
-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_LT_OQ);
     }
 
@@ -768,10 +752,10 @@ namespace xsimd {
     }
 
     // max
-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_max_ps(self, other);
     }
-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_max_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -793,10 +777,10 @@ namespace xsimd {
     }
 
     // min
-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_min_ps(self, other);
     }
-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_min_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -818,10 +802,10 @@ namespace xsimd {
     }
 
     // mul
-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_mul_ps(self, other);
     }
-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_mul_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -834,24 +818,23 @@ namespace xsimd {
     }
 
     // nearbyint
-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);
     }
-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);
     }
 
     // neg
-    template<class A, class T>
-    batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {
+    template<class A, class T> inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {
       return 0 - self;
     }
 
     // neq
-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_ps_mask(self, other, _CMP_NEQ_OQ);
     }
-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_cmp_pd_mask(self, other, _CMP_NEQ_OQ);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -859,17 +842,16 @@ namespace xsimd {
         return ~(self == other);
     }
 
-    template<class A, class T>
-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
+    template<class A, class T> inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       return register_type(self.data ^ other.data);
     }
 
     // sadd
-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return add(self, other); // no saturated arithmetic on floating point numbers
     }
-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return add(self, other); // no saturated arithmetic on floating point numbers
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -888,10 +870,10 @@ namespace xsimd {
     }
 
     // select
-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {
                 return _mm512_mask_blend_ps(cond, false_br, true_br);
     }
-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {
                 return _mm512_mask_blend_pd(cond, false_br, true_br);
     }
 
@@ -943,23 +925,19 @@ namespace xsimd {
 
     namespace detail
     {
-        template <class T>
-        using enable_signed_integer_t = typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value,
+        template <class T> inline using enable_signed_integer_t = typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value,
                                                                 int>::type;
 
-        template <class T>
-        using enable_unsigned_integer_t = typename std::enable_if<std::is_integral<T>::value && std::is_unsigned<T>::value,
+        template <class T> inline using enable_unsigned_integer_t = typename std::enable_if<std::is_integral<T>::value && std::is_unsigned<T>::value,
                                                                   int>::type;
     }
 
     // set
-    template<class A>
-    batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {
+    template<class A> inline batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {
       return _mm512_setr_ps(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);
     }
 
-    template<class A>
-    batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {
+    template<class A> inline batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {
       return _mm512_setr_pd(v0, v1, v2, v3, v4, v5, v6, v7);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -971,7 +949,7 @@ namespace xsimd {
                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {
       return _mm512_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);
     }
-    template<class A, class T, detail::enable_signed_integer_t<T> = 0>
+    template<class A, class T, detail::enable_signed_integer_t<T> inline = 0>
     batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,
                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,
                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,
@@ -987,7 +965,7 @@ namespace xsimd {
                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);
 #endif
     }
-    template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>
+    template<class A, class T, detail::enable_unsigned_integer_t<T> inline = 0>
     batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,
                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,
                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,
@@ -1003,7 +981,7 @@ namespace xsimd {
                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);
 #endif
     }
-    template<class A, class T, detail::enable_signed_integer_t<T> = 0>
+    template<class A, class T, detail::enable_signed_integer_t<T> inline = 0>
     batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,
                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,
                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,
@@ -1029,7 +1007,7 @@ namespace xsimd {
           v48, v49, v50, v51, v52, v53, v54, v55, v56, v57, v58, v59, v60, v61, v62, v63);
 #endif
     }
-    template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>
+    template<class A, class T, detail::enable_unsigned_integer_t<T> inline = 0>
     batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,
                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,
                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,
@@ -1055,8 +1033,7 @@ namespace xsimd {
           v48, v49, v50, v51, v52, v53, v54, v55, v56, v57, v58, v59, v60, v61, v62, v63);
 #endif
     }
-    template<class A, class T, class... Values>
-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {
+    template<class A, class T, class... Values> inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {
       static_assert(sizeof...(Values) == batch_bool<T, A>::size, "consistent init");
       using register_type = typename batch_bool<T, A>::register_type;
       register_type r = 0;
@@ -1066,18 +1043,18 @@ namespace xsimd {
     }
 
     // sqrt
-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {
       return _mm512_sqrt_ps(val);
     }
-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {
       return _mm512_sqrt_pd(val);
     }
 
     // ssub
-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_sub_ps(self, other); // no saturated arithmetic on floating point numbers
     }
-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_sub_pd(self, other); // no saturated arithmetic on floating point numbers
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -1092,8 +1069,7 @@ namespace xsimd {
     }
 
     // store
-    template<class T, class A>
-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {
+    template<class T, class A> inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {
       using register_type = typename batch_bool<T, A>::register_type;
       constexpr auto size = batch_bool<T, A>::size;
       for(std::size_t i = 0; i < size; ++i)
@@ -1109,10 +1085,10 @@ namespace xsimd {
     void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {
       return _mm512_store_si512((__m512i *)mem, self);
     }
-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_store_ps(mem, self);
     }
-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_store_pd(mem, self);
     }
 
@@ -1125,10 +1101,10 @@ namespace xsimd {
     void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {
       return _mm512_storeu_si512((__m512i *)mem, self);
     }
-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_storeu_ps(mem, self);
     }
-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_storeu_pd(mem, self);
     }
 
@@ -1143,20 +1119,18 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_sub_ps(self, other);
     }
-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_sub_pd(self, other);
     }
 
     // to_float
-    template<class A>
-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {
       return _mm512_cvtepi32_ps(self);
     }
-    template<class A>
-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {
       // FIXME: call _mm_cvtepi64_pd
       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];
       self.store_aligned(&buffer[0]);
@@ -1165,13 +1139,11 @@ namespace xsimd {
     }
 
     // to_int
-    template<class A>
-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_cvttps_epi32(self);
     }
 
-    template<class A>
-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {
       // FIXME: call _mm_cvttpd_epi64
       alignas(A::alignment()) double buffer[batch<double, A>::size];
       self.store_aligned(&buffer[0]);
@@ -1180,10 +1152,10 @@ namespace xsimd {
     }
 
     // trunc
-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);
     }
-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {
       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);
     }
 
@@ -1198,10 +1170,10 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_unpackhi_ps(self, other);
     }
-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_unpackhi_pd(self, other);
     }
 
@@ -1216,10 +1188,10 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {
       return _mm512_unpacklo_ps(self, other);
     }
-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
+    template<class A> inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {
       return _mm512_unpacklo_pd(self, other);
     }
 
diff --git a/include/xsimd/arch/xsimd_constants.hpp b/include/xsimd/arch/xsimd_constants.hpp
index a9ff97f..70edcca 100644
--- a/include/xsimd/arch/xsimd_constants.hpp
+++ b/include/xsimd/arch/xsimd_constants.hpp
@@ -22,34 +22,34 @@ namespace xsimd
 namespace constants {
 
 #define XSIMD_DEFINE_CONSTANT(NAME, SINGLE, DOUBLE) \
-    template <class T>                              \
+    template <class T> inline \
     inline T NAME() noexcept                     \
     {                                               \
         return T(NAME<typename T::value_type>());   \
     }                                               \
-    template <>                                     \
+    template <> inline \
     inline float NAME<float>() noexcept          \
     {                                               \
         return SINGLE;                              \
     }                                               \
-    template <>                                     \
+    template <> inline \
     inline double NAME<double>() noexcept        \
     {                                               \
         return DOUBLE;                              \
     }
 
 #define XSIMD_DEFINE_CONSTANT_HEX(NAME, SINGLE, DOUBLE) \
-    template <class T>                                  \
+    template <class T> inline \
     inline T NAME() noexcept                         \
     {                                                   \
         return T(NAME<typename T::value_type>());       \
     }                                                   \
-    template <>                                         \
+    template <> inline \
     inline float NAME<float>() noexcept              \
     {                                                   \
       return bit_cast<float>((uint32_t)SINGLE); \
     }                                                   \
-    template <>                                         \
+    template <> inline \
     inline double NAME<double>() noexcept            \
     {                                                   \
       return bit_cast<double>((uint64_t)DOUBLE); \
@@ -107,32 +107,23 @@ namespace constants {
 #undef XSIMD_DEFINE_CONSTANT
 #undef XSIMD_DEFINE_CONSTANT_HEX
 
-    template <class T>
-    constexpr T allbits() noexcept;
+    template <class T> inline constexpr T allbits() noexcept;
 
-    template <class T>
-    constexpr as_integer_t<T> mask1frexp() noexcept;
+    template <class T> inline constexpr as_integer_t<T> mask1frexp() noexcept;
 
-    template <class T>
-    constexpr as_integer_t<T> mask2frexp() noexcept;
+    template <class T> inline constexpr as_integer_t<T> mask2frexp() noexcept;
 
-    template <class T>
-    constexpr as_integer_t<T> maxexponent() noexcept;
+    template <class T> inline constexpr as_integer_t<T> maxexponent() noexcept;
 
-    template <class T>
-    constexpr as_integer_t<T> maxexponentm1() noexcept;
+    template <class T> inline constexpr as_integer_t<T> maxexponentm1() noexcept;
 
-    template <class T>
-    constexpr int32_t nmb() noexcept;
+    template <class T> inline constexpr int32_t nmb() noexcept;
 
-    template <class T>
-    constexpr T zero() noexcept;
+    template <class T> inline constexpr T zero() noexcept;
 
-    template <class T>
-    constexpr T minvalue() noexcept;
+    template <class T> inline constexpr T minvalue() noexcept;
 
-    template <class T>
-    constexpr T maxvalue() noexcept;
+    template <class T> inline constexpr T maxvalue() noexcept;
 
     /**************************
      * allbits implementation *
@@ -149,8 +140,7 @@ namespace constants {
             }
         };
 
-        template <class T>
-        struct allbits_impl<T, false>
+        template <class T> inline struct allbits_impl<T, false>
         {
             static constexpr T get_value() noexcept
             {
@@ -159,8 +149,7 @@ namespace constants {
         };
     }
 
-    template <class T>
-    constexpr T allbits() noexcept
+    template <class T> inline constexpr T allbits() noexcept
     {
         return T(detail::allbits_impl<typename T::value_type>::get_value());
     }
@@ -169,20 +158,17 @@ namespace constants {
      * mask1frexp implementation *
      *****************************/
 
-    template <class T>
-    constexpr as_integer_t<T> mask1frexp() noexcept
+    template <class T> inline constexpr as_integer_t<T> mask1frexp() noexcept
     {
         return as_integer_t<T>(mask1frexp<typename T::value_type>());
     }
 
-    template <>
-    constexpr int32_t mask1frexp<float>() noexcept
+    template <> inline constexpr int32_t mask1frexp<float>() noexcept
     {
         return 0x7f800000;
     }
 
-    template <>
-    constexpr int64_t mask1frexp<double>() noexcept
+    template <> inline constexpr int64_t mask1frexp<double>() noexcept
     {
         return 0x7ff0000000000000;
     }
@@ -191,20 +177,17 @@ namespace constants {
      * mask2frexp implementation *
      *****************************/
 
-    template <class T>
-    constexpr as_integer_t<T> mask2frexp() noexcept
+    template <class T> inline constexpr as_integer_t<T> mask2frexp() noexcept
     {
         return as_integer_t<T>(mask2frexp<typename T::value_type>());
     }
 
-    template <>
-    constexpr int32_t mask2frexp<float>() noexcept
+    template <> inline constexpr int32_t mask2frexp<float>() noexcept
     {
         return 0x3f000000;
     }
 
-    template <>
-    constexpr int64_t mask2frexp<double>() noexcept
+    template <> inline constexpr int64_t mask2frexp<double>() noexcept
     {
         return 0x3fe0000000000000;
     }
@@ -213,20 +196,17 @@ namespace constants {
      * maxexponent implementation *
      ******************************/
 
-    template <class T>
-    constexpr as_integer_t<T> maxexponent() noexcept
+    template <class T> inline constexpr as_integer_t<T> maxexponent() noexcept
     {
         return as_integer_t<T>(maxexponent<typename T::value_type>());
     }
 
-    template <>
-    constexpr int32_t maxexponent<float>() noexcept
+    template <> inline constexpr int32_t maxexponent<float>() noexcept
     {
         return 127;
     }
 
-    template <>
-    constexpr int64_t maxexponent<double>() noexcept
+    template <> inline constexpr int64_t maxexponent<double>() noexcept
     {
         return 1023;
     }
@@ -235,20 +215,17 @@ namespace constants {
      * maxexponent implementation *
      ******************************/
 
-    template <class T>
-    constexpr as_integer_t<T> maxexponentm1() noexcept
+    template <class T> inline constexpr as_integer_t<T> maxexponentm1() noexcept
     {
         return as_integer_t<T>(maxexponentm1<typename T::value_type>());
     }
 
-    template <>
-    constexpr int32_t maxexponentm1<float>() noexcept
+    template <> inline constexpr int32_t maxexponentm1<float>() noexcept
     {
         return 126;
     }
 
-    template <>
-    constexpr int64_t maxexponentm1<double>() noexcept
+    template <> inline constexpr int64_t maxexponentm1<double>() noexcept
     {
         return 1022;
     }
@@ -257,20 +234,17 @@ namespace constants {
      * nmb implementation *
      **********************/
 
-    template <class T>
-    constexpr int32_t nmb() noexcept
+    template <class T> inline constexpr int32_t nmb() noexcept
     {
         return nmb<typename T::value_type>();
     }
 
-    template <>
-    constexpr int32_t nmb<float>() noexcept
+    template <> inline constexpr int32_t nmb<float>() noexcept
     {
         return 23;
     }
 
-    template <>
-    constexpr int32_t nmb<double>() noexcept
+    template <> inline constexpr int32_t nmb<double>() noexcept
     {
         return 52;
     }
@@ -279,8 +253,7 @@ namespace constants {
      * zero implementation *
      ***********************/
 
-    template <class T>
-    constexpr T zero() noexcept
+    template <class T> inline constexpr T zero() noexcept
     {
         return T(typename T::value_type(0));
     }
@@ -291,8 +264,7 @@ namespace constants {
 
     namespace detail
     {
-        template <class T>
-        struct minvalue_impl
+        template <class T> inline struct minvalue_impl
         {
             static constexpr T get_value() noexcept
             {
@@ -300,8 +272,7 @@ namespace constants {
             }
         };
 
-        template <class T>
-        struct minvalue_common
+        template <class T> inline struct minvalue_common
         {
             static constexpr T get_value() noexcept
             {
@@ -309,25 +280,16 @@ namespace constants {
             }
         };
 
-        template <>
-        struct minvalue_impl<int8_t> : minvalue_common<int8_t> {};
-        template <>
-        struct minvalue_impl<uint8_t> : minvalue_common<uint8_t> {};
-        template <>
-        struct minvalue_impl<int16_t> : minvalue_common<int16_t> {};
-        template <>
-        struct minvalue_impl<uint16_t> : minvalue_common<uint16_t> {};
-        template <>
-        struct minvalue_impl<int32_t> : minvalue_common<int32_t> {};
-        template <>
-        struct minvalue_impl<uint32_t> : minvalue_common<uint32_t> {};
-        template <>
-        struct minvalue_impl<int64_t> : minvalue_common<int64_t> {};
-        template <>
-        struct minvalue_impl<uint64_t> : minvalue_common<uint64_t> {};
-
-        template <>
-        struct minvalue_impl<float>
+        template <> inline struct minvalue_impl<int8_t> : minvalue_common<int8_t> {};
+        template <> inline struct minvalue_impl<uint8_t> : minvalue_common<uint8_t> {};
+        template <> inline struct minvalue_impl<int16_t> : minvalue_common<int16_t> {};
+        template <> inline struct minvalue_impl<uint16_t> : minvalue_common<uint16_t> {};
+        template <> inline struct minvalue_impl<int32_t> : minvalue_common<int32_t> {};
+        template <> inline struct minvalue_impl<uint32_t> : minvalue_common<uint32_t> {};
+        template <> inline struct minvalue_impl<int64_t> : minvalue_common<int64_t> {};
+        template <> inline struct minvalue_impl<uint64_t> : minvalue_common<uint64_t> {};
+
+        template <> inline struct minvalue_impl<float>
         {
             static float get_value() noexcept
             {
@@ -335,8 +297,7 @@ namespace constants {
             }
         };
 
-        template <>
-        struct minvalue_impl<double>
+        template <> inline struct minvalue_impl<double>
         {
             static double get_value() noexcept
             {
@@ -345,8 +306,7 @@ namespace constants {
         };
     }
 
-    template <class T>
-    constexpr T minvalue() noexcept
+    template <class T> inline constexpr T minvalue() noexcept
     {
         return T(detail::minvalue_impl<typename T::value_type>::get_value());
     }
@@ -355,8 +315,7 @@ namespace constants {
      * maxvalue implementation *
      ***************************/
 
-    template <class T>
-    constexpr T maxvalue() noexcept
+    template <class T> inline constexpr T maxvalue() noexcept
     {
         return T(std::numeric_limits<typename T::value_type>::max());
     }
diff --git a/include/xsimd/arch/xsimd_fma3.hpp b/include/xsimd/arch/xsimd_fma3.hpp
index e4646e9..1953672 100644
--- a/include/xsimd/arch/xsimd_fma3.hpp
+++ b/include/xsimd/arch/xsimd_fma3.hpp
@@ -20,38 +20,38 @@ namespace xsimd {
   namespace kernel {
     using namespace types;
     // fnma
-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
       return _mm_fnmadd_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
       return _mm_fnmadd_pd(x, y, z);
     }
 
     // fnms
-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
       return _mm_fnmsub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
       return _mm_fnmsub_pd(x, y, z);
     }
 
     // fma
-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
       return _mm_fmadd_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
       return _mm_fmadd_pd(x, y, z);
     }
 
     // fms
-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {
       return _mm_fmsub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
+    template<class A> inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {
       return _mm_fmsub_pd(x, y, z);
     }
 
diff --git a/include/xsimd/arch/xsimd_fma4.hpp b/include/xsimd/arch/xsimd_fma4.hpp
index 814bcd1..ef28172 100644
--- a/include/xsimd/arch/xsimd_fma4.hpp
+++ b/include/xsimd/arch/xsimd_fma4.hpp
@@ -21,38 +21,38 @@ namespace xsimd {
     using namespace types;
 
     // fnma
-    template<class A> batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
       return _mm_nmacc_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
       return _mm_nmacc_pd(x, y, z);
     }
 
     // fnms
-    template<class A> batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
       return _mm_nmsub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
       return _mm_nmsub_pd(x, y, z);
     }
 
     // fma
-    template<class A> batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
       return _mm_macc_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
       return _mm_macc_pd(x, y, z);
     }
 
     // fms
-    template<class A> batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {
       return _mm_msub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
+    template<class A> inline batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {
       return _mm_msub_pd(x, y, z);
     }
   }
diff --git a/include/xsimd/arch/xsimd_fma5.hpp b/include/xsimd/arch/xsimd_fma5.hpp
index 786949d..b991c91 100644
--- a/include/xsimd/arch/xsimd_fma5.hpp
+++ b/include/xsimd/arch/xsimd_fma5.hpp
@@ -21,38 +21,38 @@ namespace xsimd {
     using namespace types;
 
     // fnma
-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
       return _mm256_fnmadd_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
       return _mm256_fnmadd_pd(x, y, z);
     }
 
     // fnms
-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
       return _mm256_fnmsub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
       return _mm256_fnmsub_pd(x, y, z);
     }
 
     // fma
-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
       return _mm256_fmadd_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
       return _mm256_fmadd_pd(x, y, z);
     }
 
     // fms
-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {
       return _mm256_fmsub_ps(x, y, z);
     }
 
-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
+    template<class A> inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {
       return _mm256_fmsub_pd(x, y, z);
     }
 
diff --git a/include/xsimd/arch/xsimd_generic_fwd.hpp b/include/xsimd/arch/xsimd_generic_fwd.hpp
index 8326066..d6e8856 100644
--- a/include/xsimd/arch/xsimd_generic_fwd.hpp
+++ b/include/xsimd/arch/xsimd_generic_fwd.hpp
@@ -24,7 +24,7 @@ namespace xsimd {
     batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);
-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);
+    template<class A, class T> inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);
 
diff --git a/include/xsimd/arch/xsimd_neon.hpp b/include/xsimd/arch/xsimd_neon.hpp
index e3b0201..a809048 100644
--- a/include/xsimd/arch/xsimd_neon.hpp
+++ b/include/xsimd/arch/xsimd_neon.hpp
@@ -86,7 +86,7 @@ namespace xsimd
 
         namespace detail
         {
-            template <template <class> class return_type, class... T>
+            template <template <class> inline class return_type, class... T>
             struct neon_dispatcher_base
             {
                 struct unary
@@ -94,8 +94,7 @@ namespace xsimd
                     using container_type = std::tuple<return_type<T> (*)(T)...>;
                     const container_type m_func;
 
-                    template <class U>
-                    return_type<U> apply(U rhs) const
+                    template <class U> inline return_type<U> apply(U rhs) const
                     {
                         using func_type = return_type<U> (*)(U);
                         auto func = xsimd::detail::get<func_type>(m_func);
@@ -108,8 +107,7 @@ namespace xsimd
                     using container_type = std::tuple<return_type<T> (*)(T, T) ...>;
                     const container_type m_func;
 
-                    template <class U>
-                    return_type<U> apply(U lhs, U rhs) const
+                    template <class U> inline return_type<U> apply(U lhs, U rhs) const
                     {
                         using func_type = return_type<U> (*)(U, U);
                         auto func = xsimd::detail::get<func_type>(m_func);
@@ -122,11 +120,9 @@ namespace xsimd
              *  arithmetic dispatchers *
              ***************************/
 
-            template <class T>
-            using identity_return_type = T;
+            template <class T> inline using identity_return_type = T;
             
-            template <class... T>
-            struct neon_dispatcher_impl : neon_dispatcher_base<identity_return_type, T...>
+            template <class... T> inline struct neon_dispatcher_impl : neon_dispatcher_base<identity_return_type, T...>
             {
             };
 
@@ -146,68 +142,56 @@ namespace xsimd
              * comparison dispatchers *
              **************************/
 
-            template <class T>
-            struct comp_return_type_impl;
+            template <class T> inline struct comp_return_type_impl;
 
-            template <>
-            struct comp_return_type_impl<uint8x16_t>
+            template <> inline struct comp_return_type_impl<uint8x16_t>
             {
                 using type = uint8x16_t;
             };
 
-            template <>
-            struct comp_return_type_impl<int8x16_t>
+            template <> inline struct comp_return_type_impl<int8x16_t>
             {
                 using type = uint8x16_t;
             };
 
-            template <>
-            struct comp_return_type_impl<uint16x8_t>
+            template <> inline struct comp_return_type_impl<uint16x8_t>
             {
                 using type = uint16x8_t;
             };
 
-            template <>
-            struct comp_return_type_impl<int16x8_t>
+            template <> inline struct comp_return_type_impl<int16x8_t>
             {
                 using type = uint16x8_t;
             };
 
-            template <>
-            struct comp_return_type_impl<uint32x4_t>
+            template <> inline struct comp_return_type_impl<uint32x4_t>
             {
                 using type = uint32x4_t;
             };
 
-            template <>
-            struct comp_return_type_impl<int32x4_t>
+            template <> inline struct comp_return_type_impl<int32x4_t>
             {
                 using type = uint32x4_t;
             };
 
-            template <>
-            struct comp_return_type_impl<uint64x2_t>
+            template <> inline struct comp_return_type_impl<uint64x2_t>
             {
                 using type = uint64x2_t;
             };
 
-            template <>
-            struct comp_return_type_impl<int64x2_t>
+            template <> inline struct comp_return_type_impl<int64x2_t>
             {
                 using type = uint64x2_t;
             };
             
-            template <>
-            struct comp_return_type_impl<float32x4_t>
+            template <> inline struct comp_return_type_impl<float32x4_t>
             {
                 using type = uint32x4_t;
             };
 
-            template <class T>
-            using comp_return_type = typename comp_return_type_impl<T>::type;
+            template <class T> inline using comp_return_type = typename comp_return_type_impl<T>::type;
 
-            template <class... T>
-            struct neon_comp_dispatcher_impl : neon_dispatcher_base<comp_return_type, T...>
+            template <class... T> inline struct neon_comp_dispatcher_impl : neon_dispatcher_base<comp_return_type, T...>
             {
             };
 
@@ -220,32 +204,25 @@ namespace xsimd
              * enabling / disabling metafunctions *
              **************************************/
 
-            template <class T>
-            using enable_integral_t = typename std::enable_if<std::is_integral<T>::value, int>::type;
+            template <class T> inline using enable_integral_t = typename std::enable_if<std::is_integral<T>::value, int>::type;
 
-            template <class T>
-            using enable_neon_type_t = typename std::enable_if<std::is_integral<T>::value || std::is_same<T, float>::value,
+            template <class T> inline using enable_neon_type_t = typename std::enable_if<std::is_integral<T>::value || std::is_same<T, float>::value,
                                                                int>::type;
 
-            template <class T, size_t S>
-            using enable_sized_signed_t = typename std::enable_if<std::is_integral<T>::value &&
+            template <class T, size_t S> inline using enable_sized_signed_t = typename std::enable_if<std::is_integral<T>::value &&
                                                                   std::is_signed<T>::value &&
                                                                   sizeof(T) == S, int>::type;
 
-            template <class T, size_t S>
-            using enable_sized_unsigned_t = typename std::enable_if<std::is_integral<T>::value &&
+            template <class T, size_t S> inline using enable_sized_unsigned_t = typename std::enable_if<std::is_integral<T>::value &&
                                                                     !std::is_signed<T>::value &&
                                                                     sizeof(T) == S, int>::type;
 
-            template <class T, size_t S>
-            using enable_sized_integral_t = typename std::enable_if<std::is_integral<T>::value &&
+            template <class T, size_t S> inline using enable_sized_integral_t = typename std::enable_if<std::is_integral<T>::value &&
                                                                    sizeof(T) == S, int>::type;
 
-            template <class T, size_t S>
-            using enable_sized_t = typename std::enable_if<sizeof(T) == S, int>::type;
+            template <class T, size_t S> inline using enable_sized_t = typename std::enable_if<sizeof(T) == S, int>::type;
 
-            template <class T>
-            using exclude_int64_neon_t
+            template <class T> inline using exclude_int64_neon_t
                  = typename std::enable_if<(std::is_integral<T>::value && sizeof(T) != 8) || std::is_same<T, float>::value, int>::type;
         }
 
@@ -253,56 +230,55 @@ namespace xsimd
          * broadcast *
          *************/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_u8(uint8_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_s8(int8_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_u16(uint16_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_s16(int16_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_u32(uint32_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_s32(int32_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_u64(uint64_t(val));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> broadcast(T val, requires_arch<neon>)
         {
             return vdupq_n_s64(int64_t(val));
         }
 
-        template <class A>
-        batch<float, A> broadcast(float val, requires_arch<neon>)
+        template <class A> inline batch<float, A> broadcast(float val, requires_arch<neon>)
         {
             return vdupq_n_f32(val);
         }
@@ -311,13 +287,13 @@ namespace xsimd
          * set *
          *******/
 
-        template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>
+        template <class A, class T, class... Args, detail::enable_integral_t<T> inline = 0>
         batch<T, A> set(batch<T, A> const&, requires_arch<neon>, Args... args)
         {
             return xsimd::types::detail::neon_vector_type<T>{args...};
         }
 
-        template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>
+        template <class A, class T, class... Args, detail::enable_integral_t<T> inline = 0>
         batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<neon>, Args... args)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -325,14 +301,12 @@ namespace xsimd
             return register_type{static_cast<unsigned_type>(args ? -1LL : 0LL)...};
         }
 
-        template <class A>
-        batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)
+        template <class A> inline batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)
         {
             return float32x4_t{f0, f1, f2, f3};
         }
 
-        template <class A, class... Args>
-        batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)
+        template <class A, class... Args> inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)
         {
             using register_type = typename batch_bool<float, A>::register_type;
             using unsigned_type = as_unsigned_integer_t<float>;
@@ -343,56 +317,55 @@ namespace xsimd
          * from_bool *
          *************/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_u8(arg, vdupq_n_u8(1));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_s8(reinterpret_cast<int8x16_t>(arg.data), vdupq_n_s8(1));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_u16(arg, vdupq_n_u16(1));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_s16(reinterpret_cast<int16x8_t>(arg.data), vdupq_n_s16(1));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_u32(arg, vdupq_n_u32(1));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_s32(reinterpret_cast<int32x4_t>(arg.data), vdupq_n_s32(1));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_u64(arg, vdupq_n_u64(1));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             return vandq_s64(reinterpret_cast<int64x2_t>(arg.data), vdupq_n_s64(1));
         }
 
-        template <class A>
-        batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)
+        template <class A> inline batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)
         {
             return vreinterpretq_f32_u32(vandq_u32(arg, vreinterpretq_u32_f32(vdupq_n_f32(1.f))));
         }
@@ -401,57 +374,55 @@ namespace xsimd
          * load *
          ********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_u8(src);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_s8(src);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_u16(src);
         }
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_s16(src);
         }
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_u32(src);
         }
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_s32(src);
         }
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_u64(src);
         }
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return vld1q_s64(src);
         }
 
-        template <class A>
-        batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)
+        template <class A> inline batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)
         {
             return vld1q_f32(src);
         }
 
-        template <class A, class T>
-        batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)
+        template <class A, class T> inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)
         {
             return load_aligned<A>(src, convert<T>(), A{});
         }
@@ -460,62 +431,60 @@ namespace xsimd
          * store *
          *********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_u8(dst, src);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_s8(dst, src);
         }
         
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_u16(dst, src);
         }
         
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_s16(dst, src);
         }
         
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_u32(dst, src);
         }
         
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_s32(dst, src);
         }
         
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_u64(dst, src);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             vst1q_s64(dst, src);
         }
 
-        template <class A>
-        void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)
+        template <class A> inline void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)
         {
             vst1q_f32(dst, src);
         }
 
-        template <class A, class T>
-        void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
+        template <class A, class T> inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)
         {
             store_aligned<A>(dst, src, A{});
         }
@@ -524,8 +493,7 @@ namespace xsimd
          * load_complex *
          ****************/
 
-        template <class A>
-        batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)
+        template <class A> inline batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)
         {
             using real_batch = batch<float, A>;
             const float* buf = reinterpret_cast<const float*>(mem);
@@ -535,8 +503,7 @@ namespace xsimd
             return batch<std::complex<float>, A>{real, imag};
         }
 
-        template <class A>
-        batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)
+        template <class A> inline batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)
         {
             return load_complex_aligned<A>(mem, cvt, A{});
         }
@@ -545,8 +512,7 @@ namespace xsimd
          * store_complex *
          *****************/
 
-        template <class A>
-        void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)
+        template <class A> inline void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)
         {
             float32x4x2_t tmp;
             tmp.val[0] = src.real();
@@ -555,8 +521,7 @@ namespace xsimd
             vst2q_f32(buf, tmp);
         }
 
-        template <class A>
-        void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)
+        template <class A> inline void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)
         {
             store_complex_aligned(dst, src, A{});
         }
@@ -565,56 +530,55 @@ namespace xsimd
          * neg *
          *******/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vreinterpretq_u8_s8(vnegq_s8(vreinterpretq_s8_u8(rhs)));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vnegq_s8(rhs);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vreinterpretq_u16_s16(vnegq_s16(vreinterpretq_s16_u16(rhs)));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vnegq_s16(rhs);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vreinterpretq_u32_s32(vnegq_s32(vreinterpretq_s32_u32(rhs)));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vnegq_s32(rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch<T, A>({-rhs.get(0), -rhs.get(1)});
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch<T, A>({-rhs.get(0), -rhs.get(1)});
         }
         
-        template <class A>
-        batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)
+        template <class A> inline batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)
         {
             return vnegq_f32(rhs);
         }
@@ -626,7 +590,7 @@ namespace xsimd
         WRAP_BINARY_INT(vaddq, detail::identity_return_type)
         WRAP_BINARY_FLOAT(vaddq, detail::identity_return_type)
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -645,7 +609,7 @@ namespace xsimd
 
         WRAP_BINARY_INT(vqaddq, detail::identity_return_type)
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -665,7 +629,7 @@ namespace xsimd
         WRAP_BINARY_INT(vsubq, detail::identity_return_type)
         WRAP_BINARY_FLOAT(vsubq, detail::identity_return_type)
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -684,7 +648,7 @@ namespace xsimd
 
         WRAP_BINARY_INT(vqsubq, detail::identity_return_type)
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -705,7 +669,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vmulq, detail::identity_return_type)
         WRAP_BINARY_FLOAT(vmulq, detail::identity_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -722,21 +686,20 @@ namespace xsimd
          *******/
 
 #if defined(XSIMD_FAST_INTEGER_DIVISION)
-        template <class A, class T,  detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcvtq_s32_f32(vcvtq_f32_s32(lhs) / vcvtq_f32_s32(rhs));
         }
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcvtq_u32_f32(vcvtq_f32_u32(lhs) / vcvtq_f32_u32(rhs));
         }
 #endif
 
-        template <class A>
-        batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
+        template <class A> inline batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
         {
             // from stackoverflow & https://projectne10.github.io/Ne10/doc/NE10__divc_8neon_8c_source.html
             // get an initial estimate of 1/b.
@@ -759,7 +722,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vceqq, detail::comp_return_type)
         WRAP_BINARY_FLOAT(vceqq, detail::comp_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -771,7 +734,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -783,13 +746,13 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});
@@ -802,7 +765,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vcltq, detail::comp_return_type)
         WRAP_BINARY_FLOAT(vcltq, detail::comp_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -814,7 +777,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) < rhs.get(0), lhs.get(1) < rhs.get(1)});
@@ -827,7 +790,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vcleq, detail::comp_return_type)
         WRAP_BINARY_FLOAT(vcleq, detail::comp_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -839,7 +802,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) <= rhs.get(0), lhs.get(1) <= rhs.get(1)});
@@ -852,7 +815,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vcgtq, detail::comp_return_type)
         WRAP_BINARY_FLOAT(vcgtq, detail::comp_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -864,7 +827,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) > rhs.get(0), lhs.get(1) > rhs.get(1)});
@@ -877,7 +840,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vcgeq, detail::comp_return_type)
         WRAP_BINARY_FLOAT(vcgeq, detail::comp_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -889,7 +852,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return batch_bool<T, A>({lhs.get(0) >= rhs.get(0), lhs.get(1) >= rhs.get(1)});
@@ -909,8 +872,7 @@ namespace xsimd
                                                        vreinterpretq_u32_f32(rhs)));
             }
 
-            template <class V>
-            V bitwise_and_neon(V const& lhs, V const& rhs)
+            template <class V> inline V bitwise_and_neon(V const& lhs, V const& rhs)
             {
                 const neon_dispatcher::binary dispatcher =
                 {
@@ -922,14 +884,14 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
             return detail::bitwise_and_neon(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -950,8 +912,7 @@ namespace xsimd
                                                        vreinterpretq_u32_f32(rhs)));
             }
 
-            template <class V>
-            V bitwise_or_neon(V const& lhs, V const& rhs)
+            template <class V> inline V bitwise_or_neon(V const& lhs, V const& rhs)
             {
                 const neon_dispatcher::binary dispatcher =
                 {
@@ -963,14 +924,14 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
             return detail::bitwise_or_neon(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -991,8 +952,7 @@ namespace xsimd
                                                        vreinterpretq_u32_f32(rhs)));
             }
 
-            template <class V>
-            V bitwise_xor_neon(V const& lhs, V const& rhs)
+            template <class V> inline V bitwise_xor_neon(V const& lhs, V const& rhs)
             {
                 const neon_dispatcher::binary dispatcher =
                 {
@@ -1004,14 +964,14 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
             return detail::bitwise_xor_neon(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -1022,8 +982,7 @@ namespace xsimd
          * neq *
          *******/
 
-        template <class A, class T>
-        batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
+        template <class A, class T> inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             return bitwise_xor(lhs, rhs, A{});
         }
@@ -1051,8 +1010,7 @@ namespace xsimd
                 return vreinterpretq_f32_u32(vmvnq_u32(vreinterpretq_u32_f32(arg)));
             }
 
-            template <class V>
-            V bitwise_not_neon(V const& arg)
+            template <class V> inline V bitwise_not_neon(V const& arg)
             {
                 const neon_dispatcher::unary dispatcher =
                 {
@@ -1065,14 +1023,14 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
             return detail::bitwise_not_neon(register_type(arg));
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -1092,8 +1050,7 @@ namespace xsimd
                 return vreinterpretq_f32_u32(vbicq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs)));
             }
 
-            template <class V>
-            V bitwise_andnot_neon(V const& lhs, V const& rhs)
+            template <class V> inline V bitwise_andnot_neon(V const& lhs, V const& rhs)
             {
                 const detail::neon_dispatcher::binary dispatcher =
                 {
@@ -1105,14 +1062,14 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
             return detail::bitwise_andnot_neon(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch_bool<T, A>::register_type;
@@ -1126,7 +1083,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vminq, detail::identity_return_type)
         WRAP_BINARY_FLOAT(vminq, detail::identity_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -1138,7 +1095,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return { std::min(lhs.get(0), rhs.get(0)), std::min(lhs.get(1), rhs.get(1)) };
@@ -1151,7 +1108,7 @@ namespace xsimd
         WRAP_BINARY_INT_EXCLUDING_64(vmaxq, detail::identity_return_type)
         WRAP_BINARY_FLOAT(vmaxq, detail::identity_return_type)
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -1163,7 +1120,7 @@ namespace xsimd
             return dispatcher.apply(register_type(lhs), register_type(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return { std::max(lhs.get(0), rhs.get(0)), std::max(lhs.get(1), rhs.get(1)) };
@@ -1198,7 +1155,7 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::exclude_int64_neon_t<T> = 0>
+        template <class A, class T, detail::exclude_int64_neon_t<T> inline = 0>
         batch<T, A> abs(batch<T, A> const& arg, requires_arch<neon>)
         {
             using register_type = typename batch<T, A>::register_type;
@@ -1214,8 +1171,7 @@ namespace xsimd
          * sqrt *
          ********/
 
-        template <class A>
-        batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)
+        template <class A> inline batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)
         {
             batch<float, A> sqrt_reciprocal = vrsqrteq_f32(arg);
             // one iter
@@ -1230,14 +1186,12 @@ namespace xsimd
          ********************/
 
 #ifdef __ARM_FEATURE_FMA
-        template <class A>
-        batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)
+        template <class A> inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)
         {
             return vfmaq_f32(z, x, y);
         }
 
-        template <class A>
-        batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)
+        template <class A> inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)
         {
             return vfmaq_f32(-z, x, y);
         }
@@ -1249,8 +1203,7 @@ namespace xsimd
 
         namespace detail
         {
-            template <class T, class A, class V>
-            T sum_batch(V const& arg)
+            template <class T, class A, class V> inline T sum_batch(V const& arg)
             {
                 T res = T(0);
                 for (std::size_t i = 0; i < batch<T, A>::size; ++i)
@@ -1261,35 +1214,35 @@ namespace xsimd
             }
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             uint8x8_t tmp = vpadd_u8(vget_low_u8(arg), vget_high_u8(arg));
             return detail::sum_batch<T, A>(tmp);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             int8x8_t tmp = vpadd_s8(vget_low_s8(arg), vget_high_s8(arg));
             return detail::sum_batch<T, A>(tmp);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             uint16x4_t tmp = vpadd_u16(vget_low_u16(arg), vget_high_u16(arg));
             return detail::sum_batch<T, A>(tmp);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             int16x4_t tmp = vpadd_s16(vget_low_s16(arg), vget_high_s16(arg));
             return detail::sum_batch<T, A>(tmp);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             uint32x2_t tmp = vpadd_u32(vget_low_u32(arg), vget_high_u32(arg));
@@ -1297,7 +1250,7 @@ namespace xsimd
             return vget_lane_u32(tmp, 0);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             int32x2_t tmp = vpadd_s32(vget_low_s32(arg), vget_high_s32(arg));
@@ -1305,14 +1258,13 @@ namespace xsimd
             return vget_lane_s32(tmp, 0);
         }
 
-        template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_integral_t<T, 8> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)
         {
             return arg.get(0) + arg.get(1);
         }
 
-        template <class A>
-        float hadd(batch<float, A> const& arg, requires_arch<neon>)
+        template <class A> inline float hadd(batch<float, A> const& arg, requires_arch<neon>)
         {
             float32x2_t tmp = vpadd_f32(vget_low_f32(arg), vget_high_f32(arg));
             tmp = vpadd_f32(tmp, tmp);
@@ -1323,8 +1275,7 @@ namespace xsimd
          * haddp *
          *********/
 
-        template <class A>
-        batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)
+        template <class A> inline batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)
         {
             // row = (a,b,c,d)
             float32x2_t tmp1, tmp2, tmp3;
@@ -1362,14 +1313,12 @@ namespace xsimd
 
         namespace detail
         {
-            template <class... T>
-            struct neon_select_dispatcher_impl
+            template <class... T> inline struct neon_select_dispatcher_impl
             {
                 using container_type = std::tuple<T (*)(comp_return_type<T>, T, T)...>;
                 const container_type m_func;
 
-                template <class U>
-                U apply(comp_return_type<U> cond, U lhs, U rhs) const
+                template <class U> inline U apply(comp_return_type<U> cond, U lhs, U rhs) const
                 {
                     using func_type = U (*)(comp_return_type<U>, U, U);
                     auto func = xsimd::detail::get<func_type>(m_func);
@@ -1384,7 +1333,7 @@ namespace xsimd
                                                                      float32x4_t>;
         }
 
-        template <class A, class T, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<neon>)
         {
             using bool_register_type = typename batch_bool<T, A>::register_type;
@@ -1398,7 +1347,7 @@ namespace xsimd
             return dispatcher.apply(bool_register_type(cond), register_type(a), register_type(b));
         }
 
-        template <class A, class T, bool... b, detail::enable_neon_type_t<T> = 0>
+        template <class A, class T, bool... b, detail::enable_neon_type_t<T> inline = 0>
         batch<T, A> select(batch_bool_constant<batch<T, A>, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<neon>)
         {
             return select(batch_bool<T, A>{b...}, true_br, false_br, neon{});
@@ -1408,62 +1357,61 @@ namespace xsimd
          * zip_lo *
          **********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint8x8x2_t tmp = vzip_u8(vget_low_u8(lhs), vget_low_u8(rhs));
             return vcombine_u8(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int8x8x2_t tmp = vzip_s8(vget_low_s8(lhs), vget_low_s8(rhs));
             return vcombine_s8(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint16x4x2_t tmp = vzip_u16(vget_low_u16(lhs), vget_low_u16(rhs));
             return vcombine_u16(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int16x4x2_t tmp = vzip_s16(vget_low_s16(lhs), vget_low_s16(rhs));
             return vcombine_s16(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint32x2x2_t tmp = vzip_u32(vget_low_u32(lhs), vget_low_u32(rhs));
             return vcombine_u32(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int32x2x2_t tmp = vzip_s32(vget_low_s32(lhs), vget_low_s32(rhs));
             return vcombine_s32(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcombine_u64(vget_low_u64(lhs), vget_low_u64(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcombine_s64(vget_low_s64(lhs), vget_low_s64(rhs));
         }
 
-        template <class A>
-        batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
+        template <class A> inline batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
         {
             float32x2x2_t tmp = vzip_f32(vget_low_f32(lhs), vget_low_f32(rhs));
             return vcombine_f32(tmp.val[0], tmp.val[1]);
@@ -1473,62 +1421,61 @@ namespace xsimd
          * zip_hi *
          **********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint8x8x2_t tmp = vzip_u8(vget_high_u8(lhs), vget_high_u8(rhs));
             return vcombine_u8(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int8x8x2_t tmp = vzip_s8(vget_high_s8(lhs), vget_high_s8(rhs));
             return vcombine_s8(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint16x4x2_t tmp = vzip_u16(vget_high_u16(lhs), vget_high_u16(rhs));
             return vcombine_u16(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int16x4x2_t tmp = vzip_s16(vget_high_s16(lhs), vget_high_s16(rhs));
             return vcombine_s16(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             uint32x2x2_t tmp = vzip_u32(vget_high_u32(lhs), vget_high_u32(rhs));
             return vcombine_u32(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             int32x2x2_t tmp = vzip_s32(vget_high_s32(lhs), vget_high_s32(rhs));
             return vcombine_s32(tmp.val[0], tmp.val[1]);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcombine_u64(vget_high_u64(lhs), vget_high_u64(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vcombine_s64(vget_high_s64(lhs), vget_high_s64(rhs));
         }
 
-        template <class A>
-        batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
+        template <class A> inline batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)
         {
             float32x2x2_t tmp = vzip_f32(vget_high_f32(lhs), vget_high_f32(rhs));
             return vcombine_f32(tmp.val[0], tmp.val[1]);
@@ -1540,14 +1487,13 @@ namespace xsimd
 
         namespace detail
         {
-            template <class A, class T>
-            batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)
+            template <class A, class T> inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)
             {
                 assert(false && "extract_pair out of bounds");
                 return  batch<T, A>{};
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 1> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 1> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1560,7 +1506,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 1> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 1> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1573,7 +1519,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 2> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 2> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1586,7 +1532,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 2> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 2> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1599,7 +1545,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 4> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 4> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1612,7 +1558,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 4> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 4> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1625,7 +1571,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 8> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 8> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1638,7 +1584,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 8> = 0>
+            template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 8> inline = 0>
             batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1651,8 +1597,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, size_t I, size_t... Is>
-            batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
+            template <class A, size_t I, size_t... Is> inline batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
                 {
@@ -1664,8 +1609,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, size_t... Is>
-            batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)
+            template <class A, class T, size_t... Is> inline batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)
             {
                 if (n == 0)
                 {
@@ -1678,8 +1622,7 @@ namespace xsimd
             }
         }
 
-        template <class A, class T>
-        batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)
+        template <class A, class T> inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)
         {
             constexpr std::size_t size = batch<T, A>::size;
             assert(0<= n && n< size && "index in bounds");
@@ -1692,14 +1635,13 @@ namespace xsimd
 
         namespace detail
         {
-            template <class A, class T>
-            batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)
+            template <class A, class T> inline batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)
             {
                 assert(false && "bitwise_lshift out of bounds");
                 return batch<T, A>{};
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1712,7 +1654,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1725,7 +1667,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1738,7 +1680,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1751,7 +1693,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1764,7 +1706,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1777,7 +1719,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1790,7 +1732,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> inline = 0>
             batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1803,8 +1745,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int... Is>
-            batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)
+            template <class A, class T, int... Is> inline batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)
             {
                 if (n == 0)
                 {
@@ -1817,57 +1758,56 @@ namespace xsimd
             }
         }
         
-        template <class A, class T>
-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)
+        template <class A, class T> inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)
         {
             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;
             assert(0<= n && n< size && "index in bounds");
             return detail::bitwise_lshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u8(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s8(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u16(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s16(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u32(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s32(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u64(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s64(lhs, rhs);
@@ -1879,14 +1819,13 @@ namespace xsimd
 
         namespace detail
         {
-            template <class A, class T>
-            batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)
+            template <class A, class T> inline batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)
             {
                 assert(false && "bitwise_rshift out of bounds");
                 return batch<T, A>{};
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1899,7 +1838,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1912,7 +1851,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1925,7 +1864,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1938,7 +1877,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1951,7 +1890,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1964,7 +1903,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1977,7 +1916,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>
+            template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> inline = 0>
             batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)
             {
                 if (n == I)
@@ -1990,8 +1929,7 @@ namespace xsimd
                 }
             }
 
-            template <class A, class T, int... Is>
-            batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)
+            template <class A, class T, int... Is> inline batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)
             {
                 if (n == 0)
                 {
@@ -2004,45 +1942,44 @@ namespace xsimd
             }
         }
         
-        template <class A, class T>
-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)
+        template <class A, class T> inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)
         {
             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;
             assert(0<= n && n< size && "index in bounds");
             return detail::bitwise_rshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());
         }
         
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u8(lhs, vnegq_s8(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s8(lhs, vnegq_s8(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u16(lhs, vnegq_s16(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s16(lhs, vnegq_s16(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_u32(lhs, vnegq_s32(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)
         {
             return vshlq_s32(lhs, vnegq_s32(rhs));
@@ -2054,7 +1991,7 @@ namespace xsimd
          * all *
          *******/
 
-        template <class A, class T, detail::enable_sized_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 1> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint8x8_t tmp = vand_u8(vget_low_u8(arg), vget_high_u8(arg));
@@ -2064,7 +2001,7 @@ namespace xsimd
             return vget_lane_u8(tmp, 0);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 2> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint16x4_t tmp = vand_u16(vget_low_u16(arg), vget_high_u16(arg));
@@ -2073,14 +2010,14 @@ namespace xsimd
             return vget_lane_u16(tmp, 0) != 0;
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 4> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint32x2_t tmp = vand_u32(vget_low_u32(arg), vget_high_u32(arg));
             return vget_lane_u32(vpmin_u32(tmp, tmp), 0) != 0;
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 8> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint64x1_t tmp = vand_u64(vget_low_u64(arg), vget_high_u64(arg));
@@ -2091,7 +2028,7 @@ namespace xsimd
          * any *
          *******/
 
-        template <class A, class T, detail::enable_sized_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 1> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint8x8_t tmp = vorr_u8(vget_low_u8(arg), vget_high_u8(arg));
@@ -2101,7 +2038,7 @@ namespace xsimd
             return vget_lane_u8(tmp, 0);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 2> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint16x4_t tmp = vorr_u16(vget_low_u16(arg), vget_high_u16(arg));
@@ -2110,14 +2047,14 @@ namespace xsimd
             return vget_lane_u16(tmp, 0);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 4> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint32x2_t tmp = vorr_u32(vget_low_u32(arg), vget_high_u32(arg));
             return vget_lane_u32(vpmax_u32(tmp, tmp), 0);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 8> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon>)
         {
             uint64x1_t tmp = vorr_u64(vget_low_u64(arg), vget_high_u64(arg));
@@ -2155,14 +2092,12 @@ namespace xsimd
 
         namespace detail
         {
-            template <class R, class... T>
-            struct bitwise_caster_impl
+            template <class R, class... T> inline struct bitwise_caster_impl
             {
                 using container_type = std::tuple<R (*)(T)...>;
                 container_type m_func;
 
-                template <class U>
-                R apply(U rhs) const
+                template <class U> inline R apply(U rhs) const
                 {
                     using func_type = R (*)(U);
                     auto func = xsimd::detail::get<func_type>(m_func);
@@ -2170,26 +2105,21 @@ namespace xsimd
                 }
             };
 
-            template <class R, class... T>
-            const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))
+            template <class R, class... T> inline const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))
             {
                 return {std::make_tuple(arg...)};
             }
 
-            template <class... T>
-            struct type_list {};
+            template <class... T> inline struct type_list {};
             
-            template <class RTL, class TTL>
-            struct bitwise_caster;
+            template <class RTL, class TTL> inline struct bitwise_caster;
 
-            template <class... R, class... T>
-            struct bitwise_caster<type_list<R...>, type_list<T...>>
+            template <class... R, class... T> inline struct bitwise_caster<type_list<R...>, type_list<T...>>
             {
                 using container_type = std::tuple<bitwise_caster_impl<R, T...>...>;
                 container_type m_caster;
 
-                template <class V, class U>
-                V apply(U rhs) const
+                template <class V, class U> inline V apply(U rhs) const
                 {
                     using caster_type = bitwise_caster_impl<V, T...>;
                     auto caster = xsimd::detail::get<caster_type>(m_caster);
@@ -2197,8 +2127,7 @@ namespace xsimd
                 }
             };
 
-            template <class... T>
-            using bitwise_caster_t = bitwise_caster<type_list<T...>, type_list<T...>>;
+            template <class... T> inline using bitwise_caster_t = bitwise_caster<type_list<T...>, type_list<T...>>;
                     
             using neon_bitwise_caster = bitwise_caster_t<uint8x16_t, int8x16_t,
                                                         uint16x8_t, int16x8_t,
@@ -2207,8 +2136,7 @@ namespace xsimd
                                                         float32x4_t>;
         }
 
-        template <class A, class T, class R>
-        batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)
+        template <class A, class T, class R> inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)
         {
             const detail::neon_bitwise_caster caster = {
                 std::make_tuple(
@@ -2249,15 +2177,13 @@ namespace xsimd
          * bool_cast *
          *************/
 
-        template <class A>
-        batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)
+        template <class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)
         {
             using register_type = typename batch_bool<int32_t, A>::register_type;
             return register_type(arg);
         }
 
-        template <class A>
-        batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)
+        template <class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)
         {
             using register_type = typename batch_bool<float, A>::register_type;
             return register_type(arg);
@@ -2267,8 +2193,7 @@ namespace xsimd
          * to_int *
          **********/
 
-        template <class A>
-        batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)
+        template <class A> inline batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)
         {
             return vcvtq_s32_f32(x);
         }
@@ -2277,8 +2202,7 @@ namespace xsimd
          * to_float *
          ************/
 
-        template <class A>
-        batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)
+        template <class A> inline batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)
         {
             return vcvtq_f32_s32(x);
         }
@@ -2289,8 +2213,7 @@ namespace xsimd
 
         namespace detail
         {
-            template <class Tin, class Tout, class A>
-            batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)
+            template <class Tin, class Tout, class A> inline batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)
             {
                 return bitwise_cast(in, out, A{});
             }
@@ -2300,8 +2223,7 @@ namespace xsimd
          * isnan *
          *********/
 
-        template <class A>
-        batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)
+        template <class A> inline batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)
         {
             return !(arg == arg);
         }
diff --git a/include/xsimd/arch/xsimd_neon64.hpp b/include/xsimd/arch/xsimd_neon64.hpp
index f63254a..31004bc 100644
--- a/include/xsimd/arch/xsimd_neon64.hpp
+++ b/include/xsimd/arch/xsimd_neon64.hpp
@@ -28,25 +28,25 @@ namespace xsimd
          * all *
          *******/
 
-        template <class A, class T, detail::enable_sized_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 1> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vminvq_u8(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 2> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vminvq_u16(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 4> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vminvq_u32(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 8> inline = 0>
         bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});
@@ -56,25 +56,25 @@ namespace xsimd
          * any *
          *******/
 
-        template <class A, class T, detail::enable_sized_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 1> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vmaxvq_u8(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 2> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vmaxvq_u16(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 4> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return vmaxvq_u32(arg);
         }
 
-        template <class A, class T, detail::enable_sized_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_t<T, 8> inline = 0>
         bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)
         {
             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});
@@ -85,14 +85,12 @@ namespace xsimd
          *************/
 
         // Required to avoid ambiguous call
-        template <class A, class T>
-        batch<T, A> broadcast(T val, requires_arch<neon64>)
+        template <class A, class T> inline batch<T, A> broadcast(T val, requires_arch<neon64>)
         {
             return broadcast<neon64>(val, neon{});
         }
 
-        template <class A>
-        batch<double, A> broadcast(double val, requires_arch<neon64>)
+        template <class A> inline batch<double, A> broadcast(double val, requires_arch<neon64>)
         {
             return vdupq_n_f64(val);
         }
@@ -101,14 +99,12 @@ namespace xsimd
          * set *
          *******/
 
-        template <class A>
-        batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)
+        template <class A> inline batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)
         {
             return float64x2_t{d0, d1};
         }
 
-        template <class A>
-        batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)
+        template <class A> inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)
         {
             using register_type = typename batch_bool<double, A>::register_type;
             using unsigned_type = as_unsigned_integer_t<double>;
@@ -120,8 +116,7 @@ namespace xsimd
          * from_bool *
          *************/
 
-        template <class A>
-        batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)
+        template <class A> inline batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u64(vandq_u64(arg, vreinterpretq_u64_f64(vdupq_n_f64(1.))));
         }
@@ -130,14 +125,12 @@ namespace xsimd
          * load *
          ********/
 
-        template <class A>
-        batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)
+        template <class A> inline batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)
         {
             return vld1q_f64(src);
         }
 
-        template <class A>
-        batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)
+        template <class A> inline batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)
         {
             return load_aligned<A>(src, convert<double>(), A{});
         }
@@ -146,14 +139,12 @@ namespace xsimd
          * store *
          *********/
 
-        template <class A>
-        void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)
+        template <class A> inline void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)
         {
             vst1q_f64(dst, src);
         }
 
-        template <class A>
-        void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)
+        template <class A> inline void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)
         {
             return store_aligned<A>(dst, src, A{});
         }
@@ -162,8 +153,7 @@ namespace xsimd
          * load_complex *
          ****************/
 
-        template <class A>
-        batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)
+        template <class A> inline batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)
         {
             using real_batch = batch<double, A>;
             const double* buf = reinterpret_cast<const double*>(mem);
@@ -173,8 +163,7 @@ namespace xsimd
             return batch<std::complex<double>, A>{real, imag};
         }
 
-        template <class A>
-        batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)
+        template <class A> inline batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)
         {
             return load_complex_aligned<A>(mem, cvt, A{});
         }
@@ -183,8 +172,7 @@ namespace xsimd
          * store_complex *
          *****************/
 
-        template <class A>
-        void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)
+        template <class A> inline void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)
         {
             float64x2x2_t tmp;
             tmp.val[0] = src.real();
@@ -193,8 +181,7 @@ namespace xsimd
             vst2q_f64(buf, tmp);
         }
 
-        template <class A>
-        void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)
+        template <class A> inline void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)
         {
             store_complex_aligned(dst, src, A{});
         }
@@ -203,20 +190,19 @@ namespace xsimd
          * neg *
          *******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_u64_s64(vnegq_s64(vreinterpretq_s64_u64(rhs)));
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vnegq_s64(rhs);
         }
 
-        template <class A>
-        batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vnegq_f64(rhs);
         }
@@ -225,8 +211,7 @@ namespace xsimd
          * add *
          *******/
 
-        template <class A>
-        batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vaddq_f64(lhs, rhs);
         }
@@ -235,8 +220,7 @@ namespace xsimd
          * sadd *
          ********/
 
-        template <class A>
-        batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return add(lhs, rhs, neon64{});
         }
@@ -245,8 +229,7 @@ namespace xsimd
          * sub *
          *******/
 
-        template <class A>
-        batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vsubq_f64(lhs, rhs);
         }
@@ -255,8 +238,7 @@ namespace xsimd
          * ssub *
          ********/
 
-        template <class A>
-        batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return sub(lhs, rhs, neon64{});
         }
@@ -265,8 +247,7 @@ namespace xsimd
          * mul *
          *******/
 
-        template <class A>
-        batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vmulq_f64(lhs, rhs);
         }
@@ -276,20 +257,19 @@ namespace xsimd
          *******/
 
 #if defined(XSIMD_FAST_INTEGER_DIVISION)
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcvtq_u64_f64(vcvtq_f64_u64(lhs) / vcvtq_f64_u64(rhs));
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcvtq_s64_f64(vcvtq_f64_s64(lhs) / vcvtq_f64_s64(rhs));
         }
 #endif
-        template <class A>
-        batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vdivq_f64(lhs, rhs);
         }
@@ -298,38 +278,36 @@ namespace xsimd
          * eq *
          ******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_f64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_u64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return vceqq_u64(lhs, rhs);
         }
@@ -338,20 +316,19 @@ namespace xsimd
          * lt *
          ******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcltq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcltq_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vcltq_f64(lhs, rhs);
         }
@@ -360,20 +337,19 @@ namespace xsimd
          * le *
          ******/
         
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcleq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcleq_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vcleq_f64(lhs, rhs);
         }
@@ -382,20 +358,19 @@ namespace xsimd
          * gt *
          ******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcgtq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcgtq_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vcgtq_f64(lhs, rhs);
         }
@@ -404,20 +379,19 @@ namespace xsimd
          * ge *
          ******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcgeq_u64(lhs, rhs);
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vcgeq_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vcgeq_f64(lhs, rhs);
         }
@@ -426,15 +400,13 @@ namespace xsimd
          * bitwise_and *
          ***************/
 
-        template <class A>
-        batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u64(vandq_u64(vreinterpretq_u64_f64(lhs),
                                                    vreinterpretq_u64_f64(rhs)));
         }
 
-        template <class A>
-        batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return vandq_u64(lhs, rhs);
         }
@@ -443,15 +415,13 @@ namespace xsimd
          * bitwise_or *
          **************/
 
-        template <class A>
-        batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(lhs),
                                                    vreinterpretq_u64_f64(rhs)));
         }
 
-        template <class A>
-        batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return vorrq_u64(lhs, rhs);
         }
@@ -460,15 +430,13 @@ namespace xsimd
          * bitwise_xor *
          ***************/
 
-        template <class A>
-        batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u64(veorq_u64(vreinterpretq_u64_f64(lhs),
                                                    vreinterpretq_u64_f64(rhs)));
         }
 
-        template <class A>
-        batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return veorq_u64(lhs, rhs);
         }
@@ -477,8 +445,7 @@ namespace xsimd
          * neq *
          *******/
 
-        template <class A>
-        batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return bitwise_xor(lhs, rhs, A{});
         }
@@ -487,14 +454,12 @@ namespace xsimd
          * bitwise_not *
          ***************/
 
-        template <class A>
-        batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u32(vmvnq_u32(vreinterpretq_u32_f64(rhs)));
         }
 
-        template <class A>
-        batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return detail::bitwise_not_u64(rhs);
         }
@@ -503,15 +468,13 @@ namespace xsimd
          * bitwise_andnot *
          ******************/
 
-        template <class A>
-        batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vreinterpretq_f64_u64(vbicq_u64(vreinterpretq_u64_f64(lhs),
                                                    vreinterpretq_u64_f64(rhs)));
         }
         
-        template <class A>
-        batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)
         {
             return vbicq_u64(lhs, rhs);
         }
@@ -520,8 +483,7 @@ namespace xsimd
          * min *
          *******/
 
-        template <class A>
-        batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vminq_f64(lhs, rhs);
         }
@@ -530,8 +492,7 @@ namespace xsimd
          * max *
          *******/
 
-        template <class A>
-        batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vmaxq_f64(lhs, rhs);
         }
@@ -540,20 +501,19 @@ namespace xsimd
          * abs *
          *******/
 
-        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return rhs;
         }
 
-        template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T,  detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vabsq_s64(rhs);
         }
 
-        template <class A>
-        batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vabsq_f64(rhs);
         }
@@ -562,8 +522,7 @@ namespace xsimd
          * sqrt *
          ********/
 
-        template <class A>
-        batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vsqrtq_f64(rhs);
         }
@@ -573,14 +532,12 @@ namespace xsimd
          ********************/
         
 #ifdef __ARM_FEATURE_FMA
-        template <class A>
-        batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)
+        template <class A> inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)
         {
             return vfmaq_f64(z, x, y);
         }
 
-        template <class A>
-        batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)
+        template <class A> inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)
         {
             return vfmaq_f64(-z, x, y);
         }
@@ -590,56 +547,55 @@ namespace xsimd
          * hadd *
          ********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 1> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_u8(arg);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 1> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_s8(arg);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 2> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_u16(arg);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 2> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_s16(arg);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 4> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_u32(arg);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 4> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_s32(arg);
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_u64(arg);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_s64(arg);
         }
 
-        template <class A>
-        double hadd(batch<double, A> const& arg, requires_arch<neon64>)
+        template <class A> inline double hadd(batch<double, A> const& arg, requires_arch<neon64>)
         {
             return vaddvq_f64(arg);
         }
@@ -648,8 +604,7 @@ namespace xsimd
          * haddp *
          *********/
 
-        template <class A>
-        batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)
+        template <class A> inline batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)
         {
             return vpaddq_f64(row[0], row[1]);
         }
@@ -658,14 +613,12 @@ namespace xsimd
          * select *
          **********/
 
-        template <class A>
-        batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)
+        template <class A> inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)
         {
             return vbslq_f64(cond, a, b);
         }
 
-        template <class A, bool... b>
-        batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,
+        template <class A, bool... b> inline batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,
                                 batch<double, A> const& true_br,
                                 batch<double, A> const& false_br,
                                 requires_arch<neon64>)
@@ -676,20 +629,19 @@ namespace xsimd
          * zip_lo *
          **********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vzip1q_u64(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vzip1q_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vzip1q_f64(lhs, rhs);
         }
@@ -698,20 +650,19 @@ namespace xsimd
          * zip_hi *
          **********/
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vzip2q_u64(lhs, rhs);
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vzip2q_s64(lhs, rhs);
         }
 
-        template <class A>
-        batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
+        template <class A> inline batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)
         {
             return vzip2q_f64(lhs, rhs);
         }
@@ -722,8 +673,7 @@ namespace xsimd
 
         namespace detail
         {
-            template <class A, size_t I, size_t... Is>
-            batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,
+            template <class A, size_t I, size_t... Is> inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,
                                           ::xsimd::detail::index_sequence<I, Is...>)
             {
                 if (n == I)
@@ -737,8 +687,7 @@ namespace xsimd
             }
         }
 
-        template <class A>
-        batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)
+        template <class A> inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)
         {
             constexpr std::size_t size = batch<double, A>::size;
             assert(0<= n && n< size && "index in bounds");
@@ -749,25 +698,25 @@ namespace xsimd
          * bitwise_rshift *
          ******************/
         
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)
         {
             return bitwise_rshift<A>(lhs, n, neon{}); 
         }
 
-        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_unsigned_t<T, 8> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>)
         {
             return vshlq_u64(lhs, vnegq_s64(rhs));
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)
         {
             return bitwise_rshift<A>(lhs, n, neon{}); 
         }
 
-        template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>
+        template <class A, class T, detail::enable_sized_signed_t<T, 8> inline = 0>
         batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)
         {
             return vshlq_s64(lhs, vnegq_s64(rhs));
@@ -795,8 +744,7 @@ namespace xsimd
 
         #undef WRAP_CAST
 
-        template <class A, class T>
-        batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)
+        template <class A, class T> inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)
         {
             using caster_type = detail::bitwise_caster_impl<float64x2_t,
                                                             uint8x16_t, int8x16_t,
@@ -815,14 +763,12 @@ namespace xsimd
 
         namespace detail
         {
-            template <class S, class... R>
-            struct bitwise_caster_neon64
+            template <class S, class... R> inline struct bitwise_caster_neon64
             {
                 using container_type = std::tuple<R (*)(S)...>;
                 container_type m_func;
 
-                template <class V>
-                V apply(float64x2_t rhs) const
+                template <class V> inline V apply(float64x2_t rhs) const
                 {
                     using func_type = V (*)(float64x2_t);
                     auto func = xsimd::detail::get<func_type>(m_func);
@@ -831,8 +777,7 @@ namespace xsimd
             };
         }
 
-        template <class A, class R>
-        batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)
+        template <class A, class R> inline batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)
         {
             using caster_type = detail::bitwise_caster_neon64<float64x2_t,
                                                               uint8x16_t, int8x16_t,
@@ -850,8 +795,7 @@ namespace xsimd
             return caster.apply<dst_register_type>(src_register_type(arg));
         }
 
-        template <class A>
-        batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)
+        template <class A> inline batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)
         {
             return arg;
         }
@@ -860,15 +804,13 @@ namespace xsimd
          * bool_cast *
          *************/
 
-        template <class A>
-        batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)
         {
             using register_type = typename batch_bool<int64_t, A>::register_type;
             return register_type(arg);
         }
 
-        template <class A>
-        batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)
+        template <class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)
         {
             using register_type = typename batch_bool<double, A>::register_type;
             return register_type(arg);
@@ -878,8 +820,7 @@ namespace xsimd
          * to_int *
          **********/
 
-        template <class A>
-        batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)
+        template <class A> inline batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)
         {
             return vcvtq_s64_f64(x);
         }
@@ -888,8 +829,7 @@ namespace xsimd
          * to_float *
          ************/
 
-        template <class A>
-        batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)
+        template <class A> inline batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)
         {
             return vcvtq_f64_s64(x);
         }
@@ -898,8 +838,7 @@ namespace xsimd
          * isnan *
          *********/
 
-        template <class A>
-        batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)
+        template <class A> inline batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)
         {
             return !(arg == arg);
         }
diff --git a/include/xsimd/arch/xsimd_scalar.hpp b/include/xsimd/arch/xsimd_scalar.hpp
index 42adfc9..6402d4e 100644
--- a/include/xsimd/arch/xsimd_scalar.hpp
+++ b/include/xsimd/arch/xsimd_scalar.hpp
@@ -75,43 +75,37 @@ namespace xsimd
     using std::isnan;
 #else
     // Windows defines catch all templates
-    template <class T>
-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type
     isfinite(T var)
     {
         return std::isfinite(var);
     }
 
-    template <class T>
-    typename std::enable_if<std::is_integral<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_integral<T>::value, bool>::type
     isfinite(T var)
     {
         return isfinite(double(var));
     }
 
-    template <class T>
-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type
     isinf(T var)
     {
         return std::isinf(var);
     }
 
-    template <class T>
-    typename std::enable_if<std::is_integral<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_integral<T>::value, bool>::type
     isinf(T var)
     {
         return isinf(double(var));
     }
 
-    template <class T>
-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type
     isnan(T var)
     {
         return std::isnan(var);
     }
 
-    template <class T>
-    typename std::enable_if<std::is_integral<T>::value, bool>::type
+    template <class T> inline typename std::enable_if<std::is_integral<T>::value, bool>::type
     isnan(T var)
     {
         return isnan(double(var));
@@ -119,14 +113,12 @@ namespace xsimd
 #endif
 
 #ifdef XSIMD_ENABLE_NUMPY_COMPLEX
-    template <class T>
-    bool isnan(std::complex<T> var)
+    template <class T> inline bool isnan(std::complex<T> var)
     {
         return std::isnan(std::real(var)) || std::isnan(std::imag(var));
     }
 
-    template <class T>
-    bool isinf(std::complex<T> var)
+    template <class T> inline bool isinf(std::complex<T> var)
     {
         return std::isinf(std::real(var)) || std::isinf(std::imag(var));
     }
@@ -183,8 +175,7 @@ namespace xsimd
 
     namespace detail
     {
-        template <class C>
-        inline C expm1_complex_scalar_impl(const C& val)
+        template <class C> inline inline C expm1_complex_scalar_impl(const C& val)
         {
             using T = typename C::value_type;
             T isin = std::sin(val.imag());
@@ -196,15 +187,13 @@ namespace xsimd
     }
 
 
-    template <class T>
-    inline std::complex<T> expm1(const std::complex<T>& val)
+    template <class T> inline inline std::complex<T> expm1(const std::complex<T>& val)
     {
         return detail::expm1_complex_scalar_impl(val);
     }
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T, bool i3ec>
-    inline xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val)
+    template <class T, bool i3ec> inline inline xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val)
     {
         return detail::expm1_complex_scalar_impl(val);
     }
@@ -212,8 +201,7 @@ namespace xsimd
 
     namespace detail
     {
-        template <class C>
-        inline C log1p_complex_scalar_impl(const C& val)
+        template <class C> inline inline C log1p_complex_scalar_impl(const C& val)
         {
             using T = typename C::value_type;
             C u = C(1.) + val;
@@ -221,14 +209,12 @@ namespace xsimd
         }
     }
 
-    template <class T>
-    inline std::complex<T> log1p(const std::complex<T>& val)
+    template <class T> inline inline std::complex<T> log1p(const std::complex<T>& val)
     {
         return detail::log1p_complex_scalar_impl(val);
     }
 
-    template <class T>
-    std::complex<T> log2(const std::complex<T>& val)
+    template <class T> inline std::complex<T> log2(const std::complex<T>& val)
     {
         return log(val) / std::log(T(2));
     }
@@ -286,18 +272,16 @@ namespace xsimd
     }
 
     namespace detail {
-      template <class T> struct value_type_or_type_helper {
+      template <class T> inline struct value_type_or_type_helper {
         using type = T;
       };
-      template<class T, class A> struct value_type_or_type_helper<batch<T, A>> {
+      template<class T, class A> inline struct value_type_or_type_helper<batch<T, A>> {
         using type = T;
       };
 
-      template<class T>
-      using value_type_or_type = typename value_type_or_type_helper<T>::type;
+      template<class T> inline using value_type_or_type = typename value_type_or_type_helper<T>::type;
 
-      template <class T0, class T1>
-      inline typename std::enable_if<std::is_integral<T1>::value, T0>::type
+      template <class T0, class T1> inline inline typename std::enable_if<std::is_integral<T1>::value, T0>::type
       ipow(const T0& x, const T1& n)
       {
         static_assert(std::is_integral<T1>::value, "second argument must be an integer");
@@ -322,51 +306,44 @@ namespace xsimd
       }
     }
 
-    template <class T0, class T1>
-    inline typename std::enable_if<std::is_integral<T1>::value, T0>::type
+    template <class T0, class T1> inline inline typename std::enable_if<std::is_integral<T1>::value, T0>::type
     pow(const T0& x, const T1& n)
     {
       return detail::ipow(x, n);
     }
 
-    template <class T0, class T1>
-    inline auto
+    template <class T0, class T1> inline inline auto
     pow(const T0& t0, const T1& t1)
         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_floating_point<T1>::value, decltype(std::pow(t0, t1))>::type
     {
         return std::pow(t0, t1);
     }
 
-    template <class T0, class T1>
-    inline typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type
+    template <class T0, class T1> inline inline typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type
     pow(const std::complex<T0>& t0, const T1& t1)
     {
         return detail::ipow(t0, t1);
     }
 
-    template <class T0, class T1>
-    inline typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type
+    template <class T0, class T1> inline inline typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type
     pow(const std::complex<T0>& t0, const T1& t1)
     {
       return std::pow(t0, t1);
     }
 
-    template <class T0, class T1>
-    inline auto
+    template <class T0, class T1> inline inline auto
     pow(const T0& t0, const std::complex<T1>& t1)
         -> typename std::enable_if<std::is_scalar<T0>::value, decltype(std::pow(t0, t1))>::type
     {
       return std::pow(t0, t1);
     }
 
-    template <class T>
-    inline auto bitofsign(T const& x) -> decltype(std::signbit(x))
+    template <class T> inline inline auto bitofsign(T const& x) -> decltype(std::signbit(x))
     {
         return std::signbit(x);
     }
 
-    template <class T>
-    inline auto signbit(T const& v) -> decltype(bitofsign(v))
+    template <class T> inline inline auto signbit(T const& v) -> decltype(bitofsign(v))
     {
         return bitofsign(v);
     }
@@ -384,8 +361,7 @@ namespace xsimd
 
     namespace detail
     {
-        template <class C>
-        inline C sign_complex_scalar_impl(const C& v)
+        template <class C> inline inline C sign_complex_scalar_impl(const C& v)
         {
             using value_type = typename C::value_type;
             if (v.real())
@@ -399,23 +375,20 @@ namespace xsimd
         }
     }
 
-    template <class T>
-    inline std::complex<T> sign(const std::complex<T>& v)
+    template <class T> inline inline std::complex<T> sign(const std::complex<T>& v)
     {
         return detail::sign_complex_scalar_impl(v);
     }
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T, bool i3ec>
-    inline xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v)
+    template <class T, bool i3ec> inline inline xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v)
     {
         return detail::sign_complex_scalar_impl(v);
     }
 #endif
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T, bool i3ec>
-    inline xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val)
+    template <class T, bool i3ec> inline inline xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val)
     {
         return log(val) / log(T(2));
     }
@@ -423,15 +396,13 @@ namespace xsimd
 
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T, bool i3ec>
-    inline xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val)
+    template <class T, bool i3ec> inline inline xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val)
     {
         return detail::log1p_complex_scalar_impl(val);
     }
 #endif
 
-    template <class T0, class T1>
-    inline auto min(T0 const &self, T1 const &other) ->
+    template <class T0, class T1> inline inline auto min(T0 const &self, T1 const &other) ->
         typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,
                                 typename std::decay<decltype(self > other ? other : self)>::type>::type
     {
@@ -439,15 +410,13 @@ namespace xsimd
     }
 
     // numpy defines minimum operator on complex using lexical comparison
-    template <class T0, class T1>
-    inline std::complex<typename std::common_type<T0, T1>::type>
+    template <class T0, class T1> inline inline std::complex<typename std::common_type<T0, T1>::type>
     min(std::complex<T0> const &self, std::complex<T1> const &other)
     {
         return (self.real() < other.real()) ? (self) : (self.real() == other.real() ? (self.imag() < other.imag() ? self : other) : other);
     }
 
-   template <class T0, class T1>
-    inline auto max(T0 const &self, T1 const &other) ->
+   template <class T0, class T1> inline inline auto max(T0 const &self, T1 const &other) ->
         typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,
                                 typename std::decay<decltype(self > other ? other : self)>::type>::type
     {
@@ -455,38 +424,33 @@ namespace xsimd
     }
 
     // numpy defines maximum operator on complex using lexical comparison
-    template <class T0, class T1>
-    inline std::complex<typename std::common_type<T0, T1>::type>
+    template <class T0, class T1> inline inline std::complex<typename std::common_type<T0, T1>::type>
     max(std::complex<T0> const &self, std::complex<T1> const &other)
     {
         return (self.real() > other.real()) ? (self) : (self.real() == other.real() ? (self.imag() > other.imag() ? self : other) : other);
     }
 
-    template <class T>
-    inline typename std::enable_if<std::is_scalar<T>::value, T>::type fma(const T& a, const T& b, const T& c)
+    template <class T> inline inline typename std::enable_if<std::is_scalar<T>::value, T>::type fma(const T& a, const T& b, const T& c)
     {
         return std::fma(a, b, c);
     }
 
     namespace detail
     {
-        template <class C>
-        inline C fma_complex_scalar_impl(const C& a, const C& b, const C& c)
+        template <class C> inline inline C fma_complex_scalar_impl(const C& a, const C& b, const C& c)
         {
             return {fms(a.real(), b.real(), fms(a.imag(), b.imag(), c.real())),
                     fma(a.real(), b.imag(), fma(a.imag(), b.real(), c.imag()))};
         }
     }
 
-    template <class T>
-    inline std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c)
+    template <class T> inline inline std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c)
     {
         return detail::fma_complex_scalar_impl(a, b, c);
     }
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T, bool i3ec>
-    inline xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c)
+    template <class T, bool i3ec> inline inline xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c)
     {
         return detail::fma_complex_scalar_impl(a, b, c);
     }
@@ -495,10 +459,10 @@ namespace xsimd
     namespace detail
     {
 #define XSIMD_HASSINCOS_TRAIT(func) \
-        template<class S> \
+        template<class S> inline \
         struct has##func \
         { \
-          template<class T> static auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type{});\
+          template<class T> inline static auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type{});\
           static std::false_type get(...); \
           static constexpr bool value = decltype(get((S*)nullptr))::value; \
         }
@@ -512,22 +476,19 @@ namespace xsimd
 
         struct generic_sincosf
         {
-            template<class T>
-            typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type
+            template<class T> inline typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type
             operator()(float val, T &s, T &c)
             {
                 sincosf(val, &s, &c);
             }
 
-            template<class T>
-            typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type
+            template<class T> inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type
             operator()(float val, T &s, T &c)
             {
                 __sincosf(val, &s, &c);
             }
 
-            template<class T>
-            typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type
+            template<class T> inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type
             operator()(float val, T &s, T &c)
             {
                 s = std::sin(val);
@@ -537,22 +498,19 @@ namespace xsimd
 
        struct generic_sincos
         {
-            template<class T>
-            typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type
+            template<class T> inline typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type
             operator()(double val, T &s, T &c)
             {
                 sincos(val, &s, &c);
             }
 
-            template<class T>
-            typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type
+            template<class T> inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type
             operator()(double val, T &s, T &c)
             {
                 __sincos(val, &s, &c);
             }
 
-            template<class T>
-            typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type
+            template<class T> inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type
             operator()(double val, T &s, T &c)
             {
                 s = std::sin(val);
@@ -574,30 +532,26 @@ namespace xsimd
         detail::generic_sincos{}(val, s, c);
     }
 
-    template <class T>
-    inline void sincos(const std::complex<T>& val, std::complex<T>& s, std::complex<T>& c)
+    template <class T> inline inline void sincos(const std::complex<T>& val, std::complex<T>& s, std::complex<T>& c)
     {
         s = std::sin(val);
         c = std::cos(val);
     }
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-    template <class T>
-    inline void sincos(const xtl::xcomplex<T>& val, xtl::xcomplex<T>& s, xtl::xcomplex<T>& c)
+    template <class T> inline inline void sincos(const xtl::xcomplex<T>& val, xtl::xcomplex<T>& s, xtl::xcomplex<T>& c)
     {
         s = sin(val);
         c = cos(val);
     }
 #endif
 
-    template <class T>
-    inline T frexp(T const& val, int& exp)
+    template <class T> inline inline T frexp(T const& val, int& exp)
     {
         return std::frexp(val, &exp);
     }
 
-    template <class T>
-    inline decltype(abs(std::declval<T>())) norm(const T& val)
+    template <class T> inline inline decltype(abs(std::declval<T>())) norm(const T& val)
     {
         auto tmp = abs(val);
         return tmp * tmp;
diff --git a/include/xsimd/arch/xsimd_sse2.hpp b/include/xsimd/arch/xsimd_sse2.hpp
index 755956d..fecaf07 100644
--- a/include/xsimd/arch/xsimd_sse2.hpp
+++ b/include/xsimd/arch/xsimd_sse2.hpp
@@ -24,11 +24,11 @@ namespace xsimd {
     using namespace types;
 
     // abs
-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {
       __m128d sign_mask = _mm_set1_pd(-0.f);  // -0.f = 1 << 31
       return _mm_andnot_pd(sign_mask, self);
     }
-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {
       __m128 sign_mask = _mm_set1_ps(-0.f);  // -0.f = 1 << 31
       return _mm_andnot_ps(sign_mask, self);
     }
@@ -45,19 +45,19 @@ namespace xsimd {
       }
     }
 
-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_add_ps(self, other);
     }
 
-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_add_pd(self, other);
     }
 
     // all
-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {
       return _mm_movemask_ps(self) == 0x0F;
     }
-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {
       return _mm_movemask_pd(self) == 0x03;
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -66,10 +66,10 @@ namespace xsimd {
     }
 
     // any
-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {
       return _mm_movemask_ps(self) != 0;
     }
-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {
       return _mm_movemask_pd(self) != 0;
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -78,10 +78,10 @@ namespace xsimd {
     }
 
     // bitwise_and
-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_and_ps(self, other);
     }
-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return _mm_and_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -93,20 +93,20 @@ namespace xsimd {
       return _mm_and_si128(self, other);
     }
 
-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_and_pd(self, other);
     }
 
-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return _mm_and_pd(self, other);
     }
 
     // bitwise_andnot
-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_andnot_ps(self, other);
     }
 
-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return _mm_andnot_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -118,11 +118,11 @@ namespace xsimd {
       return _mm_andnot_si128(self, other);
     }
 
-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_andnot_pd(self, other);
     }
     
-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return _mm_andnot_pd(self, other);
     }
 
@@ -139,10 +139,10 @@ namespace xsimd {
     }
 
     // bitwise_not
-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));
     }
-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {
       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -153,20 +153,18 @@ namespace xsimd {
     batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) {
       return _mm_xor_si128(self, _mm_set1_epi32(-1));
     }
-    template <class A>
-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {
+    template <class A> inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {
       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));
     }
-    template <class A>
-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {
+    template <class A> inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {
       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));
     }
 
     // bitwise_or
-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_or_ps(self, other);
     }
-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return _mm_or_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -178,11 +176,11 @@ namespace xsimd {
       return _mm_or_si128(self, other);
     }
 
-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_or_pd(self, other);
     }
 
-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return _mm_or_pd(self, other);
     }
 
@@ -222,20 +220,20 @@ namespace xsimd {
     }
 
     // bitwise_xor
-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_xor_ps(self, other);
     }
-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return _mm_xor_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {
       return _mm_xor_si128(self, other);
     }
-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_xor_pd(self, other);
     }
-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return _mm_xor_pd(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -260,12 +258,10 @@ namespace xsimd {
     batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<sse2>) {
       return _mm_castsi128_pd(self);
     }
-    template<class A>
-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {
       return _mm_castps_pd(self);
     }
-    template<class A>
-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {
       return _mm_castpd_ps(self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -274,21 +270,21 @@ namespace xsimd {
     }
 
     // bool_cast
-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {
         return _mm_castps_si128(self);
     }
-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {
         return _mm_castsi128_ps(self);
     }
-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {
         return _mm_castpd_si128(self);
     }
-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {
         return _mm_castsi128_pd(self);
     }
 
     // broadcast
-    template<class A> batch<float, A> broadcast(float val, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> broadcast(float val, requires_arch<sse2>) {
       return _mm_set1_ps(val);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -301,7 +297,7 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<double, A> broadcast(double val, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> broadcast(double val, requires_arch<sse2>) {
       return _mm_set1_pd(val);
     }
 
@@ -310,44 +306,44 @@ namespace xsimd {
     {
       // Override these methods in SSE-based archs, no need to override store_aligned / store_unaligned
       // complex_low
-      template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {
+      template<class A> inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {
         return _mm_unpacklo_ps(self.real(), self.imag());
       }
       // complex_high
-      template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {
+      template<class A> inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {
         return _mm_unpackhi_ps(self.real(), self.imag());
       }
-      template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {
+      template<class A> inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {
         return _mm_unpacklo_pd(self.real(), self.imag());
       }
-      template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {
+      template<class A> inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {
         return _mm_unpackhi_pd(self.real(), self.imag());
       }
     }
 
     // div
-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_div_ps(self, other);
     }
-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_div_pd(self, other);
     }
 
     // convert
     namespace detail {
-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {
       return _mm_cvtepi32_ps(self);
     }
-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {
+    template<class A> inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {
       return _mm_cvttps_epi32(self);
     }
     }
 
     // eq
-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmpeq_ps(self, other);
     }
-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return  _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_castps_si128(self), _mm_castps_si128(other)));
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -370,23 +366,23 @@ namespace xsimd {
     batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {
       return eq(batch<T, A>(self.data), batch<T, A>(other.data));
     }
-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmpeq_pd(self, other);
     }
-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return  _mm_castsi128_pd(_mm_cmpeq_epi32(_mm_castpd_si128(self), _mm_castpd_si128(other)));
     }
 
     // ge
-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmpge_ps(self, other);
     }
-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmpge_pd(self, other);
     }
     // gt
 
-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmpgt_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -404,12 +400,12 @@ namespace xsimd {
       }
     }
 
-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmpgt_pd(self, other);
     }
     
     // hadd
-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline float hadd(batch<float, A> const& self, requires_arch<sse2>) {
       __m128 tmp0 = _mm_add_ps(self, _mm_movehl_ps(self, self));
       __m128 tmp1 = _mm_add_ss(tmp0, _mm_shuffle_ps(tmp0, tmp0, 1));
       return _mm_cvtss_f32(tmp1);
@@ -455,13 +451,12 @@ namespace xsimd {
         default: return detail::hadd_default(self, A{});
       }
     }
-    template <class A>
-    double hadd(batch<double, A> const &self, requires_arch<sse2>) {
+    template <class A> inline double hadd(batch<double, A> const &self, requires_arch<sse2>) {
       return _mm_cvtsd_f64(_mm_add_sd(self, _mm_unpackhi_pd(self, self)));
     }
 
     // haddp
-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {
       __m128 tmp0 = _mm_unpacklo_ps(row[0], row[1]);
       __m128 tmp1 = _mm_unpackhi_ps(row[0], row[1]);
       __m128 tmp2 = _mm_unpackhi_ps(row[2], row[3]);
@@ -472,41 +467,40 @@ namespace xsimd {
       tmp0 = _mm_movelh_ps(tmp0, tmp1);
       return _mm_add_ps(tmp0, tmp2);
     }
-    template <class A>
-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {
+    template <class A> inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {
       return _mm_add_pd(_mm_unpacklo_pd(row[0], row[1]),
           _mm_unpackhi_pd(row[0], row[1]));
     }
 
     // isnan
-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_cmpunord_ps(self, self);
     }
-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {
       return _mm_cmpunord_pd(self, self);
     }
 
     // load_aligned
-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {
       return _mm_load_ps(mem);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) {
       return _mm_load_si128((__m128i const*)mem);
     }
-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {
       return _mm_load_pd(mem);
     }
 
     // load_unaligned
-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){
+    template<class A> inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){
       return _mm_loadu_ps(mem);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) {
       return _mm_loadu_si128((__m128i const*)mem);
     }
-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){
+    template<class A> inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){
       return _mm_loadu_pd(mem);
     }
 
@@ -514,24 +508,24 @@ namespace xsimd {
     namespace detail
     {
       // Redefine these methods in the SSE-based archs if required
-      template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {
+      template<class A> inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {
         return {_mm_shuffle_ps(hi, lo, _MM_SHUFFLE(2, 0, 2, 0)), _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(3, 1, 3, 1))};
       }
-        template<class A> batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {
+        template<class A> inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {
             return {_mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1))};
         }
     }
 
     // le
-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmple_ps(self, other);
     }
-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmple_pd(self, other);
     }
 
     // lt
-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmplt_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -573,39 +567,39 @@ namespace xsimd {
         }
       }
     }
-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmplt_pd(self, other);
     }
 
     // max
-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_max_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {
       return select(self > other, self, other);
     }
-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_max_pd(self, other);
     }
 
     // min
-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_min_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {
       return select(self <= other, self, other);
     }
-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_min_pd(self, other);
     }
 
     // mul
-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_mul_ps(self, other);
     }
-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_mul_pd(self, other);
     }
 
@@ -614,26 +608,25 @@ namespace xsimd {
     batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) {
       return 0 - self;
     }
-    template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(0x80000000)));
     }
 
 
-    template <class A>
-    batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {
+    template <class A> inline batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {
       return _mm_xor_pd(
           self, _mm_castsi128_pd(_mm_setr_epi32(0, 0x80000000, 0, 0x80000000)));
     }
 
     // neq
-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmpneq_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
     batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {
         return ~(self == other);
     }
-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {
       return _mm_cmpneq_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -642,15 +635,15 @@ namespace xsimd {
     }
 
 
-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmpneq_pd(self, other);
     }
-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {
       return _mm_cmpneq_pd(self, other);
     }
 
     // select
-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {
       return _mm_or_ps(_mm_and_ps(cond, true_br), _mm_andnot_ps(cond, false_br));
     }
 
@@ -663,19 +656,19 @@ namespace xsimd {
     batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {
       return select(batch_bool<T, A>{Values...}, true_br, false_br, sse2{});
     }
-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {
       return _mm_or_pd(_mm_and_pd(cond, true_br), _mm_andnot_pd(cond, false_br));
     }
 
     // sqrt
-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {
       return _mm_sqrt_ps(val);
     }
-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {
       return _mm_sqrt_pd(val);
     }
     // sadd
-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_add_ps(self, other); // no saturated arithmetic on floating point numbers
     }
     // TODO: move this in xsimd_generic
@@ -715,13 +708,12 @@ namespace xsimd {
         }
       }
     }
-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_add_pd(self, other); // no saturated arithmetic on floating point numbers
     }
 
     // set
-    template<class A, class... Values>
-    batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {
+    template<class A, class... Values> inline batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {
       static_assert(sizeof...(Values) == batch<float, A>::size, "consistent init");
       return _mm_setr_ps(values...);
     }
@@ -744,8 +736,7 @@ namespace xsimd {
       return _mm_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);
     }
 
-    template<class A, class... Values>
-    batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {
+    template<class A, class... Values> inline batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {
       static_assert(sizeof...(Values) == batch<double, A>::size, "consistent init");
       return _mm_setr_pd(values...);
     }
@@ -755,20 +746,18 @@ namespace xsimd {
       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;
     }
 
-    template<class A, class... Values>
-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {
+    template<class A, class... Values> inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {
       static_assert(sizeof...(Values) == batch_bool<float, A>::size, "consistent init");
       return _mm_castsi128_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);
     }
 
-    template<class A, class... Values>
-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {
+    template<class A, class... Values> inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {
       static_assert(sizeof...(Values) == batch_bool<double, A>::size, "consistent init");
       return _mm_castsi128_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);
     }
 
     // ssub
-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_sub_ps(self, other); // no saturated arithmetic on floating point numbers
     }
     // TODO: move this in xsimd_generic
@@ -804,12 +793,12 @@ namespace xsimd {
         }
       }
     }
-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_sub_pd(self, other); // no saturated arithmetic on floating point numbers
     }
 
     // store_aligned
-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_store_ps(mem, self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -820,12 +809,12 @@ namespace xsimd {
     void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {
       return _mm_store_si128((__m128i *)mem, self);
     }
-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {
       return _mm_store_pd(mem, self);
     }
 
     // store_unaligned
-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_storeu_ps(mem, self);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -836,12 +825,12 @@ namespace xsimd {
     void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {
       return _mm_storeu_si128((__m128i *)mem, self);
     }
-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {
       return _mm_storeu_pd(mem, self);
     }
 
     // sub
-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_sub_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -854,17 +843,15 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_sub_pd(self, other);
     }
 
     // to_float
-    template<class A>
-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {
       return _mm_cvtepi32_ps(self);
     }
-    template<class A>
-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {
       // FIXME: call _mm_cvtepi64_pd
       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];
       self.store_aligned(&buffer[0]);
@@ -872,13 +859,11 @@ namespace xsimd {
     }
 
     // to_int
-    template<class A>
-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {
       return _mm_cvttps_epi32(self);
     }
 
-    template<class A>
-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {
+    template<class A> inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {
       // FIXME: call _mm_cvttpd_epi64
       alignas(A::alignment()) double buffer[batch<double, A>::size];
       self.store_aligned(&buffer[0]);
@@ -886,7 +871,7 @@ namespace xsimd {
     }
 
     // zip_hi
-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_unpackhi_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -899,12 +884,12 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_unpackhi_pd(self, other);
     }
 
     // zip_lo
-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {
       return _mm_unpacklo_ps(self, other);
     }
     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -917,7 +902,7 @@ namespace xsimd {
         default: assert(false && "unsupported arch/op combination"); return {};
       }
     }
-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
+    template<class A> inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {
       return _mm_unpacklo_pd(self, other);
     }
   }
diff --git a/include/xsimd/arch/xsimd_sse3.hpp b/include/xsimd/arch/xsimd_sse3.hpp
index 5c521e9..c2f9e25 100644
--- a/include/xsimd/arch/xsimd_sse3.hpp
+++ b/include/xsimd/arch/xsimd_sse3.hpp
@@ -27,24 +27,22 @@ namespace xsimd {
     }
 
     // hadd
-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse3>) {
+    template<class A> inline float hadd(batch<float, A> const& self, requires_arch<sse3>) {
       __m128 tmp0 = _mm_hadd_ps(self, self);
       __m128 tmp1 = _mm_hadd_ps(tmp0, tmp0);
       return _mm_cvtss_f32(tmp1);
     }
-    template <class A>
-    double hadd(batch<double, A> const &self, requires_arch<sse3>) {
+    template <class A> inline double hadd(batch<double, A> const &self, requires_arch<sse3>) {
       __m128d tmp0 = _mm_hadd_pd(self, self);
       return _mm_cvtsd_f64(tmp0);
     }
 
     // haddp
-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {
+    template<class A> inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {
       return _mm_hadd_ps(_mm_hadd_ps(row[0], row[1]),
                               _mm_hadd_ps(row[2], row[3]));
     }
-    template <class A>
-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {
+    template <class A> inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {
       return _mm_hadd_pd(row[0], row[1]);
     }
 
diff --git a/include/xsimd/arch/xsimd_sse4_1.hpp b/include/xsimd/arch/xsimd_sse4_1.hpp
index dbd35ff..9ce6fe9 100644
--- a/include/xsimd/arch/xsimd_sse4_1.hpp
+++ b/include/xsimd/arch/xsimd_sse4_1.hpp
@@ -26,10 +26,10 @@ namespace xsimd {
       return !_mm_testz_si128(self, self);
     }
     // ceil
-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {
       return _mm_ceil_ps(self);
     }
-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {
       return _mm_ceil_pd(self);
     }
 
@@ -43,10 +43,10 @@ namespace xsimd {
     }
 
     // floor
-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {
       return _mm_floor_ps(self);
     }
-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {
       return _mm_floor_pd(self);
     }
 
@@ -115,17 +115,16 @@ namespace xsimd {
     }
 
     // nearbyint
-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {
       return _mm_round_ps(self, _MM_FROUND_TO_NEAREST_INT);
     }
-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {
       return _mm_round_pd(self, _MM_FROUND_TO_NEAREST_INT);
     }
 
     // select
     namespace detail {
-      template<class T>
-      constexpr T interleave(T const &cond) {
+      template<class T> inline constexpr T interleave(T const &cond) {
         return (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 49) & 0x5555) |
                (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 48) & 0xAAAA);
       }
@@ -135,10 +134,10 @@ namespace xsimd {
     batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {
       return _mm_blendv_epi8(false_br, true_br, cond);
     }
-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {
+    template<class A> inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {
       return _mm_blendv_ps(false_br, true_br, cond);
     }
-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {
+    template<class A> inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {
       return _mm_blendv_pd(false_br, true_br, cond);
     }
 
@@ -159,21 +158,21 @@ namespace xsimd {
         default: return select(batch_bool_constant<batch<T, A>, Values...>(), true_br, false_br, ssse3{});
       }
     }
-    template<class A, bool... Values> batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {
+    template<class A, bool... Values> inline batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {
       constexpr int mask = batch_bool_constant<batch<float, A>, Values...>::mask();
       return _mm_blend_ps(false_br, true_br, mask);
     }
-    template<class A, bool... Values> batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {
+    template<class A, bool... Values> inline batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {
       constexpr int mask = batch_bool_constant<batch<double, A>, Values...>::mask();
       return _mm_blend_pd(false_br, true_br, mask);
     }
 
 
     // trunc
-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {
       return _mm_round_ps(self, _MM_FROUND_TO_ZERO);
     }
-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {
+    template<class A> inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {
       return _mm_round_pd(self, _MM_FROUND_TO_ZERO);
     }
 
diff --git a/include/xsimd/arch/xsimd_sse4_2.hpp b/include/xsimd/arch/xsimd_sse4_2.hpp
index 2746302..73388b2 100644
--- a/include/xsimd/arch/xsimd_sse4_2.hpp
+++ b/include/xsimd/arch/xsimd_sse4_2.hpp
@@ -22,12 +22,10 @@ namespace xsimd {
     using namespace types;
 
     // lt
-    template<class A>
-    batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {
+    template<class A> inline batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {
       return _mm_cmpgt_epi64(other, self);
     }
-    template<class A>
-    batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {
+    template<class A> inline batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {
       auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));
       auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));
       return _mm_cmpgt_epi64(xother, xself);
diff --git a/include/xsimd/arch/xsimd_ssse3.hpp b/include/xsimd/arch/xsimd_ssse3.hpp
index 4bc2bcf..e3c2303 100644
--- a/include/xsimd/arch/xsimd_ssse3.hpp
+++ b/include/xsimd/arch/xsimd_ssse3.hpp
@@ -37,13 +37,11 @@ namespace xsimd {
     // extract_pair
     namespace detail {
 
-      template<class T, class A>
-      batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {
+      template<class T, class A> inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {
         return other;
       }
 
-      template<class T, class A, std::size_t I, std::size_t... Is>
-      batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {
+      template<class T, class A, std::size_t I, std::size_t... Is> inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {
         if(i == I) {
           return _mm_alignr_epi8(self, other, sizeof(T) * I);
         }
diff --git a/include/xsimd/types/xsimd_api.hpp b/include/xsimd/types/xsimd_api.hpp
index e2e1795..ed1f151 100644
--- a/include/xsimd/types/xsimd_api.hpp
+++ b/include/xsimd/types/xsimd_api.hpp
@@ -52,8 +52,7 @@ namespace xsimd {
  * @param x batch of integer or floating point values.
  * @return the absolute values of \c x.
  */
-template<class T, class A>
-batch<T, A> abs(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> abs(batch<T, A> const& x) {
   return kernel::abs<A>(x, A{});
 }
 
@@ -64,8 +63,7 @@ batch<T, A> abs(batch<T, A> const& x) {
  * @param z batch of complex values.
  * @return the absolute values of \c z.
  */
-template<class T, class A>
-batch<T, A> abs(batch<std::complex<T>, A> const& z) {
+template<class T, class A> inline batch<T, A> abs(batch<std::complex<T>, A> const& z) {
   return kernel::abs<A>(z, A{});
 }
 
@@ -77,8 +75,7 @@ batch<T, A> abs(batch<std::complex<T>, A> const& z) {
  * @param y batch or scalar involved in the addition.
  * @return the sum of \c x and \c y
  */
-template<class T, class Tp>
-auto add(T const& x, Tp const& y) -> decltype(x + y){
+template<class T, class Tp> inline auto add(T const& x, Tp const& y) -> decltype(x + y){
   return x + y;
 }
 
@@ -89,8 +86,7 @@ auto add(T const& x, Tp const& y) -> decltype(x + y){
  * @param x batch of floating point values.
  * @return the arc cosine of \c x.
  */
-template<class T, class A>
-batch<T, A> acos(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> acos(batch<T, A> const& x) {
   return kernel::acos<A>(x, A{});
 }
 
@@ -101,8 +97,7 @@ batch<T, A> acos(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the inverse hyperbolic cosine of \c x.
  */
-template<class T, class A>
-batch<T, A> acosh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> acosh(batch<T, A> const& x) {
   return kernel::acosh<A>(x, A{});
 }
 
@@ -113,8 +108,7 @@ batch<T, A> acosh(batch<T, A> const& x) {
  * @param z batch of complex or real values.
  * @return the argument of \c z.
  */
-template<class T, class A>
-real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {
+template<class T, class A> inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {
   return kernel::arg<A>(z, A{});
 }
 
@@ -125,8 +119,7 @@ real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {
  * @param x batch of floating point values.
  * @return the arc sine of \c x.
  */
-template<class T, class A>
-batch<T, A> asin(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> asin(batch<T, A> const& x) {
   return kernel::asin<A>(x, A{});
 }
 
@@ -137,8 +130,7 @@ batch<T, A> asin(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the inverse hyperbolic sine of \c x.
  */
-template<class T, class A>
-batch<T, A> asinh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> asinh(batch<T, A> const& x) {
   return kernel::asinh<A>(x, A{});
 }
 
@@ -149,8 +141,7 @@ batch<T, A> asinh(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the arc tangent of \c x.
  */
-template<class T, class A>
-batch<T, A> atan(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> atan(batch<T, A> const& x) {
   return kernel::atan<A>(x, A{});
 }
 
@@ -163,8 +154,7 @@ batch<T, A> atan(batch<T, A> const& x) {
  * @param y batch of floating point values.
  * @return the arc tangent of \c x/y.
  */
-template<class T, class A>
-batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::atan2<A>(x, y, A{});
 }
 
@@ -175,8 +165,7 @@ batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the inverse hyperbolic tangent of \c x.
  */
-template<class T, class A>
-batch<T, A> atanh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> atanh(batch<T, A> const& x) {
   return kernel::atanh<A>(x, A{});
 }
 
@@ -187,8 +176,7 @@ batch<T, A> atanh(batch<T, A> const& x) {
  * @param x batch of \c T_in
  * @return \c x casted to \c T_out
  */
-template<class T_out, class T_in, class A>
-batch<T_out, A> batch_cast(batch<T_in, A> const & x) {
+template<class T_out, class T_in, class A> inline batch<T_out, A> batch_cast(batch<T_in, A> const & x) {
   return kernel::batch_cast<A>(x, batch<T_out, A>{}, A{});
 }
 
@@ -199,8 +187,7 @@ batch<T_out, A> batch_cast(batch<T_in, A> const & x) {
  * @param x batch of scalar
  * @return bit of sign of \c x
  */
-template<class T, class A>
-batch<T, A> bitofsign(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> bitofsign(batch<T, A> const& x) {
   return kernel::bitofsign<A>(x, A{});
 }
 
@@ -212,8 +199,7 @@ batch<T, A> bitofsign(batch<T, A> const& x) {
  * @param y batch involved in the operation.
  * @return the result of the bitwise and.
  */
-template<class T, class Tp>
-auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){
+template<class T, class Tp> inline auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){
   return x & y;
 }
 
@@ -225,8 +211,7 @@ auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){
  * @param y batch involved in the operation.
  * @return the result of the bitwise and not.
  */
-template<class T, class A>
-batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::bitwise_andnot<A>(x, y, A{});
 }
 
@@ -239,8 +224,7 @@ batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y batch involved in the operation.
  * @return the result of the bitwise and not.
  */
-template<class T, class A>
-batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {
   return kernel::bitwise_andnot<A>(x, y, A{});
 }
 
@@ -251,8 +235,7 @@ batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> cons
  * @param x batch of \c T_in
  * @return \c x reinterpreted as \c T_out
  */
-template<class B, class T, class A>
-B bitwise_cast(batch<T, A> const& x) {
+template<class B, class T, class A> inline B bitwise_cast(batch<T, A> const& x) {
   return kernel::bitwise_cast<A>(x, B{}, A{});
 }
 
@@ -263,8 +246,7 @@ B bitwise_cast(batch<T, A> const& x) {
  * @param x batch involved in the operation.
  * @return the result of the bitwise not.
  */
-template<class T, class A>
-batch<T, A> bitwise_not(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> bitwise_not(batch<T, A> const& x) {
   return kernel::bitwise_not<A>(x, A{});
 }
 
@@ -276,8 +258,7 @@ batch<T, A> bitwise_not(batch<T, A> const& x) {
  * @param y scalar or batch of scalars
  * @return the result of the bitwise or.
  */
-template<class T, class Tp>
-auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){
+template<class T, class Tp> inline auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){
   return x | y;
 }
 
@@ -289,26 +270,21 @@ auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){
  * @param y scalar or batch of scalars
  * @return the result of the bitwise xor.
  */
-template<class T, class Tp>
-auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){
+template<class T, class Tp> inline auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){
   return x ^ y;
 }
 
 // FIXME: check if these need to be exposed, or removed (?)
-template<class A>
-batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {
+template<class A> inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {
   return kernel::bool_cast<A>(x, A{});
 }
-template<class A>
-batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {
+template<class A> inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {
   return kernel::bool_cast<A>(x, A{});
 }
-template<class A>
-batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {
+template<class A> inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {
   return kernel::bool_cast<A>(x, A{});
 }
-template<class A>
-batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {
+template<class A> inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {
   return kernel::bool_cast<A>(x, A{});
 }
 
@@ -319,8 +295,7 @@ batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {
  * @param v the value used to initialize the batch
  * @return a new batch instance
  */
-template<class T, class A=default_arch>
-batch<T, A> broadcast(T v) {
+template<class T, class A=default_arch> inline batch<T, A> broadcast(T v) {
   return kernel::broadcast<A>(v, A{});
 }
 
@@ -332,8 +307,7 @@ batch<T, A> broadcast(T v) {
  * @param v the value used to initialize the batch
  * @return a new batch instance
  */
-template <class To, class A=default_arch, class From>
-simd_return_type<From, To> broadcast_as(From v) {
+template <class To, class A=default_arch, class From> inline simd_return_type<From, To> broadcast_as(From v) {
     using batch_value_type = typename simd_return_type<From, To>::value_type;
     using value_type = typename std::conditional<std::is_same<From, bool>::value,
                                                  bool,
@@ -348,8 +322,7 @@ simd_return_type<From, To> broadcast_as(From v) {
  * @param x batch of floating point values.
  * @return the cubic root of \c x.
  */
-template<class T, class A>
-batch<T, A> cbrt(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> cbrt(batch<T, A> const& x) {
   return kernel::cbrt<A>(x, A{});
 }
 
@@ -361,8 +334,7 @@ batch<T, A> cbrt(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the batch of smallest integer values not less than \c x.
  */
-template<class T, class A>
-batch<T, A> ceil(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> ceil(batch<T, A> const& x) {
   return kernel::ceil<A>(x, A{});
 }
 
@@ -376,8 +348,7 @@ batch<T, A> ceil(batch<T, A> const& x) {
  * @param hi batch of floating point values.
  * @return the result of the clipping.
  */
-template<class A, class T>
-batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {
+template<class A, class T> inline batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {
   return kernel::clip(x, lo, hi, A{});
 }
 
@@ -388,8 +359,7 @@ batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const&
  * @param z batch of complex values.
  * @return the argument of \c z.
  */
-template<class A, class T>
-complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {
+template<class A, class T> inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {
   return kernel::conj(z, A{});
 }
 
@@ -403,8 +373,7 @@ complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {
  * @return batch whose absolute  value  matches that of \c x, but whose sign bit
  * matches that of \c y.
  */
-template<class A, class T>
-batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {
+template<class A, class T> inline batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::copysign<A>(x, y, A{});
 }
 
@@ -415,8 +384,7 @@ batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the cosine of \c x.
  */
-template<class T, class A>
-batch<T, A> cos(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> cos(batch<T, A> const& x) {
   return kernel::cos<A>(x, A{});
 }
 
@@ -427,8 +395,7 @@ batch<T, A> cos(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the hyperbolic cosine of \c x.
  */
-template<class T, class A>
-batch<T, A> cosh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> cosh(batch<T, A> const& x) {
   return kernel::cosh<A>(x, A{});
 }
 
@@ -440,8 +407,7 @@ batch<T, A> cosh(batch<T, A> const& x) {
  * @param y scalar or batch of scalars
  * @return the result of the division.
  */
-template<class T, class Tp>
-auto div(T const& x, Tp const& y) -> decltype(x / y){
+template<class T, class Tp> inline auto div(T const& x, Tp const& y) -> decltype(x / y){
   return x / y;
 }
 
@@ -453,8 +419,7 @@ auto div(T const& x, Tp const& y) -> decltype(x / y){
  * @param y batch of scalars
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {
   return x == y;
 }
 
@@ -465,8 +430,7 @@ batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the natural exponential of \c x.
  */
-template<class T, class A>
-batch<T, A> exp(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> exp(batch<T, A> const& x) {
   return kernel::exp<A>(x, A{});
 }
 
@@ -477,8 +441,7 @@ batch<T, A> exp(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the base 10 exponential of \c x.
  */
-template<class T, class A>
-batch<T, A> exp10(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> exp10(batch<T, A> const& x) {
   return kernel::exp10<A>(x, A{});
 }
 
@@ -489,8 +452,7 @@ batch<T, A> exp10(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the base 2 exponential of \c x.
  */
-template<class T, class A>
-batch<T, A> exp2(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> exp2(batch<T, A> const& x) {
   return kernel::exp2<A>(x, A{});
 }
 
@@ -501,8 +463,7 @@ batch<T, A> exp2(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the natural exponential of \c x, minus one.
  */
-template<class T, class A>
-batch<T, A> expm1(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> expm1(batch<T, A> const& x) {
   return kernel::expm1<A>(x, A{});
 }
 
@@ -513,8 +474,7 @@ batch<T, A> expm1(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the error function of \c x.
  */
-template<class T, class A>
-batch<T, A> erf(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> erf(batch<T, A> const& x) {
   return kernel::erf<A>(x, A{});
 }
 
@@ -525,8 +485,7 @@ batch<T, A> erf(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the error function of \c x.
  */
-template<class T, class A>
-batch<T, A> erfc(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> erfc(batch<T, A> const& x) {
   return kernel::erfc<A>(x, A{});
 }
 
@@ -538,8 +497,7 @@ batch<T, A> erfc(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the evaluation ofpolynomial with coefficient \c Coefs on point \c x.
  */
-template <class T, class A, uint64_t... Coefs>
-batch<T, A> estrin(const batch<T, A>& x) {
+template <class T, class A, uint64_t... Coefs> inline batch<T, A> estrin(const batch<T, A>& x) {
   return kernel::estrin<T, A, Coefs...>(x);
 }
 
@@ -553,8 +511,7 @@ batch<T, A> estrin(const batch<T, A>& x) {
  * @param i integer specifuing the lowest vector element to extract from the first source register
  * @return.
  */
-template <class T, class A>
-batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {
+template <class T, class A> inline batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {
   return kernel::extract_pair<A>(x, y, i, A{});
 }
 
@@ -565,8 +522,7 @@ batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_
  * @param x batch floating point values.
  * @return the asbolute values of \c x.
  */
-template<class T, class A>
-batch<T, A> fabs(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> fabs(batch<T, A> const& x) {
   return kernel::abs<A>(x, A{});
 }
 
@@ -579,8 +535,7 @@ batch<T, A> fabs(batch<T, A> const& x) {
  * @param y batch of floating point values.
  * @return the positive difference.
  */
-template<class T, class A>
-batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::fdim<A>(x, y, A{});
 }
 
@@ -592,8 +547,7 @@ batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the batch of largest integer values not greater than \c x.
  */
-template<class T, class A>
-batch<T, A> floor(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> floor(batch<T, A> const& x) {
   return kernel::floor<A>(x, A{});
 }
 
@@ -606,8 +560,7 @@ batch<T, A> floor(batch<T, A> const& x) {
  * @param z a batch of integer or floating point values.
  * @return the result of the fused multiply-add operation.
  */
-template<class T, class A>
-batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
+template<class T, class A> inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
   return kernel::fma<A>(x, y, z, A{});
 }
 
@@ -620,8 +573,7 @@ batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z
  * @param y a batch of integer or floating point values.
  * @return a batch of the larger values.
  */
-template<class T, class A>
-batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::max<A>(x, y, A{});
 }
 
@@ -634,8 +586,7 @@ batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y a batch of integer or floating point values.
  * @return a batch of the larger values.
  */
-template<class T, class A>
-batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::min<A>(x, y, A{});
 }
 
@@ -647,8 +598,7 @@ batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y batch involved in the modulo.
  * @return the result of the modulo.
  */
-template<class T, class A>
-batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::fmod<A>(x, y, A{});
 }
 
@@ -661,8 +611,7 @@ batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {
  * @param z a batch of integer or floating point values.
  * @return the result of the fused multiply-sub operation.
  */
-template<class T, class A>
-batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
+template<class T, class A> inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
   return kernel::fms<A>(x, y, z, A{});
 }
 
@@ -675,8 +624,7 @@ batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z
  * @param z a batch of integer or floating point values.
  * @return the result of the fused negated multiply-add operation.
  */
-template<class T, class A>
-batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
+template<class T, class A> inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
   return kernel::fnma<A>(x, y, z, A{});
 }
 
@@ -689,8 +637,7 @@ batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&
  * @param z a batch of integer or floating point values.
  * @return the result of the fused negated multiply-sub operation.
  */
-template<class T, class A>
-batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
+template<class T, class A> inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {
   return kernel::fnms<A>(x, y, z, A{});
 }
 
@@ -702,8 +649,7 @@ batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&
  * @param y a batch of integer or floating point values.
  * @return the normalized fraction of x
  */
-template <class T, class A>
-batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {
+template <class T, class A> inline batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {
   return kernel::frexp<A>(x, y, A{});
 }
 
@@ -716,8 +662,7 @@ batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {
  * @param y batch involved in the comparison.
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {
   return x >= y;
 }
 
@@ -730,8 +675,7 @@ batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y batch involved in the comparison.
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {
   return x > y;
 }
 
@@ -742,8 +686,7 @@ batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch involved in the reduction
  * @return the result of the reduction.
  */
-template<class T, class A>
-T hadd(batch<T, A> const& x) {
+template<class T, class A> inline T hadd(batch<T, A> const& x) {
   return kernel::hadd<A>(x, A{});
 }
 
@@ -756,8 +699,7 @@ T hadd(batch<T, A> const& x) {
  * @param row an array of \c N batches
  * @return the result of the reduction.
  */
-template<class T, class A>
-batch<T, A> haddp(batch<T, A> const* row) {
+template<class T, class A> inline batch<T, A> haddp(batch<T, A> const* row) {
   return kernel::haddp<A>(row, A{});
 }
 
@@ -770,8 +712,7 @@ batch<T, A> haddp(batch<T, A> const* row) {
  * @param x batch of floating point values.
  * @return the evaluation ofpolynomial with coefficient \c Coefs on point \c x.
  */
-template <class T, class A, uint64_t... Coefs>
-batch<T, A> horner(const batch<T, A>& x) {
+template <class T, class A, uint64_t... Coefs> inline batch<T, A> horner(const batch<T, A>& x) {
   return kernel::horner<T, A, Coefs...>(x);
 }
 
@@ -784,8 +725,7 @@ batch<T, A> horner(const batch<T, A>& x) {
  * @param y batch of floating point values.
  * @return the square root of the sum of the squares of \c x and \c y.
  */
-template<class T, class A>
-batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::hypot<A>(x, y, A{});
 }
 
@@ -796,8 +736,7 @@ batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {
  * @param z batch of complex or real values.
  * @return the argument of \c z.
  */
-template <class T, class A>
-real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {
+template <class T, class A> inline real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {
   return kernel::imag<A>(x, A{});
 }
 
@@ -807,8 +746,7 @@ real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {
  * Return a batch of scalars representing positive infinity
  * @return a batch of positive infinity
  */
-template<class B>
-B infinity() {
+template<class B> inline B infinity() {
   using T = typename B::value_type;
   return B(std::numeric_limits<T>::infinity());
 }
@@ -820,8 +758,7 @@ B infinity() {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-batch_bool<T, A> is_even(batch<T, A> const& x) {
+template<class T, class A> inline batch_bool<T, A> is_even(batch<T, A> const& x) {
   return kernel::is_even<A>(x, A{});
 }
 
@@ -832,8 +769,7 @@ batch_bool<T, A> is_even(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-batch_bool<T, A> is_flint(batch<T, A> const& x) {
+template<class T, class A> inline batch_bool<T, A> is_flint(batch<T, A> const& x) {
   return kernel::is_flint<A>(x, A{});
 }
 
@@ -844,8 +780,7 @@ batch_bool<T, A> is_flint(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-batch_bool<T, A> is_odd(batch<T, A> const& x) {
+template<class T, class A> inline batch_bool<T, A> is_odd(batch<T, A> const& x) {
   return kernel::is_odd<A>(x, A{});
 }
 
@@ -857,8 +792,7 @@ batch_bool<T, A> is_odd(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-batch_bool<T, A> isinf(batch<T, A> const& x) {
+template<class T, class A> inline batch_bool<T, A> isinf(batch<T, A> const& x) {
   return kernel::isinf<A>(x, A{});
 }
 
@@ -870,8 +804,7 @@ batch_bool<T, A> isinf(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-batch_bool<T, A> isfinite(batch<T, A> const& x) {
+template<class T, class A> inline batch_bool<T, A> isfinite(batch<T, A> const& x) {
   return kernel::isfinite<A>(x, A{});
 }
 
@@ -882,8 +815,7 @@ batch_bool<T, A> isfinite(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a batch of booleans.
  */
-template<class T, class A>
-typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {
+template<class T, class A> inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {
   return kernel::isnan<A>(x, A{});
 }
 
@@ -895,8 +827,7 @@ typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {
  * @param y batch of floating point values.
  * @return the natural logarithm of the gamma function of \c x.
  */
-template <class T, class A>
-batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {
+template <class T, class A> inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {
   return kernel::ldexp<A>(x, y, A{});
 }
 
@@ -908,8 +839,7 @@ batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {
  * @param y batch involved in the comparison.
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {
   return x <= y;
 }
 
@@ -920,8 +850,7 @@ batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the natural logarithm of the gamma function of \c x.
  */
-template<class T, class A>
-batch<T, A> lgamma(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> lgamma(batch<T, A> const& x) {
   return kernel::lgamma<A>(x, A{});
 }
 
@@ -933,19 +862,16 @@ batch<T, A> lgamma(batch<T, A> const& x) {
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template <class To, class A=default_arch, class From>
-simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {
+template <class To, class A=default_arch, class From> inline simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {
   using batch_value_type = typename simd_return_type<From, To>::value_type;
   return kernel::load_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});
 }
 
-template <class To, class A = default_arch>
-simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {
+template <class To, class A = default_arch> inline simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {
   return simd_return_type<bool, To>::load_aligned(ptr);
 }
 
-template <class To, class A=default_arch, class From>
-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)
+template <class To, class A=default_arch, class From> inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)
 {
   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;
   return kernel::load_complex_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});
@@ -959,19 +885,16 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template <class To, class A=default_arch, class From>
-simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {
+template <class To, class A=default_arch, class From> inline simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {
   using batch_value_type = typename simd_return_type<From, To>::value_type;
   return kernel::load_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});
 }
 
-template <class To, class A = default_arch>
-simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {
+template <class To, class A = default_arch> inline simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {
   return simd_return_type<bool, To>::load_unaligned(ptr);
 }
 
-template <class To, class A=default_arch, class From>
-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)
+template <class To, class A=default_arch, class From> inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)
 {
   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;
   return kernel::load_complex_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});
@@ -985,8 +908,7 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template<class A=default_arch, class From>
-batch<From, A> load(From const* ptr, aligned_mode= {}) {
+template<class A=default_arch, class From> inline batch<From, A> load(From const* ptr, aligned_mode= {}) {
   return load_as<From, A>(ptr, aligned_mode{});
 }
 
@@ -998,8 +920,7 @@ batch<From, A> load(From const* ptr, aligned_mode= {}) {
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template<class A=default_arch, class From>
-batch<From, A> load(From const* ptr, unaligned_mode) {
+template<class A=default_arch, class From> inline batch<From, A> load(From const* ptr, unaligned_mode) {
   return load_as<From, A>(ptr, unaligned_mode{});
 }
 
@@ -1011,8 +932,7 @@ batch<From, A> load(From const* ptr, unaligned_mode) {
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template<class A=default_arch, class From>
-batch<From, A> load_aligned(From const* ptr) {
+template<class A=default_arch, class From> inline batch<From, A> load_aligned(From const* ptr) {
   return load_as<From, A>(ptr, aligned_mode{});
 }
 
@@ -1024,8 +944,7 @@ batch<From, A> load_aligned(From const* ptr) {
  * @param ptr the memory buffer to read
  * @return a new batch instance
  */
-template <class A=default_arch, class From>
-batch<From, A> load_unaligned(From const* ptr) {
+template <class A=default_arch, class From> inline batch<From, A> load_unaligned(From const* ptr) {
   return load_as<From, A>(ptr, unaligned_mode{});
 }
 
@@ -1036,8 +955,7 @@ batch<From, A> load_unaligned(From const* ptr) {
  * @param x batch of floating point values.
  * @return the natural logarithm of \c x.
  */
-template<class T, class A>
-batch<T, A> log(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> log(batch<T, A> const& x) {
   return kernel::log<A>(x, A{});
 }
 
@@ -1047,8 +965,7 @@ batch<T, A> log(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the base 2 logarithm of \c x.
  */
-template<class T, class A>
-batch<T, A> log2(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> log2(batch<T, A> const& x) {
   return kernel::log2<A>(x, A{});
 }
 
@@ -1058,8 +975,7 @@ batch<T, A> log2(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the base 10 logarithm of \c x.
  */
-template<class T, class A>
-batch<T, A> log10(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> log10(batch<T, A> const& x) {
   return kernel::log10<A>(x, A{});
 }
 
@@ -1069,8 +985,7 @@ batch<T, A> log10(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the natural logarithm of one plus \c x.
  */
-template<class T, class A>
-batch<T, A> log1p(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> log1p(batch<T, A> const& x) {
   return kernel::log1p<A>(x, A{});
 }
 
@@ -1082,8 +997,7 @@ batch<T, A> log1p(batch<T, A> const& x) {
  * @param y batch involved in the comparison.
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {
   return x < y;
 }
 
@@ -1095,8 +1009,7 @@ batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y a batch of integer or floating point values.
  * @return a batch of the larger values.
  */
-template<class T, class A>
-batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::max<A>(x, y, A{});
 }
 
@@ -1108,8 +1021,7 @@ batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y a batch of integer or floating point values.
  * @return a batch of the smaller values.
  */
-template<class T, class A>
-batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::min<A>(x, y, A{});
 }
 
@@ -1119,8 +1031,7 @@ batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {
  * Return a batch of scalars representing positive infinity
  * @return a batch of positive infinity
  */
-template<class B>
-B minusinfinity() {
+template<class B> inline B minusinfinity() {
   using T = typename B::value_type;
   return B(-std::numeric_limits<T>::infinity());
 }
@@ -1133,8 +1044,7 @@ B minusinfinity() {
  * @param y batch involved in the modulo.
  * @return the result of the modulo.
  */
-template<class T, class Tp>
-auto mod(T const& x, Tp const& y) -> decltype(x % y){
+template<class T, class Tp> inline auto mod(T const& x, Tp const& y) -> decltype(x % y){
   return x % y;
 }
 
@@ -1147,8 +1057,7 @@ auto mod(T const& x, Tp const& y) -> decltype(x % y){
  * @param y batch involved in the product.
  * @return the result of the product.
  */
-template<class T, class Tp>
-auto mul(T const& x, Tp const& y) -> decltype(x * y){
+template<class T, class Tp> inline auto mul(T const& x, Tp const& y) -> decltype(x * y){
   return x * y;
 }
 
@@ -1160,8 +1069,7 @@ auto mul(T const& x, Tp const& y) -> decltype(x * y){
  * @param x batch of flaoting point values.
  * @return the batch of nearest integer values.
  */
-template<class T, class A>
-batch<T, A> nearbyint(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> nearbyint(batch<T, A> const& x) {
   return kernel::nearbyint<A>(x, A{});
 }
 
@@ -1173,8 +1081,7 @@ batch<T, A> nearbyint(batch<T, A> const& x) {
  * @param y batch involved in the comparison.
  * @return a boolean batch.
  */
-template<class T, class A>
-batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {
   return x != y;
 }
 
@@ -1186,8 +1093,7 @@ batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch involved in the operation.
  * @return the opposite of \c x.
  */
-template<class T, class A>
-batch<T, A> neg(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> neg(batch<T, A> const& x) {
   return -x;
 }
 
@@ -1200,8 +1106,7 @@ batch<T, A> neg(batch<T, A> const& x) {
  * @param y batch of floating point values.
  * @return \c x raised to the power \c y.
  */
-template<class T, class A>
-batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::nextafter<A>(x, y, A{});
 }
 
@@ -1212,8 +1117,7 @@ batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of complex or real values.
  * @return the norm of \c x.
  */
-template<class A, class T>
-real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {
+template<class A, class T> inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {
   return kernel::norm(x, A{});
 }
 
@@ -1224,8 +1128,7 @@ real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {
  * @param x batch involved in the operation.
  * @return \c x.
  */
-template<class T, class A>
-batch<T, A> pos(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> pos(batch<T, A> const& x) {
   return +x;
 }
 
@@ -1238,8 +1141,7 @@ batch<T, A> pos(batch<T, A> const& x) {
  * @param y batch of floating point values.
  * @return \c x raised to the power \c y.
  */
-template<class T, class A>
-batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::pow<A>(x, y, A{});
 }
 
@@ -1264,8 +1166,7 @@ batch<T, A> pow(batch<T, A> const& x, ITy y) {
  * @param x batch of complex or real values.
  * @return the projection of \c x.
  */
-template<class A, class T>
-complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {
+template<class A, class T> inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {
   return kernel::proj(x, A{});
 }
 
@@ -1276,8 +1177,7 @@ complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {
  * @param z batch of complex or real values.
  * @return the argument of \c z.
  */
-template <class T, class A>
-real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {
+template <class T, class A> inline real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {
   return kernel::real<A>(x, A{});
 }
 
@@ -1289,8 +1189,7 @@ real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {
  * @param y batch of scalar values
  * @return the result of the addition.
  */
-template<class T, class A>
-batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::remainder<A>(x, y, A{});
 }
 
@@ -1302,8 +1201,7 @@ batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {
  * @param x batch of floating point values.
  * @return the batch of rounded values.
  */
-template<class T, class A>
-batch<T, A> rint(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> rint(batch<T, A> const& x) {
   return nearbyint(x);
 }
 
@@ -1316,8 +1214,7 @@ batch<T, A> rint(batch<T, A> const& x) {
  * @param x batch of flaoting point values.
  * @return the batch of nearest integer values. 
  */
-template<class T, class A>
-batch<T, A> round(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> round(batch<T, A> const& x) {
   return kernel::round<A>(x, A{});
 }
 
@@ -1331,8 +1228,7 @@ batch<T, A> round(batch<T, A> const& x) {
  * @param y batch involved in the saturated addition.
  * @return the result of the saturated addition.
  */
-template<class T, class Tp>
-auto sadd(T const& x, Tp const& y) -> decltype(x + y) {
+template<class T, class Tp> inline auto sadd(T const& x, Tp const& y) -> decltype(x + y) {
   using B = decltype(x + y);
   using A = typename B::arch_type;
   return kernel::sadd<A>(B(x), B(y), A{});
@@ -1352,8 +1248,7 @@ auto sadd(T const& x, Tp const& y) -> decltype(x + y) {
  * @param false_br batch value for falsy condition.
  * @return the result of the selection.
  */
-template<class T, class A>
-batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {
+template<class T, class A> inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {
   return kernel::select<A>(cond, true_br, false_br, A{});
 }
 
@@ -1371,8 +1266,7 @@ batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, bat
  * @param false_br batch value for falsy condition.
  * @return the result of the selection.
  */
-template<class T, class A>
-batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {
+template<class T, class A> inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {
   return kernel::select<A>(cond, true_br, false_br, A{});
 }
 
@@ -1390,8 +1284,7 @@ batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::comple
  * @param false_br batch value for falsy condition.
  * @return the result of the selection.
  */
-template<class T, class A, bool... Values>
-batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {
+template<class T, class A, bool... Values> inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {
   return kernel::select<A>(cond, true_br, false_br, A{});
 }
 
@@ -1402,8 +1295,7 @@ batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batc
  * @param x batch
  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element
  */
-template<class T, class A>
-batch<T, A> sign(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> sign(batch<T, A> const& x) {
   return kernel::sign<A>(x, A{});
 }
 
@@ -1414,8 +1306,7 @@ batch<T, A> sign(batch<T, A> const& x) {
  * @param x batch
  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element
  */
-template<class T, class A>
-batch<T, A> signnz(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> signnz(batch<T, A> const& x) {
   return kernel::signnz<A>(x, A{});
 }
 
@@ -1426,8 +1317,7 @@ batch<T, A> signnz(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the sine of \c x.
  */
-template<class T, class A>
-batch<T, A> sin(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> sin(batch<T, A> const& x) {
   return kernel::sin<A>(x, A{});
 }
 
@@ -1438,8 +1328,7 @@ batch<T, A> sin(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the hyperbolic sine of \c x.
  */
-template<class T, class A>
-batch<T, A> sinh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> sinh(batch<T, A> const& x) {
   return kernel::sinh<A>(x, A{});
 }
 
@@ -1451,8 +1340,7 @@ batch<T, A> sinh(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return a pair containing the sine then the cosine of  batch \c x
  */
-template<class T, class A>
-std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {
+template<class T, class A> inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {
   return kernel::sincos<A>(x, A{});
 }
 
@@ -1463,8 +1351,7 @@ std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the square root of \c x.
  */
-template<class T, class A>
-batch<T, A> sqrt(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> sqrt(batch<T, A> const& x) {
   return kernel::sqrt<A>(x, A{});
 }
 
@@ -1478,8 +1365,7 @@ batch<T, A> sqrt(batch<T, A> const& x) {
  * @param y batch involved in the saturated difference.
  * @return the result of the saturated difference.
  */
-template<class T, class Tp>
-auto ssub(T const& x, Tp const& y) -> decltype(x - y) {
+template<class T, class Tp> inline auto ssub(T const& x, Tp const& y) -> decltype(x - y) {
   using B = decltype(x + y);
   using A = typename B::arch_type;
   return kernel::ssub<A>(B(x), B(y), A{});
@@ -1493,18 +1379,15 @@ auto ssub(T const& x, Tp const& y) -> decltype(x - y) {
  * @param mem the memory buffer to write to
  * @param val the batch to copy
  */
-template <class To, class A=default_arch, class From>
-void store_as(To* dst, batch<From, A> const& src, aligned_mode) {
+template <class To, class A=default_arch, class From> inline void store_as(To* dst, batch<From, A> const& src, aligned_mode) {
   kernel::store_aligned(dst, src, A{});
 }
 
-template <class A=default_arch, class From>
-void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {
+template <class A=default_arch, class From> inline void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {
   kernel::store(src, dst, A{});
 }
 
-template <class To, class A=default_arch, class From>
-void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {
+template <class To, class A=default_arch, class From> inline void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {
   kernel::store_complex_aligned(dst, src, A{});
 }
 
@@ -1516,18 +1399,15 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, ali
  * @param mem the memory buffer to write to
  * @param val the batch to copy
  */
-template <class To, class A=default_arch, class From>
-void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {
+template <class To, class A=default_arch, class From> inline void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {
   kernel::store_unaligned(dst, src, A{});
 }
 
-template <class A=default_arch, class From>
-void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {
+template <class A=default_arch, class From> inline void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {
   kernel::store(src, dst, A{});
 }
 
-template <class To, class A=default_arch, class From>
-void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {
+template <class To, class A=default_arch, class From> inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {
   kernel::store_complex_unaligned(dst, src, A{});
 }
 
@@ -1539,8 +1419,7 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, un
  * @param mem the memory buffer to write to
  * @param val the batch to copy from
  */
-template<class A, class T>
-void store(T* mem, batch<T, A> const& val, aligned_mode={}) {
+template<class A, class T> inline void store(T* mem, batch<T, A> const& val, aligned_mode={}) {
   store_as<T, A>(mem, val, aligned_mode{});
 }
 
@@ -1552,8 +1431,7 @@ void store(T* mem, batch<T, A> const& val, aligned_mode={}) {
  * @param mem the memory buffer to write to
  * @param val the batch to copy from
  */
-template<class A, class T>
-void store(T* mem, batch<T, A> const& val, unaligned_mode) {
+template<class A, class T> inline void store(T* mem, batch<T, A> const& val, unaligned_mode) {
   store_as<T, A>(mem, val, unaligned_mode{});
 }
 
@@ -1565,8 +1443,7 @@ void store(T* mem, batch<T, A> const& val, unaligned_mode) {
  * @param mem the memory buffer to write to
  * @param val the batch to copy from
  */
-template<class A, class T>
-void store_aligned(T* mem, batch<T, A> const& val) {
+template<class A, class T> inline void store_aligned(T* mem, batch<T, A> const& val) {
   store_as<T, A>(mem, val, aligned_mode{});
 }
 
@@ -1578,8 +1455,7 @@ void store_aligned(T* mem, batch<T, A> const& val) {
  * @param mem the memory buffer to write to
  * @param val the batch to copy
  */
-template<class A, class T>
-void store_unaligned(T* mem, batch<T, A> const& val) {
+template<class A, class T> inline void store_unaligned(T* mem, batch<T, A> const& val) {
   store_as<T, A>(mem, val, unaligned_mode{});
 }
 
@@ -1592,8 +1468,7 @@ void store_unaligned(T* mem, batch<T, A> const& val) {
  * @param y scalar or batch of scalars
  * @return the difference between \c x and \c y
  */
-template<class T, class Tp>
-auto sub(T const& x, Tp const& y) -> decltype(x - y){
+template<class T, class Tp> inline auto sub(T const& x, Tp const& y) -> decltype(x - y){
   return x - y;
 }
 
@@ -1604,8 +1479,7 @@ auto sub(T const& x, Tp const& y) -> decltype(x - y){
  * @param x batch of floating point values.
  * @return the tangent of \c x.
  */
-template<class T, class A>
-batch<T, A> tan(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> tan(batch<T, A> const& x) {
   return kernel::tan<A>(x, A{});
 }
 
@@ -1616,8 +1490,7 @@ batch<T, A> tan(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the hyperbolic tangent of \c x.
  */
-template<class T, class A>
-batch<T, A> tanh(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> tanh(batch<T, A> const& x) {
   return kernel::tanh<A>(x, A{});
 }
 
@@ -1628,8 +1501,7 @@ batch<T, A> tanh(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the gamma function of \c x.
  */
-template<class T, class A>
-batch<T, A> tgamma(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> tgamma(batch<T, A> const& x) {
   return kernel::tgamma<A>(x, A{});
 }
 
@@ -1640,8 +1512,7 @@ batch<T, A> tgamma(batch<T, A> const& x) {
  * @param i batch of integers.
  * @return \c i converted to a value of an floating point type of the same size as \c T
  */
-template<class T, class A>
-batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {
+template<class T, class A> inline batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {
   return kernel::to_float<A>(i, A{});
 }
 
@@ -1652,8 +1523,7 @@ batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {
  * @param x batch.
  * @return \c x converted to a value of an integer type of the same size as \c T
  */
-template<class T, class A>
-batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {
+template<class T, class A> inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {
   return kernel::to_int<A>(x, A{});
 }
 
@@ -1665,8 +1535,7 @@ batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {
  * @param x batch of floating point values.
  * @return the batch of nearest integer values not greater in magnitude than \c x.
  */
-template<class T, class A>
-batch<T, A> trunc(batch<T, A> const& x) {
+template<class T, class A> inline batch<T, A> trunc(batch<T, A> const& x) {
   return kernel::trunc<A>(x, A{});
 }
 
@@ -1679,8 +1548,7 @@ batch<T, A> trunc(batch<T, A> const& x) {
  * @param y a batch of integer or floating point or double precision values.
  * @return a batch of the high part of shuffled values.
  */
-template<class T, class A>
-batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::zip_hi<A>(x, y, A{});
 }
 
@@ -1693,8 +1561,7 @@ batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {
  * @param y a batch of integer or floating point or double precision values.
  * @return a batch of the low part of shuffled values.
  */
-template<class T, class A>
-batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {
+template<class T, class A> inline batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {
   return kernel::zip_lo<A>(x, y, A{});
 }
 
@@ -1724,8 +1591,7 @@ batch<T, A> bitwise_cast(batch_bool<T, A> const& self)
  * @param x the batch to reduce.
  * @return a boolean scalar.
  */
-template<class T, class A>
-bool all(batch_bool<T, A> const& x) {
+template<class T, class A> inline bool all(batch_bool<T, A> const& x) {
   return kernel::all<A>(x, A{});
 }
 
@@ -1737,8 +1603,7 @@ bool all(batch_bool<T, A> const& x) {
  * @param x the batch to reduce.
  * @return a boolean scalar.
  */
-template<class T, class A>
-bool any(batch_bool<T, A> const& x) {
+template<class T, class A> inline bool any(batch_bool<T, A> const& x) {
   return kernel::any<A>(x, A{});
 }
 
@@ -1750,8 +1615,7 @@ bool any(batch_bool<T, A> const& x) {
  * @param x batch to dump.
  * @return a reference to \c o
  */
-template<class T, class A>
-std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {
+template<class T, class A> inline std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {
   constexpr auto size = batch<T, A>::size;
   alignas(A::alignment()) T buffer[size];
   x.store_aligned(&buffer[0]);
diff --git a/include/xsimd/types/xsimd_batch.hpp b/include/xsimd/types/xsimd_batch.hpp
index 00febd7..ffe801f 100644
--- a/include/xsimd/types/xsimd_batch.hpp
+++ b/include/xsimd/types/xsimd_batch.hpp
@@ -31,8 +31,7 @@ namespace xsimd
      * @tparam T the type of the underlying values.
      * @tparam A the architecture this batch is tied too.
      **/
-    template<class T, class A=default_arch>
-    class batch : public types::simd_register<T, A>
+    template<class T, class A=default_arch> inline class batch : public types::simd_register<T, A>
     {
     public:
 
@@ -50,27 +49,18 @@ namespace xsimd
         explicit batch(batch_bool_type const &b);
         batch(register_type reg);
 
-        template<class U>
-        static XSIMD_NO_DISCARD batch broadcast(U val);
+        template<class U> inline static XSIMD_NO_DISCARD batch broadcast(U val);
 
         // memory operators
-        template<class U>
-        void store_aligned(U * mem) const;
-        template<class U>
-        void store_unaligned(U * mem) const;
-        template<class U>
-        void store(U * mem, aligned_mode) const;
-        template<class U>
-        void store(U * mem, unaligned_mode) const;
-
-        template<class U>
-        static XSIMD_NO_DISCARD batch load_aligned(U const* mem) ;
-        template<class U>
-        static XSIMD_NO_DISCARD batch load_unaligned(U const* mem);
-        template<class U>
-        static XSIMD_NO_DISCARD batch load(U const* mem, aligned_mode);
-        template<class U>
-        static XSIMD_NO_DISCARD batch load(U const* mem, unaligned_mode);
+        template<class U> inline void store_aligned(U * mem) const;
+        template<class U> inline void store_unaligned(U * mem) const;
+        template<class U> inline void store(U * mem, aligned_mode) const;
+        template<class U> inline void store(U * mem, unaligned_mode) const;
+
+        template<class U> inline static XSIMD_NO_DISCARD batch load_aligned(U const* mem) ;
+        template<class U> inline static XSIMD_NO_DISCARD batch load_unaligned(U const* mem);
+        template<class U> inline static XSIMD_NO_DISCARD batch load(U const* mem, aligned_mode);
+        template<class U> inline static XSIMD_NO_DISCARD batch load(U const* mem, unaligned_mode);
 
         T get(std::size_t i) const;
 
@@ -182,15 +172,13 @@ namespace xsimd
         }
     private:
   
-        template<size_t... Is>
-        batch(T const* data, detail::index_sequence<Is...>);
+        template<size_t... Is> inline batch(T const* data, detail::index_sequence<Is...>);
 
         batch logical_and(batch const& other) const;
         batch logical_or(batch const& other) const;
     };
 
-    template <class T, class A>
-    constexpr std::size_t batch<T, A>::size;
+    template <class T, class A> inline constexpr std::size_t batch<T, A>::size;
 
     /**
      * @brief batch of predicate over scalar or complex values.
@@ -201,8 +189,7 @@ namespace xsimd
      * @tparam T the type of the predicated values.
      * @tparam A the architecture this batch is tied too.
      **/
-    template<class T, class A=default_arch>
-    class batch_bool : public types::get_bool_simd_register_t<T, A>
+    template<class T, class A=default_arch> inline class batch_bool : public types::get_bool_simd_register_t<T, A>
     {
     public:
 
@@ -241,18 +228,14 @@ namespace xsimd
 
     private:
 
-        template<size_t... Is>
-        batch_bool(bool const* data, detail::index_sequence<Is...>);
+        template<size_t... Is> inline batch_bool(bool const* data, detail::index_sequence<Is...>);
 
-        template <class U, class... V, size_t I, size_t... Is>
-        static register_type make_register(detail::index_sequence<I, Is...>, U u, V... v);
+        template <class U, class... V, size_t I, size_t... Is> inline static register_type make_register(detail::index_sequence<I, Is...>, U u, V... v);
 
-        template <class... V>
-        static register_type make_register(detail::index_sequence<>, V... v);
+        template <class... V> inline static register_type make_register(detail::index_sequence<>, V... v);
     };
 
-    template <class T, class A>
-    constexpr std::size_t batch_bool<T, A>::size;
+    template <class T, class A> inline constexpr std::size_t batch_bool<T, A>::size;
 
     /**
      * @brief batch of complex values.
@@ -262,8 +245,7 @@ namespace xsimd
      * @tparam T the type of the underlying values.
      * @tparam A the architecture this batch is tied too.
      **/
-    template<class T, class A>
-    class batch<std::complex<T>, A>
+    template<class T, class A> inline class batch<std::complex<T>, A>
     {
     public:
 
@@ -294,14 +276,10 @@ namespace xsimd
         void store_aligned(value_type* dst) const;
         void store_unaligned(value_type* dst) const;
 
-        template<class U>
-        static XSIMD_NO_DISCARD batch load(U const* mem, aligned_mode);
-        template<class U>
-        static XSIMD_NO_DISCARD batch load(U const* mem, unaligned_mode);
-        template<class U>
-        void store(U * mem, aligned_mode) const;
-        template<class U>
-        void store(U * mem, unaligned_mode) const;
+        template<class U> inline static XSIMD_NO_DISCARD batch load(U const* mem, aligned_mode);
+        template<class U> inline static XSIMD_NO_DISCARD batch load(U const* mem, unaligned_mode);
+        template<class U> inline void store(U * mem, aligned_mode) const;
+        template<class U> inline void store(U * mem, unaligned_mode) const;
 
         real_batch real() const;
         real_batch imag() const;
@@ -310,19 +288,13 @@ namespace xsimd
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
         // xtl-related methods
-        template<bool i3ec>
-        batch(xtl::xcomplex<T, T, i3ec> const& val);
-        template<bool i3ec>
-        batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data);
-
-        template<bool i3ec>
-        static XSIMD_NO_DISCARD batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src);
-        template<bool i3ec>
-        static XSIMD_NO_DISCARD batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src);
-        template<bool i3ec>
-        void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const;
-        template<bool i3ec>
-        void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const;
+        template<bool i3ec> inline batch(xtl::xcomplex<T, T, i3ec> const& val);
+        template<bool i3ec> inline batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data);
+
+        template<bool i3ec> inline static XSIMD_NO_DISCARD batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src);
+        template<bool i3ec> inline static XSIMD_NO_DISCARD batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src);
+        template<bool i3ec> inline void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const;
+        template<bool i3ec> inline void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const;
 #endif
 
         // comparison operators
@@ -375,48 +347,41 @@ namespace xsimd
         real_batch m_imag;
     };
 
-    template <class T, class A>
-    constexpr std::size_t batch<std::complex<T>, A>::size;
+    template <class T, class A> inline constexpr std::size_t batch<std::complex<T>, A>::size;
 
     /*******************
      * real_batch_type *
      *******************/
 
-    template <class B>
-    struct real_batch_type
+    template <class B> inline struct real_batch_type
     {
         using type = B;
     };
 
-    template <class T, class A>
-    struct real_batch_type<batch<std::complex<T>, A>>
+    template <class T, class A> inline struct real_batch_type<batch<std::complex<T>, A>>
     {
         using type = batch<T, A>;
     };
 
-    template <class B>
-    using real_batch_type_t = typename real_batch_type<B>::type;
+    template <class B> inline using real_batch_type_t = typename real_batch_type<B>::type;
 
     /**********************
      * complex_batch_type *
      **********************/
 
-    template <class B>
-    struct complex_batch_type
+    template <class B> inline struct complex_batch_type
     {
         using real_value_type = typename B::value_type;
         using arch_type = typename B::arch_type;
         using type = batch<std::complex<real_value_type>, arch_type>;
     };
 
-    template <class T, class A>
-    struct complex_batch_type<batch<std::complex<T>, A>>
+    template <class T, class A> inline struct complex_batch_type<batch<std::complex<T>, A>>
     {
         using type = batch<std::complex<T>, A>;
     };
 
-    template <class B>
-    using complex_batch_type_t = typename complex_batch_type<B>::type;
+    template <class B> inline using complex_batch_type_t = typename complex_batch_type<B>::type;
 }
 
 #include "../types/xsimd_batch_constant.hpp"
@@ -429,41 +394,33 @@ namespace xsimd
      * batch constructors *
      **********************/
 
-    template<class T, class A>
-    batch<T, A>::batch(T val)
+    template<class T, class A> inline batch<T, A>::batch(T val)
         : types::simd_register<T, A>(kernel::broadcast<A>(val, A{}))
     {
     }
 
-    template<class T, class A>
-    batch<T, A>::batch(std::initializer_list<T> data)
+    template<class T, class A> inline batch<T, A>::batch(std::initializer_list<T> data)
         : batch(data.begin(), detail::make_index_sequence<size>())
     {
         assert(data.size() == size && "consistent initialization");
     }
 
-    template<class T, class A>
-    batch<T, A>::batch(batch_bool<T, A> const &b)
+    template<class T, class A> inline batch<T, A>::batch(batch_bool<T, A> const &b)
         : batch(kernel::from_bool(b, A{}))
     {
     }
 
-    template<class T, class A>
-    batch<T, A>::batch(register_type reg)
+    template<class T, class A> inline batch<T, A>::batch(register_type reg)
         : types::simd_register<T, A>({reg})
     {
     }
 
-    template<class T, class A>
-    template<size_t... Is>
-    batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)
+    template<class T, class A> inline template<size_t... Is> inline batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)
         : batch(kernel::set<A>(batch{}, A{}, data[Is]...))
     {
     }
 
-    template <class T, class A>
-    template<class U>
-    XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)
+    template <class T, class A> inline template<class U> inline XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)
     {
         return batch(static_cast<T>(val));
     }
@@ -477,9 +434,7 @@ namespace xsimd
     * memory needs to be aligned.
     * @param mem the memory buffer to read
     */
-    template<class T, class A>
-    template<class U>
-    void batch<T, A>::store_aligned(U* mem) const
+    template<class T, class A> inline template<class U> inline void batch<T, A>::store_aligned(U* mem) const
     {
         kernel::store_aligned<A>(mem, *this, A{});
     }
@@ -489,23 +444,17 @@ namespace xsimd
      * memory does not need to be aligned.
      * @param mem the memory buffer to write to
      */
-    template<class T, class A>
-    template<class U>
-    void batch<T, A>::store_unaligned(U* mem) const
+    template<class T, class A> inline template<class U> inline void batch<T, A>::store_unaligned(U* mem) const
     {
         kernel::store_unaligned<A>(mem, *this, A{});
     }
 
-    template<class T, class A>
-    template<class U>
-    void batch<T, A>::store(U * mem, aligned_mode) const
+    template<class T, class A> inline template<class U> inline void batch<T, A>::store(U * mem, aligned_mode) const
     {
         return store_aligned(mem);
     }
 
-    template<class T, class A>
-    template<class U>
-    void batch<T, A>::store(U * mem, unaligned_mode) const
+    template<class T, class A> inline template<class U> inline void batch<T, A>::store(U * mem, unaligned_mode) const
     {
         return store_unaligned(mem);
     }
@@ -517,9 +466,7 @@ namespace xsimd
      * @param mem the memory buffer to read from.
      * @return a new batch instance.
      */
-    template<class T, class A>
-    template<class U>
-    batch<T, A> batch<T, A>::load_aligned(U const* mem)
+    template<class T, class A> inline template<class U> inline batch<T, A> batch<T, A>::load_aligned(U const* mem)
     {
         return kernel::load_aligned<A>(mem, kernel::convert<T>{}, A{});
     }
@@ -531,29 +478,22 @@ namespace xsimd
      * @param mem the memory buffer to read from.
      * @return a new batch instance.
      */
-    template<class T, class A>
-    template<class U>
-    batch<T, A> batch<T, A>::load_unaligned(U const* mem)
+    template<class T, class A> inline template<class U> inline batch<T, A> batch<T, A>::load_unaligned(U const* mem)
     {
         return kernel::load_unaligned<A>(mem, kernel::convert<T>{}, A{});
     }
 
-    template<class T, class A>
-    template<class U>
-    batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)
+    template<class T, class A> inline template<class U> inline batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)
     {
         return load_aligned(mem);
     }
 
-    template<class T, class A>
-    template<class U>
-    batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)
+    template<class T, class A> inline template<class U> inline batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)
     {
         return load_unaligned(mem);
     }
 
-    template <class T, class A>
-    T batch<T, A>::get(std::size_t i) const
+    template <class T, class A> inline T batch<T, A>::get(std::size_t i) const
     {
         alignas(A::alignment()) T buffer[size];
         store_aligned(&buffer[0]);
@@ -564,38 +504,32 @@ namespace xsimd
      * batch comparison operators *
      ******************************/
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const
     {
         return kernel::eq<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const
     {
         return kernel::neq<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const
     {
         return kernel::ge<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const
     { 
         return kernel::le<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const
     { 
         return kernel::gt<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const
     { 
         return kernel::lt<A>(*this, other, A{});
     }
@@ -604,74 +538,62 @@ namespace xsimd
      * batch update operators *
      **************************/
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)
     {
         return *this = kernel::add<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)
     {
         return *this = kernel::sub<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)
     {
         return *this = kernel::mul<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)
     {
         return *this = kernel::div<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)
     {
         return *this = kernel::mod<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)
     {
         return *this = kernel::bitwise_and<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)
     {
         return *this = kernel::bitwise_or<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)
     {
         return *this = kernel::bitwise_xor<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)
     {
         return *this = kernel::bitwise_rshift<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)
     { 
         return *this = kernel::bitwise_lshift<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator>>=(int32_t other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator>>=(int32_t other)
     { 
         return *this = kernel::bitwise_rshift<A>(*this, other, A{});
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator<<=(int32_t other)
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator<<=(int32_t other)
     { 
         return *this = kernel::bitwise_lshift<A>(*this, other, A{});
     }
@@ -680,28 +602,24 @@ namespace xsimd
      * batch incr/decr operators *
      *****************************/
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator++()
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator++()
     { 
         return operator+=(1);
     }
 
-    template<class T, class A>
-    batch<T, A>& batch<T, A>::operator--()
+    template<class T, class A> inline batch<T, A>& batch<T, A>::operator--()
     { 
         return operator-=(1);
     }
     
-    template<class T, class A>
-    batch<T, A> batch<T, A>::operator++(int)
+    template<class T, class A> inline batch<T, A> batch<T, A>::operator++(int)
     {
         batch<T, A> copy(*this);
         operator+=(1);
         return copy;
     }
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::operator--(int)
+    template<class T, class A> inline batch<T, A> batch<T, A>::operator--(int)
     { 
         batch copy(*this);
         operator-=(1);
@@ -712,26 +630,22 @@ namespace xsimd
      * batch unary operators *
      *************************/
 
-    template<class T, class A>
-    batch_bool<T, A> batch<T, A>::operator!() const
+    template<class T, class A> inline batch_bool<T, A> batch<T, A>::operator!() const
     {
         return kernel::eq<A>(*this, batch(0), A{});
     }
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::operator~() const
+    template<class T, class A> inline batch<T, A> batch<T, A>::operator~() const
     {
         return kernel::bitwise_not<A>(*this, A{});
     }
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::operator-() const
+    template<class T, class A> inline batch<T, A> batch<T, A>::operator-() const
     { 
         return kernel::neg<A>(*this, A{});
     }
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::operator+() const
+    template<class T, class A> inline batch<T, A> batch<T, A>::operator+() const
     {
         return *this;
     }
@@ -740,14 +654,12 @@ namespace xsimd
      * batch private method *
      ************************/
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const
+    template<class T, class A> inline batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const
     {
         return kernel::logical_and<A>(*this, other, A());
     }
 
-    template<class T, class A>
-    batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const
+    template<class T, class A> inline batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const
     {
         return kernel::logical_or<A>(*this, other, A());
     }
@@ -756,21 +668,17 @@ namespace xsimd
      * batch_bool constructors *
      ***************************/
 
-    template<class T, class A>
-    template<size_t... Is>
-    batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)
+    template<class T, class A> inline template<size_t... Is> inline batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)
         : batch_bool(kernel::set<A>(batch_bool{}, A{}, data[Is]...))
     {
     }
 
-    template<class T, class A>
-    batch_bool<T, A>::batch_bool(register_type reg)
+    template<class T, class A> inline batch_bool<T, A>::batch_bool(register_type reg)
         : types::get_bool_simd_register_t<T, A>({reg})
     {
     }
 
-    template<class T, class A>
-    batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)
+    template<class T, class A> inline batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)
         : batch_bool(data.begin(), detail::make_index_sequence<size>())
     {
         assert(data.size() == size && "consistent initialization");
@@ -780,20 +688,17 @@ namespace xsimd
      * batch_bool memory operators *
      *******************************/
 
-    template<class T, class A>
-    void batch_bool<T, A>::store_aligned(bool* mem) const
+    template<class T, class A> inline void batch_bool<T, A>::store_aligned(bool* mem) const
     {
         kernel::store(*this, mem, A{});
     }
 
-    template<class T, class A>
-    void batch_bool<T, A>::store_unaligned(bool* mem) const
+    template<class T, class A> inline void batch_bool<T, A>::store_unaligned(bool* mem) const
     {
         store_aligned(mem);
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)
     {
         batch_type ref(0);
         alignas(A::alignment()) T buffer[size];
@@ -802,14 +707,12 @@ namespace xsimd
         return ref != batch_type::load_aligned(&buffer[0]);
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)
     {
         return load_aligned(mem);
     }
 
-    template<class T, class A>
-    bool batch_bool<T, A>::get(std::size_t i) const
+    template<class T, class A> inline bool batch_bool<T, A>::get(std::size_t i) const
     {
         alignas(A::alignment()) bool buffer[size];
         store_aligned(&buffer[0]);
@@ -820,14 +723,12 @@ namespace xsimd
      * batch_bool comparison operators *
      ***********************************/
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const
     {
         return kernel::eq<A>(*this, other, A{}).data;
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const
     {
         return kernel::neq<A>(*this, other, A{}).data;
     }
@@ -836,38 +737,32 @@ namespace xsimd
      * batch_bool logical operators *
      ********************************/
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator~() const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator~() const
     {
         return kernel::bitwise_not<A>(*this, A{}).data;
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator!() const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator!() const
     { 
         return operator==(batch_bool(false));
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const
     {
         return kernel::bitwise_and<A>(*this, other, A{}).data;
     }
 
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const
     {
         return kernel::bitwise_or<A>(*this, other, A{}).data;
     }
         
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const
     {
         return operator&(other);
     }
     
-    template<class T, class A>
-    batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const
+    template<class T, class A> inline batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const
     {
         return operator|(other);
     }
@@ -876,22 +771,17 @@ namespace xsimd
      * batch_bool private methods *
      ******************************/
 
-    template<class T, class A>
-    batch_bool<T, A>::batch_bool(bool val)
+    template<class T, class A> inline batch_bool<T, A>::batch_bool(bool val)
         : base_type{make_register(detail::make_index_sequence<size-1>(), val)}
     {
     }
 
-    template <class T, class A>
-    template <class U, class... V, size_t I, size_t... Is>
-    auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type
+    template <class T, class A> inline template <class U, class... V, size_t I, size_t... Is> inline auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type
     {
         return make_register(detail::index_sequence<Is...>(), u, u, v...);
     }
 
-    template <class T, class A>
-    template <class... V>
-    auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type
+    template <class T, class A> inline template <class... V> inline auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type
     {
         return kernel::set<A>(batch_bool<T, A>(), A{}, v...).data;
     }
@@ -900,39 +790,33 @@ namespace xsimd
      * batch<complex> constructors *
      *******************************/
 
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(value_type const& val)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(value_type const& val)
         : m_real(val.real()), m_imag(val.imag())
     {
     }
     
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)
         : m_real(real), m_imag(imag)
     {
     }
         
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(real_batch const& real)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(real_batch const& real)
         : m_real(real), m_imag(0)
     {
     }
     
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(T val)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(T val)
         : m_real(val), m_imag(0)
     {
     }
     
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)
     { 
         assert(data.size() == size && "consistent initialization");
         *this = load_unaligned(data.begin());
     }
 
-    template <class T, class A>
-    batch<std::complex<T>, A>::batch(batch_bool_type const& b)
+    template <class T, class A> inline batch<std::complex<T>, A>::batch(batch_bool_type const& b)
         : m_real(b), m_imag(0)
     {
     }
@@ -941,97 +825,78 @@ namespace xsimd
      * batch<complex> memory operators *
      ***********************************/
 
-    template <class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)
+    template <class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)
     {
         return {batch<T, A>::load_aligned(real_src), imag_src ? batch<T, A>::load_aligned(imag_src) : batch<T, A>(0)};
     }
-    template <class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)
+    template <class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)
     {
         return {batch<T, A>::load_unaligned(real_src), imag_src?batch<T, A>::load_unaligned(imag_src):batch<T, A>(0)};
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)
+    template<class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)
     {
         return kernel::load_complex_aligned<A>(src, kernel::convert<value_type>{}, A{});
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)
+    template<class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)
     {
         return kernel::load_complex_unaligned<A>(src, kernel::convert<value_type>{}, A{});
     }
 
-    template<class T, class A>
-    void batch<std::complex<T>, A>::store_aligned(value_type* dst) const
+    template<class T, class A> inline void batch<std::complex<T>, A>::store_aligned(value_type* dst) const
     {
         return kernel::store_complex_aligned(dst, *this, A{});
     }
 
-    template<class T, class A>
-    void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const
+    template<class T, class A> inline void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const
     {
         return kernel::store_complex_unaligned(dst, *this, A{});
     }
 
-    template<class T, class A>
-    void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const
+    template<class T, class A> inline void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const
     {
         m_real.store_aligned(real_dst);
         m_imag.store_aligned(imag_dst);
     }
 
-    template<class T, class A>
-    void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const
+    template<class T, class A> inline void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const
     {
         m_real.store_unaligned(real_dst);
         m_imag.store_unaligned(imag_dst);
     }
 
-    template<class T, class A>
-    template<class U>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)
+    template<class T, class A> inline template<class U> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)
     {
         return load_aligned(mem);
     }
 
-    template<class T, class A>
-    template<class U>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)
+    template<class T, class A> inline template<class U> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)
     { 
         return load_unaligned(mem);
     }
 
-    template<class T, class A>
-    template<class U>
-    void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const
+    template<class T, class A> inline template<class U> inline void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const
     {
         return store_aligned(mem);
     }
     
-    template<class T, class A>
-    template<class U>
-    void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const
+    template<class T, class A> inline template<class U> inline void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const
     { 
         return store_unaligned(mem);
     }
 
-    template<class T, class A>
-    auto batch<std::complex<T>, A>::real() const -> real_batch
+    template<class T, class A> inline auto batch<std::complex<T>, A>::real() const -> real_batch
     {
         return m_real; 
     }
     
-    template<class T, class A>
-    auto batch<std::complex<T>, A>::imag() const -> real_batch 
+    template<class T, class A> inline auto batch<std::complex<T>, A>::imag() const -> real_batch 
     {
         return m_imag;
     }
 
-    template<class T, class A>
-    auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type
+    template<class T, class A> inline auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type
     {
         alignas(A::alignment()) value_type buffer[size];
         store_aligned(&buffer[0]);
@@ -1044,16 +909,12 @@ namespace xsimd
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
 
-    template<class T, class A>
-    template<bool i3ec>
-    batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)
+    template<class T, class A> inline template<bool i3ec> inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)
         : m_real(val.real()), m_imag(val.imag())
     {
     }
 
-    template<class T, class A>
-    template<bool i3ec>
-    batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)
+    template<class T, class A> inline template<bool i3ec> inline batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)
     {
         assert(data.size() == size && "consistent initialization");
         *this = load_unaligned(data.begin());
@@ -1063,30 +924,22 @@ namespace xsimd
     // stores values and not reference. Unfortunately, this breaks strict
     // aliasing...
 
-    template<class T, class A>
-    template<bool i3ec>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)
+    template<class T, class A> inline template<bool i3ec> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)
     {
         return load_aligned(reinterpret_cast<std::complex<T> const*>(src));
     }
 
-    template<class T, class A>
-    template<bool i3ec>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)
+    template<class T, class A> inline template<bool i3ec> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)
     {
         return load_unaligned(reinterpret_cast<std::complex<T> const*>(src));
     }
 
-    template<class T, class A>
-    template<bool i3ec>
-    void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const
+    template<class T, class A> inline template<bool i3ec> inline void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const
     {
         store_aligned(reinterpret_cast<std::complex<T> *>(dst));
     }
 
-    template<class T, class A>
-    template<bool i3ec>
-    void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const
+    template<class T, class A> inline template<bool i3ec> inline void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const
     {
         store_unaligned(reinterpret_cast<std::complex<T>*>(dst));
     }
@@ -1097,14 +950,12 @@ namespace xsimd
      * batch<complex> comparison operators *
      ***************************************/
 
-    template <class T, class A>
-    batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const
+    template <class T, class A> inline batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const
     {
         return m_real == other.m_real && m_imag == other.m_imag;
     }
     
-    template <class T, class A>
-    batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const
+    template <class T, class A> inline batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const
     { 
         return m_real != other.m_real || m_imag != other.m_imag;
     }
@@ -1113,24 +964,21 @@ namespace xsimd
      * batch<complex> update operators *
      ***********************************/
 
-    template <class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)
+    template <class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)
     {
         m_real += other.m_real;
         m_imag += other.m_imag;
         return *this;
     }
     
-    template <class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)
+    template <class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)
     {
         m_real -= other.m_real;
         m_imag -= other.m_imag;
         return *this;
     }
 
-    template <class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)
+    template <class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)
     {
         real_batch new_real = real() * other.real() - imag() * other.imag();
         real_batch new_imag = real() * other.imag() + imag() * other.real();
@@ -1139,8 +987,7 @@ namespace xsimd
         return *this;
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)
+    template<class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)
     {
         real_batch a = real();
         real_batch b = imag();
@@ -1156,28 +1003,24 @@ namespace xsimd
      * batch<complex> incr/decr operators *
      **************************************/
 
-    template<class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()
+    template<class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()
     { 
         return operator+=(1);
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()
+    template<class T, class A> inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()
     { 
         return operator-=(1);
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)
+    template<class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)
     {
         batch copy(*this);
         operator+=(1);
         return copy;
     }
 
-    template<class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)
+    template<class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)
     {
         batch copy(*this);
         operator-=(1);
@@ -1188,26 +1031,22 @@ namespace xsimd
      * batch<complex> unary operators *
      **********************************/
 
-    template <class T, class A>
-    batch_bool<T, A> batch<std::complex<T>, A>::operator!() const
+    template <class T, class A> inline batch_bool<T, A> batch<std::complex<T>, A>::operator!() const
     { 
         return operator==(batch(0));
     }
     
-    template <class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const
+    template <class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const
     { 
         return {~m_real, ~m_imag};
     }
 
-    template <class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const
+    template <class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const
     { 
         return {-m_real, -m_imag};
     }
 
-    template <class T, class A>
-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const
+    template <class T, class A> inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const
     { 
         return {+m_real, +m_imag};
     }
diff --git a/include/xsimd/types/xsimd_batch_constant.hpp b/include/xsimd/types/xsimd_batch_constant.hpp
index c37784f..99ed9c8 100644
--- a/include/xsimd/types/xsimd_batch_constant.hpp
+++ b/include/xsimd/types/xsimd_batch_constant.hpp
@@ -17,8 +17,7 @@
 
 namespace xsimd
 {
-    template <class batch_type, bool... Values>
-    struct batch_bool_constant
+    template <class batch_type, bool... Values> inline struct batch_bool_constant
     {
         static constexpr std::size_t size = sizeof...(Values);
         using arch_type = typename batch_type::arch_type;
@@ -39,15 +38,13 @@ namespace xsimd
 
       private:
         static constexpr int mask_helper(int acc) { return acc; }
-        template <class... Tys>
-        static constexpr int mask_helper(int acc, int mask, Tys... masks)
+        template <class... Tys> inline static constexpr int mask_helper(int acc, int mask, Tys... masks)
         {
             return mask_helper(acc | mask, (masks << 1)...);
         }
     };
 
-    template <class batch_type, typename batch_type::value_type... Values>
-    struct batch_constant
+    template <class batch_type, typename batch_type::value_type... Values> inline struct batch_constant
     {
         static constexpr std::size_t size = sizeof...(Values);
         using arch_type = typename batch_type::arch_type;
@@ -64,14 +61,12 @@ namespace xsimd
 
     namespace detail
     {
-        template <class batch_type, class G, std::size_t... Is>
-        constexpr auto make_batch_constant(detail::index_sequence<Is...>)
+        template <class batch_type, class G, std::size_t... Is> inline constexpr auto make_batch_constant(detail::index_sequence<Is...>)
             -> batch_constant<batch_type, G::get(Is, sizeof...(Is))...>
         {
             return {};
         }
-        template <class batch_type, class G, std::size_t... Is>
-        constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)
+        template <class batch_type, class G, std::size_t... Is> inline constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)
             -> batch_bool_constant<batch_type, G::get(Is, sizeof...(Is))...>
         {
             return {};
@@ -79,15 +74,13 @@ namespace xsimd
 
     } // namespace detail
 
-    template <class batch_type, class G>
-    constexpr auto make_batch_constant() -> decltype(
+    template <class batch_type, class G> inline constexpr auto make_batch_constant() -> decltype(
         detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>()))
     {
         return detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>());
     }
 
-    template <class batch_type, class G>
-    constexpr auto make_batch_bool_constant()
+    template <class batch_type, class G> inline constexpr auto make_batch_bool_constant()
         -> decltype(detail::make_batch_bool_constant<batch_type, G>(
             detail::make_index_sequence<batch_type::size>()))
     {
diff --git a/include/xsimd/types/xsimd_utils.hpp b/include/xsimd/types/xsimd_utils.hpp
index 87f6afc..1368fab 100644
--- a/include/xsimd/types/xsimd_utils.hpp
+++ b/include/xsimd/types/xsimd_utils.hpp
@@ -25,83 +25,69 @@
 namespace xsimd
 {
 
-    template <class T, class A>
-    class batch;
+    template <class T, class A> inline class batch;
 
-    template <class T, class A>
-    class batch_bool;
+    template <class T, class A> inline class batch_bool;
 
     /**************
      * as_integer *
      **************/
 
-    template <class T>
-    struct as_integer : std::make_signed<T>
+    template <class T> inline struct as_integer : std::make_signed<T>
     {
     };
 
-    template <>
-    struct as_integer<float>
+    template <> inline struct as_integer<float>
     {
         using type = int32_t;
     };
 
-    template <>
-    struct as_integer<double>
+    template <> inline struct as_integer<double>
     {
         using type = int64_t;
     };
 
-    template <class T, class A>
-    struct as_integer<batch<T, A>>
+    template <class T, class A> inline struct as_integer<batch<T, A>>
     {
         using type = batch<typename as_integer<T>::type, A>;
     };
 
-    template <class B>
-    using as_integer_t = typename as_integer<B>::type;
+    template <class B> inline using as_integer_t = typename as_integer<B>::type;
 
     /***********************
      * as_unsigned_integer *
      ***********************/
 
-    template <class T>
-    struct as_unsigned_integer : std::make_unsigned<T>
+    template <class T> inline struct as_unsigned_integer : std::make_unsigned<T>
     {
     };
 
-    template <>
-    struct as_unsigned_integer<float>
+    template <> inline struct as_unsigned_integer<float>
     {
         using type = uint32_t;
     };
 
-    template <>
-    struct as_unsigned_integer<double>
+    template <> inline struct as_unsigned_integer<double>
     {
         using type = uint64_t;
     };
 
-    template <class T, class A>
-    struct as_unsigned_integer<batch<T, A>>
+    template <class T, class A> inline struct as_unsigned_integer<batch<T, A>>
     {
         using type = batch<typename as_unsigned_integer<T>::type, A>;
     };
 
-    template <class T>
-    using as_unsigned_integer_t = typename as_unsigned_integer<T>::type;
+    template <class T> inline using as_unsigned_integer_t = typename as_unsigned_integer<T>::type;
 
     /*********************
      * as_signed_integer *
      *********************/
 
-    template <class T>
-    struct as_signed_integer : std::make_signed<T>
+    template <class T> inline struct as_signed_integer : std::make_signed<T>
     {
     };
 
-    template <class T>
-    using as_signed_integer_t = typename as_signed_integer<T>::type;
+    template <class T> inline using as_signed_integer_t = typename as_signed_integer<T>::type;
 
     /******************
      * flip_sign_type *
@@ -109,76 +95,63 @@ namespace xsimd
 
     namespace detail
     {
-        template <class T, bool is_signed>
-        struct flipped_sign_type_impl : std::make_signed<T>
+        template <class T, bool is_signed> inline struct flipped_sign_type_impl : std::make_signed<T>
         {
         };
 
-        template <class T>
-        struct flipped_sign_type_impl<T, true> : std::make_unsigned<T>
+        template <class T> inline struct flipped_sign_type_impl<T, true> : std::make_unsigned<T>
         {
         };
     }
 
-    template <class T>
-    struct flipped_sign_type
+    template <class T> inline struct flipped_sign_type
         : detail::flipped_sign_type_impl<T, std::is_signed<T>::value>
     {
     };
 
-    template <class T>
-    using flipped_sign_type_t = typename flipped_sign_type<T>::type;
+    template <class T> inline using flipped_sign_type_t = typename flipped_sign_type<T>::type;
 
     /***********
      * as_float *
      ************/
 
-    template <class T>
-    struct as_float;
+    template <class T> inline struct as_float;
 
-    template <>
-    struct as_float<int32_t>
+    template <> inline struct as_float<int32_t>
     {
         using type = float;
     };
 
-    template <>
-    struct as_float<int64_t>
+    template <> inline struct as_float<int64_t>
     {
         using type = double;
     };
 
-    template <class T, class A>
-    struct as_float<batch<T, A>>
+    template <class T, class A> inline struct as_float<batch<T, A>>
     {
         using type = batch<typename as_float<T>::type, A>;
     };
 
-    template <class T>
-    using as_float_t = typename as_float<T>::type;
+    template <class T> inline using as_float_t = typename as_float<T>::type;
 
     /**************
      * as_logical *
      **************/
 
-    template <class T>
-    struct as_logical;
+    template <class T> inline struct as_logical;
 
-    template <class T, class A>
-    struct as_logical<batch<T, A>>
+    template <class T, class A> inline struct as_logical<batch<T, A>>
     {
         using type = batch_bool<T, A>;
     };
 
-    template <class T>
-    using as_logical_t = typename as_logical<T>::type;
+    template <class T> inline using as_logical_t = typename as_logical<T>::type;
 
     /********************
      * bit_cast *
      ********************/
 
-    template<class To, class From>
-    To bit_cast(From val) {
+    template<class To, class From> inline To bit_cast(From val) {
       static_assert(sizeof(From) == sizeof(To), "casting between compatible layout");
       // FIXME: Some old version of GCC don't support that trait
       //static_assert(std::is_trivially_copyable<From>::value, "input type is trivially copyable");
@@ -196,8 +169,7 @@ namespace xsimd
     // TODO: Remove this once we drop C++11 support
     namespace detail
     {
-        template <typename T>
-        struct identity { using type = T; };
+        template <typename T> inline struct identity { using type = T; };
 
         #ifdef __cpp_lib_integer_sequence
             using std::integer_sequence;
@@ -207,58 +179,44 @@ namespace xsimd
 
             using std::index_sequence_for;
         #else
-            template <typename T, T... Is>
-            struct integer_sequence {
+            template <typename T, T... Is> inline struct integer_sequence {
               using value_type = T;
               static constexpr std::size_t size() noexcept { return sizeof...(Is); }
             };
 
-            template <typename Lhs, typename Rhs>
-            struct make_integer_sequence_concat;
+            template <typename Lhs, typename Rhs> inline struct make_integer_sequence_concat;
 
-            template <typename T, T... Lhs, T... Rhs>
-            struct make_integer_sequence_concat<integer_sequence<T, Lhs...>,
+            template <typename T, T... Lhs, T... Rhs> inline struct make_integer_sequence_concat<integer_sequence<T, Lhs...>,
                                                 integer_sequence<T, Rhs...>>
               : identity<integer_sequence<T, Lhs..., (sizeof...(Lhs) + Rhs)...>> {};
 
-            template <typename T>
-            struct make_integer_sequence_impl;
+            template <typename T> inline struct make_integer_sequence_impl;
 
-            template <typename T>
-            struct make_integer_sequence_impl<std::integral_constant<T, (T)0>> : identity<integer_sequence<T>> {};
+            template <typename T> inline struct make_integer_sequence_impl<std::integral_constant<T, (T)0>> : identity<integer_sequence<T>> {};
 
-            template <typename T>
-            struct make_integer_sequence_impl<std::integral_constant<T, (T)1>> : identity<integer_sequence<T, 0>> {};
+            template <typename T> inline struct make_integer_sequence_impl<std::integral_constant<T, (T)1>> : identity<integer_sequence<T, 0>> {};
 
-            template <typename T, T N>
-            struct make_integer_sequence_impl<std::integral_constant<T, N>>
+            template <typename T, T N> inline struct make_integer_sequence_impl<std::integral_constant<T, N>>
               : make_integer_sequence_concat<typename make_integer_sequence_impl<std::integral_constant<T, N / 2>>::type,
                                              typename make_integer_sequence_impl<std::integral_constant<T, N - (N / 2)>>::type> {};
 
 
-            template <typename T, T N>
-            using make_integer_sequence = typename make_integer_sequence_impl<std::integral_constant<T, N>>::type;
+            template <typename T, T N> inline using make_integer_sequence = typename make_integer_sequence_impl<std::integral_constant<T, N>>::type;
 
 
-            template <std::size_t... Is>
-            using index_sequence = integer_sequence<std::size_t, Is...>;
+            template <std::size_t... Is> inline using index_sequence = integer_sequence<std::size_t, Is...>;
 
-            template <std::size_t N>
-            using make_index_sequence = make_integer_sequence<std::size_t, N>;
+            template <std::size_t N> inline using make_index_sequence = make_integer_sequence<std::size_t, N>;
 
-            template <typename... Ts>
-            using index_sequence_for = make_index_sequence<sizeof...(Ts)>;
+            template <typename... Ts> inline using index_sequence_for = make_index_sequence<sizeof...(Ts)>;
 
         #endif
 
-          template <int... Is>
-          using int_sequence = integer_sequence<int, Is...>;
+          template <int... Is> inline using int_sequence = integer_sequence<int, Is...>;
 
-          template <int N>
-          using make_int_sequence = make_integer_sequence<int, N>;
+          template <int N> inline using make_int_sequence = make_integer_sequence<int, N>;
 
-          template <typename... Ts>
-          using int_sequence_for = make_int_sequence<sizeof...(Ts)>;
+          template <typename... Ts> inline using int_sequence_for = make_int_sequence<sizeof...(Ts)>;
 
     }
 
@@ -268,21 +226,18 @@ namespace xsimd
 
     namespace detail
     {
-        template <class T, class... Types, size_t I, size_t... Is>
-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)
+        template <class T, class... Types, size_t I, size_t... Is> inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)
         {
             return std::get<I>(t);
         }
 
-        template <class T, class U, class... Types, size_t I, size_t... Is>
-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)
+        template <class T, class U, class... Types, size_t I, size_t... Is> inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)
         {
             using tuple_elem = typename std::tuple_element<I+1, std::tuple<Types...>>::type;
             return get_impl<T>(t, std::is_same<T, tuple_elem>(), index_sequence<Is...>());
         }
 
-        template <class T, class... Types>
-        const T& get(const std::tuple<Types...>& t)
+        template <class T, class... Types> inline const T& get(const std::tuple<Types...>& t)
         {
             using tuple_elem = typename std::tuple_element<0, std::tuple<Types...>>::type;
             return get_impl<T>(t, std::is_same<T, tuple_elem>(), make_index_sequence<sizeof...(Types)>());
@@ -295,14 +250,12 @@ namespace xsimd
 
     namespace detail
     {
-        template <class... T>
-        struct make_void
+        template <class... T> inline struct make_void
         {
             using type = void;
         };
 
-        template <class... T>
-        using void_t = typename make_void<T...>::type;
+        template <class... T> inline using void_t = typename make_void<T...>::type;
     }
 
     /**************************************************
@@ -311,14 +264,12 @@ namespace xsimd
 
     namespace detail
     {
-        template <std::size_t>
-        struct check_size
+        template <std::size_t> inline struct check_size
         {
             using type = void;
         };
 
-        template <std::size_t S>
-        using check_size_t = typename check_size<S>::type;
+        template <std::size_t S> inline using check_size_t = typename check_size<S>::type;
     }
 
     /*****************************************
@@ -328,8 +279,7 @@ namespace xsimd
     namespace detail
     {
         // std::array constructor from scalar value ("broadcast")
-        template <typename T, std::size_t... Is>
-        constexpr std::array<T, sizeof...(Is)>
+        template <typename T, std::size_t... Is> inline constexpr std::array<T, sizeof...(Is)>
         array_from_scalar_impl(const T& scalar, index_sequence<Is...>)
         {
             // You can safely ignore this silly ternary, the "scalar" is all
@@ -337,23 +287,20 @@ namespace xsimd
             return std::array<T, sizeof...(Is)>{ (Is+1) ? scalar : T() ... };
         }
 
-        template <typename T, std::size_t N>
-        constexpr std::array<T, N>
+        template <typename T, std::size_t N> inline constexpr std::array<T, N>
         array_from_scalar(const T& scalar)
         {
             return array_from_scalar_impl(scalar, make_index_sequence<N>());
         }
 
         // std::array constructor from C-style pointer (handled as an array)
-        template <typename T, std::size_t... Is>
-        constexpr std::array<T, sizeof...(Is)>
+        template <typename T, std::size_t... Is> inline constexpr std::array<T, sizeof...(Is)>
         array_from_pointer_impl(const T* c_array, index_sequence<Is...>)
         {
             return std::array<T, sizeof...(Is)>{ c_array[Is]... };
         }
 
-        template <typename T, std::size_t N>
-        constexpr std::array<T, N>
+        template <typename T, std::size_t N> inline constexpr std::array<T, N>
         array_from_pointer(const T* c_array)
         {
             return array_from_pointer_impl(c_array, make_index_sequence<N>());
@@ -366,25 +313,21 @@ namespace xsimd
 
     namespace detail
     {
-        template <bool...> struct bool_pack;
+        template <bool...> inline struct bool_pack;
 
-        template <bool... bs>
-        using all_true = std::is_same<
+        template <bool... bs> inline using all_true = std::is_same<
             bool_pack<bs..., true>, bool_pack<true, bs...>
         >;
 
-        template <typename T, typename... Args>
-        using is_all_convertible = all_true<std::is_convertible<Args, T>::value...>;
+        template <typename T, typename... Args> inline using is_all_convertible = all_true<std::is_convertible<Args, T>::value...>;
 
-        template <typename T, std::size_t N, typename... Args>
-        using is_array_initializer = std::enable_if<
+        template <typename T, std::size_t N, typename... Args> inline using is_array_initializer = std::enable_if<
             (sizeof...(Args) == N) && is_all_convertible<T, Args...>::value
         >;
 
         // Check that a variadic argument pack is a list of N values of type T,
         // as usable for instantiating a value of type std::array<T, N>.
-        template <typename T, std::size_t N, typename... Args>
-        using is_array_initializer_t = typename is_array_initializer<T, N, Args...>::type;
+        template <typename T, std::size_t N, typename... Args> inline using is_array_initializer_t = typename is_array_initializer<T, N, Args...>::type;
     }
 
     /**************
@@ -399,19 +342,16 @@ namespace xsimd
 
     namespace detail
     {
-        template <class T>
-        struct is_complex : std::false_type
+        template <class T> inline struct is_complex : std::false_type
         {
         };
 
-        template <class T>
-        struct is_complex<std::complex<T>> : std::true_type
+        template <class T> inline struct is_complex<std::complex<T>> : std::true_type
         {
         };
 
 #ifdef XSIMD_ENABLE_XTL_COMPLEX
-        template <class T, bool i3ec>
-        struct is_complex<xtl::xcomplex<T, T, i3ec>> : std::true_type
+        template <class T, bool i3ec> inline struct is_complex<xtl::xcomplex<T, T, i3ec>> : std::true_type
         {
         };
 #endif
