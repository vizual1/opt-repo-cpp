diff --git a/snappy-internal.h b/snappy-internal.h
index ad2b36a..846f1ce 100644
--- a/snappy-internal.h
+++ b/snappy-internal.h
@@ -97,6 +97,11 @@ class WorkingMemory {
   // stores the number of buckets in "*table_size" and returns a pointer to
   // the base of the hash table.
   uint16_t* GetHashTable(size_t fragment_size, int* table_size) const;
+
+  // FastMemset fills memory with a byte value using vector stores when available.
+  // It's used instead of std::memset on hot paths because some compilers generate
+  // suboptimal code for memset. Implemented in snappy.cc.
+  static void FastMemset(void* dst, int c, size_t n);
   char* GetScratchInput() const { return input_; }
   char* GetScratchOutput() const { return output_; }
 
diff --git a/snappy.cc b/snappy.cc
index 7d0ff71..b7a125e 100644
--- a/snappy.cc
+++ b/snappy.cc
@@ -697,10 +697,49 @@ WorkingMemory::~WorkingMemory() {
   std::allocator<char>().deallocate(mem_, size_);
 }
 
+// FastMemset: optimized memset used on hot paths. Uses vector stores when
+// available (via V128_StoreU), and falls back to 8-byte memcpy loops which are
+// safe on platforms that disallow unaligned integer stores.
+void WorkingMemory::FastMemset(void* dst, int c, size_t n) {
+  unsigned char* p = reinterpret_cast<unsigned char*>(dst);
+  if (n == 0) return;
+  const unsigned char byte = static_cast<unsigned char>(c);
+
+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE
+  // Build a 16-byte pattern and store it repeatedly. V128_StoreU supports
+  // unaligned stores.
+  alignas(16) unsigned char pattern[16];
+  for (int i = 0; i < 16; ++i) pattern[i] = byte;
+  V128 v = V128_LoadU(reinterpret_cast<const V128*>(pattern));
+  while (n >= 16) {
+    V128_StoreU(reinterpret_cast<V128*>(p), v);
+    p += 16;
+    n -= 16;
+  }
+#else
+  // For portability (including architectures that fault on unaligned integer
+  // stores), use memcpy with a repeated 8-byte value as the fast path.
+  uint64_t rep = static_cast<uint64_t>(byte);
+  rep = rep | (rep << 8);
+  rep = rep | (rep << 16);
+  rep = rep | (rep << 32);
+  while (n >= 8) {
+    std::memcpy(p, &rep, 8);
+    p += 8;
+    n -= 8;
+  }
+#endif
+
+  // Tail bytes
+  while (n--) *p++ = byte;
+}
+
 uint16_t* WorkingMemory::GetHashTable(size_t fragment_size,
                                       int* table_size) const {
   const size_t htsize = CalculateTableSize(fragment_size);
-  memset(table_, 0, htsize * sizeof(*table_));
+  // Use optimized FastMemset to avoid suboptimal code generated by some compilers
+  // for std::memset on hot paths.
+  WorkingMemory::FastMemset(table_, 0, htsize * sizeof(*table_));
   *table_size = htsize;
   return table_;
 }
