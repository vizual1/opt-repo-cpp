diff --git a/include/xsimd/arch/generic/xsimd_generic_memory.hpp b/include/xsimd/arch/generic/xsimd_generic_memory.hpp
index 7f1f09c..aa19ce7 100644
--- a/include/xsimd/arch/generic/xsimd_generic_memory.hpp
+++ b/include/xsimd/arch/generic/xsimd_generic_memory.hpp
@@ -267,6 +267,19 @@ namespace xsimd
             using batch_type = batch<T, A>;
             batch_type ref(0);
             constexpr auto size = batch_bool<T, A>::size;
+
+            // Fast path: if the storage type of the boolean batch is a byte and the
+            // input pointer is properly aligned for the architecture, we can load
+            // the raw bytes directly and then compare to zero. This avoids the
+            // per-element branch and may be notably faster on some platforms.
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+            {
+                if (((reinterpret_cast<uintptr_t>(mem) % A::alignment()) == 0))
+                {
+                    return ref != batch_type::load_aligned(reinterpret_cast<T const*>(mem));
+                }
+            }
+
             alignas(A::alignment()) T buffer[size];
             for (std::size_t i = 0; i < size; ++i)
                 buffer[i] = mem[i] ? 1 : 0;
diff --git a/include/xsimd/arch/xsimd_avx512bw.hpp b/include/xsimd/arch/xsimd_avx512bw.hpp
index 6a76b59..1c986c2 100644
--- a/include/xsimd/arch/xsimd_avx512bw.hpp
+++ b/include/xsimd/arch/xsimd_avx512bw.hpp
@@ -322,8 +322,18 @@ namespace xsimd
             using register_type = typename batch_bool<T, A>::register_type;
             XSIMD_IF_CONSTEXPR(batch_bool<T, A>::size == 64)
             {
-                __m512i bool_val = _mm512_loadu_si512((__m512i const*)mem);
-                return (register_type)_mm512_cmpgt_epu8_mask(bool_val, _mm512_setzero_si512());
+                // If the boolean buffer is 64 bytes and properly aligned we can do a
+                // single 512-bit load which is faster than loading unaligned.
+                if (((reinterpret_cast<uintptr_t>(mem) % 64) == 0))
+                {
+                    __m512i bool_val = _mm512_load_si512((__m512i const*)mem);
+                    return (register_type)_mm512_cmpgt_epu8_mask(bool_val, _mm512_setzero_si512());
+                }
+                else
+                {
+                    __m512i bool_val = _mm512_loadu_si512((__m512i const*)mem);
+                    return (register_type)_mm512_cmpgt_epu8_mask(bool_val, _mm512_setzero_si512());
+                }
             }
             else
             {
diff --git a/include/xsimd/arch/xsimd_avx512f.hpp b/include/xsimd/arch/xsimd_avx512f.hpp
index 1d2ee2d..86d4fa5 100644
--- a/include/xsimd/arch/xsimd_avx512f.hpp
+++ b/include/xsimd/arch/xsimd_avx512f.hpp
@@ -1217,6 +1217,15 @@ namespace xsimd
             constexpr auto size = batch_bool<T, A>::size;
             constexpr auto iter = size / 8;
             static_assert(size % 8 == 0, "incorrect size of bool batch");
+            // If the boolean buffer is 64 bytes and properly aligned we can do a
+            // single 512-bit load which is faster than the per-8-byte tobitset
+            // approach.
+            if (size == 64 && ((reinterpret_cast<uintptr_t>(mem) % 64) == 0))
+            {
+                __m512i bool_val = _mm512_load_si512((__m512i const*)mem);
+                return (register_type)_mm512_cmpgt_epu8_mask(bool_val, _mm512_setzero_si512());
+            }
+
             register_type mask = 0;
             for (std::size_t i = 0; i < iter; ++i)
             {
diff --git a/include/xsimd/types/xsimd_batch.hpp b/include/xsimd/types/xsimd_batch.hpp
index c3c0d7f..3086206 100644
--- a/include/xsimd/types/xsimd_batch.hpp
+++ b/include/xsimd/types/xsimd_batch.hpp
@@ -974,7 +974,10 @@ namespace xsimd
     template <class T, class A>
     XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem) noexcept
     {
-        return load_aligned(mem);
+        // Prefer calling the kernel directly for unaligned loads so that
+        // architecture-specific implementations can optimize the access
+        // depending on alignment and available intrinsics.
+        return kernel::load<A>(mem, batch_bool<T, A>(), A {});
     }
 
     /**
