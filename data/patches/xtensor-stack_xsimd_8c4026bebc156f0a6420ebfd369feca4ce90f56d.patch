diff --git a/include/xsimd/arch/xsimd_avx.hpp b/include/xsimd/arch/xsimd_avx.hpp
index 901547f..8d18cd5 100644
--- a/include/xsimd/arch/xsimd_avx.hpp
+++ b/include/xsimd/arch/xsimd_avx.hpp
@@ -407,20 +407,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> broadcast(T val, requires_arch<avx>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_set1_epi8(val);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_set1_epi16(val);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_set1_epi32(val);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_set1_epi64x(val);
-            default:
+}
+else
+{
                 assert(false && "unsupported");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> broadcast(float val, requires_arch<avx>) noexcept
@@ -681,22 +690,29 @@ namespace xsimd
                 0xFFFFFFFFFFFF0000ul,
                 0xFFFFFFFFFFFFFFFFul,
             };
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 assert(!(mask & ~0xFFFFFFFFul) && "inbound mask");
                 return _mm256_setr_epi32(lut32[mask & 0xF], lut32[(mask >> 4) & 0xF],
                                          lut32[(mask >> 8) & 0xF], lut32[(mask >> 12) & 0xF],
                                          lut32[(mask >> 16) & 0xF], lut32[(mask >> 20) & 0xF],
                                          lut32[(mask >> 24) & 0xF], lut32[mask >> 28]);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 assert(!(mask & ~0xFFFFul) && "inbound mask");
                 return _mm256_setr_epi64x(lut64[mask & 0xF], lut64[(mask >> 4) & 0xF], lut64[(mask >> 8) & 0xF], lut64[(mask >> 12) & 0xF]);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_castps_si256(from_mask(batch_bool<float, A> {}, mask, avx {}));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_castpd_si256(from_mask(batch_bool<double, A> {}, mask, avx {}));
             }
+}
+
         }
 
         // hadd
@@ -785,19 +801,25 @@ namespace xsimd
         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<avx>) noexcept
         {
-            switch (sizeof(T))
-            {
-#if !defined(_MSC_VER) || _MSC_VER > 1900
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_insert_epi8(self, val, I);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_insert_epi16(self, val, I);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_insert_epi32(self, val, I);
 #endif
-            default:
+}
+else
+{
                 return insert(self, val, pos, generic {});
             }
+}
+
         }
 
         // isnan
@@ -924,23 +946,33 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
+
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 __m128i self_low, self_high;
                 detail::split_avx(self, self_low, self_high);
                 return mask(batch_bool<T, sse4_2>(self_low), sse4_2 {}) | (mask(batch_bool<T, sse4_2>(self_high), sse4_2 {}) << (128 / (8 * sizeof(T))));
             }
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_movemask_ps(_mm256_castsi256_ps(self));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_movemask_pd(_mm256_castsi256_pd(self));
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<avx>) noexcept
@@ -1511,10 +1543,12 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
+
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 // extract low word
                 __m128i self_lo = _mm256_extractf128_si256(self, 0);
@@ -1543,15 +1577,22 @@ namespace xsimd
                         _mm_castsi128_ps(res_hi),
                         1));
             }
-
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_castps_si256(_mm256_unpackhi_ps(_mm256_castsi256_ps(self), _mm256_castsi256_ps(other)));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_castpd_si256(_mm256_unpackhi_pd(_mm256_castsi256_pd(self), _mm256_castsi256_pd(other)));
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept
@@ -1568,10 +1609,12 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
+
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 // extract low word
                 __m128i self_lo = _mm256_extractf128_si256(self, 0);
@@ -1600,14 +1643,22 @@ namespace xsimd
                         _mm_castsi128_ps(res_hi),
                         1));
             }
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_castps_si256(_mm256_unpacklo_ps(_mm256_castsi256_ps(self), _mm256_castsi256_ps(other)));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_castpd_si256(_mm256_unpacklo_pd(_mm256_castsi256_pd(self), _mm256_castsi256_pd(other)));
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         template <class A>
diff --git a/include/xsimd/arch/xsimd_avx2.hpp b/include/xsimd/arch/xsimd_avx2.hpp
index d96c50c..f078a33 100644
--- a/include/xsimd/arch/xsimd_avx2.hpp
+++ b/include/xsimd/arch/xsimd_avx2.hpp
@@ -30,17 +30,24 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_abs_epi8(self);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_abs_epi16(self);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_abs_epi32(self);
-                default:
+}
+else
+{
                     return abs(self, avx {});
                 }
+}
+
             }
             return self;
         }
@@ -49,19 +56,28 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_add_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_add_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_add_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_add_epi64(self, other);
-            default:
+}
+else
+{
                 return add(self, other, avx {});
             }
+}
+
         }
 
         // bitwise_and
@@ -104,31 +120,43 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_slli_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_slli_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_slli_epi64(self, other);
-            default:
+}
+else
+{
                 return bitwise_lshift(self, other, avx {});
             }
+}
+
         }
 
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 4:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_sllv_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_sllv_epi64(self, other);
-            default:
+}
+else
+{
                 return bitwise_lshift(self, other, avx {});
             }
+}
+
         }
 
         // bitwise_or
@@ -149,9 +177,8 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 {
                     __m256i sign_mask = _mm256_set1_epi16((0xFF00 >> other) & 0x00FF);
                     __m256i cmp_is_negative = _mm256_cmpgt_epi8(_mm256_setzero_si256(), self);
@@ -162,27 +189,42 @@ namespace xsimd
                                            sign_mask, cmp_is_negative),
                         _mm256_andnot_si256(sign_mask, res));
                 }
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_srai_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_srai_epi32(self, other);
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 2:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_srli_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_srli_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm256_srli_epi64(self, other);
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -191,25 +233,33 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_srav_epi32(self, other);
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_srlv_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm256_srlv_epi64(self, other);
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -296,19 +346,28 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_cmpeq_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_cmpeq_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_cmpeq_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_cmpeq_epi64(self, other);
-            default:
+}
+else
+{
                 return eq(self, other, avx {});
             }
+}
+
         }
 
         // gather
@@ -374,19 +433,28 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_cmpgt_epi8(other, self);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_cmpgt_epi16(other, self);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_cmpgt_epi32(other, self);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm256_cmpgt_epi64(other, self);
-                default:
+}
+else
+{
                     return lt(self, other, avx {});
                 }
+}
+
             }
             else
             {
@@ -398,9 +466,8 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline T hadd(batch<T, A> const& self, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 4:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
             {
                 __m256i tmp1 = _mm256_hadd_epi32(self, self);
                 __m256i tmp2 = _mm256_hadd_epi32(tmp1, tmp1);
@@ -408,7 +475,9 @@ namespace xsimd
                 __m128i tmp4 = _mm_add_epi32(_mm256_castsi256_si128(tmp2), tmp3);
                 return _mm_cvtsi128_si32(tmp4);
             }
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
             {
                 __m256i tmp1 = _mm256_shuffle_epi32(self, 0x0E);
                 __m256i tmp2 = _mm256_add_epi64(self, tmp1);
@@ -424,9 +493,13 @@ namespace xsimd
                 return i;
 #endif
             }
-            default:
+}
+else
+{
                 return hadd(self, avx {});
             }
+}
+
         }
         // load_complex
         template <class A>
@@ -455,18 +528,23 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return 0xFFFFFFFF & (uint64_t)_mm256_movemask_epi8(self);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 uint64_t mask8 = 0xFFFFFFFF & (uint64_t)_mm256_movemask_epi8(self);
                 return detail::mask_lut(mask8) | (detail::mask_lut(mask8 >> 8) << 4) | (detail::mask_lut(mask8 >> 16) << 8) | (detail::mask_lut(mask8 >> 24) << 12);
             }
-            default:
+}
+else
+{
                 return mask(self, avx {});
             }
+}
+
         }
 
         // max
@@ -475,31 +553,45 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_max_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_max_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_max_epi32(self, other);
-                default:
+}
+else
+{
                     return max(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_max_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_max_epu16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_max_epu32(self, other);
-                default:
+}
+else
+{
                     return max(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -509,31 +601,45 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_min_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_min_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_min_epi32(self, other);
-                default:
+}
+else
+{
                     return min(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_min_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_min_epu16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm256_min_epu32(self, other);
-                default:
+}
+else
+{
                     return min(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -541,15 +647,20 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_mullo_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_mullo_epi32(self, other);
-            default:
+}
+else
+{
                 return mul(self, other, avx {});
             }
+}
+
         }
 
         // sadd
@@ -558,27 +669,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_adds_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_adds_epi16(self, other);
-                default:
+}
+else
+{
                     return sadd(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_adds_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_adds_epu16(self, other);
-                default:
+}
+else
+{
                     return sadd(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -586,39 +707,50 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_blendv_epi8(false_br, true_br, cond);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_blendv_epi8(false_br, true_br, cond);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_blendv_epi8(false_br, true_br, cond);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_blendv_epi8(false_br, true_br, cond);
-            default:
+}
+else
+{
                 return select(cond, true_br, false_br, avx {});
             }
+}
+
         }
         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept
         {
             constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();
-            switch (sizeof(T))
-            {
-                // FIXME: for some reason mask here is not considered as an immediate,
-                // but it's okay for _mm256_blend_epi32
-                // case 2: return _mm256_blend_epi16(false_br, true_br, mask);
-            case 4:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_blend_epi32(false_br, true_br, mask);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
             {
                 constexpr int imask = detail::interleave(mask);
                 return _mm256_blend_epi32(false_br, true_br, imask);
             }
-            default:
+}
+else
+{
                 return select(batch_bool<T, A> { Values... }, true_br, false_br, avx2 {});
             }
+}
+
         }
 
         // slide_left
@@ -689,27 +821,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_subs_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_subs_epi16(self, other);
-                default:
+}
+else
+{
                     return ssub(self, other, avx {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm256_subs_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm256_subs_epu16(self, other);
-                default:
+}
+else
+{
                     return ssub(self, other, avx {});
                 }
+}
+
             }
         }
 
@@ -717,19 +859,28 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_sub_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_sub_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_sub_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_sub_epi64(self, other);
-            default:
+}
+else
+{
                 return sub(self, other, avx {});
             }
+}
+
         }
 
         // swizzle
@@ -772,40 +923,58 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_unpackhi_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_unpackhi_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_unpackhi_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_unpackhi_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // zip_lo
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm256_unpacklo_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm256_unpacklo_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm256_unpacklo_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm256_unpacklo_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
     }
 }
diff --git a/include/xsimd/arch/xsimd_avx512bw.hpp b/include/xsimd/arch/xsimd_avx512bw.hpp
index 7363eee..c7e9263 100644
--- a/include/xsimd/arch/xsimd_avx512bw.hpp
+++ b/include/xsimd/arch/xsimd_avx512bw.hpp
@@ -32,31 +32,45 @@ namespace xsimd
                 using register_type = typename batch_bool<T, A>::register_type;
                 if (std::is_signed<T>::value)
                 {
-                    switch (sizeof(T))
-                    {
-                    case 1:
+                    XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                         return (register_type)_mm512_cmp_epi8_mask(self, other, Cmp);
-                    case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                         return (register_type)_mm512_cmp_epi16_mask(self, other, Cmp);
-                    case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                         return (register_type)_mm512_cmp_epi32_mask(self, other, Cmp);
-                    case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                         return (register_type)_mm512_cmp_epi64_mask(self, other, Cmp);
                     }
+}
+
                 }
                 else
                 {
-                    switch (sizeof(T))
-                    {
-                    case 1:
+                    XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                         return (register_type)_mm512_cmp_epu8_mask(self, other, Cmp);
-                    case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                         return (register_type)_mm512_cmp_epu16_mask(self, other, Cmp);
-                    case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                         return (register_type)_mm512_cmp_epu32_mask(self, other, Cmp);
-                    case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                         return (register_type)_mm512_cmp_epu64_mask(self, other, Cmp);
                     }
+}
+
                 }
             }
         }
@@ -68,48 +82,62 @@ namespace xsimd
             if (std::is_unsigned<T>::value)
                 return self;
 
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_abs_epi8(self);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_abs_epi16(self);
-            default:
+}
+else
+{
                 return abs(self, avx512dq {});
             }
+}
+
         }
 
         // add
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_add_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_add_epi16(self, other);
-            default:
+}
+else
+{
                 return add(self, other, avx512dq {});
             }
+}
+
         }
 
         // bitwise_lshift
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept
         {
-            switch (sizeof(T))
-            {
-#if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_sllv_epi16(self, _mm512_set1_epi16(other));
 #else
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_slli_epi16(self, other);
 #endif
-            default:
+}
+else
+{
                 return bitwise_lshift(self, other, avx512dq {});
             }
+}
+
         }
 
         // bitwise_rshift
@@ -118,9 +146,8 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 {
                     __m512i sign_mask = _mm512_set1_epi16((0xFF00 >> other) & 0x00FF);
                     __m512i zeros = _mm512_setzero_si512();
@@ -134,30 +161,42 @@ namespace xsimd
                     return _mm512_or_si512(cmp_sign_mask, _mm512_andnot_si512(sign_mask, res));
                 }
 #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_srav_epi16(self, _mm512_set1_epi16(other));
 #else
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_srai_epi16(self, other);
 #endif
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx512dq {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-#if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-                case 2:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_srlv_epi16(self, _mm512_set1_epi16(other));
 #else
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_srli_epi16(self, other);
 #endif
-                default:
+}
+else
+{
                     return bitwise_rshift(self, other, avx512dq {});
                 }
+}
+
             }
         }
 
@@ -202,27 +241,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_max_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_max_epi16(self, other);
-                default:
+}
+else
+{
                     return max(self, other, avx512dq {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_max_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_max_epu16(self, other);
-                default:
+}
+else
+{
                     return max(self, other, avx512dq {});
                 }
+}
+
             }
         }
 
@@ -232,27 +281,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_min_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_min_epi16(self, other);
-                default:
+}
+else
+{
                     return min(self, other, avx512dq {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_min_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_min_epu16(self, other);
-                default:
+}
+else
+{
                     return min(self, other, avx512dq {});
                 }
+}
+
             }
         }
 
@@ -260,19 +319,24 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
             {
                 __m512i upper = _mm512_and_si512(_mm512_mullo_epi16(self, other), _mm512_srli_epi16(_mm512_set1_epi16(-1), 8));
                 __m512i lower = _mm512_slli_epi16(_mm512_mullo_epi16(_mm512_srli_epi16(self, 8), _mm512_srli_epi16(other, 8)), 8);
                 return _mm512_or_si512(upper, lower);
             }
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_mullo_epi16(self, other);
-            default:
+}
+else
+{
                 return mul(self, other, avx512dq {});
             }
+}
+
         }
 
         // neq
@@ -288,27 +352,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_adds_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_adds_epi16(self, other);
-                default:
+}
+else
+{
                     return sadd(self, other, avx512dq {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_adds_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_adds_epu16(self, other);
-                default:
+}
+else
+{
                     return sadd(self, other, avx512dq {});
                 }
+}
+
             }
         }
 
@@ -316,15 +390,20 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_mask_blend_epi8(cond, false_br, true_br);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_mask_blend_epi16(cond, false_br, true_br);
-            default:
+}
+else
+{
                 return select(cond, true_br, false_br, avx512dq {});
-            };
+            }
+}
+;
         }
 
         // slide_left
@@ -445,27 +524,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_subs_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_subs_epi16(self, other);
-                default:
+}
+else
+{
                     return ssub(self, other, avx512dq {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm512_subs_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm512_subs_epu16(self, other);
-                default:
+}
+else
+{
                     return ssub(self, other, avx512dq {});
                 }
+}
+
             }
         }
 
@@ -473,15 +562,20 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_sub_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_sub_epi16(self, other);
-            default:
+}
+else
+{
                 return sub(self, other, avx512dq {});
             }
+}
+
         }
 
         // swizzle
diff --git a/include/xsimd/arch/xsimd_avx512f.hpp b/include/xsimd/arch/xsimd_avx512f.hpp
index 05bfa37..602b58f 100644
--- a/include/xsimd/arch/xsimd_avx512f.hpp
+++ b/include/xsimd/arch/xsimd_avx512f.hpp
@@ -134,9 +134,8 @@ namespace xsimd
                 using register_type = typename batch_bool<T, A>::register_type;
                 if (std::is_signed<T>::value)
                 {
-                    switch (sizeof(T))
-                    {
-                    case 1:
+                    XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     {
                         // shifting to take sign into account
                         uint64_t mask_low0 = _mm512_cmp_epi32_mask((batch<int32_t, A>(self.data) & batch<int32_t, A>(0x000000FF)) << 24,
@@ -161,7 +160,9 @@ namespace xsimd
                         }
                         return (register_type)mask;
                     }
-                    case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     {
                         // shifting to take sign into account
                         uint16_t mask_low = _mm512_cmp_epi32_mask((batch<int32_t, A>(self.data) & batch<int32_t, A>(0x0000FFFF)) << 16,
@@ -172,17 +173,22 @@ namespace xsimd
                                                                    Cmp);
                         return static_cast<register_type>(morton(mask_low, mask_high));
                     }
-                    case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                         return (register_type)_mm512_cmp_epi32_mask(self, other, Cmp);
-                    case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                         return (register_type)_mm512_cmp_epi64_mask(self, other, Cmp);
                     }
+}
+
                 }
                 else
                 {
-                    switch (sizeof(T))
-                    {
-                    case 1:
+                    XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     {
                         uint64_t mask_low0 = _mm512_cmp_epu32_mask((batch<uint32_t, A>(self.data) & batch<uint32_t, A>(0x000000FF)), (batch<uint32_t, A>(other.data) & batch<uint32_t, A>(0x000000FF)), Cmp);
                         uint64_t mask_low1 = _mm512_cmp_epu32_mask((batch<uint32_t, A>(self.data) & batch<uint32_t, A>(0x0000FF00)), (batch<uint32_t, A>(other.data) & batch<uint32_t, A>(0x0000FF00)), Cmp);
@@ -198,17 +204,25 @@ namespace xsimd
                         }
                         return (register_type)mask;
                     }
-                    case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     {
                         uint16_t mask_low = _mm512_cmp_epu32_mask((batch<uint32_t, A>(self.data) & batch<uint32_t, A>(0x0000FFFF)), (batch<uint32_t, A>(other.data) & batch<uint32_t, A>(0x0000FFFF)), Cmp);
                         uint16_t mask_high = _mm512_cmp_epu32_mask((batch<uint32_t, A>(self.data) & batch<uint32_t, A>(0xFFFF0000)), (batch<uint32_t, A>(other.data) & batch<uint32_t, A>(0xFFFF0000)), Cmp);
                         return static_cast<register_type>(morton(mask_low, mask_high));
                     }
-                    case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                         return (register_type)_mm512_cmp_epu32_mask(self, other, Cmp);
-                    case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                         return (register_type)_mm512_cmp_epu64_mask(self, other, Cmp);
                     }
+}
+
                 }
             }
         }
@@ -237,48 +251,66 @@ namespace xsimd
             if (std::is_unsigned<T>::value)
                 return self;
 
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return detail::fwd_to_avx([](__m256i s) noexcept
                                           { return abs(batch<T, avx2>(s)); },
                                           self);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return detail::fwd_to_avx([](__m256i s) noexcept
                                           { return abs(batch<T, avx2>(s)); },
                                           self);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_abs_epi32(self);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_abs_epi64(self);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // add
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                           { return add(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                           self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                           { return add(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                           self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_add_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_add_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept
@@ -368,9 +400,8 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
             {
 #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
                 __m512i tmp = _mm512_sllv_epi32(self, _mm512_set1_epi32(other));
@@ -379,25 +410,39 @@ namespace xsimd
 #endif
                 return _mm512_and_si512(_mm512_set1_epi8(0xFF << other), tmp);
             }
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return detail::fwd_to_avx([](__m256i s, int32_t o) noexcept
                                           { return bitwise_lshift(batch<T, avx2>(s), o, avx2 {}); },
                                           self, other);
 #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_sllv_epi32(self, _mm512_set1_epi32(other));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_sllv_epi64(self, _mm512_set1_epi64(other));
 #else
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_slli_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_slli_epi64(self, other);
 #endif
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // bitwise_not
@@ -455,30 +500,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-#if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_srav_epi32(self, _mm512_set1_epi32(other));
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_srav_epi64(self, _mm512_set1_epi64(other));
 #else
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_srai_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_srai_epi64(self, other);
 #endif
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, int32_t o) noexcept
                                               { return bitwise_rshift(batch<T, avx2>(s), o, avx2 {}); },
                                               self, other);
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 {
 #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
                     __m512i tmp = _mm512_srlv_epi32(self, _mm512_set1_epi32(other));
@@ -488,21 +540,33 @@ namespace xsimd
                     return _mm512_and_si512(_mm512_set1_epi8(0xFF >> other), tmp);
                 }
 #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_srlv_epi32(self, _mm512_set1_epi32(other));
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_srlv_epi64(self, _mm512_set1_epi64(other));
 #else
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_srli_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_srli_epi64(self, other);
 #endif
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, int32_t o) noexcept
                                               { return bitwise_rshift(batch<T, avx2>(s), o, avx2 {}); },
                                               self, other);
                 }
+}
+
             }
         }
 
@@ -572,20 +636,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> broadcast(T val, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_set1_epi8(val);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_set1_epi16(val);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_set1_epi32(val);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_set1_epi64(val);
-            default:
+}
+else
+{
                 assert(false && "unsupported");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> broadcast(float val, requires_arch<avx512f>) noexcept
@@ -1066,31 +1139,41 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_max_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_max_epi64(self, other);
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                               { return max(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                               self, other);
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_max_epu32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_max_epu64(self, other);
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                               { return max(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                               self, other);
                 }
+}
+
             }
         }
 
@@ -1110,31 +1193,41 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_min_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_min_epi64(self, other);
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                               { return min(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                               self, other);
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 4:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm512_min_epu32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm512_min_epu64(self, other);
-                default:
+}
+else
+{
                     return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                               { return min(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                               self, other);
                 }
+}
+
             }
         }
 
@@ -1152,17 +1245,22 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 4:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_mullo_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_mullo_epi64(self, other);
-            default:
+}
+else
+{
                 return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                           { return mul(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                           self, other);
             }
+}
+
         }
 
         // nearbyint
@@ -1326,9 +1424,8 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
             {
                 alignas(avx2::alignment()) uint8_t buffer[64];
                 // FIXME: ultra inefficient
@@ -1347,7 +1444,9 @@ namespace xsimd
                 __m256i res_hi = select(batch_bool<T, avx2>(cond_hi), batch<T, avx2>(true_hi), batch<T, avx2>(false_hi), avx2 {});
                 return detail::merge_avx(res_low, res_hi);
             }
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 __m256i cond_low = _mm512_maskz_cvtepi32_epi16((uint64_t)cond.data & 0xFFFF, _mm512_set1_epi32(~0));
                 __m256i cond_hi = _mm512_maskz_cvtepi32_epi16((uint64_t)cond.data >> 16, _mm512_set1_epi32(~0));
@@ -1362,14 +1461,22 @@ namespace xsimd
                 __m256i res_hi = select(batch_bool<T, avx2>(cond_hi), batch<T, avx2>(true_hi), batch<T, avx2>(false_hi), avx2 {});
                 return detail::merge_avx(res_low, res_hi);
             }
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_mask_blend_epi32(cond, false_br, true_br);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_mask_blend_epi64(cond, false_br, true_br);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/type combination");
                 return {};
-            };
+            }
+}
+;
         }
 
         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
@@ -1603,24 +1710,33 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                           { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                           self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return detail::fwd_to_avx([](__m256i s, __m256i o) noexcept
                                           { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); },
                                           self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_sub_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_sub_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept
@@ -1686,20 +1802,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_unpackhi_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_unpackhi_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_unpackhi_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_unpackhi_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept
@@ -1716,20 +1841,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm512_unpacklo_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm512_unpacklo_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm512_unpacklo_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm512_unpacklo_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept
diff --git a/include/xsimd/arch/xsimd_sse2.hpp b/include/xsimd/arch/xsimd_sse2.hpp
index c3acce3..2b6d6c5 100644
--- a/include/xsimd/arch/xsimd_sse2.hpp
+++ b/include/xsimd/arch/xsimd_sse2.hpp
@@ -55,20 +55,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_add_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_add_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_add_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_add_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         template <class A>
@@ -197,20 +206,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_and_si128(_mm_set1_epi8(0xFF << other), _mm_slli_epi32(self, other));
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_slli_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_slli_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_slli_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // bitwise_not
@@ -285,20 +303,25 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 {
                     __m128i sign_mask = _mm_set1_epi16((0xFF00 >> other) & 0x00FF);
                     __m128i cmp_is_negative = _mm_cmpgt_epi8(_mm_setzero_si128(), self);
                     __m128i res = _mm_srai_epi16(self, other);
                     return _mm_or_si128(_mm_and_si128(sign_mask, cmp_is_negative), _mm_andnot_si128(sign_mask, res));
                 }
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_srai_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_srai_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 {
                     // from https://github.com/samyvilar/vect/blob/master/vect_128.h
                     return _mm_or_si128(
@@ -307,27 +330,40 @@ namespace xsimd
                             _mm_srai_epi32(_mm_shuffle_epi32(self, _MM_SHUFFLE(3, 3, 1, 1)), 32),
                             64 - other));
                 }
-                default:
+}
+else
+{
                     assert(false && "unsupported arch/op combination");
                     return {};
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_and_si128(_mm_set1_epi8(0xFF >> other), _mm_srli_epi32(self, other));
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_srli_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_srli_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                     return _mm_srli_epi64(self, other);
-                default:
+}
+else
+{
                     assert(false && "unsupported arch/op combination");
                     return {};
                 }
+}
+
             }
         }
 
@@ -409,20 +445,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> broadcast(T val, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_set1_epi8(val);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_set1_epi16(val);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_set1_epi32(val);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_set1_epi64x(val);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<double, A> broadcast(double val, requires_arch<sse2>) noexcept
@@ -554,15 +599,20 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_cmpeq_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_cmpeq_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_cmpeq_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
             {
                 __m128i tmp1 = _mm_cmpeq_epi32(self, other);
                 __m128i tmp2 = _mm_shuffle_epi32(tmp1, 0xB1);
@@ -570,10 +620,14 @@ namespace xsimd
                 __m128i tmp4 = _mm_srai_epi32(tmp3, 31);
                 return _mm_shuffle_epi32(tmp4, 0xF5);
             }
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept
@@ -667,19 +721,26 @@ namespace xsimd
                 0xFFFFFF00,
                 0xFFFFFFFF,
             };
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 assert(!(mask & ~0xFFFF) && "inbound mask");
                 return _mm_setr_epi32(lut32[mask & 0xF], lut32[(mask >> 4) & 0xF], lut32[(mask >> 8) & 0xF], lut32[mask >> 12]);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 assert(!(mask & ~0xFF) && "inbound mask");
                 return _mm_set_epi64x(lut64[mask >> 4], lut64[mask & 0xF]);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_castps_si128(from_mask(batch_bool<float, A> {}, mask, sse2 {}));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_castpd_si128(from_mask(batch_bool<double, A> {}, mask, sse2 {}));
             }
+}
+
         }
 
         // ge
@@ -705,17 +766,24 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_cmpgt_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_cmpgt_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_cmpgt_epi32(self, other);
-                default:
+}
+else
+{
                     return gt(self, other, generic {});
                 }
+}
+
             }
             else
             {
@@ -756,9 +824,8 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline T hadd(batch<T, A> const& self, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 4:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
             {
                 __m128i tmp1 = _mm_shuffle_epi32(self, 0x0E);
                 __m128i tmp2 = _mm_add_epi32(self, tmp1);
@@ -766,7 +833,9 @@ namespace xsimd
                 __m128i tmp4 = _mm_add_epi32(tmp2, tmp3);
                 return _mm_cvtsi128_si32(tmp4);
             }
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
             {
                 __m128i tmp1 = _mm_shuffle_epi32(self, 0x0E);
                 __m128i tmp2 = _mm_add_epi64(self, tmp1);
@@ -780,9 +849,13 @@ namespace xsimd
                 return i;
 #endif
             }
-            default:
+}
+else
+{
                 return detail::hadd_default(self, A {});
             }
+}
+
         }
         template <class A>
         inline double hadd(batch<double, A> const& self, requires_arch<sse2>) noexcept
@@ -815,13 +888,16 @@ namespace xsimd
         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_insert_epi16(self, val, I);
-            default:
+}
+else
+{
                 return insert(self, val, pos, generic {});
             }
+}
+
         }
 
         // isnan
@@ -909,15 +985,20 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_cmplt_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_cmplt_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_cmplt_epi32(self, other);
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 {
                     __m128i tmp1 = _mm_sub_epi64(self, other);
                     __m128i tmp2 = _mm_xor_si128(self, other);
@@ -927,22 +1008,31 @@ namespace xsimd
                     __m128i tmp6 = _mm_srai_epi32(tmp5, 31);
                     return _mm_shuffle_epi32(tmp6, 0xF5);
                 }
-                default:
+}
+else
+{
                     assert(false && "unsupported arch/op combination");
                     return {};
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_cmplt_epi8(_mm_xor_si128(self, _mm_set1_epi8(std::numeric_limits<int8_t>::lowest())), _mm_xor_si128(other, _mm_set1_epi8(std::numeric_limits<int8_t>::lowest())));
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_cmplt_epi16(_mm_xor_si128(self, _mm_set1_epi16(std::numeric_limits<int16_t>::lowest())), _mm_xor_si128(other, _mm_set1_epi16(std::numeric_limits<int16_t>::lowest())));
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_cmplt_epi32(_mm_xor_si128(self, _mm_set1_epi32(std::numeric_limits<int32_t>::lowest())), _mm_xor_si128(other, _mm_set1_epi32(std::numeric_limits<int32_t>::lowest())));
-                case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 {
                     auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));
                     auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));
@@ -954,10 +1044,14 @@ namespace xsimd
                     __m128i tmp6 = _mm_srai_epi32(tmp5, 31);
                     return _mm_shuffle_epi32(tmp6, 0xF5);
                 }
-                default:
+}
+else
+{
                     assert(false && "unsupported arch/op combination");
                     return {};
                 }
+}
+
             }
         }
 
@@ -1002,23 +1096,32 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_movemask_epi8(self);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 uint64_t mask8 = _mm_movemask_epi8(self);
                 return detail::mask_lut(mask8) | (detail::mask_lut(mask8 >> 8) << 4);
             }
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_movemask_ps(_mm_castsi128_ps(self));
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_movemask_pd(_mm_castsi128_pd(self));
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept
@@ -1240,27 +1343,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_adds_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_adds_epi16(self, other);
-                default:
+}
+else
+{
                     return detail::sadd_default(self, other, A {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_adds_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_adds_epu16(self, other);
-                default:
+}
+else
+{
                     return detail::sadd_default(self, other, A {});
                 }
+}
+
             }
         }
         template <class A>
@@ -1354,27 +1467,37 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_subs_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_subs_epi16(self, other);
-                default:
+}
+else
+{
                     return detail::ssub_default(self, other, A {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_subs_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_subs_epu16(self, other);
-                default:
+}
+else
+{
                     return detail::ssub_default(self, other, A {});
                 }
+}
+
             }
         }
 
@@ -1437,20 +1560,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_sub_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_sub_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_sub_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_sub_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept
@@ -1521,20 +1653,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_unpackhi_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_unpackhi_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_unpackhi_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_unpackhi_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept
@@ -1551,20 +1692,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_unpacklo_epi8(self, other);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_unpacklo_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_unpacklo_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_unpacklo_epi64(self, other);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
         template <class A>
         inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept
diff --git a/include/xsimd/arch/xsimd_sse4_1.hpp b/include/xsimd/arch/xsimd_sse4_1.hpp
index 73c4157..a524356 100644
--- a/include/xsimd/arch/xsimd_sse4_1.hpp
+++ b/include/xsimd/arch/xsimd_sse4_1.hpp
@@ -82,13 +82,16 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 8:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_cmpeq_epi64(self, other);
-            default:
+}
+else
+{
                 return eq(self, other, ssse3 {});
             }
+}
+
         }
 
         // floor
@@ -107,19 +110,26 @@ namespace xsimd
         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse4_1>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_insert_epi8(self, val, I);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_insert_epi32(self, val, I);
 #if !defined(_MSC_VER) || _MSC_VER > 1900 && defined(_M_X64)
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_insert_epi64(self, val, I);
 #endif
-            default:
+}
+else
+{
                 return insert(self, val, pos, ssse3 {});
             }
+}
+
         }
 
         // max
@@ -128,31 +138,45 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_max_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_max_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_max_epi32(self, other);
-                default:
+}
+else
+{
                     return max(self, other, ssse3 {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_max_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_max_epu16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_max_epu32(self, other);
-                default:
+}
+else
+{
                     return max(self, other, ssse3 {});
                 }
+}
+
             }
         }
 
@@ -162,31 +186,45 @@ namespace xsimd
         {
             if (std::is_signed<T>::value)
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_min_epi8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_min_epi16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_min_epi32(self, other);
-                default:
+}
+else
+{
                     return min(self, other, ssse3 {});
                 }
+}
+
             }
             else
             {
-                switch (sizeof(T))
-                {
-                case 1:
+                XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                     return _mm_min_epu8(self, other);
-                case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                     return _mm_min_epu16(self, other);
-                case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                     return _mm_min_epu32(self, other);
-                default:
+}
+else
+{
                     return min(self, other, ssse3 {});
                 }
+}
+
             }
         }
 
@@ -194,17 +232,22 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_or_si128(
                     _mm_and_si128(_mm_mullo_epi16(self, other), _mm_srli_epi16(_mm_cmpeq_epi8(self, self), 8)),
                     _mm_slli_epi16(_mm_mullo_epi16(_mm_srli_epi16(self, 8), _mm_srli_epi16(other, 8)), 8));
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_mullo_epi16(self, other);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_mullo_epi32(self, other);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_add_epi64(
                     _mm_mul_epu32(self, other),
                     _mm_slli_epi64(
@@ -212,10 +255,14 @@ namespace xsimd
                             _mm_mul_epu32(other, _mm_shuffle_epi32(self, _MM_SHUFFLE(2, 3, 0, 1))),
                             _mm_mul_epu32(self, _mm_shuffle_epi32(other, _MM_SHUFFLE(2, 3, 0, 1)))),
                         32));
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // nearbyint
@@ -260,24 +307,31 @@ namespace xsimd
         inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept
         {
             constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();
-            switch (sizeof(T))
-            {
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_blend_epi16(false_br, true_br, mask);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
             {
                 constexpr int imask = detail::interleave(mask);
                 return _mm_blend_epi16(false_br, true_br, imask);
             }
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
             {
                 constexpr int imask = detail::interleave(mask);
                 constexpr int imask2 = detail::interleave(imask);
                 return _mm_blend_epi16(false_br, true_br, imask2);
             }
-            default:
+}
+else
+{
                 return select(batch_bool_constant<batch<T, A>, Values...>(), true_br, false_br, ssse3 {});
             }
+}
+
         }
         template <class A, bool... Values>
         inline batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept
diff --git a/include/xsimd/arch/xsimd_ssse3.hpp b/include/xsimd/arch/xsimd_ssse3.hpp
index ab4c42f..2a39f05 100644
--- a/include/xsimd/arch/xsimd_ssse3.hpp
+++ b/include/xsimd/arch/xsimd_ssse3.hpp
@@ -29,20 +29,29 @@ namespace xsimd
         template <class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>
         inline batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 1:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 1)
+{
                 return _mm_abs_epi8(self);
-            case 2:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
                 return _mm_abs_epi16(self);
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
                 return _mm_abs_epi32(self);
-            case 8:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 8)
+{
                 return _mm_abs_epi64(self);
-            default:
+}
+else
+{
                 assert(false && "unsupported arch/op combination");
                 return {};
             }
+}
+
         }
 
         // extract_pair
@@ -79,24 +88,29 @@ namespace xsimd
         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>
         inline T hadd(batch<T, A> const& self, requires_arch<ssse3>) noexcept
         {
-            switch (sizeof(T))
-            {
-            case 2:
+            XSIMD_IF_CONSTEXPR(sizeof(T) == 2)
+{
             {
                 __m128i tmp1 = _mm_hadd_epi16(self, self);
                 __m128i tmp2 = _mm_hadd_epi16(tmp1, tmp1);
                 __m128i tmp3 = _mm_hadd_epi16(tmp2, tmp2);
                 return _mm_cvtsi128_si32(tmp3) & 0xFFFF;
             }
-            case 4:
+}
+else XSIMD_IF_CONSTEXPR(sizeof(T) == 4)
+{
             {
                 __m128i tmp1 = _mm_hadd_epi32(self, self);
                 __m128i tmp2 = _mm_hadd_epi32(tmp1, tmp1);
                 return _mm_cvtsi128_si32(tmp2);
             }
-            default:
+}
+else
+{
                 return hadd(self, sse3 {});
             }
+}
+
         }
 
         // swizzle
diff --git a/include/xsimd/xsimd.hpp b/include/xsimd/xsimd.hpp
index 4ac8dd1..c832c12 100644
--- a/include/xsimd/xsimd.hpp
+++ b/include/xsimd/xsimd.hpp
@@ -37,6 +37,13 @@
 
 #include "config/xsimd_config.hpp"
 
+#if __cplusplus >= 201703L
+#define XSIMD_IF_CONSTEXPR(...) if constexpr(__VA_ARGS__)
+#else
+#define XSIMD_IF_CONSTEXPR(...) if(__VA_ARGS__)
+#endif
+
+
 #include "arch/xsimd_scalar.hpp"
 #include "memory/xsimd_aligned_allocator.hpp"
 
