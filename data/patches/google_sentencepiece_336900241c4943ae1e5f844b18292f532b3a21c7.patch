diff --git a/src/unigram_model_trainer.cc b/src/unigram_model_trainer.cc
index 2fbc057..59114eb 100644
--- a/src/unigram_model_trainer.cc
+++ b/src/unigram_model_trainer.cc
@@ -432,22 +432,18 @@ TrainerModel::SentencePieces Trainer::PruneSentencePieces(
   }
 
   // Second, segments all sentences to compute likelihood
-  // with a unigram language model. inverted[i] stores
-  // the set of sentence index where the sentencepieces[i] appears.
+  // with a unigram language model. We compute freq[i] as total
+  // frequency of sentencepieces[i] in Viterbi paths.
   float vsum = 0.0;
   std::vector<float> freq(sentencepieces.size(), 0.0);
-  std::vector<std::vector<int>> inverted(sentencepieces.size());
   {
     std::vector<float> vsums(trainer_spec_.num_threads(), 0.0);
     std::vector<std::vector<float>> freqs(trainer_spec_.num_threads());
-    std::vector<std::vector<std::vector<int>>> inverteds(
-        trainer_spec_.num_threads());
 
     auto pool = std::make_unique<ThreadPool>(trainer_spec_.num_threads());
     pool->StartWorkers();
     for (int n = 0; n < trainer_spec_.num_threads(); ++n) {
       freqs[n].resize(sentencepieces.size(), 0.0);
-      inverteds[n].resize(sentencepieces.size());
 
       pool->Schedule([&, n]() {
         Lattice lattice;
@@ -460,7 +456,6 @@ TrainerModel::SentencePieces Trainer::PruneSentencePieces(
           for (const auto *node : lattice.Viterbi().first) {
             if (node->id >= 0) {
               freqs[n][node->id] += w.second;
-              inverteds[n][node->id].push_back(i);
             }
           }
         }
@@ -472,8 +467,6 @@ TrainerModel::SentencePieces Trainer::PruneSentencePieces(
       vsum += vsums[n];
       for (size_t i = 0; i < sentencepieces.size(); ++i) {
         freq[i] += freqs[n][i];
-        std::copy(inverteds[n][i].begin(), inverteds[n][i].end(),
-                  std::back_inserter(inverted[i]));
       }
     }
   }
@@ -496,11 +489,7 @@ TrainerModel::SentencePieces Trainer::PruneSentencePieces(
       // no alternatives. Keeps this entry.
       new_sentencepieces.push_back(sentencepieces[i]);
     } else {
-      float F = 0.0;  // the frequency of sentencepieces[i].
-      for (const int n : inverted[i]) {
-        F += sentences_[n].second;
-      }
-      F /= vsum;  // normalizes by all sentence frequency.
+      const float F = (vsum > 0.0f) ? (freq[i] / vsum) : 0.0f;  // normalized frequency of sentencepieces[i].
 
       // The logprob with the sentencepiece[i].
       const float logprob_sp = std::log(static_cast<double>(freq[i])) - logsum;
