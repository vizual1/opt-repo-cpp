diff --git a/include/xsimd/arch/xsimd_avx512bw.hpp b/include/xsimd/arch/xsimd_avx512bw.hpp
index ce211a6..5e702bf 100644
--- a/include/xsimd/arch/xsimd_avx512bw.hpp
+++ b/include/xsimd/arch/xsimd_avx512bw.hpp
@@ -497,6 +497,28 @@ namespace xsimd
             {
                 return { (Is >= N ? Is - N : 0)... };
             }
+
+            struct slide_perm_hi_generator
+            {
+                static constexpr uint64_t get(size_t i, size_t n) noexcept { return (i == 0 ? 8 : i - 1); }
+            };
+
+            struct slide_perm_low_generator
+            {
+                static constexpr uint64_t get(size_t i, size_t n) noexcept { return i + 1; }
+            };
+
+            template <size_t N>
+            struct slide_left_pattern_generator
+            {
+                static constexpr uint16_t get(size_t i, size_t n) noexcept { return (i >= N ? static_cast<uint16_t>(i - N) : 0); }
+            };
+
+            template <size_t N>
+            struct slide_right_pattern_generator
+            {
+                static constexpr uint16_t get(size_t i, size_t n) noexcept { return (i < (32 - N) ? static_cast<uint16_t>(i + N) : 0); }
+            };
         }
 
         template <size_t N, class A, class T>
@@ -520,10 +542,9 @@ namespace xsimd
                 buffer[0] = buffer[0] << 8;
                 xx = _mm512_load_epi64(&buffer[0]);
 
-                alignas(A::alignment()) auto slide_perm = detail::make_slide_perm_hi(::xsimd::detail::make_index_sequence<512 / 64>());
                 __m512i xl = _mm512_slli_epi64(x, 8);
                 __m512i xr = _mm512_srli_epi64(x, 56);
-                xr = _mm512_permutex2var_epi64(xr, _mm512_load_epi64(slide_perm.data()), _mm512_setzero_si512());
+                xr = _mm512_permutex2var_epi64(xr, make_batch_constant<uint64_t, detail::slide_perm_hi_generator, A>().as_batch(), _mm512_setzero_si512());
                 xx = _mm512_or_si512(xr, xl);
                 if (N == 1)
                     return xx;
@@ -533,8 +554,7 @@ namespace xsimd
                 xx = x;
             }
             __mmask32 mask = 0xFFFFFFFFu << ((N / 2) & 31);
-            alignas(A::alignment()) auto slide_pattern = detail::make_slide_left_pattern<N / 2>(::xsimd::detail::make_index_sequence<512 / 16>());
-            return _mm512_maskz_permutexvar_epi16(mask, _mm512_load_epi32(slide_pattern.data()), xx);
+            return _mm512_maskz_permutexvar_epi16(mask, make_batch_constant<uint16_t, detail::slide_left_pattern_generator<(N / 2)>, A>().as_batch(), xx);
         }
 
         // slide_right
@@ -566,10 +586,9 @@ namespace xsimd
             batch<T, A> xx;
             if (N & 1)
             {
-                alignas(A::alignment()) auto slide_perm = detail::make_slide_perm_low(::xsimd::detail::make_index_sequence<512 / 64>());
                 __m512i xr = _mm512_srli_epi64(x, 8);
                 __m512i xl = _mm512_slli_epi64(x, 56);
-                xl = _mm512_permutex2var_epi64(xl, _mm512_load_epi64(slide_perm.data()), _mm512_setzero_si512());
+                xl = _mm512_permutex2var_epi64(xl, make_batch_constant<uint64_t, detail::slide_perm_low_generator, A>().as_batch(), _mm512_setzero_si512());
                 xx = _mm512_or_si512(xr, xl);
                 if (N == 1)
                     return xx;
@@ -579,8 +598,7 @@ namespace xsimd
                 xx = x;
             }
             __mmask32 mask = 0xFFFFFFFFu >> ((N / 2) & 31);
-            alignas(A::alignment()) auto slide_pattern = detail::make_slide_right_pattern<N / 2>(::xsimd::detail::make_index_sequence<512 / 16>());
-            return _mm512_maskz_permutexvar_epi16(mask, _mm512_load_epi32(slide_pattern.data()), xx);
+            return _mm512_maskz_permutexvar_epi16(mask, make_batch_constant<uint16_t, detail::slide_right_pattern_generator<(N / 2)>, A>().as_batch(), xx);
         }
 
         // ssub
diff --git a/include/xsimd/arch/xsimd_avx512vbmi.hpp b/include/xsimd/arch/xsimd_avx512vbmi.hpp
index 680b238..a5599ac 100644
--- a/include/xsimd/arch/xsimd_avx512vbmi.hpp
+++ b/include/xsimd/arch/xsimd_avx512vbmi.hpp
@@ -37,6 +37,18 @@ namespace xsimd
             {
                 return { (Is < (64 - N) ? Is + N : 0)... };
             }
+
+            template <size_t N>
+            struct slide_left_bytes_pattern_generator
+            {
+                static constexpr uint8_t get(size_t i, size_t n) noexcept { return (i >= N ? static_cast<uint8_t>(i - N) : 0); }
+            };
+
+            template <size_t N>
+            struct slide_right_bytes_pattern_generator
+            {
+                static constexpr uint8_t get(size_t i, size_t n) noexcept { return (i < (64 - N) ? static_cast<uint8_t>(i + N) : 0); }
+            };
         }
 
         // slide_left
@@ -53,8 +65,7 @@ namespace xsimd
             }
 
             __mmask64 mask = 0xFFFFFFFFFFFFFFFFull << (N & 63);
-            alignas(A::alignment()) auto slide_pattern = detail::make_slide_left_bytes_pattern<N>(::xsimd::detail::make_index_sequence<512 / 8>());
-            return _mm512_maskz_permutexvar_epi8(mask, _mm512_load_epi32(slide_pattern.data()), x);
+            return _mm512_maskz_permutexvar_epi8(mask, make_batch_constant<uint8_t, detail::slide_left_bytes_pattern_generator<N>, A>().as_batch(), x);
         }
 
         // slide_right
@@ -70,8 +81,7 @@ namespace xsimd
                 return batch<T, A>(T(0));
             }
             __mmask64 mask = 0xFFFFFFFFFFFFFFFFull >> (N & 63);
-            alignas(A::alignment()) auto slide_pattern = detail::make_slide_right_bytes_pattern<N>(::xsimd::detail::make_index_sequence<512 / 8>());
-            return _mm512_maskz_permutexvar_epi8(mask, _mm512_load_epi32(slide_pattern.data()), x);
+            return _mm512_maskz_permutexvar_epi8(mask, make_batch_constant<uint8_t, detail::slide_right_bytes_pattern_generator<N>, A>().as_batch(), x);
         }
 
         // swizzle (dynamic version)
