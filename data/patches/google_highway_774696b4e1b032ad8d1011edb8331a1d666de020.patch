diff --git a/hwy/ops/x86_128-inl.h b/hwy/ops/x86_128-inl.h
index 1574d4e8..1d970029 100644
--- a/hwy/ops/x86_128-inl.h
+++ b/hwy/ops/x86_128-inl.h
@@ -6567,9 +6567,12 @@ HWY_API Vec128<int64_t> ConvertTo(DI di, const Vec128<double> v) {
 #if HWY_TARGET <= HWY_AVX3 && HWY_ARCH_X86_64
   return detail::FixConversionOverflow(di, v, _mm_cvttpd_epi64(v.raw));
 #elif HWY_ARCH_X86_64
-  const __m128i i0 = _mm_cvtsi64_si128(_mm_cvttsd_si64(v.raw));
-  const Full64<double> dd2;
-  const __m128i i1 = _mm_cvtsi64_si128(_mm_cvttsd_si64(UpperHalf(dd2, v).raw));
+  // Use scalar conversions on x86_64 for the two lanes which is typically
+  // faster than the generic bit-manipulation fallback.
+  const __m128d lo = v.raw;
+  const __m128d lo_hi = _mm_unpackhi_pd(lo, lo);
+  const __m128i i0 = _mm_cvtsi64_si128(_mm_cvttsd_si64(lo));
+  const __m128i i1 = _mm_cvtsi64_si128(_mm_cvttsd_si64(lo_hi));
   return detail::FixConversionOverflow(di, v, _mm_unpacklo_epi64(i0, i1));
 #else
   using VI = VFromD<decltype(di)>;
diff --git a/hwy/ops/x86_256-inl.h b/hwy/ops/x86_256-inl.h
index 8e180aad..32c2e0e2 100644
--- a/hwy/ops/x86_256-inl.h
+++ b/hwy/ops/x86_256-inl.h
@@ -4608,6 +4608,14 @@ template <class D, HWY_IF_I64_D(D)>
 HWY_API Vec256<int64_t> ConvertTo(D di, Vec256<double> v) {
 #if HWY_TARGET <= HWY_AVX3
   return detail::FixConversionOverflow(di, v, _mm256_cvttpd_epi64(v.raw));
+#elif HWY_ARCH_X86_64
+  // Convert each 128-bit half using the 128-bit ConvertTo implementation and
+  // combine; scalar conversions are typically faster than the generic
+  // bit-manipulation fallback on x86_64.
+  const auto dh = Half<decltype(di)>();
+  const Vec128<int64_t> lo = ConvertTo(dh, LowerHalf(dh, v));
+  const Vec128<int64_t> hi = ConvertTo(dh, UpperHalf(dh, v));
+  return Combine(di, hi, lo);
 #else
   using VI = decltype(Zero(di));
   const VI k0 = Zero(di);
