diff --git a/include/LoadBalancerAC.hpp b/include/LoadBalancerAC.hpp
index e94639f1..86f432a5 100644
--- a/include/LoadBalancerAC.hpp
+++ b/include/LoadBalancerAC.hpp
@@ -18,6 +18,7 @@
 
 #include <OmpLock.hpp>
 #include <stdint.h>
+#include <atomic>
 
 namespace primecount {
 
@@ -30,13 +31,13 @@ public:
 private:
   void print_status();
 
-  int64_t low_ = 0;
+  std::atomic<int64_t> low_ = 0;
   int64_t sqrtx_ = 0;
   int64_t y_ = 0;
-  int64_t segment_size_ = 0;
+  std::atomic<int64_t> segment_size_ = 0;
   int64_t max_segment_size_ = 0;
-  int64_t segment_nr_ = 0;
-  int64_t total_segments_ = 0;
+  std::atomic<int64_t> segment_nr_ = 0;
+  std::atomic<int64_t> total_segments_ = 0;
   double time_ = 0;
   int threads_ = 0;
   bool is_print_ = false;
diff --git a/src/gourdon/LoadBalancerAC.cpp b/src/gourdon/LoadBalancerAC.cpp
index 711feaf6..46fcfc69 100644
--- a/src/gourdon/LoadBalancerAC.cpp
+++ b/src/gourdon/LoadBalancerAC.cpp
@@ -23,6 +23,7 @@
 #include <algorithm>
 #include <iostream>
 #include <sstream>
+#include <atomic>
 
 namespace {
 
@@ -51,25 +52,28 @@ LoadBalancerAC::LoadBalancerAC(int64_t sqrtx,
   // The default segment size is x^(1/4). This
   // is tiny, will fit into the CPU's cache.
   int64_t x14 = isqrt(sqrtx);
-  segment_size_ = x14;
+  segment_size_.store(x14, std::memory_order_relaxed);
 
   // When a single thread is used (and printing is
   // disabled) we can use a segment size larger
   // than x^(1/4) because load balancing is only
   // useful for multi-threading.
   if (threads == 1 && !is_print)
-    segment_size_ = std::max(x14, l2_segment_size);
+  {
+    int64_t ss = std::max(x14, l2_segment_size);
+    segment_size_.store(ss, std::memory_order_relaxed);
+  }
 
-  segment_size_ = std::max(min_segment_size, segment_size_);
-  segment_size_ = SegmentedPiTable::get_segment_size(segment_size_);
-  total_segments_ = ceil_div(sqrtx, segment_size_);
+  {
+    int64_t ss = std::max(min_segment_size, segment_size_.load(std::memory_order_relaxed));
+    ss = SegmentedPiTable::get_segment_size(ss);
+    segment_size_.store(ss, std::memory_order_relaxed);
+    total_segments_.store(ceil_div(sqrtx, ss), std::memory_order_relaxed);
+    max_segment_size_ = std::max(l2_segment_size, ss);
+  }
 
-  // Most special leaves are below y (~ x^(1/3) * log(x)).
-  // We make sure this interval is evenly distributed
-  // amongst all threads by using a small segment size.
-  // Above y we use a larger segment size but still ensure
-  // that it fits into the CPU's cache.
-  max_segment_size_ = std::max(l2_segment_size, segment_size_);
+  low_.store(0, std::memory_order_relaxed);
+  segment_nr_.store(0, std::memory_order_relaxed);
 
   print_status();
 }
@@ -83,10 +87,11 @@ bool LoadBalancerAC::get_work(int64_t& low,
 
   LockGuard lockGuard(lock_);
 
-  if (low_ >= sqrtx_)
+  int64_t curr_low = low_.load(std::memory_order_relaxed);
+  if (curr_low >= sqrtx_)
     return false;
 
-  int64_t remaining_dist = sqrtx_ - low_;
+  int64_t remaining_dist = sqrtx_ - curr_low;
   int64_t thread_segment_size = high - low;
 
   // Most special leaves are below y (~ x^(1/3) * log(x)).
@@ -94,22 +99,27 @@ bool LoadBalancerAC::get_work(int64_t& low,
   // amongst all threads by using a small segment size.
   // Above y we increase the segment size by 2x if the
   // thread runtime is close to 0.
-  if (low_ > y_ &&
+  int64_t seg_size = segment_size_.load(std::memory_order_relaxed);
+  int64_t seg_nr = segment_nr_.load(std::memory_order_relaxed);
+
+  if (curr_low > y_ &&
       thread_secs < 0.01 &&
-      thread_segment_size >= segment_size_ &&
-      segment_size_ * (threads_ * 4) < remaining_dist)
+      thread_segment_size >= seg_size &&
+      seg_size * (static_cast<int64_t>(threads_) * 4) < remaining_dist)
   {
     int64_t increase_factor = 2;
-    segment_size_ = std::min(segment_size_ * increase_factor, max_segment_size_);
-    segment_size_ = SegmentedPiTable::get_segment_size(segment_size_);
-    total_segments_ = segment_nr_ + ceil_div(remaining_dist, segment_size_);
+    int64_t new_seg = std::min(seg_size * increase_factor, max_segment_size_);
+    new_seg = SegmentedPiTable::get_segment_size(new_seg);
+    segment_size_.store(new_seg, std::memory_order_relaxed);
+    total_segments_.store(seg_nr + ceil_div(remaining_dist, new_seg), std::memory_order_relaxed);
+    seg_size = new_seg;
   }
 
-  low = low_;
-  high = low + segment_size_;
+  low = curr_low;
+  high = low + seg_size;
   high = std::min(high, sqrtx_);
-  low_ = high;
-  segment_nr_++;
+  low_.store(high, std::memory_order_relaxed);
+  segment_nr_.fetch_add(1, std::memory_order_relaxed);
   print_status();
 
   return low < sqrtx_;
@@ -128,8 +138,10 @@ void LoadBalancerAC::print_status()
       time_ = time;
       std::ostringstream status;
       // Clear line because total_segments_ may become smaller
+      int64_t seg_nr = segment_nr_.load(std::memory_order_relaxed);
+      int64_t tot_seg = total_segments_.load(std::memory_order_relaxed);
       status << "\r                                    "
-             << "\rSegments: " << segment_nr_ << '/' << total_segments_;
+             << "\rSegments: " << seg_nr << '/' << tot_seg;
       std::cout << status.str() << std::flush;
     }
   }
