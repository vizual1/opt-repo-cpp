diff --git a/cache/cmake-dep.json b/cache/cmake-dep.json
index 281c1af..703e61f 100644
--- a/cache/cmake-dep.json
+++ b/cache/cmake-dep.json
@@ -1076,7 +1076,11 @@
         }
     },
     "x11": {
-        "apt": ["libx11-dev", "libxi-dev", "libxtst-dev"],
+        "apt": [
+            "libx11-dev",
+            "libxi-dev",
+            "libxtst-dev"
+        ],
         "vcpkg": "libx11",
         "flags": {
             "apt": [
@@ -1158,7 +1162,13 @@
         }
     },
     "ffmpeg": {
-        "apt": ["ffmpeg", "libavcodec-dev", "libavformat-dev", "libavutil-dev" , "libswscale-dev"],
+        "apt": [
+            "ffmpeg",
+            "libavcodec-dev",
+            "libavformat-dev",
+            "libavutil-dev",
+            "libswscale-dev"
+        ],
         "vcpkg": "ffmpeg",
         "flags": {
             "apt": [
diff --git a/data/commits/filtered.txt b/data/commits/filtered.txt
old mode 100644
new mode 100755
diff --git a/data/testcrawl_3285_fullrun.txt b/data/testcrawl_3285_fullrun.txt
old mode 100644
new mode 100755
diff --git a/src/__init__.py b/src/__init__.py
old mode 100644
new mode 100755
diff --git a/src/cmake/__init__.py b/src/cmake/__init__.py
old mode 100644
new mode 100755
diff --git a/src/cmake/analyzer.py b/src/cmake/analyzer.py
old mode 100644
new mode 100755
index 07af747..ade8c29
--- a/src/cmake/analyzer.py
+++ b/src/cmake/analyzer.py
@@ -1,49 +1,49 @@
-from src.cmake.parser import CMakeParser
-from src.config.constants import DOCKER_IMAGE_MAP
-from pathlib import Path
-
-class CMakeAnalyzer:
-    """High-level interface for analyzing CMake repositories."""
-    def __init__(self, root: Path):
-        self.root = root
-        self.parser = CMakeParser(self.root)
-
-    def reset(self) -> None:
-        self.root = self.root
-        self.parser = CMakeParser(self.root)
-
-    def has_root_cmake(self) -> bool:
-        return self.parser.has_root_cmake()
-
-    def has_testing(self, nolist: bool = False) -> bool:
-        list_tests: bool = self.parser.can_list_tests()
-        return (self.parser.find_enable_testing() and (nolist or list_tests) and 
-               (self.parser.find_add_tests() or self.parser.find_discover_tests()))
-    
-    def get_list_test_arg(self) -> list[tuple[str, str]]:
-        return list(self.parser.list_test_arg)
-
-    def has_build_testing_flag(self) -> dict[str, dict[str, str]]:
-        return self.parser.find_test_flags()
-    
-    def get_enable_testing_path(self) -> list[Path]:
-        return self.parser.enable_testing_path
-    
-    def parse_ctest_file(self, text: str) -> list[str]:
-        return self.parser.parse_ctest_file(text)
-    
-    def parse_subdirs(self, text: str) -> list[str]:
-        return self.parser.parse_subdirs(text)
-    
-    def find_unit_tests(self, text: str, framework: str) -> list[str]:
-        return self.parser.find_unit_tests(text, framework)
-
-    def get_dependencies(self) -> set[str]:
-        deps = self.parser.find_dependencies()
-        return deps
-    
-    def get_ubuntu_version(self) -> str:
-        return self.parser.get_ubuntu_for_cmake(self.parser.find_cmake_minimum_required())
-
-    def get_docker(self) -> str:
-        return DOCKER_IMAGE_MAP[self.get_ubuntu_version()]
+from src.cmake.parser import CMakeParser
+from src.config.constants import DOCKER_IMAGE_MAP
+from pathlib import Path
+
+class CMakeAnalyzer:
+    """High-level interface for analyzing and parsing CMakeLists.txt files."""
+    def __init__(self, repo_path: Path):
+        self.root = repo_path
+        self.parser = CMakeParser(self.root)
+
+    def reset(self) -> None:
+        self.root = self.root
+        self.parser = CMakeParser(self.root)
+
+    def has_root_cmake(self) -> bool:
+        return self.parser.has_root_cmake()
+
+    def has_testing(self, nolist: bool = False) -> bool:
+        list_tests: bool = self.parser.can_list_tests()
+        return (self.parser.find_enable_testing() and (nolist or list_tests) and 
+               (self.parser.find_add_tests() or self.parser.find_discover_tests()))
+    
+    def get_list_test_arg(self) -> list[tuple[str, str]]:
+        return list(self.parser.list_test_arg)
+
+    def extract_build_testing_flag(self) -> dict[str, dict[str, str]]:
+        return self.parser.find_cmake_test_flags()
+    
+    def get_enable_testing_path(self) -> list[Path]:
+        return self.parser.enable_testing_path
+    
+    def parse_ctest_file(self, text: str) -> list[str]:
+        return self.parser.parse_ctest_file(text)
+    
+    def parse_subdirs(self, text: str) -> list[str]:
+        return self.parser.parse_cmake_subdirs(text)
+    
+    def extract_unit_tests(self, text: str, framework: str) -> list[str]:
+        return self.parser.extract_unit_tests(text, framework)
+
+    def get_dependencies(self) -> set[str]:
+        deps = self.parser.find_dependencies()
+        return deps
+    
+    def get_ubuntu_version(self) -> str:
+        return self.parser.get_ubuntu_for_cmake(self.parser.find_cmake_minimum_required())
+
+    def get_docker(self) -> str:
+        return DOCKER_IMAGE_MAP[self.get_ubuntu_version()]
diff --git a/src/cmake/parser.py b/src/cmake/parser.py
old mode 100644
new mode 100755
index 1b9a256..86330b0
--- a/src/cmake/parser.py
+++ b/src/cmake/parser.py
@@ -1,434 +1,513 @@
-import os, logging, re
-from cmakeast.printer import ast
-from typing import Optional, Generator
-from pathlib import Path
-
-class CMakeParser:
-    def __init__(self, root: Path):
-        self.root = root
-
-        self.enable_testing_path: list[Path] = []
-        self.add_test_path: list[Path] = []
-        self.discover_tests_path: list[Path] = []
-        self.target_link_path: list[Path] = []
-        self.list_test_arg: set[tuple[str, str]] = set()
-
-        self._cmake_files: Optional[list[Path]] = None
-        self._cmake_function_calls: Optional[list[tuple[ast.FunctionCall, Path]]] = None
-
-    @property
-    def cmake_files(self) -> list[Path]:
-        if self._cmake_files is None:
-            self._cmake_files = self.find_files(search="CMakeLists.txt")
-        return self._cmake_files
-
-    @property
-    def cmake_function_calls(self) -> list[tuple[ast.FunctionCall, Path]]:
-        if self._cmake_function_calls is None:
-            self._cmake_function_calls = self._find_all_function_calls(self.cmake_files)
-        return self._cmake_function_calls
-
-    def has_root_cmake(self) -> bool:
-        return (self.root / "CMakeLists.txt").exists()
-
-    def find_files(self, search: str = "", pattern: Optional[re.Pattern] = None) -> list[Path]:
-        """Search all files 'search' from root."""
-        found_files: list[Path] = []
-        for root, _, files in os.walk(self.root):
-            for file in files: 
-                if file == search or (pattern and pattern.match(file)):
-                    found_files.append(Path(root, file))
-        return found_files
-    
-    def find_cmake_minimum_required(self) -> str:
-        cmake_file: Path = Path(self.root, "CMakeLists.txt")
-        root_function_calls: list[tuple[ast.FunctionCall, Path]] = self._find_all_function_calls([cmake_file]) 
-        all_function_calls = self._find_function_calls(name="cmake_minimum_required", fcalls=root_function_calls)
-        cmake_minimum_calls, cf = all_function_calls[0] if 0 < len(all_function_calls) else ('', '')
-        
-        if not cmake_minimum_calls:
-            logging.warning("No cmake_minimum_required found, using default 3.16")
-            return "3.16"
-        
-        arguments: list = cmake_minimum_calls.arguments
-
-        for arg in arguments:
-            arg_str = str(arg)
-            version_match = re.search(r'(\d+\.\d+(?:\.\d+)*)', arg_str)
-            if version_match:
-                logging.info(f"cmake_minimum_required found: {version_match.group()}")
-                return version_match.group(1)
-        
-        logging.warning("No cmake_minimum_required found, using default 3.16")
-        return "3.16"
-    
-    def find_ctest_exec(self) -> list[str]:
-        logging.debug("Searching for ctest executables...")
-        test_files = self.find_files("CTestTestfile.cmake")
-        self.ctest_function_calls: list[tuple[ast.FunctionCall, Path]] = self._find_all_function_calls(test_files) 
-        all_exec: list[str] = []
-        calls = self._find_function_calls(name="add_test", fcalls=self.ctest_function_calls)
-        for call, tf in calls:
-            arguments: list = call.arguments
-            if len(arguments) == 2:
-                name = arguments[0].contents if hasattr(arguments[0], "contents") else ""
-                exec = arguments[1].contents if hasattr(arguments[1], "contents") else ""
-                if exec:
-                    all_exec.append(exec)
-        logging.debug("Found ctest executables:")
-        for exec in all_exec:
-            logging.debug(f"  -./{exec}")
-        return all_exec
-    
-    def find_enable_testing(self) -> bool:
-        """
-        Checks if enable_testing() is called anywhere in CMakeLists.txt. 
-        Either CMakeLists.txt calls enable_testing() directly or it calls enable_testing() via include(CTest).
-        """
-        logging.debug("Searching for enable_testing()...")
-        calls = self._find_function_calls(name="enable_testing")
-        calls += self._find_function_calls(name="include", _args=["CTest"])
-        for _, cf in calls:
-            self.enable_testing_path.append(Path(cf))
-        if self.enable_testing_path:
-            logging.debug(f"CMakeLists.txt with enable_testing(): {str(self.enable_testing_path)}.")
-            return True
-        
-        return False
-    
-    def find_add_tests(self) -> bool:
-        logging.debug("Search for add_tests...")
-        calls = self._find_function_calls(name="add_test")
-        for _, cf in calls:
-            self.add_test_path.append(Path(cf))
-        if self.add_test_path:
-            logging.debug(f"CMakeLists.txt with add_test(): {str(self.add_test_path)}.")
-            return True
-        
-        return False
-    
-    def find_discover_tests(self) -> bool:
-        logging.info("Search for *_discover_tests...")
-        calls = self._find_function_calls(ends="_discover_tests")
-        for _, cf in calls:
-            self.discover_tests_path.append(Path(cf))
-        if self.discover_tests_path:
-            logging.info(f"CMakeLists.txt with *_discover_tests(): {str(self.discover_tests_path)}.")
-            return True
-        
-        return False
-    
-    def can_list_tests(self) -> bool:
-        libraries: list[str] = [
-            "GTest::gtest", "GTest::gtest_main", "GTest::gmock", "GTest::gmock_main", 
-            "gtest", "gtest_main", "gmock", "gmock_main",
-            "Catch2::Catch2", "Catch::Main",
-            "Catch2::Catch2WithMain", "Catch2::Catch2WithMainNoExit", "Catch2::Catch2WithRunner"
-            "doctest", "doctest::doctest", "doctest::doctest_main",
-            "Boost::unit_test_framework", "boost_unit_test_framework",
-            "Qt::Test", "Qt5::Test", "Qt6::Test"
-        ]
-
-        logging.debug("Searching for target_link_libraries to isolate test cases...")
-        all_calls: list[tuple[ast.FunctionCall, Path]] = []
-        for library in libraries:
-            calls = self._find_function_calls(name="target_link_libraries", _args=[library])
-            for _, cf in calls:
-                logging.debug(f"target_link_libraries found {library} in {cf}.")
-                all_calls += calls
-                if "gtest" in library.lower():
-                    # run individual tests with ./test_executable --gtest_filter=TESTNAME
-                    self.list_test_arg.add(("gtest", "--gtest_list_tests"))
-                elif "catch" in library.lower():
-                    # run individual tests with ./test_executable TESTNAME
-                    self.list_test_arg.add(("catch", "--list-tests")) #maybe also --list-test-cases possible
-                elif "doctest" in library.lower():
-                    # run individual tests with ./test_executable TESTNAME
-                    self.list_test_arg.add(("doctest", "--list-test-cases")) #maybe also --list-test-suites possible
-                elif "boost" in library.lower():
-                    # run individual tests with ./test_executable --run_test=suite/test
-                    self.list_test_arg.add(("boost", "--list_content"))
-                elif "qt" in library.lower():
-                    # run individual tests wiht ./test_executable TESTNAME
-                    self.list_test_arg.add(("qt", "-functions"))
-
-        if all_calls:
-            logging.debug(f"target_link_libraries(...) to isolate test cases found.")
-            return True
-
-        return False
-    
-    def find_test_flags(self) -> dict[str, dict[str, str]]:
-        test_flags = {}
-
-        logging.debug("Searching for possible test flags...")
-        if "BUILD_TESTING" not in test_flags and self._find_function_calls(name="include", _args=["CTest"]):
-            test_flags["BUILD_TESTING"] = {
-                "desc": "Enable CTest-based testing",
-                "default": "ON"
-            }
-
-        options = self._find_function_calls(name="option")
-        for option, cf in options:
-            arguments: list = option.arguments
-            if len(arguments) == 0:
-                continue
-            arg_name = arguments[0].contents.strip() if hasattr(arguments[0], "contents") else None
-            if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
-                desc = arguments[1].contents.strip() if len(arguments) > 1 and hasattr(arguments[1], "contents") else "option"
-                default = arguments[2].contents.strip() if len(arguments) > 2 and hasattr(arguments[2], "contents") else ""
-                test_flags[arg_name] = {"type": "BOOL", "desc": desc, "default": default}
-
-        set_caches = self._find_function_calls(name="set")
-        for set_cache, cf in set_caches:
-            arguments: list = set_cache.arguments
-            if len(arguments) == 0:
-                continue
-            exist_cache = any(hasattr(argument, "contents") and argument.contents == "CACHE" for argument in arguments)
-            if exist_cache:
-                arg_name = arguments[0].contents if hasattr(arguments[0], "contents") else None
-                if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
-                    default = arguments[1].contents.strip() if len(arguments) > 1 and hasattr(arguments[1], "contents") else ""
-                    test_flags[arg_name] = {"default": default, "desc": "cache"}
-
-        branches = self._find_function_calls(name="if")
-        branches += self._find_function_calls(name="elseif")
-        for branch, cf in branches:
-            arguments: list = branch.arguments
-            for argument in arguments:
-                arg_name = argument.contents if hasattr(argument, "contents") else None
-                if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
-                    test_flags[arg_name] = {"type": "BOOL", "desc": f"Implicit test flag used in {cf}", "default": "undefined"}
-
-        logging.debug("Discovered test flags:")
-        for k, v in test_flags.items():
-            logging.debug(f"  - {k} (default={v['default']}): {v['desc']}")
-
-        return test_flags
-
-    def parse_ctest_file(self, text: str) -> list[str]:
-        executables = set()
-        # 1. Handle new form: add_test(NAME ... COMMAND ...)
-        for match in re.finditer(r"add_test\s*\(\s*NAME\s+\S+\s+COMMAND\s+([^\s\)]+)", text):
-            exec_path = match.group(1)
-            if not exec_path.startswith("$"):
-                executables.add(exec_path.strip('"').strip("'"))
-
-        # 2. Handle old form: add_test(<name> <exec>)
-        for match in re.finditer(r"add_test\s*\(\s*[^\s\)]+\s+([^\s\)]+)", text):
-            exec_path = match.group(1)
-            if not exec_path.startswith("$"):
-                executables.add(exec_path.strip('"').strip("'"))
-
-        return sorted(executables)
-    
-    def parse_subdirs(self, text: str) -> list[str]:
-        subdirs: list[str] = re.findall(r'subdirs\("([^"]+)"\)', text)
-        return [s.replace("\\", "/") for s in subdirs]
-    
-    def find_unit_tests(self, text: str, framework: str) -> list[str]:
-        tests = []
-
-        if framework == "gtest":
-            # Example:
-            # MySuite.
-            #   TestA
-            #   TestB
-            current_suite = ""
-            for line in text.splitlines():
-                if line.endswith("ms"):
-                    continue
-                if line.endswith('.'):
-                    current_suite = line.strip().strip('.')
-                elif line.strip():
-                    tests.append(f"{current_suite}.{line.strip()}")
-                
-        elif framework == "catch":
-            tests = [line.strip() for line in text.splitlines() 
-                     if line.strip() and 
-                     not "All available test cases:" in line.strip() and 
-                     not "test cases" in line.strip() and
-                     not line.strip().startswith("[") and 
-                     not line.strip().endswith("]") and 
-                     not line.strip().endswith("ms")]
-        elif framework == "doctest":
-            tests = []
-            for line in text.splitlines():
-                line = line.strip()
-                if not line:
-                    continue
-                if line.endswith("ms"):
-                    continue
-                if line.startswith("[doctest]"):
-                    continue
-                if set(line) == {"="}:
-                    continue
-                tests.append(line)
-        elif framework == "boost":
-            # Boost lists suites/tests as suite/test
-            tests = [line.strip() for line in text.splitlines() if "/" in line and not line.endswith("ms")]
-        elif framework == "qt":
-            # QTest lists test functions prefixed with "PASS", "FAIL", etc. when run
-            tests = [line.strip() for line in text.splitlines() if line.strip() and not line.endswith("ms")]
-        return tests
-
-    def find_dependencies(self) -> set[str]:
-        """Find CMake dependency names from CMakeLists.txt."""
-        logging.debug("Searching for possible dependencies...")
-        calls: list[tuple[ast.FunctionCall, Path]] = []
-        calls += self._find_function_calls(name="include", starts="Find")
-        calls += self._find_function_calls(name="find_package")
-        calls += self._find_function_calls(name="pkg_check_modules")
-        #calls += self._find_function_calls(name="target_link_libraries")
-        #calls += self._find_function_calls(name="include_directories")
-        #calls += self._find_function_calls(name="target_include_directories")
-        #calls += self._find_function_calls(name="add_subdirectory")
-            
-        packages: set[str] = set()
-        for call, _ in calls:
-            arguments = call.arguments
-            if arguments and hasattr(arguments[0], "contents"):
-                arg_name = arguments[0].contents.strip()
-                if self._valid_name(arg_name):
-                    if "::" in arg_name:
-                        packages.add(arg_name.split("::")[0])
-                    else:
-                        packages.add(arg_name)
-
-        logging.debug(f"Found possible dependencies: {packages}")
-        return packages
-
-    def _check_cmakeast_class(self, node) -> bool:
-        return hasattr(node, "__class__") and node.__class__.__module__.startswith("cmakeast")
-
-    def _walk_ast(self, node) -> Generator[ast.FunctionCall, None, None]:
-        if isinstance(node, list):
-            for item in node:
-                yield from self._walk_ast(item)
-        elif self._check_cmakeast_class(node):
-            yield node
-            for attr_name in dir(node):
-                if attr_name.startswith("_"):
-                    continue
-                try:
-                    value = getattr(node, attr_name)
-                    if isinstance(value, (list,)) or self._check_cmakeast_class(value):
-                        yield from self._walk_ast(value)
-                except Exception:
-                    continue
-
-    def _find_function_calls(self, name: str = "", _args: list[str] = [], starts: str = "", ends: str = "", fcalls: list[tuple[ast.FunctionCall, Path]] = []) -> list[tuple[ast.FunctionCall, Path]]:
-        calls: list[tuple[ast.FunctionCall, Path]] = []
-        if not fcalls:
-            fcalls = self.cmake_function_calls
-        for statement, cf in fcalls:
-            if (name and statement.name == name) or (ends and statement.name.endswith(ends)) or (starts and statement.name.startswith(starts)):
-                arguments: list = statement.arguments
-                if not _args:
-                    calls.append((statement, Path(cf)))
-                count = 0
-                for argument in arguments:
-                    if (hasattr(argument, "contents") and argument.contents.strip() in _args):
-                        count += 1
-                if count >= len(_args):
-                    calls.append((statement, Path(cf)))   
-        return calls
-    
-    def _find_all_function_calls(self, files: list[Path]) -> list[tuple[ast.FunctionCall, Path]]:
-        calls: list[tuple[ast.FunctionCall, Path]] = []
-        for cf in files:
-            with open(cf, 'r', errors='ignore') as file:
-                content = file.read()
-            try:
-                statements = ast.parse(content).statements
-            except Exception as e:
-                logging.warning(f"{cf} has an error: {e}")
-                continue
-            cmake_statements = self._walk_ast(statements)
-            for statement in cmake_statements:
-                if isinstance(statement, ast.FunctionCall):
-                    calls.append((statement, cf))
-        return calls
-    
-    def _valid_name(self, name: str) -> bool:
-        keywords = {
-            "REQUIRED", "OPTIONAL", "QUIET", "EXACT", "CONFIG", "NO_MODULE", "PRIVATE", 
-            "PUBLIC", "INTERFACE", "BEFORE", "IMPORTED_TARGET", "REGEX", "INCLUDE", "PATH",
-            "COMPONENTS"
-        }
-        if name.upper() in keywords:
-            return False
-        if "/" in name or "\\" in name or name.startswith("."):
-            return False # relative/absolute paths
-        if "{" in name or "}" in name:
-            return False # variables
-        if "\"" in name or "'" in name:
-            return False # string
-        if re.match(r'^\d+(\.\d+)*$', name):
-            return False # version numbers
-        if name.endswith(('.so', '.a', '.lib', '.dll', '.dylib', '.cmake', '.txt', '.h', '.cc', '.cpp')):
-            return False # files
-        if name.startswith(('test_', 'example_', 'demo_', 'benchmark_')):
-            return False # internal variables
-        if not name or len(name) < 2:
-            return False # single letter
-        if "<" in name or ">" in name:
-            return False
-        return True
-    
-    def get_ubuntu_for_cmake(self, cmake_version: str) -> str:
-        version_num = self._version_to_number(cmake_version)
-
-        if version_num <= 305:
-            return "ubuntu:16.04"
-        elif version_num <= 310:
-            return "ubuntu:18.04"
-        elif version_num <= 316:
-            return "ubuntu:20.04"
-        elif version_num <= 322:
-            return "ubuntu:22.04"
-        else:
-            return "ubuntu:24.04"
-
-    def _version_to_number(self, version_str: str) -> int:
-        """
-        Parses a cmake_minimum_required(VERSION ...) line and returns a numeric version (major*100 + minor).
-        If a range is given, takes the maximum version in the range.
-        If only a single version is given, picks the latest version known to be backward-compatible with it.
-        """
-        COMPATIBILITY_MAP = {
-            208: 300, 305: 322, 310: 327, 316: 327
-        }
-        LATEST_KNOWN = 327
-
-        try:
-            clean_version = re.sub(r'[^\d.\s.]', '', version_str)
-
-            if '...' in version_str:
-                versions = [v.strip() for v in clean_version.split('...') if v.strip()]
-                if len(versions) >= 2:
-                    parts1 = [int(x) for x in versions[0].split('.')]
-                    parts2 = [int(x) for x in versions[1].split('.')]
-                    version_to_use = versions[0] if parts1 > parts2 else versions[1]
-                else:
-                    return LATEST_KNOWN
-            else:
-                parts = clean_version.strip().split('.')
-                major = int(parts[0]) if len(parts) > 0 else 0
-                minor = int(parts[1]) if len(parts) > 1 else 0
-                min_version_num = major * 100 + minor
-
-                for lower_bound, upper_bound in sorted(COMPATIBILITY_MAP.items()):
-                    if min_version_num <= lower_bound:
-                        return upper_bound
-                return LATEST_KNOWN
-            
-            parts = version_to_use.split('.')
-            major = int(parts[0]) if len(parts) > 0 else 0
-            minor = int(parts[1]) if len(parts) > 1 else 0
-
-            return major * 100 + minor
-
-        except (ValueError, IndexError):
-            return LATEST_KNOWN
-
+import os, logging, re
+from cmakeast.printer import ast
+from typing import Optional, Generator
+from pathlib import Path
+
+class CMakeParser:
+    def __init__(self, root: Path):
+        self.root = root
+
+        self.enable_testing_path: list[Path] = []
+        self.add_test_path: list[Path] = []
+        self.discover_tests_path: list[Path] = []
+        self.target_link_path: list[Path] = []
+        self.list_test_arg: set[tuple[str, str]] = set()
+
+        self._cmake_files: Optional[list[Path]] = None
+        self._cmake_function_calls: Optional[list[tuple[ast.FunctionCall, Path]]] = None
+
+    @property
+    def cmake_files(self) -> list[Path]:
+        if self._cmake_files is None:
+            self._cmake_files = self.find_files(search="CMakeLists.txt")
+        return self._cmake_files
+
+    @property
+    def cmake_function_calls(self) -> list[tuple[ast.FunctionCall, Path]]:
+        if self._cmake_function_calls is None:
+            self._cmake_function_calls = self._find_all_function_calls(self.cmake_files)
+        return self._cmake_function_calls
+
+    def has_root_cmake(self) -> bool:
+        return (self.root / "CMakeLists.txt").exists()
+
+    def find_files(self, search: str = "", pattern: Optional[re.Pattern] = None) -> list[Path]:
+        """Search all files 'search' from root."""
+        found_files: list[Path] = []
+        for root, _, files in os.walk(self.root):
+            for file in files: 
+                if file == search or (pattern and pattern.match(file)):
+                    found_files.append(Path(root, file))
+        return found_files
+    
+    def find_cmake_minimum_required(self) -> str:
+        cmake_file: Path = Path(self.root, "CMakeLists.txt")
+        root_function_calls: list[tuple[ast.FunctionCall, Path]] = self._find_all_function_calls([cmake_file]) 
+        all_function_calls = self._find_function_calls(name="cmake_minimum_required", fcalls=root_function_calls)
+        cmake_minimum_calls, cf = all_function_calls[0] if 0 < len(all_function_calls) else ('', '')
+        
+        if not cmake_minimum_calls:
+            logging.warning("No cmake_minimum_required found, using default 3.16")
+            return "3.16"
+        
+        arguments: list = cmake_minimum_calls.arguments
+
+        for arg in arguments:
+            arg_str = str(arg)
+            version_match = re.search(r'(\d+\.\d+(?:\.\d+)*)', arg_str)
+            if version_match:
+                logging.info(f"cmake_minimum_required found: {version_match.group()}")
+                return version_match.group(1)
+        
+        logging.warning("No cmake_minimum_required found, using default 3.16")
+        return "3.16"
+    
+    def find_ctest_exec(self) -> list[str]:
+        logging.debug("Searching for ctest executables...")
+        test_files = self.find_files("CTestTestfile.cmake")
+        self.ctest_function_calls: list[tuple[ast.FunctionCall, Path]] = self._find_all_function_calls(test_files) 
+        all_exec: list[str] = []
+        calls = self._find_function_calls(name="add_test", fcalls=self.ctest_function_calls)
+        for call, tf in calls:
+            arguments: list = call.arguments
+            if len(arguments) == 2:
+                name = arguments[0].contents if hasattr(arguments[0], "contents") else ""
+                exec = arguments[1].contents if hasattr(arguments[1], "contents") else ""
+                if exec:
+                    all_exec.append(exec)
+        logging.debug("Found ctest executables:")
+        for exec in all_exec:
+            logging.debug(f"  -./{exec}")
+        return all_exec
+    
+    def find_enable_testing(self) -> bool:
+        """
+        Checks if enable_testing() is called anywhere in CMakeLists.txt. 
+        Either CMakeLists.txt calls enable_testing() directly or it calls enable_testing() via include(CTest).
+        """
+        logging.debug("Searching for enable_testing()...")
+        calls = self._find_function_calls(name="enable_testing")
+        calls += self._find_function_calls(name="include", _args=["CTest"])
+        for _, cf in calls:
+            self.enable_testing_path.append(Path(cf))
+        if self.enable_testing_path:
+            logging.debug(f"CMakeLists.txt with enable_testing(): {str(self.enable_testing_path)}.")
+            return True
+        
+        return False
+    
+    def find_add_tests(self) -> bool:
+        logging.debug("Search for add_tests...")
+        calls = self._find_function_calls(name="add_test")
+        for _, cf in calls:
+            self.add_test_path.append(Path(cf))
+        if self.add_test_path:
+            logging.debug(f"CMakeLists.txt with add_test(): {str(self.add_test_path)}.")
+            return True
+        
+        return False
+    
+    def find_discover_tests(self) -> bool:
+        logging.info("Search for *_discover_tests...")
+        calls = self._find_function_calls(ends="_discover_tests")
+        for _, cf in calls:
+            self.discover_tests_path.append(Path(cf))
+        if self.discover_tests_path:
+            logging.info(f"CMakeLists.txt with *_discover_tests(): {str(self.discover_tests_path)}.")
+            return True
+        
+        return False
+    
+    def can_list_tests(self) -> bool:
+        """
+        Checks if CTest is using some test framework that can list individual tests that can be run separately
+        """
+        libraries: list[str] = [
+            "GTest::gtest", "GTest::gtest_main", "GTest::gmock", "GTest::gmock_main", 
+            "gtest", "gtest_main", "gmock", "gmock_main",
+            "Catch2::Catch2", "Catch::Main",
+            "Catch2::Catch2WithMain", "Catch2::Catch2WithMainNoExit", "Catch2::Catch2WithRunner"
+            "doctest", "doctest::doctest", "doctest::doctest_main",
+            "Boost::unit_test_framework", "boost_unit_test_framework",
+            "Qt::Test", "Qt5::Test", "Qt6::Test"
+        ]
+
+        logging.debug("Searching for target_link_libraries to isolate test cases...")
+        all_calls: list[tuple[ast.FunctionCall, Path]] = []
+        for library in libraries:
+            calls = self._find_function_calls(name="target_link_libraries", _args=[library])
+            for _, cf in calls:
+                logging.debug(f"target_link_libraries found {library} in {cf}.")
+                all_calls += calls
+                if "gtest" in library.lower():
+                    # run individual tests with ./test_executable --gtest_filter=TESTNAME
+                    self.list_test_arg.add(("gtest", "--gtest_list_tests"))
+                elif "catch" in library.lower():
+                    # run individual tests with ./test_executable TESTNAME
+                    self.list_test_arg.add(("catch", "--list-tests")) #maybe also --list-test-cases possible
+                elif "doctest" in library.lower():
+                    # run individual tests with ./test_executable TESTNAME
+                    self.list_test_arg.add(("doctest", "--list-test-cases")) #maybe also --list-test-suites possible
+                elif "boost" in library.lower():
+                    # run individual tests with ./test_executable --run_test=suite/test
+                    self.list_test_arg.add(("boost", "--list_content"))
+                elif "qt" in library.lower():
+                    # run individual tests wiht ./test_executable TESTNAME
+                    self.list_test_arg.add(("qt", "-functions"))
+
+        if all_calls:
+            logging.debug(f"target_link_libraries(...) to isolate test cases found.")
+            return True
+
+        return False
+    
+    def find_cmake_test_flags(self) -> dict[str, dict[str, str]]:
+        test_flags = {}
+
+        logging.debug("Searching for possible test flags...")
+        if "BUILD_TESTING" not in test_flags and self._find_function_calls(name="include", _args=["CTest"]):
+            test_flags["BUILD_TESTING"] = {
+                "desc": "Enable CTest-based testing",
+                "default": "ON"
+            }
+
+        options = self._find_function_calls(name="option")
+        for option, cf in options:
+            arguments: list = option.arguments
+            if len(arguments) == 0:
+                continue
+            arg_name = arguments[0].contents.strip() if hasattr(arguments[0], "contents") else None
+            if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
+                desc = arguments[1].contents.strip() if len(arguments) > 1 and hasattr(arguments[1], "contents") else "option"
+                default = arguments[2].contents.strip() if len(arguments) > 2 and hasattr(arguments[2], "contents") else ""
+                test_flags[arg_name] = {"type": "BOOL", "desc": desc, "default": default}
+
+        set_caches = self._find_function_calls(name="set")
+        for set_cache, cf in set_caches:
+            arguments: list = set_cache.arguments
+            if len(arguments) == 0:
+                continue
+            exist_cache = any(hasattr(argument, "contents") and argument.contents == "CACHE" for argument in arguments)
+            if exist_cache:
+                arg_name = arguments[0].contents if hasattr(arguments[0], "contents") else None
+                if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
+                    default = arguments[1].contents.strip() if len(arguments) > 1 and hasattr(arguments[1], "contents") else ""
+                    test_flags[arg_name] = {"default": default, "desc": "cache"}
+
+        branches = self._find_function_calls(name="if")
+        branches += self._find_function_calls(name="elseif")
+        for branch, cf in branches:
+            arguments: list = branch.arguments
+            for argument in arguments:
+                arg_name = argument.contents if hasattr(argument, "contents") else None
+                if arg_name and self._valid_name(arg_name) and "TEST" in arg_name.upper():
+                    test_flags[arg_name] = {"type": "BOOL", "desc": f"Implicit test flag used in {cf}", "default": "undefined"}
+
+        logging.debug("Discovered test flags:")
+        for k, v in test_flags.items():
+            logging.debug(f"  - {k} (default={v['default']}): {v['desc']}")
+
+        return test_flags
+
+    def parse_ctest_file(self, text: str) -> list[str]:
+        executables = set()
+        # 1. Handle new form: add_test(NAME ... COMMAND ...)
+        for match in re.finditer(r"add_test\s*\(\s*NAME\s+\S+\s+COMMAND\s+([^\s\)]+)", text):
+            exec_path = match.group(1)
+            if not exec_path.startswith("$"):
+                executables.add(exec_path.strip('"').strip("'"))
+
+        # 2. Handle old form: add_test(<name> <exec>)
+        for match in re.finditer(r"add_test\s*\(\s*[^\s\)]+\s+([^\s\)]+)", text):
+            exec_path = match.group(1)
+            if not exec_path.startswith("$"):
+                executables.add(exec_path.strip('"').strip("'"))
+
+        return sorted(executables)
+    
+    def parse_cmake_subdirs(self, text: str) -> list[str]:
+        subdirs: list[str] = re.findall(r'subdirs\("([^"]+)"\)', text)
+        return [s.replace("\\", "/") for s in subdirs]
+    
+    def _clean(self, s: str) -> str:
+        s = re.sub(r'\x1B\[[0-?]*[ -/]*[@-~]', '', s)
+        s = ''.join(ch for ch in s if ch.isprintable() or ch in "\n\t ")
+        return s
+    
+    def extract_unit_tests(self, text: str, framework: str) -> list[str]:
+        """Extracts the unit tests in <text> according to the test <framework>"""
+        tests = []
+
+        if framework == "gtest":
+            return self.extract_gtest(text)
+        elif framework == "catch":
+            return self.extract_catch(text)
+        elif framework == "doctest":
+            return self.extract_doctest(text)
+        elif framework == "boost":
+            return self.extract_boost(text)
+        elif framework == "qt":
+            return self.extract_qt(text)
+        return tests
+    
+    def extract_gtest(self, text: str) -> list[str]:
+        suites = 0
+        tests = []
+        current_suite = None
+
+        for raw in text.splitlines():
+            line = raw.rstrip()
+            line = self._clean(line)
+
+            if any(x in line for x in (
+                "[ RUN", "[ PASSED", "[ FAILED", "Running main()"
+            )):
+                return []
+
+            if line.endswith("ms") and not line.startswith(' '):
+                continue
+            elif line.endswith('.') and not line.startswith(' '):
+                current_suite = line[:-1]
+                suites += 1
+            elif current_suite and line.startswith('  '):
+                tests.append(f"{current_suite}.{line.strip()}")
+
+        if suites == 0 or not tests:
+            return []
+
+        return tests
+
+    def extract_catch(self, text: str) -> list[str]:
+        if "All available test cases:" not in text:
+            return []
+
+        tests = []
+        for line in text.splitlines():
+            line = line.strip()
+            line = self._clean(line)
+
+            if line.endswith("ms"):
+                continue
+            if not line or line.startswith("["):
+                continue
+            if line.endswith("tests"):
+                continue
+
+            tests.append(line)
+
+        return tests if tests else []
+
+    def extract_doctest(self, text: str) -> list[str]:
+        if "====" not in text and "[doctest]" not in text:
+            return []
+
+        tests = []
+        for line in text.splitlines():
+            line = line.strip()
+            line = self._clean(line)
+            if not line or line.startswith("[doctest]"):
+                continue
+            if set(line) == {"="}:
+                continue
+            if line.endswith("ms"):
+                continue
+            tests.append(line)
+
+        return tests if tests else []
+
+    def extract_boost(self, text: str) -> list[str]:
+        tests = []
+        for line in text.splitlines():
+            line = line.strip()
+            line = self._clean(line)
+            if "/" in line and not line.startswith("Running"):
+                tests.append(line)
+
+        return tests if tests else []
+
+    def extract_qt(self, text: str) -> list[str]:
+        tests = []
+        for line in text.splitlines():
+            line = line.strip()
+            line = self._clean(line)
+            if "::" in line and not any(x in line for x in ("PASS", "FAIL")):
+                tests.append(line)
+
+        return tests if tests else []
+
+
+    def find_dependencies(self) -> set[str]:
+        """Find CMake dependency names from CMakeLists.txt."""
+        logging.debug("Searching for possible dependencies...")
+        calls: list[tuple[ast.FunctionCall, Path]] = []
+        calls += self._find_function_calls(name="include", starts="Find")
+        calls += self._find_function_calls(name="find_package")
+        calls += self._find_function_calls(name="pkg_check_modules")
+        #calls += self._find_function_calls(name="target_link_libraries")
+        #calls += self._find_function_calls(name="include_directories")
+        #calls += self._find_function_calls(name="target_include_directories")
+        #calls += self._find_function_calls(name="add_subdirectory")
+            
+        packages: set[str] = set()
+        for call, _ in calls:
+            arguments = call.arguments
+            if arguments and hasattr(arguments[0], "contents"):
+                arg_name = arguments[0].contents.strip()
+                if self._valid_name(arg_name):
+                    if "::" in arg_name:
+                        packages.add(arg_name.split("::")[0])
+                    else:
+                        packages.add(arg_name)
+
+        logging.debug(f"Found possible dependencies: {packages}")
+        return packages
+
+    def _check_cmakeast_class(self, node) -> bool:
+        return hasattr(node, "__class__") and node.__class__.__module__.startswith("cmakeast")
+
+    def _walk_ast(self, node) -> Generator[ast.FunctionCall, None, None]:
+        """
+        Walks the CMake AST generated by the CMakeAst library
+        """
+        if isinstance(node, list):
+            for item in node:
+                yield from self._walk_ast(item)
+        elif self._check_cmakeast_class(node):
+            yield node
+            for attr_name in dir(node):
+                if attr_name.startswith("_"):
+                    continue
+                try:
+                    value = getattr(node, attr_name)
+                    if isinstance(value, (list,)) or self._check_cmakeast_class(value):
+                        yield from self._walk_ast(value)
+                except Exception:
+                    continue
+
+    def _find_function_calls(self, name: str = "", _args: list[str] = [], starts: str = "", ends: str = "", fcalls: list[tuple[ast.FunctionCall, Path]] = []) -> list[tuple[ast.FunctionCall, Path]]:
+        """
+        Find a specific function call <name> with specific <_args> arguments or arguments that <starts> or <ends> with a specific string according to CMakeAst library
+        """
+        calls: list[tuple[ast.FunctionCall, Path]] = []
+        if not fcalls:
+            fcalls = self.cmake_function_calls
+        for statement, cf in fcalls:
+            if (name and statement.name == name) or (ends and statement.name.endswith(ends)) or (starts and statement.name.startswith(starts)):
+                arguments: list = statement.arguments
+                if not _args:
+                    calls.append((statement, Path(cf)))
+                count = 0
+                for argument in arguments:
+                    if (hasattr(argument, "contents") and argument.contents.strip() in _args):
+                        count += 1
+                if count >= len(_args):
+                    calls.append((statement, Path(cf)))   
+        return calls
+    
+    def _find_all_function_calls(self, files: list[Path]) -> list[tuple[ast.FunctionCall, Path]]:
+        """
+        Finds all function calls in CMake according to CMakeAst library
+        """
+        calls: list[tuple[ast.FunctionCall, Path]] = []
+        for cf in files:
+            with open(cf, 'r', errors='ignore') as file:
+                content = file.read()
+            try:
+                statements = ast.parse(content).statements
+            except Exception as e:
+                logging.warning(f"{cf} has an error: {e}")
+                continue
+            cmake_statements = self._walk_ast(statements)
+            for statement in cmake_statements:
+                if isinstance(statement, ast.FunctionCall):
+                    calls.append((statement, cf))
+        return calls
+    
+    def _valid_name(self, name: str) -> bool:
+        """
+        Checks if the package/library/dependency name is valid
+        """
+        keywords = {
+            "REQUIRED", "OPTIONAL", "QUIET", "EXACT", "CONFIG", "NO_MODULE", "PRIVATE", 
+            "PUBLIC", "INTERFACE", "BEFORE", "IMPORTED_TARGET", "REGEX", "INCLUDE", "PATH",
+            "COMPONENTS"
+        }
+        if name.upper() in keywords:
+            return False
+        if "/" in name or "\\" in name or name.startswith("."):
+            return False # relative/absolute paths
+        if "{" in name or "}" in name:
+            return False # variables
+        if "\"" in name or "'" in name:
+            return False # string
+        if re.match(r'^\d+(\.\d+)*$', name):
+            return False # version numbers
+        if name.endswith(('.so', '.a', '.lib', '.dll', '.dylib', '.cmake', '.txt', '.h', '.cc', '.cpp')):
+            return False # files
+        if name.startswith(('test_', 'example_', 'demo_', 'benchmark_')):
+            return False # internal variables
+        if not name or len(name) < 2:
+            return False # single letter
+        if "<" in name or ">" in name:
+            return False
+        return True
+    
+    def get_ubuntu_for_cmake(self, cmake_version: str) -> str:
+        """
+        Maps the cmake_minimum_version number to an ubuntu version 
+        """
+        version_num = self._version_to_number(cmake_version)
+
+        if version_num <= 305:
+            return "ubuntu:16.04"
+        elif version_num <= 310:
+            return "ubuntu:18.04"
+        elif version_num <= 316:
+            return "ubuntu:20.04"
+        elif version_num <= 322:
+            return "ubuntu:22.04"
+        else:
+            return "ubuntu:24.04"
+
+    def _version_to_number(self, version_str: str) -> int:
+        """
+        Parses a cmake_minimum_required(VERSION ...) line and returns a numeric version (major*100 + minor).
+        If a range is given, takes the maximum version in the range.
+        If only a single version is given, picks the latest version known to be backward-compatible with it.
+        """
+        COMPATIBILITY_MAP = {
+            208: 300, 305: 322, 310: 327, 316: 327
+        }
+        LATEST_KNOWN = 327
+
+        try:
+            clean_version = re.sub(r'[^\d.\s.]', '', version_str)
+
+            if '...' in version_str:
+                versions = [v.strip() for v in clean_version.split('...') if v.strip()]
+                if len(versions) >= 2:
+                    parts1 = [int(x) for x in versions[0].split('.')]
+                    parts2 = [int(x) for x in versions[1].split('.')]
+                    version_to_use = versions[0] if parts1 > parts2 else versions[1]
+                else:
+                    return LATEST_KNOWN
+            else:
+                parts = clean_version.strip().split('.')
+                major = int(parts[0]) if len(parts) > 0 else 0
+                minor = int(parts[1]) if len(parts) > 1 else 0
+                min_version_num = major * 100 + minor
+
+                for lower_bound, upper_bound in sorted(COMPATIBILITY_MAP.items()):
+                    if min_version_num <= lower_bound:
+                        return upper_bound
+                return LATEST_KNOWN
+            
+            parts = version_to_use.split('.')
+            major = int(parts[0]) if len(parts) > 0 else 0
+            minor = int(parts[1]) if len(parts) > 1 else 0
+
+            return major * 100 + minor
+
+        except (ValueError, IndexError):
+            return LATEST_KNOWN
+
diff --git a/src/cmake/patterns.py b/src/cmake/patterns.py
old mode 100644
new mode 100755
index e53baaf..9dcd13e
--- a/src/cmake/patterns.py
+++ b/src/cmake/patterns.py
@@ -1,89 +1,143 @@
-CONFIG_ERROR_PATTERNS = [
-    r"No package '([a-zA-Z0-9_\-\+\.]+)' found",
-    r"Could NOT find ([A-Za-z0-9_\-\+\.]+)",
-    r"Could not find ([A-Za-z0-9_\-\+\.]+), missing",
-    r"Could not find a package configuration file provided by \"([^\"]+)\"",
-    r"Could not find a configuration file for package \"([^\"]+)\"",
-    r"([A-Za-z0-9_\-\+\.]+)\s+package NOT found",
-    r"No module named ['\"]([^'\"]+)['\"]",
-    r"executable '([a-zA-Z0-9_\-\+\.]+)' not found",
-    r"Package '([a-zA-Z0-9_\-\+\.]+)'.*not found",
-    r"package '([^']+)' not found",
-    r"Dependency '([^']+)' is required but was not found",
-    r"Failed to find ([A-Za-z0-9_\-\+\.]+)",
-    r"Looking for ([A-Za-z0-9_\-\+\.]+) - not found",
-    r"Dependency ([A-Za-z0-9_\-\+\.]+) not found",
-    r"  ([A-Za-z0-9_\-\+\.]+) is required",
-    r'([A-Z0-9_]+)_(?:INCLUDE_DIR|LIBRARIES)-NOTFOUND'
-]
-
-BUILD_ERROR_PATTERNS = [
-    r"fatal error:\s+([\w_]+)\.h:\s+No such file or directory",
-    r"fatal error:\s+([\w_/]+)\.h:\s+No such file or directory",
-    r"cannot find -l([\w_]+)"
-]
-
-FLAGS_ERROR_PATTERNS = [
-    {
-        "name": "FetchContent: Failed to checkout tag",
-        "regex": r"failed to checkout tag",
-        "action": lambda append, remove: append.update([
-            "-DFETCHCONTENT_TRY_FIND_PACKAGE_MODE=ALWAYS",
-            "-DFETCHCONTENT_UPDATES_DISCONNECTED=ON",
-            "-DFETCHCONTENT_FULLY_DISCONNECTED=OFF",
-        ]),
-    },
-    {
-        "name": "FetchContent: Git clone failure",
-        "regex": r"(fatal: .*unable to access|could not clone|timeout|operation timed out)",
-        "action": lambda append, remove: append.update([
-            "-DFETCHCONTENT_UPDATES_DISCONNECTED=ON",
-            "-DFETCHCONTENT_FULLY_DISCONNECTED=ON",
-        ]),
-    },
-    {
-        "name": "Don't use CMAKE_BUILD_TYPE",
-        "regex": r"Don't use CMAKE_BUILD_TYPE",
-        "action": lambda append, remove: (
-            remove.add("-DCMAKE_BUILD_TYPE=Debug")
-        ),
-    },
-    {
-        "name": "Requires C++14",
-        "regex": r"(C\+\+14|requires at least c\+\+14|std::make_unique.*not declared)",
-        "action": lambda append, remove: append.update([
-            "-DCMAKE_CXX_STANDARD=14",
-            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
-        ]),
-    },
-    {
-        "name": "Requires C++17",
-        "regex": r"(filesystem|optional|variant|any).*not.*member",
-        "action": lambda append, remove: append.update([
-            "-DCMAKE_CXX_STANDARD=17",
-            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
-        ]),
-    },
-    {
-        "name": "Requires C++20",
-        "regex": r"(ranges|concepts|coroutine|format).*not.*member",
-        "action": lambda append, remove: append.update([
-            "-DCMAKE_CXX_STANDARD=20",
-            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
-        ]),
-    },
-    {
-        "name": "CXX Compiler missing",
-        "regex": r"(no cmake_cxx_compiler|cxx compiler identification is unknown)",
-        "action": lambda append, remove: append.add(
-            "-DCMAKE_CXX_COMPILER=g++"
-        ),
-    },
-    {
-        "name": "Clang recommended instead of GCC",
-        "regex": r"clang.*recommended|gcc.*unsupported version",
-        "action": lambda append, remove: append.add(
-            "-DCMAKE_CXX_COMPILER=usr/bin/clang++"
-        ),
-    }
-]
+CONFIG_ERROR_PATTERNS = [
+    r"No package '([a-zA-Z0-9_\-\+\.]+)' found",
+    r"Could NOT find ([A-Za-z0-9_\-\+\.]+)",
+    r"Could not find ([A-Za-z0-9_+-]+)",
+    r"Could not find ([A-Za-z0-9_\-\+\.]+), missing",
+    r"Could not find a package configuration file provided by \"([^\"]+)\"",
+    r"Could not find a configuration file for package \"([^\"]+)\"",
+    r"([A-Za-z0-9_\-\+\.]+)\s+package NOT found",
+    r"No module named ['\"]([^'\"]+)['\"]",
+    r"executable '([a-zA-Z0-9_\-\+\.]+)' not found",
+    r"Package '([a-zA-Z0-9_\-\+\.]+)'.*not found",
+    r"package '([^']+)' not found",
+    r"Dependency '([^']+)' is required but was not found",
+    r"Failed to find ([A-Za-z0-9_\-\+\.]+)",
+    r"Looking for ([A-Za-z0-9_\-\+\.]+) - not found",
+    r"Dependency ([A-Za-z0-9_\-\+\.]+) not found",
+    r"  ([A-Za-z0-9_\-\+\.]+) is required",
+    r'([A-Z0-9_]+)_(?:INCLUDE_DIR|LIBRARIES)-NOTFOUND',
+    r"Please install the ([a-zA-Z0-9_\-\+\.]+) library package",
+    r"Requires ([a-zA-Z0-9_\-\+\.]+) >=",
+    r"([A-Za-z0-9_+\-]+)\s+component not found",
+    r"Can't find .*?of\s+([^\s]+)",
+    r"Unable to find requested ([A-Za-z0-9_+-]+) installation",
+    r"Unable to locate ([A-Za-z0-9_+\-]+) include",
+    r"  ([A-Za-z0-9_+\-]+) not found",
+    r"  ([A-Za-z0-9_+\-]+) library not found",
+    r"None of the required '([A-Za-z0-9_+-]+)",
+    r"  ([A-Za-z0-9_+-]+)\s+library missing",
+    r'references the file\s+"([^"]+)"',
+    r'([A-Z]+)_INCLUDE_DIR',
+    r'-D([A-Z]+)_INCLUDE_PATH',
+    r' component\s+"([^"]+)"',
+    r"The following required packages were not found:\s*(?:\n\s*-\s*([^\s]+))+"
+]
+
+BUILD_ERROR_PATTERNS = [
+    r"fatal error:\s+([\w_]+\.h):\s+No such file or directory",
+    r"fatal error:\s+([\w_/]+\.h):\s+No such file or directory",
+    r"cannot find -l([\w_]+)",
+    r"(?:^|[\s:])([\w.+-]+):\s+(?:not found|command not found|No such file or directory)"
+]
+
+FLAGS_ERROR_PATTERNS = [
+    {
+        "name": "Generic GCC attribute warnings under -Werror",
+        "regex": r"[-]W(ignored|array-bounds|stringop|attributes)",
+        "action": lambda append, remove, command, match: append.update([
+            "-DCMAKE_CXX_FLAGS=-Wno-error",
+        ]),
+    },
+    {
+        "name": "Dangling reference warning under -Werror",
+        "regex": r"dangling-reference",
+        "action": lambda append, remove, command, match: append.update([
+            "-DCMAKE_CXX_FLAGS=-Wno-error"
+        ]),
+    },
+    {
+        "name": "Catch2 SIGSTKSZ constexpr error",
+        "regex": r"storage size of 'altStackMem' isn't constant|sysconf",
+        "action": lambda append, remove, command, match: append.update([
+            "-DSIGSTKSZ=16384"
+        ]),
+    },
+    {
+        "name": "Older versions of Catch2 incorrectly used it inside a constexpr expression",
+        "regex": r"non-'constexpr' function 'sysconf'|altStackMem",
+        "action": lambda append, remove, command, match: append.update([
+            "-DUSE_SYSTEM_CATCH2=ON",
+            "-DCATCH_USE_SYSTEM=ON",
+        ])
+    },
+    {
+        "name": "FetchContent: Failed to checkout tag",
+        "regex": r"failed to checkout tag",
+        "action": lambda append, remove, command, match: append.update([
+            "-DFETCHCONTENT_TRY_FIND_PACKAGE_MODE=ALWAYS",
+            "-DFETCHCONTENT_UPDATES_DISCONNECTED=ON",
+            "-DFETCHCONTENT_FULLY_DISCONNECTED=OFF",
+        ]),
+    },
+    {
+        "name": "FetchContent: Git clone failure",
+        "regex": r"(fatal: .*unable to access|could not clone|timeout|operation timed out)",
+        "action": lambda append, remove, command, match: append.update([
+            "-DFETCHCONTENT_UPDATES_DISCONNECTED=ON",
+            "-DFETCHCONTENT_FULLY_DISCONNECTED=ON",
+        ]),
+    },
+    {
+        "name": "Don't use CMAKE_BUILD_TYPE",
+        "regex": r"Don't use CMAKE_BUILD_TYPE",
+        "action": lambda append, remove, command, match: (
+            command.add("-DCMAKE_BUILD_TYPE=Debug")
+        ),
+    },
+    {
+        "name": "Requires C++14",
+        "regex": r"(C\+\+14|requires at least c\+\+14|std::make_unique.*not declared)",
+        "action": lambda append, remove, command, match: append.update([
+            "-DCMAKE_CXX_STANDARD=14",
+            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
+        ]),
+    },
+    {
+        "name": "Requires C++17",
+        "regex": r"(filesystem|optional|variant|any).*not.*member",
+        "action": lambda append, remove, command, match: append.update([
+            "-DCMAKE_CXX_STANDARD=17",
+            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
+        ]),
+    },
+    {
+        "name": "Requires C++20",
+        "regex": r"(ranges|concepts|coroutine|format).*not.*member",
+        "action": lambda append, remove, command, match: append.update([
+            "-DCMAKE_CXX_STANDARD=20",
+            "-DCMAKE_CXX_STANDARD_REQUIRED=ON",
+        ]),
+    },
+    {
+        "name": "CXX Compiler missing",
+        "regex": r"(no cmake_cxx_compiler|cxx compiler identification is unknown)",
+        "action": lambda append, remove, command, match: append.add(
+            "-DCMAKE_CXX_COMPILER=g++"
+        ),
+    },
+    {
+        "name": "Clang recommended instead of GCC",
+        "regex": r"clang.*recommended|gcc.*unsupported version",
+        "action": lambda append, remove, command, match: append.add(
+            "-DCMAKE_CXX_COMPILER=usr/bin/clang++"
+        ),
+    },
+    {
+        "name": "",
+        "regex": r'cmake\s+(-D[\w_]+=OFF)',
+        "action": lambda append, remove, command, match: (
+            append.add(match.group(1)),
+            remove.add(match.group(1).replace("=OFF", "=ON")),
+        )
+    }
+]
diff --git a/src/cmake/process.py b/src/cmake/process.py
old mode 100644
new mode 100755
index 3d10737..3bdd427
--- a/src/cmake/process.py
+++ b/src/cmake/process.py
@@ -1,4 +1,3 @@
-
 import logging, subprocess, tempfile, json
 from pathlib import Path
 from src.cmake.analyzer import CMakeAnalyzer
@@ -8,11 +7,13 @@ from typing import Optional, Union
 from src.core.docker.manager import DockerManager
 from src.config.config import Config
 from src.utils.permission import check_and_fix_path_permissions
+from src.utils.image_handling import image
 
 class CMakeProcess:
     """Class configures, builds, tests, and clones commits."""
     def __init__(
         self, 
+        repo_id: str,
         config: Config,
         root: Path, 
         enable_testing_path: Optional[Path], 
@@ -23,7 +24,7 @@ class CMakeProcess:
         docker_test_dir: str = ""
     ):
         self.config = config
-
+        self.repo_id = repo_id
         self.project_root = Path(__file__).resolve().parents[2]
         self.root = (self.project_root / root).resolve() if not root.is_absolute() else root.resolve()
         self.docker_test_dir = docker_test_dir.replace("\\", "/")
@@ -43,12 +44,14 @@ class CMakeProcess:
         self.build_stderr: str = ""
         self.test_flags: set[str] = set()
         self.other_flags: dict[str, list[str]] = {"append": [], "remove": []}
+        self.other_cmds: list[list[str]] = []
         self.resolver = DependencyResolver(self.config)
         self.test_time: dict[str, list[float]] = {
             "parsed": [0.0 for _ in range(self.config.testing.warmup + self.config.testing.commit_test_times)], 
             "time": [0.0 for _ in range(self.config.testing.warmup + self.config.testing.commit_test_times)]
         }
-        self.commands: list[list[str]] = []
+        self.test_commands: list[list[str]] = []
+        self.build_commands: list[list[str]] = []
         self.cmake_config_output: list[str] = []
         self.cmake_build_output: list[str] = []
         self.ctest_output: list[str] = []
@@ -63,15 +66,20 @@ class CMakeProcess:
     def set_flags(self, flags: list[str]) -> None:
         self.flags = flags
 
-    def set_docker(self, config: Config, docker_image: str, new: bool):
-        self.docker = DockerManager(config, self.root.parent, docker_image, self.docker_test_dir, new)
+    def set_docker(self, docker_image: str, new: bool):
+        if self.config.docker_image and self.config.mount:
+            mount_path = Path(self.config.mount).resolve()
+            self.docker = DockerManager(self.config, mount_path, docker_image, self.docker_test_dir, new)
+        else:
+            self.docker = DockerManager(self.config, self.root.parent, docker_image, self.docker_test_dir, new)
 
-    def start_docker_image(self, config: Config, container_name: str, new: bool = True) -> None:
+    def start_docker_image(self, container_name: str, new: bool = True, cpuset_cpus: str = "") -> None:
         if not self.docker_image:
+            # if no docker_image set then it takes a predefined cmake version in CMakeLists.txt mapped to a Dockerfile base
             self.docker_image = self.analyzer.get_docker()
         logging.info(f"Started Docker Image: {self.docker_image}")
-        self.set_docker(config, self.docker_image, new)
-        self.docker.start_docker_container(container_name)
+        self.set_docker(self.docker_image, new)
+        self.docker.start_docker_container(container_name, cpuset_cpus)
         self.container = self.docker.container
 
         copy_cmd = ["cp", "-r", "/workspace", self.docker_test_dir]
@@ -84,7 +92,12 @@ class CMakeProcess:
             logging.info(f"Files copied into docker: {' '.join(map(str, copy_cmd))}")
             if stdout: logging.info(f"stdout: {stdout}")
 
-    def save_docker_image(self, repo_id: str, sha: str, new_cmd: list[str], old_cmd: list[str], results_json: dict) -> None:
+
+    def save_docker_image(
+            self, repo_id: str, sha: str, 
+            new_build_cmd: list[str], old_build_cmd: list[str], 
+            new_test_cmd: list[str], old_test_cmd: list[str],
+            results_json: dict) -> None:
         """
         Saved docker image structure:
         | /workspace -- mount folder
@@ -100,23 +113,55 @@ class CMakeProcess:
                 | results.json -- tested results, statistics, metadata and other informations
             | old_build.sh -- old configure and build code used => OK
             | new_build.sh -- new configure and build code used => OK
-            TODO: test restructuring?
             | old_test.sh -- old ctest used => OK
             | new_test.sh -- new ctest used => OK
         """
-        image_name = ("_".join(repo_id.split("/")) + f"_{sha}").lower()
+        image_name = image(repo_id, sha)
         container_id = self.container.id if self.container and self.container.id else "test"
         
-        self.copy_log_to_container(container_id, results_json)
-        self.docker.copy_commands_to_container(self.root, new_cmd, old_cmd)
+        inspect = subprocess.run(
+            ["docker", "image", "inspect", image_name],
+            stdout=subprocess.DEVNULL,
+            stderr=subprocess.DEVNULL
+        )
 
+        if inspect.returncode == 0:
+            logging.info(f"Removing existing image: {image_name}")
+            rm = subprocess.run(
+                ["docker", "rmi", "-f", image_name],
+                capture_output=True,
+                text=True
+            )
+            if rm.returncode != 0:
+                logging.error(f"Failed to remove old image {image_name}: {rm.stderr}")
+
+        self.copy_log_to_container(container_id, results_json)
+        self.docker.copy_commands_to_container(self.root, new_build_cmd, old_build_cmd, new_test_cmd, old_test_cmd)
+
+        clean_cmd = [
+            ["apt-get", "autoremove", "-y"], 
+            ["apt-get", "autoclean", "-y"], 
+            ["apt-get", "clean"], 
+            ["rm", "-rf", "/var/lib/apt/lists/*"], 
+            ["rm", "-rf", "/tmp/*"], 
+            ["rm", "-rf", "/root/.cache"], 
+            ["rm", "-rf", "/home/*/.cache"]
+        ]
+        for cmd in clean_cmd:
+            exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
+            if exit_code != 0:
+                logging.error(f"Failed command exit code {exit_code}: {' '.join(map(str, cmd))}")
+                if stdout: logging.info(f"stdout: {stdout}")
+                if stderr: logging.warning(f"stderr: {stderr}")
+            
         result = subprocess.run(["docker", "commit", container_id, image_name], capture_output=True, text=True)
         if result.returncode != 0:
             logging.error(f"docker commit failed: {result.stderr}")
         
-        result = subprocess.run(["docker", "save", image_name, "-o", f"{image_name}.tar"], capture_output=True, text=True)
-        if result.returncode != 0:
-            logging.error(f"docker save failed: {result.stderr}")
+        if self.config.tar:
+            result = subprocess.run(["docker", "save", image_name, "-o", f"{image_name}.tar"], capture_output=True, text=True)
+            if result.returncode != 0:
+                logging.error(f"docker save failed: {result.stderr}")
 
     def copy_log_to_container(self, container_id: str, results_json: dict) -> None:
         log_config: str = f"Configuration output:\n" + '\n'.join(self.cmake_config_output) + "\n"
@@ -148,18 +193,16 @@ class CMakeProcess:
                     logging.info(f"Copied log file to {dest_path}")
 
 ############### RUNNING ###############
+
+    def diff(self) -> bool:
+        return self._clean_old() and self._del_new() and self._copy_old() and self._apply_diff()
         
     def configure(self) -> bool:
         return self._configure()
 
     def build(self) -> bool:
-        if self.package_manager:
-            return self._configure() #and self._build()
-        else:
-            return self._configure_with_retries() #and self._build()
+        return self._configure_with_retries() #and self._build()
     
-    #def test(self, warmup: int = 0, test_repeat: int = 1, no_run: bool = False) -> bool:
-    #    return self._ctest(warmup, test_repeat, no_run)
     def test(self, cmd: list[str], has_list_args: bool) -> bool:
         return self._ctest(cmd, has_list_args)
 
@@ -170,28 +213,26 @@ class CMakeProcess:
     
     def _configure_with_retries(self, max_retries: int = 10) -> bool:
         save_dependencies = set()
-        save_flags: dict[str, list[str]] = {"append": [], "remove": []}
+        save_resolutions: dict[str, list[str]] = {"append": [], "remove": [], "command": []}
+        save_commands: list[str] = []
         unresolved_dependencies: set[str] = set() 
 
         if not self.container:
             logging.error(f"No docker container started")
             return False
 
+        parsed = False
         for attempt in range(max_retries):
             logging.info(f"[Attempt {attempt+1}/{max_retries}] Configuring project at {self.root}")
 
-            if attempt == 0:
-                missing_dependencies = self.analyzer.get_dependencies()
-                unresolv, oflags = self.resolver.resolve_all(missing_dependencies, self.container)
-                unresolved_dependencies |= unresolv
-                self.test_flags |= oflags
-
-            if self._configure():
-                return True
-            
             remove_build = ["rm", "-rf", str(self.build_path).replace("\\", "/")]
             self.docker.run_command_in_docker(remove_build, self.root, check=False)
             
+            if self._configure():
+                self.build_commands.reverse() # 1. build, 2. configure -> reversed
+                self.build_commands = self.resolver.install_cmds + self.build_commands
+                return True
+            
             missing_dependencies = self.resolver.package_handler.get_missing_dependencies(
                 self.config_stdout, 
                 self.config_stderr, 
@@ -199,25 +240,44 @@ class CMakeProcess:
                 self.build_stderr,
                 Path(self.build_path) / "CMakeCache.txt"
             )
+
+            missing_dependencies -= save_dependencies
             
-            self.other_flags = self.resolver.flag.find_flags(
+            self.other_flags, self.other_cmds = self.resolver.flag.find_resolve(
                 self.config_stdout, 
                 self.config_stderr, 
                 self.build_stdout,
                 self.build_stderr,
-                self.other_flags
+                self.other_flags,
+                self.other_cmds
             )
 
-            has_other_flags = (self.other_flags["append"] or self.other_flags["remove"])
-            if not missing_dependencies and not has_other_flags:
+            has_other_resolutions = (self.other_flags["append"] or self.other_flags["remove"] or self.other_cmds)
+            if not missing_dependencies and not has_other_resolutions and not parsed:
+                missing_dependencies = self.analyzer.get_dependencies()
+                unresolv, oflags = self.resolver.resolve_all(missing_dependencies, self.container)
+                unresolved_dependencies |= unresolv
+                self.test_flags |= oflags
+                parsed = True
+            elif not missing_dependencies and not has_other_resolutions:
                 logging.error("Configuration failed but no missing dependencies detected")
                 break
             
-            append_flags_in_save = set(self.other_flags["append"]) <= set(save_flags["append"])
-            remove_flags_in_save = set(self.other_flags["remove"]) <= set(save_flags["remove"])
-            if missing_dependencies <= save_dependencies and append_flags_in_save and remove_flags_in_save:
+            append_flags_in_save = set(self.other_flags["append"]) <= set(save_resolutions["append"])
+            remove_flags_in_save = set(self.other_flags["remove"]) <= set(save_resolutions["remove"])
+            new_commands_in_save = set([" ".join(c) for c in self.other_cmds]) <= set(save_commands)
+            if not missing_dependencies and append_flags_in_save and remove_flags_in_save and new_commands_in_save and not parsed:
+                missing_dependencies = self.analyzer.get_dependencies()
+                unresolv, oflags = self.resolver.resolve_all(missing_dependencies, self.container)
+                unresolved_dependencies |= unresolv
+                self.test_flags |= oflags
+                parsed = True
+            elif not missing_dependencies and append_flags_in_save and remove_flags_in_save and new_commands_in_save:
                 logging.error("Configuration failed but no new missing dependencies detected")
                 break
+            
+            for cmd in self.other_cmds:
+                self.docker.run_command_in_docker(cmd, self.root, check=False)
 
             unresolv, oflags = self.resolver.resolve_all(missing_dependencies, self.container)
             unresolved_dependencies |= unresolv
@@ -227,7 +287,8 @@ class CMakeProcess:
                 unresolved_dependencies, oflags = self.resolver.unresolved_dep(unresolved_dependencies)
                 self.test_flags |= oflags
 
-            save_flags = self.other_flags
+            save_resolutions = self.other_flags
+            save_commands = [" ".join(c) for c in self.other_cmds]
             save_dependencies |= missing_dependencies
 
         logging.error("All configuration attempts failed.")
@@ -235,81 +296,78 @@ class CMakeProcess:
     
 
     def _configure(self, commands: list[str] = []) -> bool:
-        if not commands:
-            cmd = [
-                #'cmake', 
-                #'-E', 'env', 
-                #'PKG_CONFIG_PATH=/opt/vcpkg/installed/x64-linux/lib/pkgconfig',
-                #'CMAKE_PREFIX_PATH=/opt/vcpkg/installed/x64-linux',
-                #'LD_LIBRARY_PATH=/opt/vcpkg/installed/x64-linux/lib',
-
-                'cmake',
-                '-S', self.to_container_path(self.root), 
-                '-B', str(self.build_path).replace("\\", "/"), 
-                #'-G', 'Ninja',
-
-                #'-DVCPKG_MANIFEST_MODE=ON',
-                #'-DVCPKG_MANIFEST_DIR=' + self.root,  # vcpkg.json location
-                #'-DVCPKG_INSTALLED_DIR=' + str(Path(self.build_path) / 'vcpkg_installed'),  # isolate deps per build
-                
-                '-DCMAKE_BUILD_TYPE=Debug',
-                #'-DCMAKE_BUILD_TYPE=RelWithDebInfo',
-                #'-DCMAKE_C_COMPILER=/usr/bin/clang',
-                #'-DCMAKE_CXX_COMPILER=/usr/bin/clang++',
-                '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',
-
-                # disable errors for warnings
-                #'-DCMAKE_CXX_FLAGS=-g -Wall -Wextra -Wpedantic -Wno-error',
-                #'-DCMAKE_C_FLAGS=-g -Wall -Wextra -Wpedantic -Wno-error',
-                #'-DCMAKE_CXX_FLAGS_DEBUG=-g -Wall -Wextra -Wpedantic -Wno-error',
-                #'-DCMAKE_C_FLAGS_DEBUG=-g -Wall -Wextra -Wpedantic -Wno-error',
-
-                #'-DCMAKE_C_FLAGS=-fprofile-instr-generate -fcoverage-mapping -O0 -g',
-                #'-DCMAKE_CXX_FLAGS=-fprofile-instr-generate -fcoverage-mapping -O0 -g',
-                #'-DCMAKE_EXE_LINKER_FLAGS=-fprofile-instr-generate',
-                #'-DCMAKE_C_COMPILER_LAUNCHER=ccache',
-                #'-DCMAKE_CXX_COMPILER_LAUNCHER=ccache',
-
-                #'-DCMAKE_VERBOSE_MAKEFILE=ON',
-                #'-DCMAKE_FIND_DEBUG_MODE=ON',
-            ]
-
-            for flag in self.flags:
-                if 'disable' in flag.lower():
-                    cmd.append(f'-D{flag}=OFF')
-                else:
-                    cmd.append(f'-D{flag}=ON')
-
-            for flag in self.other_flags["append"]:
-                cmd.append(flag)
+        cmd = [
+            #'cmake', 
+            #'-E', 'env', 
+            #'PKG_CONFIG_PATH=/opt/vcpkg/installed/x64-linux/lib/pkgconfig',
+            #'CMAKE_PREFIX_PATH=/opt/vcpkg/installed/x64-linux',
+            #'LD_LIBRARY_PATH=/opt/vcpkg/installed/x64-linux/lib',
+
+            'cmake',
+            '-S', self.to_container_path(self.root), 
+            '-B', str(self.build_path).replace("\\", "/"), 
+            #'-G', 'Ninja',
+
+            #'-DVCPKG_MANIFEST_MODE=ON',
+            #'-DVCPKG_MANIFEST_DIR=' + self.root,  # vcpkg.json location
+            #'-DVCPKG_INSTALLED_DIR=' + str(Path(self.build_path) / 'vcpkg_installed'),  # isolate deps per build
             
-            for flag in self.other_flags["remove"]:
-                if flag in cmd:
-                    cmd.remove(flag)
+            '-DCMAKE_BUILD_TYPE=Debug',
+            #'-DCMAKE_BUILD_TYPE=RelWithDebInfo',
+            #'-DCMAKE_C_COMPILER=/usr/bin/clang',
+            #'-DCMAKE_CXX_COMPILER=/usr/bin/clang++',
+            '-DCMAKE_EXPORT_COMPILE_COMMANDS=ON',
+
+            # disable errors for warnings
+            #'-DCMAKE_CXX_FLAGS=-g -Wall -Wextra -Wpedantic -Wno-error',
+            #'-DCMAKE_C_FLAGS=-g -Wall -Wextra -Wpedantic -Wno-error',
+            #'-DCMAKE_CXX_FLAGS_DEBUG=-g -Wall -Wextra -Wpedantic -Wno-error',
+            #'-DCMAKE_C_FLAGS_DEBUG=-g -Wall -Wextra -Wpedantic -Wno-error',
+
+            #'-DCMAKE_C_FLAGS=-fprofile-instr-generate -fcoverage-mapping -O0 -g',
+            #'-DCMAKE_CXX_FLAGS=-fprofile-instr-generate -fcoverage-mapping -O0 -g',
+            #'-DCMAKE_EXE_LINKER_FLAGS=-fprofile-instr-generate',
+            #'-DCMAKE_C_COMPILER_LAUNCHER=ccache',
+            #'-DCMAKE_CXX_COMPILER_LAUNCHER=ccache',
+
+            #'-DCMAKE_VERBOSE_MAKEFILE=ON',
+            #'-DCMAKE_FIND_DEBUG_MODE=ON',
+        ]
+
+        for flag in self.flags:
+            if 'disable' in flag.lower():
+                cmd.append(f'-D{flag}=OFF')
+            else:
+                cmd.append(f'-D{flag}=ON')
+
+        for flag in self.other_flags["append"]:
+            cmd.append(flag)
         
-            if self.package_manager.startswith("vcpkg"):
-                logging.info("Installing through package manager vcpkg...")
-                cmd.append('-DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake')
-                cmd.append('-DVCPKG_MANIFEST_MODE=ON')
-            """
-            elif self.package_manager.startswith("conanfile"):
-                try:
-                    logging.info("Installing through package manager conan...")
-                    install = ['conan', 'install', '.', '-build=missing'] 
-                    exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(install, self.root, workdir=self.root, check=False)
-                    self.cmake_config_output.append(stdout)
-                    if exit_code == 0:
-                        logging.info(f"Conan Output:\n{stdout}")
-                    else:
-                        if stdout: logging.error(f"Conan Output (stdout):\n{stdout}")
-                        if stderr: logging.error(f"Conan Error (stderr):\n{stderr}")
-                        return False
-                except Exception as e:
-                    logging.error(f"Conan installation failed: {e}")
+        for flag in self.other_flags["remove"]:
+            if flag in cmd:
+                cmd.remove(flag)
+    
+        if self.package_manager.startswith("vcpkg"):
+            logging.info("Installing through package manager vcpkg...")
+            cmd.append('-DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake')
+            cmd.append('-DVCPKG_MANIFEST_MODE=ON')
+        """
+        elif self.package_manager.startswith("conanfile"):
+            try:
+                logging.info("Installing through package manager conan...")
+                install = ['conan', 'install', '.', '-build=missing'] 
+                exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(install, self.root, workdir=self.root, check=False)
+                self.cmake_config_output.append(stdout)
+                if exit_code == 0:
+                    logging.info(f"Conan Output:\n{stdout}")
+                else:
+                    if stdout: logging.error(f"Conan Output (stdout):\n{stdout}")
+                    if stderr: logging.error(f"Conan Error (stderr):\n{stderr}")
                     return False
-            """
-        else:
-            cmd = commands
+            except Exception as e:
+                logging.error(f"Conan installation failed: {e}")
+                return False
+        """
 
         logging.info(" ".join(map(str, cmd)))
         exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
@@ -323,18 +381,18 @@ class CMakeProcess:
             self.config_stderr = stderr
             if stdout: logging.error(f"Output (stdout):\n{stdout}")
             if stderr: logging.error(f"Error (stderr):\n{stderr}")
-            remove_build = ["rm", "-rf", str(self.build_path).replace("\\", "/")]
-            self.docker.run_command_in_docker(remove_build, self.root, check=False)
-            if not commands:
-                for flag in self.test_flags:
-                    cmd.append(flag)
-                return self._configure(cmd)
+            #remove_build = ["rm", "-rf", str(self.build_path).replace("\\", "/")]
+            #self.docker.run_command_in_docker(remove_build, self.root, check=False)
+            #if not commands:
+            #    for flag in self.test_flags:
+            #        cmd.append(flag)
+            #    return self._configure(cmd)
             return False
         
         if not self._build():
             return False
         
-        self.commands.append(list(map(str, cmd)))
+        self.build_commands.append(list(map(str, cmd)))
         return True
         
     def to_container_path(self, path: Path) -> str:
@@ -355,7 +413,7 @@ class CMakeProcess:
         if exit_code == 0:
             logging.info(f"CMake build completed for {self.root}")
             logging.debug(f"Output:\n{stdout}")
-            self.commands.append(list(map(str, cmd)))
+            self.build_commands.append(list(map(str, cmd)))
             return True
         else:
             logging.error(f"CMake build failed for {self.root} (return code {exit_code})", exc_info=True)
@@ -376,11 +434,13 @@ class CMakeProcess:
         if test_exec_flag and not self._individual_tests_collection(test_exec_flag):
             return False
 
-        if len(self.commands) == 2: # only configuration and build commands
-            self.commands.append(list(map(str, cmd)))
+        logging.info(f"Commands: {self.test_commands}")
+        if len(self.test_commands) == 0: # no test commands
+            root = ['cd', str(self.test_path)]
+            self.test_commands.append(list(map(str, root)))
+            self.test_commands.append(list(map(str, cmd)))
         return True
 
-
     def _individual_tests_collection(self, test_exec_flag: list[tuple[str, str]]) -> bool:
         framework, test_flag = test_exec_flag[0]
 
@@ -411,12 +471,12 @@ class CMakeProcess:
         unit_tests: dict[str, list[str]] = {}
         for exe_path in test_exec:
             cmd = [exe_path, test_flag]
-            logging.info(' '.join(map(str, cmd)))
+            logging.debug(' '.join(map(str, cmd)))
             exit_code, stdout, stderr, time = self.docker.run_command_in_docker(
                 cmd, self.root, workdir=self.docker_test_dir/self.test_path, check=False
             )
-            logging.info(f"{test_flag} output:\n{stdout}")
-            tests = self.analyzer.find_unit_tests(stdout, framework)
+            tests = self.analyzer.extract_unit_tests(stdout, framework)
+            logging.info(f"{test_flag} ({framework}) output:\n{stdout}")
             if tests:
                 unit_tests[exe_path] = tests
 
@@ -430,7 +490,7 @@ class CMakeProcess:
             for test_name in test_names:
                 self.per_test_times[test_name] = {"parsed": [], "time": []}
                 command = self._single_test_collection(exe_path, framework, test_name)
-                self.commands.append(command)
+                self.test_commands.append(command)
                 self.unit_tests_map[" ".join(command)] = {"name": test_name, "exe": exe_path}
 
         logging.debug(f"Unit test map: {self.unit_tests_map}")
@@ -494,7 +554,7 @@ class CMakeProcess:
         try:
             logging.info(f"{' '.join(command)} in {self.test_path}")
             exit_code, stdout, stderr, time = self.docker.run_command_in_docker(
-                command, self.root, workdir=self.docker_test_dir/self.test_path, check=False
+                command, self.root, workdir=self.docker_test_dir/self.test_path, check=False, timeout=self.config.max_test_time, log=False,
             )
 
             # parse the times returned
@@ -511,8 +571,8 @@ class CMakeProcess:
 
             if exit_code == 0 or (elapsed != 0.0 and stats['passed'] > 0):
                 logging.info(f"CTest passed for {self.test_path}")
-                logging.info(f"Output:\n{stdout}")
-                logging.info(f"Tests run: {stats['total']}, Failures: {stats['failed']}, Skipped: {stats['skipped']}, Time elapsed: {elapsed} s")
+                logging.debug(f"Output:\n{stdout}")
+                logging.info(f"Tests run: {stats['total']}, Failures: {stats['failed']}, Skipped: {stats['skipped']}, Time elapsed: {elapsed or time} s")
             else:
                 logging.error(f"CTest failed for {self.test_path} (return code {exit_code}) with command {' '.join(command)}", exc_info=True)
                 if stdout: logging.error(f"Output (stdout):\n{stdout}", exc_info=True)
@@ -530,19 +590,22 @@ class CMakeProcess:
             return False
         
 
-    def _individual_ctest(self, command: list[str]) -> bool:
-        unit_map = self.unit_tests_map[" ".join(command)]
-        test_name = unit_map['name']
-        exe_path = unit_map['exe']
+    def _individual_ctest(self, command: list[str], extra: list[str] = []) -> bool:
+        try:
+            unit_map = self.unit_tests_map[" ".join(command)]
+            test_name = unit_map['name']
+            exe_path = unit_map['exe']
+        except:
+            return False
 
         if (len(self.per_test_times[test_name]['parsed']) >= len(self.test_time['parsed']) and
             len(self.per_test_times[test_name]['time']) >= len(self.test_time['time'])):
             # probably duplicate test_name
             return True
 
-        logging.debug(command)
+        logging.debug(command + extra)
         exit_code, stdout, stderr, time = self.docker.run_command_in_docker(
-            command, self.root, check=False
+            command + extra, self.root, check=False, timeout=self.config.max_test_time, log=False
         )
         logging.debug(f"Individual CTest stdout:\n{stdout}")
 
@@ -552,6 +615,10 @@ class CMakeProcess:
             return False
         
         if elapsed == 0.0:
+            test_exec_flag = self.analyzer.get_list_test_arg()
+            framework, _ = test_exec_flag[0]
+            if framework == "gtest" and not extra:
+                return self._individual_ctest(command, ["--gtest_repeat=100"])
             elapsed = parse_usr_bin_time(stdout)
 
         ntest = len(self.per_test_times[test_name]['parsed'])
@@ -566,9 +633,72 @@ class CMakeProcess:
             logging.debug(f"[{test_name}] Time elapsed: {elapsed or time} s")
         else:
             logging.error(f"CTest failed for {self.test_path} (return code {exit_code}) with command {' '.join(command)}", exc_info=True)
-            logging.error(f"Output (stdout):\n{stdout}", exc_info=True)
-            logging.error(f"Error (stderr):\n{stderr}", exc_info=True)
+            logging.error(f"Output (stdout):\n{stdout[:10000]}", exc_info=True)
+            logging.error(f"Error (stderr):\n{stderr[:10000]}", exc_info=True)
             return False
 
         self.ctest_output.append(stdout)
-        return True
\ No newline at end of file
+        return True
+    
+############### DIFF ###############
+    # /test_workspace/workspace/old should clean the build folder (generated by cmake build)
+    # and other artifacts
+    def _clean_old(self) -> bool:
+        #remove_build = ["rm", "-rf", str(self.build_path).replace("\\", "/")]
+        remove_old_build = ["rm", "-rf", "/test_workspace/workspace/old/build", "/test_workspace/workspace/old/CMakeCache.txt", "/test_workspace/workspace/old/CMakeFiles"]
+        exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(remove_old_build, self.root, check=False)
+        
+        if exit_code != 0:
+            logging.error(f"Clean old (original) commit failed with exit code {exit_code}: {' '.join(map(str, remove_old_build))}")
+            if stdout: logging.info(f"stdout: {stdout}")
+            if stderr: logging.warning(f"stderr: {stderr}")
+            return False
+
+        logging.info(f"Old (original) commit cleaned: {' '.join(map(str, remove_old_build))}")
+        return True
+
+    # diff patch needs to be applied to old (original) commit
+    # set chmod -R 777 to /test_workspace/workspace/new
+    # delete /test_workspace/workspace/new
+    def _del_new(self) -> bool:
+        cmd = ["chmod", "-R", "777", "/test_workspace/workspace/new"]
+        exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
+        cmd = ["rm", "-rf", "/test_workspace/workspace/new"]
+        exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
+        if exit_code != 0:
+            logging.error(f"Deleting new (patched) commit failed with exit code {exit_code}: {' '.join(map(str, cmd))}")
+            if stdout: logging.info(f"stdout: {stdout}")
+            if stderr: logging.warning(f"stderr: {stderr}")
+            return False
+
+        logging.info(f"New (patched) commit deleted: {' '.join(map(str, cmd))}")
+        return True
+
+    # copy /test_workspace/workspace/old into /test_workspace/workspace/new to be
+    # able to apply the diff patch
+    def _copy_old(self) -> bool:
+        cmd = ["cp", "-a", "/test_workspace/workspace/old", "/test_workspace/workspace/new"]
+        exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
+        if exit_code != 0:
+            logging.error(f"Copying old (original) commit failed with exit code {exit_code}: {' '.join(map(str, cmd))}")
+            if stdout: logging.info(f"stdout: {stdout}")
+            if stderr: logging.warning(f"stderr: {stderr}")
+            return False
+
+        logging.info(f"Old (original) commit copied: {' '.join(map(str, cmd))}")
+        return True
+
+    # apply the diff patch to /test_workspace/workspace/new
+    def _apply_diff(self) -> bool:
+        with open(self.config.diff, 'r', errors='ignore') as f:
+            diff_text = f.read()
+        cmd = ["patch", "-p1", "-d", "/test_workspace/workspace/new", "<<'EOF'\n" + diff_text + "\nEOF"]
+        exit_code, stdout, stderr, _ = self.docker.run_command_in_docker(cmd, self.root, check=False)
+        if exit_code != 0:
+            logging.error(f"Diff application failed with exit code {exit_code}: {' '.join(map(str, cmd))}")
+            if stdout: logging.info(f"stdout: {stdout}")
+            if stderr: logging.warning(f"stderr: {stderr}")
+            return False
+        
+        logging.info(f"Diff applied successfully: {' '.join(map(str, cmd))}")
+        return True
diff --git a/src/cmake/resolver.py b/src/cmake/resolver.py
old mode 100644
new mode 100755
index cd0b66d..39c86e2
--- a/src/cmake/resolver.py
+++ b/src/cmake/resolver.py
@@ -1,294 +1,329 @@
-import json, logging, subprocess, re, threading, jsonschema
-from typing import Any, Union, Optional
-from pathlib import Path
-from src.llm.prompt import Prompt
-from src.llm.openai import OpenRouterLLM
-from src.llm.ollama import OllamaLLM
-from docker.models.containers import Container
-from src.config.config import Config
-from src.cmake.patterns import *
-
-LLM_DEP_SCHEMA = {
-    "type": "object",
-    "patternProperties": {
-        "^.+$": {
-            "type": "object",
-            "properties": {
-                "vcpkg": {
-                    "oneOf": [
-                        {"type": "string"},
-                        {"type": "array", "items": {"type": "string"}}
-                    ]
-                },
-                "apt": {
-                    "oneOf": [
-                        {"type": "string"},
-                        {"type": "array", "items": {"type": "string"}}
-                    ]
-                },
-                "flags": {
-                    "type": "object",
-                    "properties": {
-                        "vcpkg": {"type": "array", "items": {"type": "string"}},
-                        "apt": {"type": "array", "items": {"type": "string"}},
-                    },
-                    "required": ["vcpkg", "apt"],
-                    "additionalProperties": False,
-                },
-            },
-            "required": ["vcpkg", "apt", "flags"],
-            "additionalProperties": False,
-        }
-    },
-    "additionalProperties": True,
-}
-
-class DependencyResolver:
-    
-    class DependencyCache:
-        def __init__(self, config: Config):
-            self.mapping_path = Path(config.storage_paths.get("cmake-dep", "cmake-dep.json"))
-            try:
-                with open(self.mapping_path) as f:
-                    self.mapping: dict[str, dict[str, Any]] = json.load(f)
-            except (FileNotFoundError, json.JSONDecodeError) as e:
-                logging.warning(f"Initializing empty cache ({e})")
-                self.mapping = {}
-
-        def save(self):
-            def reset_permissions(path: Path):
-                """Reset file permissions to writable"""
-                try:
-                    if path.exists():
-                        path.chmod(0o666)  # Read/write for all
-                except Exception:
-                    pass
-            
-            tmp_path = self.mapping_path.with_suffix(".tmp")
-
-            reset_permissions(self.mapping_path)
-            reset_permissions(tmp_path)
-            
-            try:
-                with open(tmp_path, "w") as f:
-                    json.dump(self.mapping, f, indent=4)
-                
-                if self.mapping_path.exists():
-                    self.mapping_path.unlink()
-                tmp_path.replace(self.mapping_path)
-                
-            except PermissionError as e:
-                try:
-                    if tmp_path.exists():
-                        tmp_path.unlink()
-                except PermissionError:
-                    pass
-                logging.error(f"Failed to save cache: {e}")
-                raise
-        
-    def __init__(self, config: Config, cache=None, handler=None, llm=None):
-        self.config = config
-        self.cache = cache or self.DependencyCache(self.config)
-        self.package_handler = handler or self.PackageHandler()
-        self.llm = llm or self.LLMResolver(self.config)
-        self.flag = self.FlagResolver(self.config)
-
-    def resolve_all(self, dep_names: set[str], container: Container) -> tuple[set[str], set[str]]:
-        self.container = container
-        unresolved: set[str] = set() 
-        flags: set[str] = set()
-
-        for dep in map(str.lower, dep_names):
-            info = self.resolve(dep)
-            if not info:
-                logging.warning(f"Unresolved dependency {dep}")
-                unresolved.add(dep)
-                continue
-
-            for method in ("apt", "vcpkg"):
-                if self.install(dep, method):
-                    flags |= self.flags(dep, method)
-                    break
-                else:
-                    self.cache.mapping.setdefault(dep, {method: ""})
-
-            self.cache.save()
-
-        return unresolved, flags
-        
-    def resolve(self, dep_name: str) -> dict[str, Union[str, list[str]]]:
-        if dep_name in self.cache.mapping:
-            return self.cache.mapping[dep_name]
-        else:
-            return {}
-    
-    def flags(self, dep_name: str, method: str) -> set[str]:
-        dep = self.cache.mapping.get(dep_name)
-        if not dep:
-            return set()
-
-        flags = dep.get("flags", {})
-        if method not in flags:
-            return set()
-
-        return set(flags[method])
-        
-    def install(self, dep_name: str, method: str) -> bool:
-        info = self.resolve(dep_name)
-        pkg_names: Optional[Union[str, list[str]]] = info.get(method)
-        if not pkg_names:
-            logging.warning(f"{method} mapping for {dep_name} is missing or empty.")
-            return False
-
-        pkg_names = [pkg_names] if isinstance(pkg_names, str) else pkg_names
-        
-        cmd = {
-            "vcpkg": ["/opt/vcpkg/vcpkg", "install"] + pkg_names,
-            "apt": ["apt-get", "install", "-y"] + pkg_names
-        }[method]
-
-        logging.info(f"Installing {dep_name} via {method}...")
-        try:
-            if method == "apt":
-                self.container.exec_run(["apt-get", "update"])
-            exit_code, output = self.container.exec_run(cmd)
-            if exit_code == 0: logging.info(f"Installed {dep_name} via {method}")
-            else: logging.info(output.decode(errors="ignore") if output else "")
-            return exit_code == 0
-        except subprocess.CalledProcessError:
-            logging.error(f"Failed to install {dep_name} via {method}")
-        except FileNotFoundError:
-            logging.error(f"{method} executable not found on system.")
-        return False
-        
-    def unresolved_dep(self, unresolved_dependencies: set[str]) -> tuple[set[str], set[str]]:
-        logging.info(f"All unresolved dependencies {unresolved_dependencies}")
-        llm_output = self.llm.llm_prompt(list(unresolved_dependencies), timeout=100)
-        logging.info(f"LLM prompt returned:\n{llm_output}")
-
-        if not llm_output.strip():
-            logging.error("LLM returned empty output.")
-            return unresolved_dependencies, set()
-        
-        try:
-            data = json.loads(llm_output)
-        except json.JSONDecodeError as e:
-            logging.error(f"Invalid JSON from LLM output: {e}")
-            return unresolved_dependencies, set()
-        
-        try:
-            jsonschema.validate(instance=data, schema=LLM_DEP_SCHEMA)
-        except jsonschema.ValidationError as e:
-            logging.error(f"LLM output failed schema validation: {e.message}")
-            logging.debug(f"Invalid data: {json.dumps(data, indent=2)}")
-            return unresolved_dependencies, set()
-
-        data = {k.lower() if isinstance(k, str) else k: v for k, v in data.items()}
-        self.cache.mapping.update(data)
-        unresolved_dependencies, other_flags = self.resolve_all(unresolved_dependencies, self.container)
-        self.cache.save()
-        
-        logging.info(f"Added {data.keys()} to dependency cache")
-        return unresolved_dependencies, other_flags
-    
-
-    class PackageHandler:
-        def get_missing_dependencies(self, stdout: str, stderr: str, build_out: str, build_err: str, cache_path: Path) -> set[str]:
-            if not cache_path.exists():
-                logging.warning("No CMakeCache.txt found, skipping")
-            
-            missing_cache = self._find_cache_missing(cache_path)
-            logging.info(f"Missing caches: {missing_cache}")
-            missing_pkgconfig = self._find_pkgconfig_missing(stdout, stderr)
-            logging.info(f"Missing packages: {missing_pkgconfig}")
-            missing_build = self._find_file_missing(build_out, build_err)
-
-            return missing_cache | missing_pkgconfig
-
-        def _find_cache_missing(self, cache_path: Path) -> set[str]:
-            if not cache_path.exists():
-                logging.warning(f"No CMakeCache.txt at {cache_path}")
-                return set()
-            
-            missing = set()
-            with open(cache_path) as f:
-                for line in f:
-                    if m := re.match(r"(\w+_DIR):PATH=(.+-NOTFOUND)", line):
-                        missing.add(m.group(1).replace("_DIR", ""))
-                    elif m := re.match(r"(\w+_FOUND):BOOL=FALSE", line):
-                        missing.add(m.group(1).replace("_FOUND", ""))
-                        
-            return missing
-        
-        def _find_pkgconfig_missing(self, stdout: str, stderr: str) -> set[str]:
-            patterns = CONFIG_ERROR_PATTERNS
-            missing = set()
-            for pattern in patterns:
-                missing.update(re.findall(pattern, stdout))
-                missing.update(re.findall(pattern, stderr))
-            return missing
-        
-        def _find_file_missing(self, stdout, stderr) -> set[str]:
-            patterns = BUILD_ERROR_PATTERNS
-            missing = set()
-            for pattern in patterns:
-                missing.update(re.findall(pattern, stdout))
-                missing.update(re.findall(pattern, stderr))
-            return missing
-    
-    class FlagResolver:
-        def __init__(self, config: Config):
-            self.config = config
-
-        def find_flags(self, conf_out: str, conf_err: str, build_out: str, build_err: str, flags: dict[str, list[str]]) -> dict[str, list[str]]:
-            append = set(flags["append"])
-            remove = set(flags["remove"])
-            
-            combined = "\n".join([conf_out, conf_err, build_out, build_err])
-
-            for pattern in FLAGS_ERROR_PATTERNS:
-                if re.search(pattern["regex"], combined, re.I):
-                    pattern["action"](append, remove)
-
-            return {"append": list(append), "remove": list(remove)}
-    
-    class LLMResolver:
-        def __init__(self, config: Config):
-            self.config = config 
-            if self.config.llm.ollama_enabled:
-                self.llm = OllamaLLM(self.config, self.config.llm.ollama_resolver_model)
-            else:
-                self.llm = OpenRouterLLM(self.config, self.config.llm.ollama_resolver_model)
-
-        def llm_prompt(self, deps: list[str], timeout: int = 60) -> str:
-            result: dict[str, str] = {"out": ""}
-
-            def run_llm():
-                p = Prompt([Prompt.Message(
-                    "user",
-                    self.config.resolver_prompt.replace("<deps>", f"{deps}")
-                )])
-                try:
-                    llm_output = self.llm.generate(p)
-                    result["out"] = self._clean_json_output(llm_output)
-                except Exception as e:
-                    logging.warning(f"LLM call failed {e}")
-
-            t = threading.Thread(target=run_llm, daemon=True)
-            t.start()
-            t.join(timeout)
-
-            if t.is_alive():
-                logging.warning(f"LLM query timed out after {timeout} seconds.")
-
-            return result["out"]
-        
-        def _clean_json_output(self, raw_text: str) -> str:
-            """Extract JSON content from LLM output."""
-            match = re.search(r"```json\s*(\{.*\})\s*```", raw_text, re.DOTALL) or re.search(r"(\{.*\})", raw_text, re.DOTALL)
-            if not match:
-                logging.warning("No JSON block found in LLM output.")
-                return "{}"
-            return match.group(1).strip()
-        
-
+import json, logging, subprocess, re, threading, jsonschema
+from typing import Any, Union, Optional
+from pathlib import Path
+from src.llm.prompt import Prompt
+from src.llm.openai import OpenRouterLLM
+from src.llm.ollama import OllamaLLM
+from docker.models.containers import Container
+from src.config.config import Config
+from src.cmake.patterns import *
+
+LLM_DEP_SCHEMA = {
+    "type": "object",
+    "patternProperties": {
+        "^.+$": {
+            "type": "object",
+            "properties": {
+                "vcpkg": {
+                    "oneOf": [
+                        {"type": "string"},
+                        {"type": "array", "items": {"type": "string"}}
+                    ]
+                },
+                "apt": {
+                    "oneOf": [
+                        {"type": "string"},
+                        {"type": "array", "items": {"type": "string"}}
+                    ]
+                },
+                "flags": {
+                    "type": "object",
+                    "properties": {
+                        "vcpkg": {"type": "array", "items": {"type": "string"}},
+                        "apt": {"type": "array", "items": {"type": "string"}},
+                    },
+                    "additionalProperties": False
+                },
+            },
+            "required": ["apt"],
+            "additionalProperties": False,
+        }
+    },
+    "additionalProperties": True,
+}
+
+class DependencyResolver:
+    
+    class DependencyCache:
+        def __init__(self, config: Config):
+            self.mapping_path = Path(config.storage_paths.get("cmake-dep", "cmake-dep.json"))
+            try:
+                with open(self.mapping_path) as f:
+                    self.mapping: dict[str, dict[str, Any]] = json.load(f)
+            except (FileNotFoundError, json.JSONDecodeError) as e:
+                logging.warning(f"Initializing empty cache ({e})")
+                self.mapping = {}
+
+        def save(self):
+            def reset_permissions(path: Path):
+                """Reset file permissions to writable"""
+                try:
+                    if path.exists():
+                        path.chmod(0o666)  # Read/write for all
+                except Exception:
+                    pass
+            
+            tmp_path = self.mapping_path.with_suffix(".tmp")
+
+            reset_permissions(self.mapping_path)
+            reset_permissions(tmp_path)
+            
+            try:
+                with open(tmp_path, "w") as f:
+                    json.dump(self.mapping, f, indent=4)
+                
+                if self.mapping_path.exists():
+                    self.mapping_path.unlink()
+                tmp_path.replace(self.mapping_path)
+                
+            except PermissionError as e:
+                try:
+                    if tmp_path.exists():
+                        tmp_path.unlink()
+                except PermissionError:
+                    pass
+                logging.error(f"Failed to save cache: {e}")
+                raise
+        
+    def __init__(self, config: Config, cache=None, handler=None, llm=None):
+        self.config = config
+        self.cache = cache or self.DependencyCache(self.config)
+        self.package_handler = handler or self.PackageHandler()
+        self.llm = llm or self.LLMResolver(self.config)
+        self.flag = self.FlagResolver(self.config)
+        self.install_cmds: list[list[str]] = [["apt-get", "update"]]
+
+    def resolve_all(self, dep_names: set[str], container: Container) -> tuple[set[str], set[str]]:
+        self.container = container
+        unresolved: set[str] = set() 
+        flags: set[str] = set()
+
+        dep_names = set([d[0] if isinstance(d, tuple) else d for d in list(dep_names)])
+        for dep in map(str.lower, dep_names):
+            info = self.resolve(dep)
+            if not info:
+                logging.warning(f"Unresolved dependency {dep}")
+                unresolved.add(dep)
+                continue
+
+            for method in ["apt"]: #, "vcpkg"):
+                if self.install(dep, method):
+                    flags |= self.flags(dep, method)
+                    break
+                else:
+                    self.cache.mapping.setdefault(dep, {method: ""})
+
+            self.cache.save()
+
+        return unresolved, flags
+        
+    def resolve(self, dep_name: str) -> dict[str, Union[str, list[str]]]:
+        if dep_name in self.cache.mapping:
+            return self.cache.mapping[dep_name]
+        else:
+            return {}
+    
+    def flags(self, dep_name: str, method: str) -> set[str]:
+        dep = self.cache.mapping.get(dep_name)
+        if not dep:
+            return set()
+
+        flags = dep.get("flags", {})
+        if method not in flags:
+            return set()
+
+        return set(flags[method])
+        
+    def install(self, dep_name: str, method: str) -> bool:
+        info = self.resolve(dep_name)
+        pkg_names: Optional[Union[str, list[str]]] = info.get(method)
+        if not pkg_names:
+            logging.warning(f"{method} mapping for {dep_name} is missing or empty.")
+            return False
+
+        pkg_names = [pkg_names] if isinstance(pkg_names, str) else pkg_names
+        
+        cmd = {
+            "vcpkg": ["/opt/vcpkg/vcpkg", "install"] + pkg_names,
+            "apt": ["apt-get", "install", "-y"] + pkg_names
+        }[method]
+
+        logging.info(f"Installing {dep_name} via {method}...")
+        try:
+            if method == "apt":
+                self.container.exec_run(["apt-get", "update"])
+            exit_code, output = self.container.exec_run(cmd)
+            if exit_code == 0: 
+                self.install_cmds.append(cmd)
+                logging.info(f"Installed {dep_name} via {method}")
+            else: 
+                logging.warning(output.decode(errors="ignore") + "\n" + str(cmd) if output else str(cmd))
+            return exit_code == 0
+        except subprocess.CalledProcessError:
+            logging.error(f"Failed to install {dep_name} via {method}")
+        except FileNotFoundError:
+            logging.error(f"{method} executable not found on system.")
+        return False
+        
+    def unresolved_dep(self, unresolved_dependencies: set[str]) -> tuple[set[str], set[str]]:
+        logging.info(f"All unresolved dependencies {unresolved_dependencies}")
+        llm_output = self.llm.llm_prompt(list(unresolved_dependencies), timeout=100)
+        logging.info(f"LLM prompt returned:\n{llm_output}")
+
+        if not llm_output.strip():
+            logging.error("LLM returned empty output.")
+            return unresolved_dependencies, set()
+        
+        try:
+            data = json.loads(llm_output)
+        except json.JSONDecodeError as e:
+            logging.error(f"Invalid JSON from LLM output: {e}")
+            return unresolved_dependencies, set()
+        
+        try:
+            jsonschema.validate(instance=data, schema=LLM_DEP_SCHEMA)
+        except jsonschema.ValidationError as e:
+            logging.error(f"LLM output failed schema validation: {e.message}")
+            logging.debug(f"Invalid data: {json.dumps(data, indent=2)}")
+            return unresolved_dependencies, set()
+
+        data = {k.lower() if isinstance(k, str) else k: v for k, v in data.items()}
+        self.cache.mapping.update(data)
+        unresolved_dependencies, other_flags = self.resolve_all(unresolved_dependencies, self.container)
+        self.cache.save()
+        
+        logging.info(f"Added {data.keys()} to dependency cache")
+        return unresolved_dependencies, other_flags
+    
+
+    class PackageHandler:
+        def get_missing_dependencies(self, stdout: str, stderr: str, build_out: str, build_err: str, cache_path: Path) -> set[str]:
+            if not cache_path.exists():
+                logging.warning("No CMakeCache.txt found, skipping")
+            
+            missing_cache = self._find_cache_missing(cache_path)
+            logging.info(f"Missing caches: {missing_cache}")
+            missing_pkgconfig = self._find_pkgconfig_missing(stdout, stderr)
+            logging.info(f"Missing packages: {missing_pkgconfig}")
+            missing_build = self._find_file_missing(build_out, build_err)
+
+            return missing_cache | missing_pkgconfig | missing_build
+
+        def _find_cache_missing(self, cache_path: Path) -> set[str]:
+            if not cache_path.exists():
+                logging.warning(f"No CMakeCache.txt at {cache_path}")
+                return set()
+            
+            missing = set()
+            with open(cache_path) as f:
+                for line in f:
+                    if m := re.match(r"(\w+_DIR):PATH=(.+-NOTFOUND)", line):
+                        missing.add(m.group(1).replace("_DIR", ""))
+                    elif m := re.match(r"(\w+_FOUND):BOOL=FALSE", line):
+                        missing.add(m.group(1).replace("_FOUND", ""))
+                        
+            return missing
+        
+        def _find_pkgconfig_missing(self, stdout: str, stderr: str) -> set[str]:
+            patterns = CONFIG_ERROR_PATTERNS
+            if "CMake Error" in stdout:
+                stdout = "CMake Error" + stdout.split("CMake Error", 1)[1]
+            if "CMake Error" in stderr:
+                stderr = "CMake Error" + stderr.split("CMake Error", 1)[1]
+            missing = set()
+            for pattern in patterns:
+                missing.update(re.findall(pattern, stdout))
+                missing.update(re.findall(pattern, stderr))
+            return missing
+        
+        def _find_file_missing(self, stdout, stderr) -> set[str]:
+            patterns = BUILD_ERROR_PATTERNS
+            missing = set()
+            for pattern in patterns:
+                missing.update(re.findall(pattern, stdout))
+                missing.update(re.findall(pattern, stderr))
+            return missing
+    
+    class FlagResolver:
+        def __init__(self, config: Config):
+            self.config = config
+
+        def find_resolve(
+            self, 
+            conf_out: str, conf_err: str, 
+            build_out: str, build_err: str, 
+            flags: dict[str, list[str]], 
+            cmds: list[list[str]]
+        ) -> tuple[dict[str, list[str]], list[list[str]]]:
+            append = set(flags["append"])
+            remove = set(flags["remove"])
+            command: list[list[str]] = cmds
+            
+            combined = "\n".join([conf_out, conf_err, build_out, build_err])
+
+            for pattern in FLAGS_ERROR_PATTERNS:
+                match = re.search(pattern["regex"], combined, re.I)
+                if match:
+                    pattern["action"](append, remove, command, match)
+
+            return {"append": list(append), "remove": list(remove)}, command
+    
+    class LLMResolver:
+        def __init__(self, config: Config):
+            self.config = config 
+            if self.config.llm.ollama_enabled:
+                self.llm = OllamaLLM(self.config, self.config.llm.ollama_resolver_model)
+            else:
+                self.llm = OpenRouterLLM(self.config, self.config.llm.ollama_resolver_model)
+
+        def llm_prompt(self, deps: list[str], timeout: int = 60) -> str:
+            result: dict[str, str] = {"out": ""}
+
+            res_user = (
+                """You are an expert in CMake, Ubuntu, and vcpkg. 
+
+                Given one or more missing dependency names, return a single JSON object where each key is a <dependency>:
+                {{
+                "<dependency>": {{
+                    "apt": "<Ubuntu 24.04 packages or libraries>",
+                    "vcpkg": "<vcpkg port>"
+                }}
+                }}
+                Rules:
+                1. Use correct libraries and package names for Ubuntu 24.04 if possible otherwise for Ubuntu 22.02.
+                2. If there are multiple possible libraries and packages, then put them into an array ["library1", "library2", ...].
+                3. Output only valid JSON (no text)
+                4. For unknown deps, set "<Ubuntu 24.04 packages or libraries>" and "<vcpkg port>" to "".
+                5. Generate it for all <dependency> in <deps>.
+                """
+            )
+
+            def run_llm():
+                p = Prompt([Prompt.Message(
+                    "user",
+                    res_user.replace("<deps>", f"{deps}")
+                )])
+                try:
+                    llm_output = self.llm.generate(p)
+                    result["out"] = self._clean_json_output(llm_output)
+                except Exception as e:
+                    logging.warning(f"LLM call failed {e}")
+
+            t = threading.Thread(target=run_llm, daemon=True)
+            t.start()
+            t.join(timeout)
+
+            if t.is_alive():
+                logging.warning(f"LLM query timed out after {timeout} seconds.")
+
+            return result["out"]
+        
+        def _clean_json_output(self, raw_text: str) -> str:
+            """Extract JSON content from LLM output."""
+            match = re.search(r"```json\s*(\{.*\})\s*```", raw_text, re.DOTALL) or re.search(r"(\{.*\})", raw_text, re.DOTALL)
+            if not match:
+                logging.warning("No JSON block found in LLM output.")
+                return "{}"
+            return match.group(1).strip()
+        
+
diff --git a/src/config/__init__.py b/src/config/__init__.py
old mode 100644
new mode 100755
diff --git a/src/config/config.py b/src/config/config.py
old mode 100644
new mode 100755
index 3239e5f..555fd8d
--- a/src/config/config.py
+++ b/src/config/config.py
@@ -1,114 +1,125 @@
-"""Runtime configuration management."""
-import logging
-from dataclasses import dataclass, field
-from typing import Optional
-from github import Auth, Github
-
-from src.config.constants import *
-from src.config.settings import LLMSettings, TestingSettings, GitHubSettings, ResourceSettings
-from src.config.prompts import STAGE1_PROMPT, STAGE2_PROMPT, RESOLVER_PROMPT
-
-@dataclass
-class Config:
-    # Core operation modes
-    popular: bool = False
-    testcrawl: bool = False 
-    commits: bool = False 
-    testcommits: bool = False
-    testdocker: bool = False
-    
-    # Limits and filters
-    limit: int = 10
-    stars: int = 1000
-    filter: str = "simple"
-    filter_type: str = field(init=False)
-    
-    input: str = ""
-    output: str = ""
-    repo: str = ""
-
-    # File paths
-    repo_id: str = field(init=False)
-    input_file: str = field(init=False)
-    output_file: str = field(init=False)
-    output_fail: str = "data/fail.txt"
-    
-    # Commit SHAs
-    sha: str = ""
-    
-    # Docker settings
-    docker: str = ""
-    docker_image: str = field(init=False)
-    mount: str = ""
-    mount_path: str = field(init=False)
-    separate: bool = False
-    analyze: bool = False
-    
-    # Configuration sections
-    llm: LLMSettings = field(default_factory=LLMSettings)
-    testing: TestingSettings = field(default_factory=TestingSettings)
-    github: GitHubSettings = field(default_factory=GitHubSettings)
-    resources: ResourceSettings = field(default_factory=ResourceSettings)
-    
-    # Constants (read-only)
-    storage_paths: dict = field(default_factory=lambda: STORAGE_PATHS)
-    valid_test_dirs: set = field(default_factory=lambda: VALID_TEST_DIRS)
-    test_keywords: list = field(default_factory=lambda: TEST_KEYWORDS)
-    docker_map: dict = field(default_factory=lambda: DOCKER_IMAGE_MAP)
-    commits_time: dict = field(default_factory=lambda: COMMIT_TIME)
-    valid_test_flags: dict = field(default_factory=lambda: VALID_TEST_FLAGS)
-    
-    # Commit analysis settings
-    min_exec_time_improvement: float = 0.05
-    min_p_value: float = 0.05
-    overall_decline_limit: float = -0.01
-    min_likelihood: int = 50
-    max_likelihood: int = 90
-    
-    # Prompts
-    stage1_prompt: str = STAGE1_PROMPT
-    stage2_prompt: str = STAGE2_PROMPT
-    resolver_prompt: str = RESOLVER_PROMPT
-    
-    # Runtime objects
-    _auth: Optional[Auth.Token] = field(init=False, default=None)
-    _git: Optional[Github] = field(init=False, default=None)
-
-    def __post_init__(self):
-        self.filter_type = self.filter
-        self.repo_id = self.repo.removeprefix("https://github.com/").strip() if self.repo else self.repo
-        self.input_file = self.input
-        self.output_file = self.output
-        self.docker_image = self.docker
-        self.mount_path = self.mount
-        self._validate()
-        self._setup_github()
-
-    def _validate(self) -> None:
-        """Validate configuration consistency."""
-        if self.filter_type not in ("simple", "llm", "issue"):
-            raise ValueError(f"Unknown filter type: {self.filter_type}")
-
-        if self.stars < 0 or self.limit <= 0:
-            raise ValueError("Stars and limit must be positive.")
-
-        if str(self.testing.docker_test_dir) == "/workspace":
-            raise ValueError("Docker test directory cannot be '/workspace' to avoid data loss")
-            
-        if not self.github.access_token:
-            raise ValueError("GitHub access token is required")
-        
-        if self.sha and not self.repo:
-            raise ValueError("SHA value needs an accompanying repository owner/name")
-
-    def _setup_github(self):
-        """Initialize GitHub client."""
-        self._auth = Auth.Token(self.github.access_token)
-        self._git = Github(auth=self._auth)
-
-    @property
-    def git_client(self) -> Github:
-        """Get the GitHub client (read-only)."""
-        if self._git is None:
-            raise RuntimeError("GitHub client not initialized")
+"""Runtime configuration management."""
+from dataclasses import dataclass, field
+from typing import Optional
+from github import Auth, Github
+
+from src.config.constants import *
+from src.config.settings import LLMSettings, TestingSettings, GitHubSettings, ResourceSettings, ResourceSettingsCrawl
+from src.utils.image_handling import dockerhub_containers, check_dockerhub
+
+@dataclass
+class Config:
+    # Core operation modes
+    collect: bool = False
+    testcollect: bool = False 
+    commits: bool = False 
+    testcommits: bool = False
+    test: bool = False
+    genimages: bool = False
+    pushimages: bool = False
+    testdocker: bool = False
+    patch: bool = False
+    testpatch: bool = False
+    
+    # Limits and filters
+    limit: int = 10 # indicates the number of repositories collected from github before stopping
+    stars: int = 1000 # indicates the maximum number of stars of repositories collected from github
+    filter: str = "llm" # filter type for filtering commits
+    filter_type: str = field(init=False)
+    
+    input: str = ""
+    output: str = ""
+    repo: str = ""
+    genforce: bool = False # --genimages
+
+    # File paths
+    repo_id: str = field(init=False)
+    input_file: str = field(init=False)
+    output_file: str = field(init=False)
+    output_fail: str = "data/fail.txt"
+    prompt: str = ""
+    
+    # Commit SHAs
+    sha: str = ""
+    
+    # Docker settings
+    docker: str = ""
+    docker_image: str = field(init=False)
+    mount: str = ""
+    diff: str = ""
+    tar: bool = False
+
+    # Dockerhub settings
+    check_dockerhub: bool = False # checks if the image is already uploaded to dockerhub
+    dockerhub_user: str = field(init=False) # export DOCKERHUB_USER=...
+    dockerhub_repo: str = field(init=False) # export DOCKERHUB_REPO=...
+    dockerhub_containers: list[str] = field(init=False)
+    dockerhub_force: bool = False # forces docker push to dockerhub via --pushimages
+    
+    # Configuration sections
+    llm: LLMSettings = field(default_factory=LLMSettings)
+    testing: TestingSettings = field(default_factory=TestingSettings)
+    github: GitHubSettings = field(default_factory=GitHubSettings)
+    resources: ResourceSettings = field(default_factory=ResourceSettings)
+    
+    # Constants (read-only)
+    storage_paths: dict = field(default_factory=lambda: STORAGE_PATHS)
+    valid_test_dirs: set = field(default_factory=lambda: VALID_TEST_DIRS)
+    test_keywords: list = field(default_factory=lambda: TEST_KEYWORDS)
+    docker_map: dict = field(default_factory=lambda: DOCKER_IMAGE_MAP)
+    commits_time: dict = field(default_factory=lambda: COMMIT_TIME)
+    valid_test_flags: dict = field(default_factory=lambda: VALID_TEST_FLAGS)
+    
+    # Commit analysis settings
+    min_exec_time_improvement: float = 0.05
+    min_p_value: float = 0.05
+    overall_decline_limit: float = -0.01
+    max_test_time: int = 600 # in seconds
+    min_stars: int = 20
+    
+    # Runtime objects
+    _auth: Optional[Auth.Token] = field(init=False, default=None)
+    _git: Optional[Github] = field(init=False, default=None)
+
+    def __post_init__(self):
+        self.filter_type = self.filter
+        self.repo_id = self.repo.removeprefix("https://github.com/").strip() if self.repo else self.repo
+        self.input_file = self.input
+        self.output_file = self.output
+        self.docker_image = self.docker
+        if self.testcollect:
+            self.resources = ResourceSettingsCrawl()
+        if self.check_dockerhub:
+            self.dockerhub_user, self.dockerhub_repo = check_dockerhub()
+            self.dockerhub_containers = dockerhub_containers(self.dockerhub_user, self.dockerhub_repo)
+        self._validate()
+        self._setup_github()
+
+    def _validate(self) -> None:
+        """Validate configuration consistency."""
+        if self.filter_type not in ("simple", "llm", "issue"):
+            raise ValueError(f"Unknown filter type: {self.filter_type}")
+
+        if self.stars < 0 or self.limit <= 0:
+            raise ValueError("Stars and limit must be positive.")
+
+        if str(self.testing.docker_test_dir) == "/workspace":
+            raise ValueError("Docker test directory cannot be '/workspace'")
+            
+        if not self.github.access_token:
+            raise ValueError("GitHub access token is required")
+        
+        if self.sha and not self.repo:
+            raise ValueError("SHA value needs an accompanying repository owner/name")
+
+    def _setup_github(self):
+        """Initialize GitHub client."""
+        self._auth = Auth.Token(self.github.access_token)
+        self._git = Github(auth=self._auth)
+
+    @property
+    def git_client(self) -> Github:
+        """Get the GitHub client (read-only)."""
+        if self._git is None:
+            raise RuntimeError("GitHub client not initialized")
         return self._git
\ No newline at end of file
diff --git a/src/config/constants.py b/src/config/constants.py
old mode 100644
new mode 100755
index 02700e9..14f50ee
--- a/src/config/constants.py
+++ b/src/config/constants.py
@@ -1,67 +1,66 @@
-from pathlib import Path
-from datetime import datetime, timezone
-
-DATA_DIR = Path("data")
-CACHE_DIR = Path("cache")
-
-STORAGE_PATHS = {
-    "performance": DATA_DIR / "performance",
-    "clones": DATA_DIR / "commits",
-
-    "popular": DATA_DIR / "popular.txt",
-    "commits": DATA_DIR / "commits.txt",
-    "repos": DATA_DIR / "repos.txt",
-    "testcrawl": DATA_DIR / "testcrawl.txt",
-    "fail": DATA_DIR / "fail.txt",
-
-    "cmake-dep": CACHE_DIR / "cmake-dep.json"
-}
-
-COMMIT_TIME = {
-    'since': datetime(2020, 1, 1, tzinfo=timezone.utc),
-    'until': datetime.now(timezone.utc),
-    'min-exec-time-improvement': 0.05,
-    'min-p-value': 0.05
-}
-
-TEST_KEYWORDS = [
-    "test", "tests", "unittest", "unittests", "testing",
-    "gtest", "googletest", "integration_tests",
-    "benchmark", "perf", "gperf"
-]
-
-VALID_TEST_DIRS = {
-    'test', 'tests', 'unittest', 'unittests', 'bench',
-    'src/test', 'src/tests', 'src/unittest', 'src/unittests'
-}
-
-VALID_TEST_FLAGS: dict[str, list[str]] = {
-    "valid": [
-        "BUILD_TESTING", "BUILD_TESTS", "BUILD_TEST",
-        "ENABLE_TESTING", "ENABLE_TESTS", "ENABLE_TEST",
-        "ENABLE_UNITTESTS",
-        "WITH_TESTING", "WITH_TESTS",
-        "WITH_UNIT_TESTS",
-        "BUILD_UNIT_TESTS",
-        "TESTING", "TESTS", "TEST",
-        "RUN_TESTS"
-    ],
-    "prefix": [],
-    "suffix": [
-        "_BUILD_TEST", "_BUILD_TESTS", "_BUILD_TESTING",
-        "_ENABLE_TEST", "_ENABLE_TESTS", "_ENABLE_TESTING",
-        "_UNIT_TESTS", "_UNITTEST"
-    ],
-    "in": [
-        "_UNIT_TEST_"
-    ]
-}
-
-
-DOCKER_IMAGE_MAP = {
-    "ubuntu:24.04": "cpp24",
-    "ubuntu:22.04": "cpp22",
-    "ubuntu:20.04": "cpp20",
-    "ubuntu:18.04": "cpp20",
-    "ubuntu:16.04": "cpp20"
+from pathlib import Path
+from datetime import datetime, timezone
+
+DATA_DIR = Path("data")
+CACHE_DIR = Path("cache")
+
+STORAGE_PATHS = {
+    "collect": DATA_DIR / "collect.txt",
+    "testcollect": DATA_DIR / "testcollect.txt",
+    "commits": DATA_DIR / "filtered_commits.txt",
+    "performance": DATA_DIR / "commits",
+    "repos": DATA_DIR / "repos.txt",
+    "fail": DATA_DIR / "fail.txt",
+    "clones": DATA_DIR / "tmp",
+
+    "cmake-dep": CACHE_DIR / "cmake-dep.json"
+}
+
+COMMIT_TIME = {
+    'since': datetime(2020, 1, 1, tzinfo=timezone.utc),
+    'until': datetime.now(timezone.utc),
+}
+
+TEST_KEYWORDS = [
+    "test", "tests", "unittest", "unittests", "testing",
+    "gtest", "googletest", "integration_tests",
+    "benchmark", "perf", "gperf", "example"
+]
+
+VALID_TEST_DIRS = {
+    'test', 'tests', 'unittest', 'unittests', 'bench', 'example',
+    'src/test', 'src/tests', 'src/unittest', 'src/unittests'
+}
+
+VALID_TEST_FLAGS: dict[str, list[str]] = {
+    "valid": [
+        "BUILD_TESTING", "BUILD_TESTS", "BUILD_TEST",
+        "ENABLE_TESTING", "ENABLE_TESTS", "ENABLE_TEST",
+        "ENABLE_UNITTESTS",
+        "WITH_TESTING", "WITH_TESTS",
+        "WITH_UNIT_TESTS",
+        "BUILD_UNIT_TESTS",
+        "TESTING", "TESTS", "TEST",
+        "RUN_TESTS", "BLACKBOX_TEST",
+        "INSTALL_TEST"
+    ],
+    "prefix": [],
+    "suffix": [
+        "_BUILD_TEST", "_BUILD_TESTS", "_BUILD_TESTING",
+        "_ENABLE_TEST", "_ENABLE_TESTS", "_ENABLE_TESTING",
+        "_UNIT_TESTS", "_UNITTEST", "DOC_TEST", "_BLACKBOX_TESTS",
+        "_INSTALL_TEST"
+    ],
+    "in": [
+        "_UNIT_TEST_", "BENCH"
+    ]
+}
+
+
+DOCKER_IMAGE_MAP = {
+    "ubuntu:24.04": "cpp24",
+    "ubuntu:22.04": "cpp22",
+    "ubuntu:20.04": "cpp20",
+    "ubuntu:18.04": "cpp20",
+    "ubuntu:16.04": "cpp20"
 }
\ No newline at end of file
diff --git a/src/config/settings.py b/src/config/settings.py
old mode 100644
new mode 100755
index 25a4189..368c770
--- a/src/config/settings.py
+++ b/src/config/settings.py
@@ -1,50 +1,67 @@
-"""Runtime configuration with environment variables and defaults."""
-import os
-from dataclasses import dataclass, field
-from pathlib import Path
-
-@dataclass
-class LLMSettings:
-    """LLM-related configuration."""
-    api_key: str = field(default_factory=lambda: os.getenv('api_key', ''))
-    base: bool = True
-    base_url: str = "https://openrouter.ai/api/v1"
-    model1: str = "openai/gpt-4.1-nano"
-    model2: str = "openai/gpt-oss-20b:free"
-    
-    # Ollama settings
-    ollama_enabled: bool = True
-    ollama_url: str = "http://127.0.0.1:11434/api/generate"
-    ollama_stage1_model: str = "qwen2.5:7b" #"llama3.1:8b"
-    ollama_stage2_model: str = "qwen3:8b" #"deepseek-coder:6.7b"
-    ollama_resolver_model: str = "qwen2.5:7b"
-    
-    # Behavior flags
-    commit_message_only: bool = False
-    issue_only: bool = False
-    with_diff: bool = True
-    
-    cache_file: Path = field(default_factory=lambda: Path("cache/commit.json"))
-
-@dataclass
-class TestingSettings:
-    """Testing-related configuration."""
-    no_list_testing: bool = True
-    warmup: int = 1
-    commit_test_times: int = 30
-    docker_test_dir: str = "/test_workspace"
-
-@dataclass
-class GitHubSettings:
-    """GitHub API configuration."""
-    access_token: str = field(default_factory=lambda: os.getenv('access_token', ''))
-    
-@dataclass
-class ResourceSettings:
-    """Docker resource limits."""
-    cpuset_cpus: str = '4,5'
-    mem_limit: str = '8g'
-    memswap_limit: str = '8g'
-    cpu_quota: int = 200000
-    cpu_period: int = 100000
-    jobs: int = 2
\ No newline at end of file
+"""Runtime configuration with environment variables and defaults."""
+import os
+from dataclasses import dataclass, field
+from pathlib import Path
+
+@dataclass
+class LLMSettings:
+    """LLM-related configuration."""
+    api_key: str = field(default_factory=lambda: os.getenv('api_key', ''))
+    base: bool = True
+    base_url: str = "https://openrouter.ai/api/v1"
+    model1: str = "openai/gpt-5-mini"
+    model2: str = "openai/gpt-5-mini"
+    
+    # Ollama settings
+    ollama_enabled: bool = False
+    ollama_url: str = "http://127.0.0.1:11434/api/generate"
+    ollama_stage1_model: str = "qwen2.5:7b"
+    ollama_diff_model: str = "qwen2.5-coder:7b"
+    ollama_stage2_model: str = "qwen3:8b"
+    ollama_resolver_model: str = "qwen3:8b"
+    
+    # Behavior flags
+    commit_message_only: bool = False
+    issue_only: bool = False
+    with_diff: bool = True
+
+    # OpenHands settings
+    sandbox_base_container_image: str = "ghcr.io/openhands/runtime:oh_v1.3.0_odjrubqcfxjb4y1s_kxyxnblfp0d1rp6p"
+    openhands_model: str = "docker.openhands.dev/openhands/openhands:1.3" 
+    docker_socket: str = "/var/run/docker-tho/docker.sock" #"/var/run/docker.sock"
+    
+    cache_file: Path = field(default_factory=lambda: Path("cache/commit.json"))
+
+@dataclass
+class TestingSettings:
+    """Testing-related configuration."""
+    no_list_testing: bool = True
+    warmup: int = 1
+    commit_test_times: int = 30
+    docker_test_dir: str = "/test_workspace"
+
+@dataclass
+class GitHubSettings:
+    """GitHub API configuration."""
+    access_token: str = field(default_factory=lambda: os.getenv('access_token', ''))
+    
+@dataclass
+class ResourceSettings:
+    """Docker resource limits for --testcommits"""
+    cpuset_cpus: str = ''
+    mem_limit: str = '8g'
+    memswap_limit: str = '8g'
+    cpu_quota: int = 200000
+    cpu_period: int = 100000
+    jobs: int = 1
+    max_parallel_jobs: int = 8 # tests multiple test commits at the same time
+    
+@dataclass
+class ResourceSettingsCrawl(ResourceSettings):
+    """Docker resource limits for --testcollect"""
+    cpuset_cpus: str = '1-4'
+    mem_limit: str = '32g'
+    memswap_limit: str = '32g'
+    cpu_quota: int = 400000
+    cpu_period: int = 100000
+    jobs: int = 4
\ No newline at end of file
diff --git a/src/core/__init__.py b/src/core/__init__.py
old mode 100644
new mode 100755
diff --git a/src/core/controller.py b/src/core/controller.py
old mode 100644
new mode 100755
index b181981..2623ada
--- a/src/core/controller.py
+++ b/src/core/controller.py
@@ -1,95 +1,147 @@
-
-import logging
-from src.core.pipelines.pipeline import (
-    CollectionPipeline, 
-    RepositoryPipeline, 
-    CommitPipeline, 
-    CommitTesterPipeline, 
-    TesterPipeline
-)
-from src.config.config import Config
-
-class Controller:
-    """
-    The Controller class initiates the main operations of the system based on the configurations.
-    It is the central manager for pipelines that handle various stages of GitHub repository analysis.
-    
-    This class supports the following operations:
-    - **Crawl popular repositories** from GitHub to build a dataset.
-    - **Filter, test and validate repositories** for structure, build and test success.
-    - **Gather and filter commits** from repositories.
-    - **Build and test new commits and old commits** to compare performance.
-    - **Run docker images to build and test new commits and old commits** to compare performance.
-    - **Run docker images to build and test old commits and the mounted modified commit** to compare performance.
-    """
-    def __init__(self, config: Config):
-        self.config = config
-
-    def run(self) -> None:
-        logging.info("Starting controller...")
-
-        try:
-            if self.config.popular:
-                self._popular()
-            
-            if self.config.testcrawl:
-                self._testcrawl()
-
-            if self.config.commits:
-                self._commits()
-
-            if self.config.testcommits:
-                self._testcommits()
-
-            if self.config.testdocker:
-                self._testdocker()
-
-            if not any([self.config.popular, self.config.testcrawl, self.config.commits, self.config.testcommits, self.config.testdocker]):
-                logging.warning("No operation selected. Use --popular, --testcrawl, --commits, --testcommits, or --testdocker")
-
-        except Exception as e:
-            logging.error(f"Controller encountered an error: {e}", exc_info=True)
-
-        finally:
-            logging.info("Controller execution completed.")
-
-    def _popular(self) -> None:
-        logging.info("Collecting popular GitHub repositories...")
-        pipeline = CollectionPipeline(self.config)
-        pipeline.query_popular_repos()
-        logging.info("Popular repository colelction completed.")
-        
-    def _testcrawl(self) -> None:
-        logging.info("Testing and validating GitHub repositories...")
-        repo_pipeline = RepositoryPipeline(self.config)
-
-        if self.config.analyze:
-            logging.info("Starting repository analysis...")
-            repo_pipeline.analyze_repos()
-            logging.info("Repository analysis completed.")
-        else:
-            repo_pipeline.test_repos()
-            valid_count = len(repo_pipeline.valid_repos)
-            logging.info(f"Found {valid_count} valid repositories.")
-
-    def _commits(self) -> None:
-        logging.info("Gathering and filtering commits...")
-        repo_ids = RepositoryPipeline(self.config).get_repos()
-        logging.info(f"Found {len(repo_ids)} repositories for commit filtering.")
-        if not repo_ids:
-            logging.warning("No repositories found for commit filtering.")
-            return
-        
-        CommitPipeline(repo_ids, self.config).filter_all_commits()
-
-    def _testcommits(self) -> None:
-        logging.info("Testing commits...")
-        tester_pipeline = CommitTesterPipeline(self.config)
-        tester_pipeline.test_commit()
-        logging.info("Commit testing completed.")
-
-    def _testdocker(self) -> None:
-        logging.info("Testing...")
-        tester_pipeline = TesterPipeline(self.config)
-        tester_pipeline.test()
-        logging.info("Testing completed.")
+
+import logging
+from src.core.pipelines.pipeline import (
+    CollectionPipeline, 
+    RepositoryPipeline, 
+    CommitPipeline, 
+    CommitTesterPipeline,
+    PushPipeline,
+    PatchPipeline
+)
+from src.config.config import Config
+
+class Controller:
+    """
+    The Controller class initiates the main operations of the system based on the configurations.
+    It is the central manager for pipelines that handle various stages of GitHub repository analysis.
+    
+    This class supports the following operations:
+    - **Crawl popular repositories** from GitHub to build a dataset.
+    - **Filter, test and validate repositories** for structure, build and test success.
+    - **Gather and filter commits** from repositories.
+    - **Build and test new commits and old commits** to compare performance.
+    - **Run docker images to build and test new commits and old commits** to compare performance.
+    """
+    def __init__(self, config: Config):
+        self.config = config
+
+    def run(self) -> None:
+        logging.info("Starting controller...")
+
+        try:
+            if self.config.collect:
+                self._collect()
+            
+            if self.config.testcollect:
+                self._testcollect()
+
+            if self.config.commits:
+                self._commits()
+
+            if self.config.testcommits or self.config.genimages:
+                self._testcommits()
+
+            if self.config.genimages:
+                self._genimages()
+
+            if self.config.pushimages:
+                self._pushimages()
+
+            if self.config.testdocker:
+                self.config.genimages = False
+                self._testdocker()
+
+            if self.config.patch:
+                self._patch()
+
+            if self.config.testpatch:
+                self.config.genimages = False
+                self._testpatch()
+
+            if not any([
+                self.config.collect, self.config.testcollect, 
+                self.config.commits, self.config.testcommits, 
+                self.config.genimages, self.config.testdocker, 
+                self.config.patch, self.config.testpatch
+            ]):
+                logging.warning("No operation selected. Use --collect, --testcollect, --commits, --testcommits, --genimages, --testdocker, --patch or --testpatch")
+                
+        except Exception as e:
+            logging.error(f"Controller encountered an error: {e}", exc_info=True)
+
+        finally:
+            logging.info("Controller execution completed.")
+
+    def _collect(self) -> None:
+        logging.info("Collecting popular GitHub repositories...")
+        pipeline = CollectionPipeline(self.config)
+        repos = pipeline.query_popular_repos()
+        logging.info(f"Collected {len(repos)} repositories.")
+
+        if self.config.test:
+            logging.info("Testing and validating GitHub repositories...")
+            repo_pipeline = RepositoryPipeline(self.config)
+            repo_pipeline.test_repos(repos)
+            valid_count = len(repo_pipeline.valid_repos)
+            logging.info(f"Collected {valid_count} valid repositories.")
+
+    def _testcollect(self) -> None:
+        logging.info("Testing and validating GitHub repositories...")
+        repo_pipeline = RepositoryPipeline(self.config)
+        repo_pipeline.test_repos()
+        valid_count = len(repo_pipeline.valid_repos)
+        logging.info(f"Found {valid_count} valid repositories.")
+
+    def _commits(self) -> None:
+        logging.info("Gathering and filtering commits...")
+        repo_ids = RepositoryPipeline(self.config).get_repos()
+        logging.info(f"Found {len(repo_ids)} repositories for commit filtering.")
+        if not repo_ids:
+            logging.warning("No repositories found for commit filtering.")
+            return
+        
+        commit_pipeline = CommitPipeline(repo_ids, self.config)
+        commit_pipeline.filter_all_commits()
+        
+        if self.config.test:
+            filtered_commits = commit_pipeline.filtered_commits
+            logging.info("Testing commits...")
+            tester_pipeline = CommitTesterPipeline(self.config)
+            tester_pipeline.test_commit(filtered_commits)
+            logging.info("Commit testing completed.")
+
+    def _testcommits(self) -> None:
+        logging.info("Testing commits...")
+        tester_pipeline = CommitTesterPipeline(self.config)
+        tester_pipeline.test_commit()
+        logging.info("Commit testing completed.")
+        
+    def _genimages(self) -> None:
+        logging.info("Generating Docker Images...")
+        image_pipeline = CommitTesterPipeline(self.config)
+        image_pipeline.test_commit()
+        logging.info("Docker images generated.")
+
+    def _pushimages(self) -> None:
+        logging.info("Pushing docker images to GHCR...")
+        push_pipeline = PushPipeline(self.config)
+        push_pipeline.push()
+        logging.info("Docker images pushed to GHCR.")
+
+    def _testdocker(self) -> None:
+        logging.info("Testing docker images...")
+        tester_pipeline = CommitTesterPipeline(self.config)
+        tester_pipeline.test_commit()
+        logging.info("Testing docker images completed.")
+
+    def _patch(self) -> None:
+        logging.info("Patching commit...")
+        patch_pipeline = PatchPipeline(self.config)
+        patch_pipeline.patch()
+        logging.info("Commit patched.")
+
+    def _testpatch(self) -> None:
+        logging.info("Testing patched docker images...")
+        image_pipeline = CommitTesterPipeline(self.config)
+        image_pipeline.test_commit()
+        logging.info("Testing patched docker images completed.")
\ No newline at end of file
diff --git a/src/core/docker/__init__.py b/src/core/docker/__init__.py
old mode 100644
new mode 100755
diff --git a/src/core/docker/manager.py b/src/core/docker/manager.py
old mode 100644
new mode 100755
index 296afd5..8d94c38
--- a/src/core/docker/manager.py
+++ b/src/core/docker/manager.py
@@ -1,112 +1,154 @@
-import logging, posixpath, docker, os, time, shlex
-from pathlib import Path
-from docker.types import Mount
-from typing import Optional
-from src.config.config import Config
-from src.utils.permission import check_and_fix_path_permissions
-
-class DockerManager:
-    def __init__(self, config: Config, mount: Path, docker_image: str, docker_test_dir: str, new: bool = False):
-        self.config = config
-        self.mount = mount
-        self.docker_image = docker_image
-        self.new = new
-        self.docker_test_dir = docker_test_dir
-
-    def stop_container(self) -> None:
-        if self.container:
-            self.container.stop()
-            self.container.remove()
-            self.container = None
-
-    def start_docker_container(self, container_name: str) -> None:
-        self.client = docker.from_env()
-        try:
-            try:
-                self.container = self.client.containers.get(container_name)
-                logging.info(f"Reusing existing container {container_name}")
-                return
-            except docker.errors.NotFound: # type: ignore
-                pass
-            
-            if not check_and_fix_path_permissions(self.mount):
-                return
-
-            logging.info(f"Run docker image ({self.docker_image}) mounted on {str(self.mount)}.")
-            mount = Mount(
-                target="/workspace", source=str(self.mount), type="bind", read_only=False
-            )
-            self.container = self.client.containers.run(
-                self.docker_image,
-                command=["/bin/bash"],
-                name=container_name,
-                mounts=[mount],
-                working_dir="/workspace",
-                detach=True,
-                tty=True,
-                remove=False,
-
-                cpuset_cpus=self.config.resources.cpuset_cpus,
-                mem_limit=self.config.resources.mem_limit,
-                memswap_limit=self.config.resources.memswap_limit,
-                cpu_quota=self.config.resources.cpu_quota,
-                cpu_period=self.config.resources.cpu_period
-            )
-            mkdir_cmd = ["mkdir", "-p", f"{self.docker_test_dir}"]
-            self.container.exec_run(mkdir_cmd)
-            mkdir_cmd = ["mkdir", "-p", f"{self.docker_test_dir}/logs"]
-            self.container.exec_run(mkdir_cmd)
-        except Exception as e:
-            logging.error(f"Docker execution failed: {e}")
-
-    def run_command_in_docker(self, cmd: list[str], root: Path, workdir: Optional[Path] = None, check: bool = True, timeout: int = -1) -> tuple[int, str, str, float]:
-        rel_root = os.path.relpath(root, self.mount)
-        container_root = posixpath.join(f"/workspace", rel_root.replace("\\", "/"))
-
-        if workdir:
-            rel_workdir = os.path.relpath(workdir, self.mount).replace("\\", "/")
-            container_workdir = posixpath.join(f"/workspace", rel_workdir)
-        else:
-            container_workdir = container_root
-        
-        if not self.container:
-            logging.error(f"No docker container started")
-            return 1, "", "", -1.0
-
-        cmd = [str(x) for x in cmd]
-        if timeout > 0:
-            cmd = ["timeout", f"{timeout}s"] + cmd
-        shell_cmd = shlex.join(cmd)
-        timed_cmd = [
-            "sh", "-c",
-            f'start=$(date +%s%N); {shell_cmd}; status=$?; end=$(date +%s%N); '
-            f'echo $(( (end - start)/1000000 ))" ms"; exit $status'
-        ]
-        start = time.perf_counter()
-        exit_code, output = self.container.exec_run(timed_cmd, workdir=str(container_workdir))
-        end = time.perf_counter()
-        output = output.decode(errors="ignore") if output else ""
-        logs = self.container.logs().decode()
-        self.container.exec_run(["bash", "-c", f"echo '{output}\n{logs}' >> {self.docker_test_dir}/logs/{'new' if self.new else 'old'}.log"])
-        return exit_code, output, logs, end-start
-    
-
-    def copy_commands_to_container(self, project_root: Path, new_cmd: list[str], old_cmd: list[str]) -> None:
-        for i, c in enumerate(new_cmd): 
-            if i < 2:
-                save = "build"
-            else:
-                save = "test"
-            cmd = ["bash", "-c", f"echo '{c}' >> {self.docker_test_dir}/new_{save}.sh"]
-            exit_code, _, _, _ = self.run_command_in_docker(cmd, project_root, check=False)
-            if exit_code != 0:
-                logging.error(f"Copying the build and test commands failed with: {exit_code}")
-        for i, c in enumerate(old_cmd):
-            if i < 2:
-                save = "build"
-            else:
-                save = "test"
-            cmd = ["bash", "-c", f"echo '{c}' >> {self.docker_test_dir}/old_{save}.sh"]
-            exit_code, _, _, _ = self.run_command_in_docker(cmd, project_root, check=False)
-            if exit_code != 0:
-                logging.error(f"Copying the build and test commands failed with: {exit_code}")
\ No newline at end of file
+import logging, posixpath, docker, os, time, shlex
+from pathlib import Path
+from docker.types import Mount
+from typing import Optional
+from src.config.config import Config
+from src.utils.permission import check_and_fix_path_permissions
+
+class DockerManager:
+    def __init__(self, config: Config, mount: Path, docker_image: str, docker_test_dir: str, new: bool = False):
+        self.config = config
+        self.mount = mount
+        self.docker_image = docker_image
+        self.new = new
+        self.docker_test_dir = docker_test_dir
+
+    def stop_container(self, repo_id: str) -> None:
+        if self.container:
+            try:
+                self.container.stop()
+                self.container.remove()
+                self.container = None
+                logging.info(f"[{repo_id}] Stopped the container")
+            except Exception as e:
+                logging.warning(f"[{repo_id}] Failed to stop container: {e}")
+
+    def start_docker_container(self, container_name: str, cpuset_cpus: str = "") -> None:
+        self.client = docker.from_env()
+        try:
+            try:
+                c = self.client.containers.get(container_name)
+                c.reload()
+
+                if c.status != "running":
+                    logging.info(
+                        f"Container {container_name} exists but is {c.status}, recreating"
+                    )
+                    c.remove(force=True)
+                    raise docker.errors.NotFound(container_name) # type: ignore
+
+                logging.info(f"Reusing running container {container_name}")
+                self.container = c
+                return
+
+            except docker.errors.NotFound: # type: ignore
+                pass
+            
+            if not check_and_fix_path_permissions(self.mount):
+                return
+
+            logging.info(f"Run docker image ({self.docker_image}) mounted on {str(self.mount)}.")
+            mounts = [Mount(target="/workspace", source=str(self.mount), type="bind", read_only=False)]
+            if self.config.mount:
+                mounts.append(Mount(target="/test_workspace/workspace/new", source=self.config.mount, type="bind", read_only=False))
+            self.container = self.client.containers.run(
+                self.docker_image,
+                command=["/bin/bash"],
+                name=container_name,
+                mounts=mounts,
+                working_dir="/workspace",
+                detach=True,
+                tty=True,
+                remove=False,
+
+                cpuset_cpus=cpuset_cpus or self.config.resources.cpuset_cpus,
+                mem_limit=self.config.resources.mem_limit,
+                memswap_limit=self.config.resources.memswap_limit,
+                cpu_quota=self.config.resources.cpu_quota,
+                cpu_period=self.config.resources.cpu_period
+            )
+            mkdir_cmd = ["mkdir", "-p", f"{self.docker_test_dir}"]
+            self.container.exec_run(mkdir_cmd)
+            mkdir_cmd = ["mkdir", "-p", f"{self.docker_test_dir}/logs"]
+            self.container.exec_run(mkdir_cmd)
+        except Exception as e:
+            logging.error(f"Docker execution failed: {e}")
+
+    def clone_in_docker(self, cmd: list[str], workdir: Optional[Path] = None, check: bool = True):
+        if not self.container:
+            logging.error(f"No docker container started")
+            return 1, "", "", -1.0
+        
+        if workdir:
+            container_workdir = posixpath.abspath(workdir)
+            exit_code, output = self.container.exec_run(cmd, workdir=str(container_workdir))
+        else:
+            exit_code, output = self.container.exec_run(cmd)
+
+        if exit_code == 0:
+            logging.info(f"Command run in docker: {cmd}")
+        else:
+            logging.warning(f"Command failed in docker: {cmd}")
+        output = output.decode(errors="ignore") if output else ""
+        logs = self.container.logs().decode()
+        if output: logging.info(f"Output: {output}")
+        if logs: logging.info(f"Logs: {logs}")
+
+
+    def run_command_in_docker(self, cmd: list[str], root: Path, workdir: Optional[Path] = None, check: bool = True, timeout: int = -1, log: bool = True) -> tuple[int, str, str, float]:
+        rel_root = os.path.relpath(root, self.mount)
+        container_root = posixpath.join(f"/workspace", rel_root.replace("\\", "/"))
+
+        if workdir:
+            rel_workdir = os.path.relpath(workdir, self.mount).replace("\\", "/")
+            container_workdir = posixpath.join(f"/workspace", rel_workdir)
+        else:
+            container_workdir = container_root
+        
+        if not self.container:
+            logging.error(f"No docker container started")
+            return 1, "", "", -1.0
+
+        cmd = [str(x) for x in cmd]
+        if timeout > 0:
+            cmd = ["timeout", f"{timeout}s"] + cmd
+        shell_cmd = shlex.join(cmd)
+        timed_cmd = [
+            "sh", "-c",
+            f'start=$(date +%s%N); {shell_cmd}; status=$?; end=$(date +%s%N); '
+            f'echo $(( (end - start)/1000000 ))" ms"; exit $status'
+        ]
+        start = time.perf_counter()
+        exit_code, output = self.container.exec_run(timed_cmd, workdir=str(container_workdir))
+        if exit_code == 0 and log:
+            logging.info(f"Command run in docker: {cmd}")
+        elif log:
+            logging.warning(f"Command failed in docker: {cmd}")
+        end = time.perf_counter()
+        output = output.decode(errors="ignore") if output else ""
+        logs = self.container.logs().decode()
+        self.container.exec_run(["bash", "-c", f"echo '{output}\n{logs}' >> {self.docker_test_dir}/logs/{'new' if self.new else 'old'}.log"])
+        return exit_code, output, logs, end-start
+    
+
+    def copy_commands_to_container(
+            self, project_root: Path, 
+            new_build_cmd: list[str], old_build_cmd: list[str], 
+            new_test_cmd: list[str], old_test_cmd: list[str]) -> None:
+        self._copy(project_root, new_build_cmd, "new_build")
+        self._copy(project_root, old_build_cmd, "old_build")
+        self._copy(project_root, new_test_cmd, "new_test")
+        self._copy(project_root, old_test_cmd, "old_test")
+
+    def _copy(self, project_root: Path, cmd: list[str], save: str) -> None:
+        for c in cmd: 
+            cmd = ["bash", "-c", f"echo '{c}' >> {self.docker_test_dir}/{save}.sh"]
+            exit_code, _, _, _ = self.run_command_in_docker(cmd, project_root, check=False, log=False)
+            if exit_code != 0:
+                logging.error(f"Copying the commands to {save}.sh failed with: {exit_code}")
+        
+
+    def load_docker_image(self, tar_path: Path):
+        self.client = docker.from_env()
+        with open(tar_path, "rb") as f:
+            self.client.images.load(f.read())
\ No newline at end of file
diff --git a/src/core/docker/tester.py b/src/core/docker/tester.py
old mode 100644
new mode 100755
index d0feffe..06dd670
--- a/src/core/docker/tester.py
+++ b/src/core/docker/tester.py
@@ -1,258 +1,252 @@
-import logging, subprocess, os, stat, shutil, random
-from tqdm import tqdm
-from src.core.docker.manager import DockerManager
-from src.cmake.analyzer import CMakeAnalyzer
-from src.core.filter.structure_filter import StructureFilter
-from src.core.filter.process_filter import ProcessFilter
-from src.config.config import Config
-from src.utils.test_analyzer import TestAnalyzer
-from pathlib import Path
-from src.utils.parser import *
-from src.utils.writer import Writer
-from contextlib import contextmanager
-from pathlib import Path
-from typing import Generator, Optional, Any
-from github.Repository import Repository
-from src.utils.exceptions import TestFailed
-
-class DockerTester:
-    def __init__(self, config: Config):
-        self.config = config
-        self.analyzer = CMakeAnalyzer(Path())
-
-    def run_commit_pair(
-        self,
-        repo: Repository,
-        new_sha: str,
-        old_sha: str,
-        new_path: Path,
-        old_path: Path,
-    ) -> None:
-        
-        with self._commit_pair_test(
-            repo, self.config, new_path, old_path, new_sha, old_sha
-        ) as (new_times, old_times, new_struct, old_struct):
-            logging.info(f"Times Old: {old_times}, New: {new_times}")
-            
-            if new_struct and new_struct.process and old_struct and old_struct.process:
-                warmup = self.config.testing.warmup
-
-                new_single_tests_d = new_struct.process.per_test_times 
-                old_single_tests_d = old_struct.process.per_test_times
-
-                new_single_tests = {
-                    test: (
-                        new_single_tests_d[test]['parsed']
-                        if 0.0 not in new_single_tests_d[test]['parsed']
-                        else new_single_tests_d[test]['time']
-                    )
-                    for test in new_single_tests_d.keys()
-                }
-
-                old_single_tests = {
-                    test: (
-                        old_single_tests_d[test]['parsed']
-                        if 0.0 not in old_single_tests_d[test]['parsed']
-                        else old_single_tests_d[test]['time']
-                    )
-                    for test in old_single_tests_d.keys()
-                }
-                
-                test = TestAnalyzer(
-                    self.config, new_single_tests, old_single_tests
-                )
-
-                total_improvement = test.get_improvement_p_value(
-                    old_times[warmup:], new_times[warmup:] 
-                )
-                logging.info(f"pvalue: {total_improvement}")
-
-                isolated_improvements = test.get_significant_test_time_changes()
-                logging.info(f"new outperforms old: {isolated_improvements['new_outperforms_old']}")
-                overall_change = test.get_overall_change()
-                logging.info(f"overall change: {overall_change}")
-                overall_change_with_new_outperforms_old = (
-                    len(isolated_improvements['new_outperforms_old']) > 0 and 
-                    overall_change > self.config.overall_decline_limit
-                )
-
-                new_cmd = [" ".join(s) for s in new_struct.process.commands]
-                old_cmd = [" ".join(s) for s in old_struct.process.commands]
-                
-                commit = repo.get_commit(new_sha)
-                results = test.create_test_log(
-                    commit, repo, old_sha, new_sha, 
-                    old_times, new_times, old_cmd, new_cmd
-                )
-                logging.info(f"Results: {results['performance_analysis']}")
-                writer = Writer(repo.full_name, self.config.storage_paths["performance"])
-                writer.write_results(results)
-
-                if total_improvement < self.config.commits_time['min-p-value'] or overall_change_with_new_outperforms_old:
-                    old_struct.process.save_docker_image(repo.full_name, new_sha, new_cmd, old_cmd, results)
-                    logging.info(f"[{repo.full_name}] ({new_sha}) significantly improves execution time.")
-                    writer.write_improve(results)
-                        
-
-    @contextmanager
-    def _commit_pair_test(
-        self, 
-        repo: Repository, 
-        config: Config, 
-        new_path: Path, 
-        old_path: Path, 
-        new_sha: str, 
-        old_sha: str
-    ) -> Generator[tuple[list[float], list[float], Optional[StructureFilter], Optional[StructureFilter]], Any, Any]:
-        """
-        Start a container for new/old commits and stop container automatically after both runs.
-        """
-        new_pf = ProcessFilter(repo, config, new_path, new_sha)
-        old_pf = ProcessFilter(repo, config, old_path, old_sha)
-        docker_image = ""
-
-        new_structure = None
-        old_structure = None
-        new_times = []
-        old_times = []
-        
-        try:
-            new_structure = new_pf.commit_setup_and_build("New", container_name=new_sha)
-            docker_image = new_structure.process.docker_image if new_structure and new_structure.process else ""
-            
-            if new_structure and new_structure.process:
-                old_structure = old_pf.commit_setup_and_build("Old", container_name=new_sha, docker_image=docker_image)
-            
-                if old_structure and old_structure.process:
-                    new_test_cmd = new_structure.process.commands[2:]
-                    old_test_cmd = old_structure.process.commands[2:]
-                    logging.debug(f"New cmd: {new_structure.process.commands}")
-                    logging.debug(f"Old cmd: {old_structure.process.commands}")
-                    assert len(new_test_cmd) == len(old_test_cmd)
-
-                    warmup = self.config.testing.warmup
-                    test_repeat = self.config.testing.commit_test_times
-                    has_list_args = len(new_test_cmd) > 1
-
-                    for _ in tqdm(range(warmup+test_repeat), total=warmup+test_repeat, desc="Commit pair test", position=1, leave=False):
-                        for new_cmd, old_cmd in zip(new_test_cmd, old_test_cmd):
-                            order = [
-                                ("New", new_cmd, new_structure, new_pf),
-                                ("Old", old_cmd, old_structure, old_pf),
-                            ]
-                            random.shuffle(order)
-
-                            for label, cmd, structure, pf in order:
-                                if not pf.test_run(label, cmd, structure, has_list_args):
-                                    raise TestFailed()
-
-                    new_cmd_times = new_structure.process.test_time
-                    old_cmd_times = old_structure.process.test_time
-
-                    new_times = new_cmd_times['time'] if 0.0 in new_cmd_times['parsed'] else new_cmd_times['parsed']
-                    old_times = old_cmd_times['time'] if 0.0 in old_cmd_times['parsed'] else old_cmd_times['parsed']
-                    #new_times, new_structure = new_pf.valid_commit_run("New", container_name=new_sha)
-                    #docker_image = new_structure.process.docker_image if new_structure and new_structure.process else ""
-                    
-                    #old_times, old_structure = old_pf.valid_commit_run("Old", container_name=new_sha, docker_image=docker_image)
-                    
-            yield new_times, old_times, new_structure, old_structure
-
-        except TestFailed:
-            logging.error("Test failed early, stopping the test loops.")
-            yield [], [], None, None
-
-        except Exception as e:
-            logging.error(f"Commit pair test failed: {e}")
-            yield [], [], None, None
-
-        finally:
-            try:
-                if new_path.exists():
-                    shutil.rmtree(new_path, onerror=self._on_rm_error)
-                if old_path.exists():
-                    shutil.rmtree(old_path, onerror=self._on_rm_error)
-                if old_path.parent.exists():
-                    shutil.rmtree(old_path.parent, onerror=self._on_rm_error)
-            except PermissionError as e:
-                logging.warning(f"[{repo}] Failed to delete {new_path} or {old_path}")
-
-            try:
-                if new_structure and new_structure.process and new_structure.process.docker.container:
-                    new_structure.process.docker.stop_container()
-                    logging.info(f"[{repo.full_name}] Stopped the container")
-                elif old_structure and old_structure.process and old_structure.process.docker.container:
-                    old_structure.process.docker.stop_container()
-                    logging.info(f"[{repo.full_name}] Stopped the container")
-            except Exception as e:
-                logging.warning(f"[{repo.full_name}] Failed to stop container: {e}")
-
-    def _on_rm_error(self, func, path, exc_info):
-        os.chmod(path, stat.S_IWRITE)
-        func(path)
-
-    def test_input_folder(self) -> None:
-        """Test all Docker images in input folder"""
-        input_file = Path(self.config.input_file)
-        if not input_file.exists():
-            raise ValueError(f"Input folder {self.config.input_file} does not exist")
-        
-        for tar_file in input_file.glob("*.tar"):
-            image_name = tar_file.stem
-            logging.info(f"Testing image: {image_name}")
-            significant = self._test_docker_image(image_name, tar_file)
-            if significant:
-                logging.info(f"[{image_name}] improves the performance significantly.")
-            else:
-                logging.info(f"[{image_name}] does not improve the performance.")
-
-    def _test_docker_image(self, image_name: str, tar_file: Path) -> bool:
-        cmd = ["docker", "load", "-i", str(tar_file)]
-        subprocess.run(cmd)
-
-        try:
-            docker = DockerManager(self.config, Path(), image_name, self.config.testing.docker_test_dir)
-            docker.start_docker_container(image_name)
-
-            new_times: list[float] = []
-            old_times: list[float] = []
-            warmup: int = self.config.testing.warmup
-            for _ in range(warmup + self.config.testing.commit_test_times):
-                cmd = ["bash", f"{self.config.testing.docker_test_dir}/new_test.sh"]
-                exit_code, stdout, stderr, time = docker.run_command_in_docker(cmd, Path())
-                stats = parse_ctest_output(stdout)
-                elapsed: float = stats['total_time_sec']
-                new_times.append(elapsed)
-
-                cmd = ["bash", f"{self.config.testing.docker_test_dir}/old_test.sh"]
-                exit_code, stdout, stderr, time = docker.run_command_in_docker(cmd, Path())
-                stats = parse_ctest_output(stdout)
-                elapsed: float = stats['total_time_sec']
-                old_times.append(elapsed)
-                
-            #test = TestAnalyzer(self.config, [], [], warmup, self.config.testing.commit_test_times)
-            return True #test.get_improvement_p_value(new_times[warmup:], old_times[warmup:]) < self.config.commits_time['min-p-value']
-        
-        except:
-            logging.error("")
-            return False
-
-        finally:
-            docker.stop_container()
-
-
-    def test_mounted_against_docker(self, docker_image: str, mount: str):
-        """Test mounted directory against saved Docker image"""
-        mount_dir = Path(mount)
-        
-        if not mount_dir.exists():
-            raise ValueError(f"Mount directory {mount_dir} does not exist")
-        
-        # TODO: .tar file or docker image, check if docker_image already exist
-        if docker_image.endswith(".tar"):
-            cmd = ["docker", "load", "-i", str(docker_image)]
-            subprocess.run(cmd)
-            docker_image = docker_image.removesuffix(".tar")
-        docker = DockerManager(self.config, mount_dir, docker_image, self.config.testing.docker_test_dir)
-        docker.start_docker_container(docker_image)
-
+import logging, os, stat, shutil, random, json
+from tqdm import tqdm
+from src.core.filter.structure_filter import StructureFilter
+from src.core.filter.process_filter import ProcessFilter
+from src.cmake.process import CMakeProcess
+from src.config.config import Config
+from src.utils.test_analyzer import TestAnalyzer
+from pathlib import Path
+from src.utils.parser import *
+from src.utils.writer import Writer
+from contextlib import contextmanager
+from pathlib import Path
+from typing import Generator, Optional, Any
+from github.Repository import Repository
+from src.utils.exceptions import TestFailed, UndefinedStructureFilter
+from src.utils.image_handling import image_exists, image
+
+class DockerTester:
+    def __init__(self, repo: Repository, config: Config):
+        self.repo = repo
+        self.repo_id = self.repo.full_name
+        self.config = config
+
+    def run_commit_pair(
+        self,
+        new_sha: str,
+        old_sha: str,
+        new_path: Path,
+        old_path: Path,
+        cpuset_cpus: str = ""
+    ) -> None:
+        
+        with self._commit_pair_test(
+            self.config, new_path, old_path, new_sha, old_sha, cpuset_cpus
+        ) as (new_times, old_times, new_process, old_process):
+            logging.info(f"Times Old: {old_times}, New: {new_times}")
+            
+            if not new_process or not old_process:
+                return
+            
+            new_build_cmd = [" ".join(s) for s in new_process.build_commands]
+            old_build_cmd = [" ".join(s) for s in old_process.build_commands]
+            new_test_cmd = [" ".join(s) for s in new_process.test_commands]
+            old_test_cmd = [" ".join(s) for s in old_process.test_commands]
+
+            # --genimages just generates the docker image from an existing json results file
+            if self.config.genimages and not self.config.test:
+                file_name = "_".join(self.repo_id.split("/") + [new_sha]) + ".json"
+                json_file = Path(self.config.input, file_name) 
+                
+                with open(json_file, 'r', errors='ignore') as f:
+                    results = json.load(f)
+
+                results["build_info"]["old_build_script"] = old_build_cmd
+                results["build_info"]["new_build_script"] = new_build_cmd
+                results["build_info"]["old_test_script"] = old_test_cmd
+                results["build_info"]["new_test_script"] = new_test_cmd
+
+                with open(json_file, 'w', errors='ignore') as f:
+                    json.dump(results, f, indent=4)
+
+                old_process.save_docker_image(self.repo_id, new_sha, new_build_cmd, old_build_cmd, new_test_cmd, old_test_cmd, results)
+                logging.info(f"[{self.repo_id}:{new_sha}] Docker image saved.")
+                return
+
+            warmup = self.config.testing.warmup
+
+            new_single_tests_d = new_process.per_test_times 
+            old_single_tests_d = old_process.per_test_times
+
+            new_single_tests = {
+                test: (
+                    new_single_tests_d[test]['parsed']
+                    if 0.0 not in new_single_tests_d[test]['parsed']
+                    else new_single_tests_d[test]['time']
+                )
+                for test in new_single_tests_d.keys()
+            }
+
+            old_single_tests = {
+                test: (
+                    old_single_tests_d[test]['parsed']
+                    if 0.0 not in old_single_tests_d[test]['parsed']
+                    else old_single_tests_d[test]['time']
+                )
+                for test in old_single_tests_d.keys()
+            }
+            
+            test = TestAnalyzer(
+                self.config, new_single_tests, old_single_tests
+            )
+
+            total_improvement = test.get_pair_improvement_p_value(
+                old_times[warmup:], new_times[warmup:] 
+            )
+            logging.info(f"pvalue: {total_improvement}")
+
+            isolated_improvements = test.get_significant_test_time_changes(test.get_pair_improvement_p_value)
+            logging.info(f"new outperforms old: {isolated_improvements['new_outperforms_old']}")
+            overall_change = test.get_overall_change()
+            logging.info(f"overall change: {overall_change}")
+            overall_change_with_new_outperforms_old = (
+                len(isolated_improvements['new_outperforms_old']) > 0 and 
+                overall_change > self.config.overall_decline_limit and
+                len(isolated_improvements['old_outperforms_new']) == 0
+            )
+
+            commit = self.repo.get_commit(new_sha)
+            results = test.create_test_log(
+                commit, self.repo, old_sha, new_sha,
+                old_times, new_times, new_build_cmd, old_build_cmd, new_test_cmd, old_test_cmd,
+            )
+            logging.info(f"Results: {results['performance_analysis']}")
+            writer = Writer(self.repo_id, self.config.storage_paths["performance"])
+            writer.write_results(results)
+
+            old_process.save_docker_image(self.repo_id, new_sha, new_build_cmd, old_build_cmd, new_test_cmd, old_test_cmd, results)
+                
+            if total_improvement < self.config.min_p_value or overall_change_with_new_outperforms_old:
+                logging.info(f"[{self.repo_id}:{new_sha}] significantly improves execution time.")
+                writer.write_improve(results)
+                        
+
+    @contextmanager
+    def _commit_pair_test(
+        self, 
+        config: Config, 
+        new_path: Path, 
+        old_path: Path, 
+        new_sha: str, 
+        old_sha: str,
+        cpuset_cpus: str = ""
+    ) -> Generator[tuple[list[float], list[float], Optional[CMakeProcess], Optional[CMakeProcess]], Any, Any]:
+        """
+        Start a container for new/old commits and stop container automatically after both runs.
+        """
+        new_pf = ProcessFilter(self.repo, config, new_path, new_sha)
+        old_pf = ProcessFilter(self.repo, config, old_path, old_sha)
+        docker_image = ""
+
+        new_times = []
+        old_times = []
+
+        new_process = None
+        old_process = None
+
+        try:
+            local_image = image(self.repo_id, new_sha)
+            container_name = local_image
+            if (self.config.testdocker or self.config.testpatch) and not image_exists(self.repo_id, new_sha) and self.config.check_dockerhub:
+                container_name = f"{self.config.dockerhub_user}/{self.config.dockerhub_repo}:{local_image}"
+
+            new_process = new_pf.commit_setup_and_build("New", container_name=container_name, cpuset_cpus=cpuset_cpus)
+            docker_image = new_process.docker_image if new_process else ""
+            
+            if not new_process:
+                raise UndefinedStructureFilter("New commit CMakeProcess is None")
+
+            old_process = old_pf.commit_setup_and_build("Old", container_name=container_name, docker_image=docker_image)
+            
+            if not old_process:
+                raise UndefinedStructureFilter("Old commit CMakeProcess is None")
+            
+            # --genimages just generates the docker image from an existing json results file
+            if (not self.config.genimages or self.config.test):
+                self._run_tests(new_process, old_process, new_pf, old_pf)
+
+            new_cmd_times = new_process.test_time
+            old_cmd_times = old_process.test_time
+
+            new_times = new_cmd_times['time'] if 0.0 in new_cmd_times['parsed'] else new_cmd_times['parsed']
+            old_times = old_cmd_times['time'] if 0.0 in old_cmd_times['parsed'] else old_cmd_times['parsed']
+
+            yield new_times, old_times, new_process, old_process
+
+        except TestFailed as e:
+            logging.error("Test failed early, stopping the test loops.")
+            logging.error(str(e))
+            yield [], [], None, None
+
+        except UndefinedStructureFilter as e:
+            logging.error(str(e))
+            yield [], [], None, None
+
+        finally:
+            self._remove_cloned_repo_folders(self.repo_id, new_path, old_path)
+            if new_process:
+                new_process.docker.stop_container(self.repo_id)
+            elif old_process:
+                old_process.docker.stop_container(self.repo_id)
+
+    def _run_tests(self, new_process: CMakeProcess, old_process: CMakeProcess, new_pf: ProcessFilter, old_pf: ProcessFilter) -> None:
+        if not new_process:
+            raise UndefinedStructureFilter("The CMakeProcess for the new commit is None")
+        
+        if not old_process:
+            raise UndefinedStructureFilter("The CMakeProcess for the old commit is None")
+            
+        new_test_cmd = new_process.test_commands
+        old_test_cmd = old_process.test_commands
+        logging.debug(f"New cmd: {new_process.test_commands}")
+        logging.debug(f"Old cmd: {old_process.test_commands}")
+        assert len(new_test_cmd) == len(old_test_cmd)
+
+        warmup = self.config.testing.warmup
+        test_repeat = self.config.testing.commit_test_times
+        has_list_args = len(new_test_cmd) > 1
+        
+        try:
+            for new_cmd, old_cmd in tqdm(zip(new_test_cmd, old_test_cmd), total=len(new_test_cmd), position=1, leave=False, mininterval=5): 
+                for _ in tqdm(range(warmup+test_repeat), total=warmup+test_repeat, desc="Commit pair test", position=2, leave=False, mininterval=5):
+                    order = [
+                        ("New", new_cmd, new_process, new_pf),
+                        ("Old", old_cmd, old_process, old_pf),
+                    ]
+                    random.shuffle(order)
+
+                    for label, cmd, process, pf in order:
+                        if cmd and cmd[0] == "cd":
+                            continue
+                        if not pf.test_run(label, cmd, process, has_list_args):
+                            raise TestFailed(f"Test run '{cmd}' failed")
+        except TestFailed:
+            has_list_args = False
+            new_test_cmd = old_test_cmd = [["ctest", "--output-on-failure"]]
+            for new_cmd, old_cmd in tqdm(zip(new_test_cmd, old_test_cmd), total=len(new_test_cmd), position=1, leave=False, mininterval=5): 
+                for _ in tqdm(range(warmup+test_repeat), total=warmup+test_repeat, desc="Commit pair test", position=2, leave=False, mininterval=5):
+                    order = [
+                        ("New", new_cmd, new_process, new_pf),
+                        ("Old", old_cmd, old_process, old_pf),
+                    ]
+                    random.shuffle(order)
+
+                    for label, cmd, process, pf in order:
+                        if not pf.test_run(label, cmd, process, has_list_args):
+                            raise TestFailed(f"Test run '{cmd}' with test framework '{label}' failed")
+            
+
+    def _remove_cloned_repo_folders(self, repo_id: str, new_path: Path, old_path: Path):
+        try:
+            if new_path.exists():
+                shutil.rmtree(new_path, onerror=self._on_rm_error)
+            if old_path.exists():
+                shutil.rmtree(old_path, onerror=self._on_rm_error)
+            if old_path.parent.exists():
+                shutil.rmtree(old_path.parent, onerror=self._on_rm_error)
+        except PermissionError as e:
+            logging.warning(f"[{repo_id}] Failed to delete {new_path} or {old_path}")
+
+    def _on_rm_error(self, func, path, exc_info):
+        os.chmod(path, stat.S_IWRITE)
+        func(path)
diff --git a/src/core/filter/__init__.py b/src/core/filter/__init__.py
old mode 100644
new mode 100755
diff --git a/src/core/filter/commit_filter.py b/src/core/filter/commit_filter.py
old mode 100644
new mode 100755
index 8a53e9c..d8594fc
--- a/src/core/filter/commit_filter.py
+++ b/src/core/filter/commit_filter.py
@@ -1,412 +1,526 @@
-import re, logging, os, json
-from github.Commit import Commit
-from github.Repository import Repository
-from src.llm.prompt import Prompt
-from src.llm.openai import OpenRouterLLM
-from src.llm.ollama import OllamaLLM
-from typing import Optional
-from src.config.config import Config
-from github.GithubException import UnknownObjectException
-
-perf_label_keywords = {
-    'performance', 'perf', 'optimization', 'optimisation', 'optimize', 'optimize',
-    'speed', 'latency', 'throughput', 'regression: performance', 'perf regression',
-    'slow', 'slowness', 'timeout', 'benchmark', 'hot path', 'hotpath', 'cpu', 'gc',
-    'jmh', 'efficiency'
-}
-perf_text_keywords = {
-    'performance', 'perf', 'optimiz', 'speed', 'latency', 'throughput',
-    'slow', 'slowness', 'timeout', 'hang', 'stall', 'regression', 'benchmark',
-    'hot path', 'hotpath', 'cpu', 'allocation', 'alloc', 'gc', 'jmh', 'efficiency',
-    'big-o', 'complexity'
-}
-runtime_hint_keywords = {
-    'latency', 'throughput', 'slow', 'slowness', 'timeout', 'hang', 'stall', 'cpu', 
-    'jmh', 'benchmark', 'regression'
-}
-
-class CommitFilter:
-    def __init__(self, commit: Commit, config: Config, repo: Repository):
-        self.commit = commit
-        self.config = config
-        self.llm1 = OpenRouterLLM(self.config, self.config.llm.model1)
-        self.llm2 = OpenRouterLLM(self.config, self.config.llm.model2)
-        self.repo = repo
-        self.cache: dict[str, dict[str, dict[str, bool]]] = self._load_cache()
-
-    def accept(self, max_msg_size: int = -1) -> bool: 
-        logging.info(f"[{self.repo.full_name}] ({self.commit.sha}) Filtering...")
-        name = self.config.llm.ollama_stage1_model + "_" + self.config.llm.ollama_stage2_model if self.config.llm.ollama_enabled else self.config.llm.model1 + "_" + self.config.llm.model2
-        cached = self.cache.get(self.repo.full_name, {}).get(self.config.filter_type + (f"_{name}" if self.config.filter_type == "llm" else ""), {}).get(self.commit.sha)
-        if cached is not None:
-            logging.info(f"Cache hit for {self.commit.sha} ({self.config.filter_type}) -> {cached}")
-            return cached
-
-        if self.config.filter_type == "simple":
-            result = self._simple_filter() and self._only_cpp_source_modified()
-            self._save_cache(self.commit, result)
-        elif self.config.filter_type == "llm":
-            result = self._only_cpp_source_modified() and self._llm_filter(max_msg_size=max_msg_size) 
-            self._save_cache(self.commit, result, extra=f"_{name}")
-        elif self.config.filter_type == "issue":
-            result = self._only_cpp_source_modified() and self.fixed_performance_issue() is not None
-            self._save_cache(self.commit, result, extra=f"_{name}")
-        else:
-            result = False
-
-        return result
-    
-    def _simple_filter(self) -> bool:
-        msg = self.commit.commit.message.lower()
-        return any(k in msg for k in perf_label_keywords | perf_text_keywords | runtime_hint_keywords)
-
-    def _llm_filter(self, max_msg_size: int) -> bool:
-
-        if self.fixed_performance_issue():
-            return True
-
-        msg = self.commit.commit.message
-        if max_msg_size != -1 and len(msg) > max_msg_size:
-            msg = msg[:max_msg_size] + "..."
-        
-        """
-        p = Prompt([Prompt.Message("user",
-            (self.config.llm['stage1']
-                .replace("<name>", self.repo.full_name)
-                .replace("<message>", self.commit.commit.message)
-            )
-        )])
-        """
-        diff = self.get_diff()
-
-        p = Prompt([
-            Prompt.Message("system", 
-                "You are a strict binary classifier. "
-                "Determine if the commit improves runtime performance (e.g., reduces CPU usage, improves memory efficiency, speeds up execution). "
-                "Do not count bug fixes, correctness changes, refactoring, or style cleanups as performance improvements. "
-                "Respond ONLY in this JSON format: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
-                "If you do not have enough information to decide, say {\"answer\": \"no\"}."
-                "Do not add any explanation or commentary."),
-            Prompt.Message("user",
-                (self.config.stage1_prompt
-                    .replace("<name>", self.repo.full_name)
-                    .replace("<message>", self.commit.commit.message)
-                    .replace("<diff>", diff))
-        )])
-
-        if self.config.llm.ollama_enabled:
-            self.llm = OllamaLLM(self.config, self.config.llm.ollama_stage1_model)
-        logging.info(f"[{self.repo.full_name}] First LLM prompt: {p.messages[1].content}")
-        res = self.llm.generate(p)
-        logging.info(f"[{self.repo.full_name}] First LLM returned: {res}")
-
-        if 'yes' in res.lower():
-            """
-            likelihood = self._extract_likelihood(res)
-            if likelihood is None:
-                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} did not return a valid likelihood.")
-                return False
-
-            if likelihood < self.config.likelihood['min_likelihood']:
-                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} has a likelihood of {likelihood}%, which is below the threshold.")
-                return False
-            if likelihood >= self.config.likelihood['max_likelihood']:
-                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} has a high likelihood of being a performance commit ({likelihood}%).")
-                return True
-            """
-
-            p = Prompt([
-                Prompt.Message("system", 
-                    "You are a strict binary classifier. "
-                    "Determine if the commit improves runtime performance (e.g., reduces CPU usage, improves memory efficiency, speeds up execution). "
-                    "Do not count bug fixes, correctness changes, refactoring, or style cleanups as performance improvements. "
-                    "Respond ONLY in this JSON format: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
-                    "If you do not have enough information to decide, say {\"answer\": \"no\"}."
-                    "Do not add any explanation or commentary."),
-                Prompt.Message("user",
-                    (self.config.stage2_prompt
-                        .replace("<name>", self.repo.full_name)
-                        .replace("<message>", self.commit.commit.message)
-                        .replace("<diff>", diff))
-            )])
-
-            if self.config.llm.ollama_enabled:
-                self.llm = OllamaLLM(self.config, self.config.llm.ollama_stage2_model)
-            logging.info(f"[{self.repo.full_name}] Second LLM prompt: {p.messages[1].content}")
-            res = self.llm.generate(p)
-            logging.info(f"[{self.repo.full_name}] Second LLM returned: {res}")
-            if 'yes' in res.lower():
-                return True
-            
-        return False
-    
-    def _extract_likelihood(self, text: str) -> Optional[float]:
-        match = re.search(r"likelihood\s*[:\-]?\s*([0-9]+(?:\.[0-9]+)?)%?", text, flags=re.IGNORECASE)
-        return float(match.group(1)) if match else None
-    
-    def get_diff(self) -> str:
-        diff_lines: list[str] = []
-        for f in self.commit.files:
-            if f.patch:
-                diff_lines.append(f"--- {f.filename}")
-                patch = f.patch
-                if len(patch) > 8000:
-                    diff_lines.append("\n... [diff truncated] ...")
-                    break
-                diff_lines.append(patch)
-        return "\n".join(diff_lines)
-
-    def _modify_cpp_filter(self) -> bool:
-        """Returns True if the commit modifies any C++ source or header files."""
-        cpp_extensions = (".cpp", ".cc", ".cxx", ".c++", ".h", ".hpp")
-        ignore_dirs = ("third_party", "vendor", "external")
-        for f in self.commit.files:
-            if f.filename.endswith(cpp_extensions) and not any(d in f.filename for d in ignore_dirs):
-                return True
-        return False
-    
-    def _only_cpp_source_modified(self) -> bool:
-        """Return True if commit modifies only C++ source/header files (no tests, no non-source)."""
-        cpp_extensions = (".cpp", ".cc", ".cxx", ".c++", ".h", ".hpp")
-        ignore_dirs = ("third_party", "vendor", "external")
-
-        for f in self.commit.files:
-            filename = f.filename
-            if any(d in filename for d in ignore_dirs):
-                continue
-
-            if not filename.endswith(cpp_extensions):
-                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} changes a non-source file: {filename}.")
-                return False
-            
-            if any(filename.startswith(tdir) for tdir in self.config.valid_test_dirs):
-                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} changes a test file: {filename}")
-                return False
-            
-        return True
-    
-    def _modify_test_filter(self) -> bool:
-        """Check whether a commit has modified a file in test directories."""
-        for f in self.commit.files:
-            for tdir in self.config.valid_test_dirs:
-                if f.filename.startswith(tdir) or tdir in f.filename:
-                    return True
-        return False
-    
-    def _save_cache(self, commit: Commit, decision: bool, extra: str = "") -> None:
-        repo_name = self.repo.full_name
-        filter_type = self.config.filter_type + extra
-
-        if repo_name not in self.cache:
-            self.cache[repo_name] = {}
-        if filter_type not in self.cache[repo_name]:
-            self.cache[repo_name][filter_type] = {}
-        self.cache[repo_name][filter_type][self.commit.sha] = decision
-
-        try:
-            with open(self.config.llm.cache_file, "w", encoding="utf-8") as f:
-                json.dump(self.cache, f, indent=2)
-        except Exception as e:
-            logging.warning(f"Failed to write cache: {e}")
-
-    def _load_cache(self) -> dict[str, dict[str, dict[str, bool]]]:
-        path = self.config.llm.cache_file
-        if os.path.exists(path):
-            try:
-                with open(path, "r", encoding="utf-8") as f:
-                    return json.load(f)
-            except json.JSONDecodeError:
-                return {}
-        return {}
-    
-    def extract_fixed_issues(self) -> dict[int, str]:
-        """
-        Extract issue numbers from commit message that are explicitly closed/fixed.
-        
-        Args:
-            commit_message: The commit message to parse
-            repo: The GitHub repository object
-            
-        Returns:
-            Set of issue numbers that are explicitly closed by this commit
-            
-        Examples:
-            - "Fixes #123" -> {123}
-            - "Closes #456 and resolves #789" -> {456, 789}
-            - "Fix GH-123" -> {123}
-            - "Resolves owner/repo#456" -> {456} (if matches current repo)
-        """
-        commit_message = self.commit.commit.message
-        if not commit_message:
-            return {}
-            
-        msg = commit_message.strip()
-        results: dict[int, str] = {}
-
-        # Enhanced regex patterns to catch more formats
-        closing_prefix = r'(?:(?<![A-Za-z])(?:fix|fixes|fixed|close|closes|closed|resolve|resolves|resolved|address|addresses|addressed))(?:\s*[:\-])?\s+'
-        
-        # Individual issue reference patterns (without named groups to avoid conflicts)
-        issue_patterns = [
-            r'#(\d+)',  # #123
-            r'GH-(\d+)',  # GH-123
-            r'issue\s*#(\d+)',  # issue #123
-            r'bug\s*#(\d+)',  # bug #123
-        ]
-        
-        # Full repository reference patterns
-        full_repo_patterns = [
-            r'([\w.-]+/[\w.-]+)#(\d+)',  # owner/repo#123
-            r'https?://github\.com/([\w.-]+/[\w.-]+)/issues/(\d+)',  # Full URL
-        ]
-        
-        # PR patterns (to be filtered out)
-        pr_patterns = [
-            r'https?://github\.com/([\w.-]+/[\w.-]+)/pull/(\d+)',  # PR URL
-        ]
-        
-        # Compile all patterns
-        all_issue_patterns = issue_patterns + full_repo_patterns
-        compiled_issue_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in all_issue_patterns]
-        compiled_pr_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in pr_patterns]
-        
-        # Main fix block pattern (simplified to avoid group conflicts)
-        fix_block = re.compile(
-            closing_prefix + r'(?:' + '|'.join(all_issue_patterns) + r')(?:\s*(?:,|\band\b|&|\bor\b|\|)\s*(?:' + '|'.join(all_issue_patterns) + r'))*', 
-            re.IGNORECASE | re.MULTILINE
-        )
-
-        issue_cache = {}
-        
-        def get_ref_type(n: int) -> str:
-            """Return 'issue', 'pull_request', or 'unknown'."""
-            if n in issue_cache:
-                return issue_cache[n]
-            try:
-                issue = self.repo.get_issue(n)
-                ref_type = "issue" if getattr(issue, "pull_request", None) is None else "pull_request"
-                issue_cache[n] = ref_type
-                return ref_type
-            except UnknownObjectException:
-                issue_cache[n] = "unknown"
-                return "unknown"
-            except Exception as e:
-                logging.warning(f"Error checking issue #{n} in {self.repo.full_name}: {e}")
-                issue_cache[n] = "unknown"
-                return "unknown"
-
-        try:
-            for block in fix_block.finditer(msg):
-                block_text = block.group(0)
-                logging.info(f"Found fix block: '{block_text}'")
-
-                # Extract PR references first
-                for pr_pattern in compiled_pr_patterns:
-                    for match in pr_pattern.finditer(block_text):
-                        repo_name = match.group(1)
-                        number = int(match.group(2))
-                        if repo_name.lower() == self.repo.full_name.lower():
-                            results[number] = "pull_request"
-                            logging.info(f"Found PR reference: {repo_name}#{number}")
-
-                # Extract issue references
-                for pattern in compiled_issue_patterns:
-                    for match in pattern.finditer(block_text):
-                        issue_number = None
-
-                        if len(match.groups()) == 1:
-                            issue_number = int(match.group(1))
-                        elif len(match.groups()) == 2:
-                            repo_name = match.group(1)
-                            if repo_name.lower() == self.repo.full_name.lower():
-                                issue_number = int(match.group(2))
-
-                        if issue_number:
-                            ref_type = get_ref_type(issue_number)
-                            if ref_type in ("issue", "pull_request"):
-                                results[issue_number] = ref_type
-                                logging.info(f"Found {ref_type} reference: #{issue_number}")
-
-        except Exception as e:
-            logging.error(f"Error parsing commit message for issues: {e}")
-            return results
-
-        return results
-
-
-    def fixed_performance_issue(self) -> Optional[int]:
-        msg = self.commit.commit.message or ""
-        refs = self.extract_fixed_issues()
-        if not refs:
-            return None
-
-        issue_title_body_tuples = []
-        for number, ref_type in refs.items():
-            try:
-                gh_issue = self.repo.get_issue(number)
-
-                title = gh_issue.title or ""
-                body = gh_issue.body or ""
-                issue_title_body_tuples.append((number, title, body))
-
-                ref_type = ref_type.replace('_', ' ')
-                p = Prompt(messages=[
-                    Prompt.Message("system", 
-                        "You are a strict binary classifier. "
-                        "Determine if the commit improves runtime performance (e.g., reduces CPU usage, improves memory efficiency, speeds up execution). "
-                        "Do not count bug fixes, correctness changes, refactoring, or style cleanups as performance improvements. "
-                        "Respond ONLY in this JSON format: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
-                        "If you do not have enough information to decide, say {\"answer\": \"no\"}."
-                        "Do not add any explanation or commentary."),
-                    Prompt.Message("user",
-                        f"The following is a {ref_type} in the {self.repo.full_name} repository:\n\n"
-                        f"###{ref_type} Title###{title}\n###{ref_type} Title End###\n\n"
-                        f"###{ref_type} Body###{body}\n###{ref_type} Body End###\n\n"
-                        f"The following is the commit message that fixes this {ref_type}:\n\n"
-                        f"###Commit Message###{msg}\n###Commit Message End###\n\n"
-                        f"Answer strictly in this JSON format (do not add any explanation):\n"
-                        f"{{\"answer\": \"yes\"}} or {{\"answer\": \"no\"}}.\n\n"
-                        f"Question: Is this issue likely related to improving execution time?"
-                )])
-                if self.config.llm.ollama_enabled:
-                    self.llm1 = OllamaLLM(self.config, self.config.llm.ollama_stage1_model)
-                logging.info(f"First LLM issue prompt: {p.messages[1].content}")
-                res = self.llm1.generate(p)
-                logging.info(f"First LLM issue response: {res}")
-
-                if "yes" in res.lower().strip():
-                    logging.info(f"Commit {self.commit.sha} in {self.repo.full_name} is related to a likely performance issue (#{number}).")
-
-                    p = Prompt(messages=[
-                        Prompt.Message("system", 
-                            "You are a strict binary classifier. "
-                            "Determine if the commit improves runtime performance (e.g., reduces CPU usage, improves memory efficiency, speeds up execution). "
-                            "Do not count bug fixes, correctness changes, refactoring, or style cleanups as performance improvements. "
-                            "Respond ONLY in this JSON format: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
-                            "If you do not have enough information to decide, say {\"answer\": \"no\"}."
-                            "Do not add any explanation or commentary."),
-                        Prompt.Message("user",
-                            f"The following is a {ref_type} in the {self.repo.full_name} repository:\n\n"
-                            f"###{ref_type} Title###{title}\n###{ref_type} Title End###\n\n"
-                            f"###{ref_type} Body###{body}\n###{ref_type} Body End###\n\n"
-                            f"The following is the commit message that fixes this {ref_type}:\n\n"
-                            f"###Commit Message###{msg}\n###Commit Message End###\n\n"
-                            f"Answer strictly in this JSON format (do not add any explanation):\n"
-                            f"{{\"answer\": \"yes\"}} or {{\"answer\": \"no\"}}.\n\n"
-                            f"Question: Is this issue likely related to improving execution time?"
-                    )])
-                    if self.config.llm.ollama_enabled:
-                        self.llm2 = OllamaLLM(self.config, self.config.llm.ollama_stage2_model)
-                    logging.info(f"Second LLM issue prompt: {p.messages[1].content}")
-                    res = self.llm2.generate(p)
-                    logging.info(f"Second LLM issue response: {res}")
-
-                    if "yes" in res.lower().strip():
-                        logging.info(f"Commit {self.commit.sha} in {self.repo.full_name} is related to a likely performance issue (#{number}).")
-                        return number
-
-            except UnknownObjectException:
-                continue
-            
-        logging.info(f"Commit {self.commit.sha} in {self.repo.full_name} is not fixing performance issues.")
-        return None
+import re, logging, os, json
+from github.Commit import Commit
+from github.Repository import Repository
+from src.llm.prompt import Prompt
+from src.llm.openai import OpenRouterLLM
+from src.llm.ollama import OllamaLLM
+from typing import Optional
+from src.config.config import Config
+from github.GithubException import UnknownObjectException
+
+class CommitFilter:
+    def __init__(self, commit: Commit, config: Config, repo: Repository):
+        self.commit = commit
+        self.config = config
+        self.repo = repo
+        self.cache: dict[str, dict[str, dict[str, bool]]] = self._load_cache()
+        self.is_issue: bool = False
+
+    def accept(self) -> bool: 
+        logging.info(f"[{self.repo.full_name}] ({self.commit.sha}) Filtering...")
+        name = self.config.llm.ollama_stage1_model + "_" + self.config.llm.ollama_diff_model + "_" + self.config.llm.ollama_stage2_model if self.config.llm.ollama_enabled else self.config.llm.model1 + "_" + self.config.llm.model2
+        cached = self.cache.get(self.repo.full_name, {}).get(self.config.filter_type + (f"_{name}" if self.config.filter_type == "llm" else ""), {}).get(self.commit.sha)
+        if cached is not None:
+            logging.info(f"Cache hit for {self.commit.sha} ({self.config.filter_type}) -> {cached}")
+            return cached
+
+        if self.config.filter_type == "simple":
+            result = self._simple_filter() and self.only_cpp_source_modified()
+            self._save_cache(self.commit, result)
+        elif self.config.filter_type == "llm":
+            result = self.only_cpp_source_modified() and self._llm_filter() 
+            self._save_cache(self.commit, result, extra=f"_{name}")
+        elif self.config.filter_type == "issue":
+            result = self.only_cpp_source_modified() and self._fixed_performance_issue() is not None
+            self._save_cache(self.commit, result, extra=f"_{name}")
+        else:
+            result = False
+
+        return result
+    
+############ LLM ############
+    
+    def _llm_filter(self) -> bool:
+        """Uses the issue/PR filter + commit message filter + diff filter"""
+        performance_issue = self._fixed_performance_issue()
+        if performance_issue:
+            return True
+
+        msg = self.commit.commit.message
+
+        # ============ STAGE 1: Commit Message Analysis ============
+        system_prompt = (
+            "You are a strict binary classifier. "
+            "Determine if the commit improves runtime performance (makes code execute faster). "
+            "Do not count bug fixes, correctness changes, refactoring, or style cleanups. "
+            "Respond ONLY in JSON: {\"answer\": \"yes\"}, {\"answer\": \"no\"}, or {\"answer\": \"maybe\"}."
+        )
+
+        user_prompt = (
+            f"Repository: {self.repo.full_name}\n"
+            f"Commit Message:\n###MESSAGE START###{msg}\n###MESSAGE END###\n"
+            f"Question: Does this commit message indicate a runtime performance improvement?"
+        )
+
+        p = Prompt([
+            Prompt.Message("system", system_prompt),
+            Prompt.Message("user", user_prompt)
+        ])
+
+        if self.config.llm.ollama_enabled:
+            self.llm1 = OllamaLLM(self.config, self.config.llm.ollama_stage1_model)
+        else:
+            self.llm1 = OpenRouterLLM(self.config, self.config.llm.model1)
+        logging.info(f"[{self.repo.full_name}] First LLM prompt: {p.messages[1].content}")
+        res = self.llm1.generate(p)
+        logging.info(f"[{self.repo.full_name}] First LLM returned: {res}")
+
+        res_lower = res.lower()
+        stage1_answer = "maybe"
+        if "yes" in res_lower and "no" not in res_lower:
+            stage1_answer = "yes"
+        elif "no" in res_lower:
+            stage1_answer = "no"
+
+        if stage1_answer == "no":
+            logging.info(f"[{self.repo.full_name}] Stage 1 rejected (no)")
+            return False
+
+        diff_text = None
+
+        # ============ STAGE 1.5: Diff Verification ============
+        if stage1_answer == "maybe":
+            logging.info(f"[{self.repo.full_name}] Stage 1 uncertain, checking file diffs...")
+            
+            any_file_performance = False
+            files_checked = 0
+            
+            for f in self.commit.files:
+                if not f.patch:
+                    continue
+                    
+                files_checked += 1
+                
+                patch = f.patch
+                if len(patch) > 8000:
+                    patch = patch[:8000] + "\n... [diff truncated] ..."
+                
+                diff_text = f"File: {f.filename}\n{patch}"
+
+                system_prompt = (
+                    "You are a strict binary classifier. "
+                    "Determine if the commit improves runtime performance (makes code execute faster). "
+                    "Do not count bug fixes, correctness changes, refactoring, or style cleanups. "
+                    "Respond ONLY in JSON: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
+                    "If you do not have enough information to decide, say {\"answer\": \"no\"}. "
+                )
+                
+                file_prompt = (
+                    f"Repository: {self.repo.full_name}\n"
+                    f"Commit Message:\n###MESSAGE START###{msg}\n###MESSAGE END###\n"
+                    f"One of the patched files (diff):###DIFF START###\n{diff_text}\n###DIFF END###\n"
+                    f"Question: Does this diff show a test measureable runtime performance improvement?"
+                )
+                
+                p = Prompt([
+                    Prompt.Message("system", system_prompt),
+                    Prompt.Message("user", file_prompt)
+                ])
+                
+                if self.config.llm.ollama_enabled:
+                    self.llm2 = OllamaLLM(self.config, self.config.llm.ollama_diff_model)
+                else:
+                    self.llm2 = OpenRouterLLM(self.config, self.config.llm.model2)
+                logging.info(f"[{self.repo.full_name}] Checking file {f.filename}:\n{p.messages[1].content}")
+                res = self.llm2.generate(p)
+                logging.info(f"[{self.repo.full_name}] File {f.filename} response: {res}")
+                
+                if "yes" in res.lower() and "no" not in res.lower():
+                    any_file_performance = True
+                    logging.info(f"[{self.repo.full_name}] File {f.filename} indicates performance improvement")
+                    return True
+            
+            if not any_file_performance:
+                logging.info(f"[{self.repo.full_name}] No files indicate performance improvement ({files_checked} checked)")
+                return False
+        
+        # ============ STAGE 2: Full Context Verification ============        
+        logging.info(f"[{self.repo.full_name}] Proceeding to Stage 2 verification")
+
+        diff_text = self.get_diff()
+        
+        stage2_system = (
+            "You are a strict binary classifier. "
+            "Determine if the commit improves runtime performance (makes code execute faster). "
+            "Do not count bug fixes, correctness changes, refactoring, or style cleanups. "
+            "Respond ONLY in JSON: {\"answer\": \"yes\"} or {\"answer\": \"no\"}."
+            "If you do not have enough information to decide, say {\"answer\": \"no\"}."
+        )
+            
+        stage2_prompt = (
+            f"Repository: {self.repo.full_name}\n"
+            f"Commit Message:\n###MESSAGE START###{msg}\n###MESSAGE END###\n"
+            f"Code Changes:###DIFF START###\n{diff_text}\n###DIFF END###\n"
+            f"Question: Does this commit improve test measurable runtime performance?"
+        )
+        
+        p = Prompt([
+            Prompt.Message("system", stage2_system),
+            Prompt.Message("user", stage2_prompt)
+        ])
+        
+        if self.config.llm.ollama_enabled:
+            self.llm2 = OllamaLLM(self.config, self.config.llm.ollama_stage2_model)
+        else:
+            self.llm2 = OpenRouterLLM(self.config, self.config.llm.model2)
+        logging.info(f"[{self.repo.full_name}] Second LLM prompt: {p.messages[1].content}")
+        res = self.llm2.generate(p)
+        logging.info(f"[{self.repo.full_name}] Second LLM returned: {res}")
+        if 'yes' in res.lower() and "no" not in res.lower():
+            return True
+            
+        return False
+    
+    def get_diff(self) -> str:
+        diff_lines: list[str] = []
+        for f in self.commit.files:
+            if f.patch:
+                diff_lines.append(f"--- {f.filename}")
+                patch = f.patch
+                if len(patch) > 8000:
+                    diff_lines.append("\n... [diff truncated] ...")
+                    break
+                diff_lines.append(patch[:8000])
+        return "\n".join(diff_lines)
+
+    def _modify_cpp_filter(self) -> bool:
+        """Returns True if the commit modifies any C++ source or header files."""
+        cpp_extensions = (".cpp", ".cc", ".cxx", ".c++", ".h", ".hpp")
+        ignore_dirs = ("third_party", "vendor", "external")
+        for f in self.commit.files:
+            if f.filename.endswith(cpp_extensions) and not any(d in f.filename for d in ignore_dirs):
+                return True
+        return False
+    
+    def only_cpp_source_modified(self) -> bool:
+        """Return True if commit modifies only C++ source/header files (no tests, no non-source)."""
+        cpp_extensions = (".cpp", ".cc", ".cxx", ".c++", ".h", ".hpp")
+        ignore_dirs = ("third_party", "vendor", "external")
+
+        for f in self.commit.files:
+            filename = f.filename
+            if any(d in filename for d in ignore_dirs):
+                continue
+
+            if not filename.endswith(cpp_extensions):
+                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} changes a non-source file: {filename}.")
+                return False
+            
+            if any(tdir in filename for tdir in self.config.valid_test_dirs):
+                logging.info(f"[{self.repo.full_name}] Commit {self.commit.sha} changes a test file: {filename}")
+                return False
+            
+        return True
+    
+    def _modify_test_filter(self) -> bool:
+        """Check whether a commit has modified a file in test directories."""
+        for f in self.commit.files:
+            for tdir in self.config.valid_test_dirs:
+                if f.filename.startswith(tdir) or tdir in f.filename:
+                    return True
+        return False
+    
+    def _save_cache(self, commit: Commit, decision: bool, extra: str = "") -> None:
+        repo_name = self.repo.full_name
+        filter_type = self.config.filter_type + extra
+
+        if repo_name not in self.cache:
+            self.cache[repo_name] = {}
+        if filter_type not in self.cache[repo_name]:
+            self.cache[repo_name][filter_type] = {}
+        self.cache[repo_name][filter_type][self.commit.sha] = decision
+
+        try:
+            with open(self.config.llm.cache_file, "w", encoding="utf-8") as f:
+                json.dump(self.cache, f, indent=2)
+        except Exception as e:
+            logging.warning(f"Failed to write cache: {e}")
+
+    def _load_cache(self) -> dict[str, dict[str, dict[str, bool]]]:
+        path = self.config.llm.cache_file
+        if os.path.exists(path):
+            try:
+                with open(path, "r", encoding="utf-8") as f:
+                    return json.load(f)
+            except json.JSONDecodeError:
+                return {}
+        return {}
+
+############ ISSUE ############
+
+    def _fixed_performance_issue(self) -> Optional[int]:
+        """Returns the first performance issue found. Prompts LLM to check."""
+        performance_issues = self.get_all_performance_issues()
+        return min(performance_issues) if performance_issues else None
+
+    def _is_performance_issue(self, number: int, ref_type: str) -> bool:
+        """
+        Check if an issue/PR is related to performance using LLM.
+        
+        Args:
+            number: Issue or PR number
+            ref_type: "issue" or "pull_request"
+            
+        Returns:
+            True if it's a performance-related issue
+        """
+        try:
+            gh_issue = self.repo.get_issue(number)
+            title = gh_issue.title or ""
+            body = gh_issue.body or ""
+            msg = self.commit.commit.message or ""
+            ref_type_display = ref_type.replace('_', ' ')
+
+            system = (
+                "You are a strict binary classifier. "
+                "Determine if the commit improves runtime performance (e.g., reduces CPU usage, improves memory efficiency, speeds up execution). "
+                "Do not count bug fixes, correctness changes, refactoring, or style cleanups as performance improvements. "
+                "Respond ONLY in this JSON format: {\"answer\": \"yes\"} or {\"answer\": \"no\"}. "
+                "If you do not have enough information to decide, say {\"answer\": \"no\"}."
+                "Do not add any explanation or commentary."
+            )
+                
+            user = (
+                f"The following is a {ref_type_display} in the {self.repo.full_name} repository:\n\n"
+                f"###{ref_type_display} Title###{title}\n###{ref_type_display} Title End###\n\n"
+                f"###{ref_type_display} Body###{body}\n###{ref_type_display} Body End###\n\n"
+                f"The following is the commit message that fixes this {ref_type_display}:\n\n"
+                f"###Commit Message###{msg}\n###Commit Message End###\n\n"
+                f"Answer strictly in this JSON format (do not add any explanation):\n"
+                f"{{\"answer\": \"yes\"}} or {{\"answer\": \"no\"}}.\n\n"
+                f"Question: Is this issue likely related to improving execution time?"
+            )
+
+            p = Prompt([
+                Prompt.Message("system", system),
+                Prompt.Message("user", user)
+            ])
+        
+            
+            # First LLM check
+            if self.config.llm.ollama_enabled:
+                self.llm1 = OllamaLLM(self.config, self.config.llm.ollama_stage1_model)
+            else:
+                self.llm1 = OpenRouterLLM(self.config, self.config.llm.model1)
+            logging.info(f"First LLM check for #{number}")
+            res = self.llm1.generate(p)
+            logging.info(f"First LLM response: {res}")
+            
+            if "yes" not in res.lower().strip():
+                return False
+            
+            # Second LLM check for confirmation
+            if self.config.llm.ollama_enabled:
+                self.llm2 = OllamaLLM(self.config, self.config.llm.ollama_stage2_model)
+            else:
+                self.llm2 = OpenRouterLLM(self.config, self.config.llm.model2)
+            logging.info(f"Second LLM check for #{number}")
+            res = self.llm2.generate(p)
+            logging.info(f"Second LLM response: {res}")
+            
+            return "yes" in res.lower().strip()
+            
+        except UnknownObjectException:
+            logging.warning(f"Issue/PR #{number} not found")
+            return False
+        except Exception as e:
+            logging.error(f"Error checking if #{number} is performance issue: {e}")
+            return False
+
+    def extract_fixed_issues(self) -> dict[int, str]:
+        """Extract issue/PR numbers from commit message that are explicitly closed/fixed."""
+        commit_message = self.commit.commit.message
+        if not commit_message:
+            return {}
+            
+        msg = commit_message.strip()
+        results: dict[int, str] = {}
+
+        closing_prefix = r'(?:(?<![A-Za-z])(?:fix|fixes|fixed|close|closes|closed|resolve|resolves|resolved|address|addresses|addressed))(?:\s*[:\-])?\s+'
+        issue_patterns = [r'#(\d+)', r'GH-(\d+)', r'issue\s*#(\d+)', r'bug\s*#(\d+)']
+        full_repo_patterns = [r'([\w.-]+/[\w.-]+)#(\d+)', r'https?://github\.com/([\w.-]+/[\w.-]+)/issues/(\d+)']
+        pr_patterns = [r'https?://github\.com/([\w.-]+/[\w.-]+)/pull/(\d+)']
+        
+        all_issue_patterns = issue_patterns + full_repo_patterns
+        compiled_issue_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in all_issue_patterns]
+        compiled_pr_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in pr_patterns]
+        
+        fix_block = re.compile(
+            closing_prefix + r'(?:' + '|'.join(all_issue_patterns) + r')(?:\s*(?:,|\band\b|&|\bor\b|\|)\s*(?:' + '|'.join(all_issue_patterns) + r'))*', 
+            re.IGNORECASE | re.MULTILINE
+        )
+
+        issue_cache = {}
+        
+        def get_ref_type(n: int) -> str:
+            """Return 'issue', 'pull_request', or 'unknown'."""
+            if n in issue_cache:
+                return issue_cache[n]
+            try:
+                issue = self.repo.get_issue(n)
+                ref_type = "issue" if getattr(issue, "pull_request", None) is None else "pull_request"
+                issue_cache[n] = ref_type
+                return ref_type
+            except UnknownObjectException:
+                issue_cache[n] = "unknown"
+                return "unknown"
+            except Exception as e:
+                logging.warning(f"Error checking issue #{n} in {self.repo.full_name}: {e}")
+                issue_cache[n] = "unknown"
+                return "unknown"
+
+        try:
+            for block in fix_block.finditer(msg):
+                block_text = block.group(0)
+                logging.info(f"Found fix block: '{block_text}'")
+
+                for pr_pattern in compiled_pr_patterns:
+                    for match in pr_pattern.finditer(block_text):
+                        repo_name = match.group(1)
+                        number = int(match.group(2))
+                        if repo_name.lower() == self.repo.full_name.lower():
+                            results[number] = "pull_request"
+                            logging.info(f"Found PR reference: {repo_name}#{number}")
+
+                for pattern in compiled_issue_patterns:
+                    for match in pattern.finditer(block_text):
+                        issue_number = None
+
+                        if len(match.groups()) == 1:
+                            issue_number = int(match.group(1))
+                        elif len(match.groups()) == 2:
+                            repo_name = match.group(1)
+                            if repo_name.lower() == self.repo.full_name.lower():
+                                issue_number = int(match.group(2))
+
+                        if issue_number:
+                            ref_type = get_ref_type(issue_number)
+                            if ref_type in ("issue", "pull_request"):
+                                results[issue_number] = ref_type
+                                logging.info(f"Found {ref_type} reference: #{issue_number}")
+
+        except Exception as e:
+            logging.error(f"Error parsing commit message for issues: {e}")
+            return results
+
+        return results
+
+    def get_issues_from_pr(self, pr_number: int) -> set[int]:
+        """
+        Get all issues that a PR fixes/closes using GitHub's API.
+        
+        Args:
+            pr_number: The pull request number
+            
+        Returns:
+            Set of issue numbers that the PR closes
+        """
+        try:
+            pr = self.repo.get_pull(pr_number)
+            closed_issues = set()
+            if pr.body:
+                closing_pattern = re.compile(
+                    r'(?:close|closes|closed|fix|fixes|fixed|resolve|resolves|resolved)s?\s+#(\d+)',
+                    re.IGNORECASE
+                )
+                for match in closing_pattern.finditer(pr.body):
+                    closed_issues.add(int(match.group(1)))
+            
+            # Use GitHub's timeline API to get linked issues
+            # This captures issues linked via the UI
+            try:
+                events = pr.get_issue_events()
+                for event in events:
+                    if event.event == "connected" and event.issue:
+                        closed_issues.add(event.issue.number)
+            except Exception as e:
+                logging.warning(f"Could not fetch events for PR #{pr_number}: {e}")
+            
+            logging.info(f"PR #{pr_number} closes issues: {closed_issues}")
+            return closed_issues
+            
+        except UnknownObjectException:
+            logging.warning(f"PR #{pr_number} not found in {self.repo.full_name}")
+            return set()
+        except Exception as e:
+            logging.error(f"Error fetching issues from PR #{pr_number}: {e}")
+            return set()
+
+
+    def get_all_performance_issues(self) -> set[int]:
+        """
+        Get all unique performance issues fixed by this commit.
+        Handles both direct issue references and issues closed by referenced PRs.
+        
+        Returns:
+            Set of unique performance issue numbers
+        """
+        refs = self.extract_fixed_issues()
+        if not refs:
+            return set()
+        
+        performance_issues = set()
+        issues_to_check: list[tuple[int, str]] = []
+        
+        for number, ref_type in refs.items():
+            if ref_type == "pull_request":
+                pr_issues = self.get_issues_from_pr(number)
+                for issue_num in pr_issues:
+                    issues_to_check.append((issue_num, "issue"))
+                issues_to_check.append((number, ref_type))
+            else:
+                issues_to_check.append((number, ref_type))
+        
+        checked = set()
+        for number, ref_type in issues_to_check:
+            if number in checked:
+                continue
+            checked.add(number)
+            
+            if self._is_performance_issue(number, ref_type):
+                self.is_issue = True
+                performance_issues.add(number)
+                logging.info(f"Identified performance issue: #{number}")
+        
+        if performance_issues:
+            logging.info(
+                f"Commit {self.commit.sha} fixes {len(performance_issues)} "
+                f"unique performance issue(s): {sorted(performance_issues)}"
+            )
+        else:
+            logging.info(f"Commit {self.commit.sha} does not fix any performance issues")
+        
+        return performance_issues
+    
+############ SIMPLE ############
+
+    def _simple_filter(self) -> bool:
+        """Simple filter only checks if there are keywords used in the commit message"""
+        msg = self.commit.commit.message.lower()
+        perf_label_keywords = {
+            'performance', 'perf', 'optimization', 'optimisation', 'optimize', 'optimize',
+            'speed', 'latency', 'throughput', 'regression: performance', 'perf regression',
+            'slow', 'slowness', 'timeout', 'benchmark', 'hot path', 'hotpath', 'cpu', 'gc',
+            'jmh', 'efficiency'
+        }
+        perf_text_keywords = {
+            'performance', 'perf', 'optimiz', 'speed', 'latency', 'throughput',
+            'slow', 'slowness', 'timeout', 'hang', 'stall', 'regression', 'benchmark',
+            'hot path', 'hotpath', 'cpu', 'allocation', 'alloc', 'gc', 'jmh', 'efficiency',
+            'big-o', 'complexity'
+        }
+        runtime_hint_keywords = {
+            'latency', 'throughput', 'slow', 'slowness', 'timeout', 'hang', 'stall', 'cpu', 
+            'jmh', 'benchmark', 'regression'
+        }
+        return any(k in msg for k in perf_label_keywords | perf_text_keywords | runtime_hint_keywords)
diff --git a/src/core/filter/flags_filter.py b/src/core/filter/flags_filter.py
old mode 100644
new mode 100755
index 07827db..f7ee39b
--- a/src/core/filter/flags_filter.py
+++ b/src/core/filter/flags_filter.py
@@ -1,18 +1,18 @@
-import logging
-
-class FlagFilter:
-    def __init__(self, valid_test_flags: dict, testing_flags: dict[str, dict[str, str]]):
-        self.valid_test_flags = valid_test_flags
-        self.testing_flags = testing_flags
-        self.valid_flags: set[str] = set()
-
-    def get_valid_flags(self) -> list[str]:
-        all_flags = set(self.testing_flags.keys())
-        valid = all_flags & set(self.valid_test_flags.get("valid", []))
-        prefix = {f for f in all_flags if any(f.startswith(p) for p in self.valid_test_flags.get("prefix", []))}
-        suffix = {f for f in all_flags if any(f.endswith(p) for p in self.valid_test_flags.get("suffix", []))}
-        contains = {f for f in all_flags if any(sub in f for sub in self.valid_test_flags.get("in", []))}
-        self.valid_flags = valid | prefix | suffix | contains
-
-        logging.debug(f"FlagFilter: Found {len(self.valid_flags)} valid flags: {sorted(self.valid_flags)}")
+import logging
+
+class FlagFilter:
+    def __init__(self, valid_test_flags: dict, testing_flags: dict[str, dict[str, str]]):
+        self.valid_test_flags = valid_test_flags
+        self.testing_flags = testing_flags
+        self.valid_flags: set[str] = set()
+
+    def get_valid_flags(self) -> list[str]:
+        all_flags = set(self.testing_flags.keys())
+        valid = all_flags & set(self.valid_test_flags.get("valid", []))
+        prefix = {f for f in all_flags if any(f.startswith(p) for p in self.valid_test_flags.get("prefix", []))}
+        suffix = {f for f in all_flags if any(f.endswith(p) for p in self.valid_test_flags.get("suffix", []))}
+        contains = {f for f in all_flags if any(sub in f for sub in self.valid_test_flags.get("in", []))}
+        self.valid_flags = valid | prefix | suffix | contains
+
+        logging.debug(f"FlagFilter: Found {len(self.valid_flags)} valid flags: {sorted(self.valid_flags)}")
         return list(self.valid_flags)
\ No newline at end of file
diff --git a/src/core/filter/process_filter.py b/src/core/filter/process_filter.py
old mode 100644
new mode 100755
index 593d7b6..aec1fc8
--- a/src/core/filter/process_filter.py
+++ b/src/core/filter/process_filter.py
@@ -1,106 +1,200 @@
-import tempfile, logging
-from src.cmake.process import CMakeProcess
-from src.gh.clone import GitHandler
-from github.Repository import Repository
-from src.cmake.analyzer import CMakeAnalyzer
-from src.config.config import Config
-from src.core.filter.structure_filter import StructureFilter
-from src.core.filter.flags_filter import FlagFilter
-from pathlib import Path
-from typing import Optional
-
-class ProcessFilter:
-    def __init__(self, repo: Repository, config: Config, root: Optional[Path] = None, sha: str = ""):
-        self.config = config
-        self.repo = repo
-        self.root = root
-        self.sha = sha if sha else self.repo.get_commits()[0].sha
-
-    def commit_setup_and_build(
-        self, 
-        msg: str, 
-        container_name: str, 
-        docker_image: str = ""
-    ) -> Optional[StructureFilter]:
-        structure = StructureFilter(self.repo, self.config, self.root, self.sha)
-        
-        logging.info(f"[{self.repo.full_name}] Testing {self.sha}...")
-        if self.root and not structure.is_valid_commit(self.root, self.sha, docker_test_dir=self.config.testing.docker_test_dir):
-            logging.error(f"[{self.repo.full_name}] commit cmake and ctest failed ({self.sha})")
-            return None
-        
-        process = structure.process
-        if not process:
-            logging.error(f"[{self.repo.full_name}] CMakeProcess for {self.repo.full_name} couldn't be found")
-            return None
-        
-        analyzer = process.analyzer
-        flags = FlagFilter(self.config.valid_test_flags, analyzer.has_build_testing_flag()).get_valid_flags()
-        sorted_testing_path = self.sort_testing_path(analyzer.get_enable_testing_path())
-        if len(sorted_testing_path) == 0:
-            logging.error(f"[{self.repo.full_name}] path to enable_testing() was not found in {self.root}: {sorted_testing_path}")
-            return None
-
-        if len(sorted_testing_path) > 1:
-            logging.warning(f"[{self.repo.full_name}] multiple paths to enable_testing() was found in {self.root}. For testing: {sorted_testing_path[0]}")
-
-        test_path = sorted_testing_path[0]
-        if test_path.name == "CMakeLists.txt":
-            test_path = test_path.parent
-        enable_testing_path = test_path.relative_to(self.root) if self.root else Path()
-        logging.info(f"[{self.repo.full_name}] path to enable_testing(): '{enable_testing_path}'")
-        try:
-            process.set_enable_testing(enable_testing_path)
-            process.set_flags(flags)
-            process.docker_image = self.config.docker_image
-            if docker_image:
-                new = False
-            else:
-                new = True
-            process.start_docker_image(self.config, container_name, new)
-            
-            if not process.build():
-                logging.error(f"[{self.repo.full_name}] {msg} build failed ({self.sha})")
-                process.docker.stop_container()
-                return None
-            
-            if not process.collect_tests():
-                logging.error(f"[{self.repo.full_name}] {msg} generating test commands failed ({self.sha})")
-                process.docker.stop_container()
-                return None
-            
-            logging.info(f"[{self.repo.full_name}] {msg} build successful ({self.sha})")
-            return structure
-        
-        except Exception as e:
-            logging.exception(f"[{self.repo.full_name}] Unexpected error during process run: {e}")
-            return None
-        
-    def test_run(self, msg: str, command: list[str], structure: StructureFilter, has_list_args: bool) -> bool:
-        if structure.process:
-            process = structure.process
-            try:
-                if not process.test(command, has_list_args) and not process.test(["ninja", "test"], has_list_args):
-                    logging.error(f"[{self.repo.full_name}] {msg} test failed ({self.sha})")
-                    process.docker.stop_container()
-                    return False
-                
-                logging.debug(f"[{self.repo.full_name}] {msg} build and test successful ({self.sha})")
-                return True
-            except Exception as e:
-                logging.exception(f"[{self.repo.full_name}] Unexpected error during process run: {e}")
-                return False
-        else:
-            logging.error(f"[{self.repo.full_name}] CMakeProcess is not defined ({self.sha})")
-            return False
-
-    def _sort_key(self, y: Path) -> tuple[int, int]:
-        valid_test_dirs = [Path(x) for x in self.config.valid_test_dirs]
-        try:
-            priority = 0 if any(y.is_relative_to(x) for x in valid_test_dirs) else 1
-        except:
-            priority = 0
-        return (priority, len(y.parts))
-    
-    def sort_testing_path(self, paths: list[Path]) -> list[Path]: 
-        return sorted(paths, key=self._sort_key)
+import tempfile, logging
+from src.cmake.process import CMakeProcess
+from src.gh.clone import GitHandler
+from github.Repository import Repository
+from src.cmake.analyzer import CMakeAnalyzer
+from src.config.config import Config
+from src.core.filter.structure_filter import StructureFilter
+from src.core.filter.flags_filter import FlagFilter
+from pathlib import Path
+from typing import Optional
+
+class ProcessFilter:
+    """
+    The Process filters checks if the repository or commit can be build and run.
+    """
+    def __init__(self, repo: Repository, config: Config, root: Optional[Path] = None, sha: str = ""):
+        self.config = config
+        self.repo = repo
+        self.root = root
+        self.sha = sha if sha else self.repo.get_commits()[0].sha
+
+    def valid_run(self, container_name: str) -> bool:
+        tmp_root = Path.cwd()/"tmp"
+        tmp_root.mkdir(parents=True, exist_ok=True)
+        with tempfile.TemporaryDirectory(dir=tmp_root) as tmpdir:
+            tmp_path = Path(tmpdir)
+            process = CMakeProcess(self.repo.full_name, self.config, tmp_path, None, [], CMakeAnalyzer(tmp_path), "", jobs=self.config.resources.jobs, docker_test_dir=self.config.testing.docker_test_dir)
+        
+            if not GitHandler().clone_repo(self.repo.full_name, tmp_path):
+                logging.error(f"[{self.repo.full_name}:{self.sha}] git cloning failed")
+                return False
+            
+            analyzer = process.analyzer
+            analyzer.reset()
+            if not analyzer.has_testing(nolist=self.config.testing.no_list_testing):
+                logging.error(f"[{self.repo.full_name}:{self.sha}] invalid ctest")
+                return False
+            
+            self.list_test_arg = analyzer.get_list_test_arg()
+
+            flags = FlagFilter(self.config.valid_test_flags, analyzer.extract_build_testing_flag()).get_valid_flags()
+            sorted_testing_path = self.sort_testing_path(analyzer.get_enable_testing_path())
+            if len(sorted_testing_path) == 0:
+                logging.error(f"[{self.repo.full_name}:{self.sha}] path to enable_testing() was not found in {tmpdir}: {sorted_testing_path}")
+                return False
+            
+            test_path = sorted_testing_path[0]
+            if test_path.name == "CMakeLists.txt":
+                test_path = test_path.parent
+            enable_testing_path = test_path.relative_to(tmp_path)
+            logging.info(f"[{self.repo.full_name}:{self.sha}] path to enable_testing(): '{enable_testing_path}'")
+            try:
+                process.set_enable_testing(enable_testing_path)
+                process.set_flags(flags)
+                process.docker_image = self.config.docker_image
+                process.start_docker_image(container_name)
+            
+                if not process.build():
+                    logging.error(f"[{self.repo.full_name}:{self.sha}] build failed")
+                    process.docker.stop_container(self.repo.full_name)
+                    return False
+                    
+                if not process.collect_tests():
+                    logging.error(f"[{self.repo.full_name}:{self.sha}] test failed")
+                    process.docker.stop_container(self.repo.full_name)
+                    return False
+                
+                test_cmd = process.test_commands
+                has_list_args = len(test_cmd) > 1
+                for cmd in test_cmd:
+                    if not process.test(cmd, has_list_args):
+                        logging.error(f"[{self.repo.full_name}:{self.sha}] test failed ({self.sha})")
+                        process.docker.stop_container(self.repo.full_name)
+                        return False
+                
+            except Exception as e:
+                logging.exception(f"[{self.repo.full_name}:{self.sha}] Unexpected error during process run: {e}")
+                return False
+
+            finally:
+                process.docker.stop_container(self.repo.full_name)
+            
+            return True
+
+    def commit_setup_and_build(
+        self, 
+        msg: str, 
+        container_name: str, 
+        docker_image: str = "",
+        cpuset_cpus: str = ""
+    ) -> Optional[CMakeProcess]:
+        
+        if not self.root:
+            logging.error(f"[{self.repo.full_name}] git project root: {self.root}")
+            return None
+
+        if not GitHandler().clone_repo(self.repo.full_name, self.root, sha=self.sha):
+            logging.error(f"[{self.repo.full_name}] git cloning failed")
+            return None
+        
+        structure = StructureFilter(self.repo, self.config, self.root, self.sha)
+        logging.info(f"[{self.repo.full_name}:{self.sha}] Testing...")
+        if not structure.is_valid_commit(self.root):
+            logging.error(f"[{self.repo.full_name}:{self.sha}] commit cmake and ctest failed")
+            return None
+
+        analyzer = structure.analyzer
+        process = CMakeProcess(self.repo.full_name, self.config, self.root, None, [], analyzer, "", jobs=self.config.resources.jobs, docker_test_dir=self.config.testing.docker_test_dir)
+        if not process:
+            logging.error(f"[{self.repo.full_name}:{self.sha}] CMakeProcess couldn't be found")
+            return None
+        
+        # parses all the possible testing flags defined under src/config/constants.py as VALID_TEST_FLAGS
+        flags = FlagFilter(self.config.valid_test_flags, analyzer.extract_build_testing_flag()).get_valid_flags()
+        
+        # possible multiple enable_testing() defined in CMakeLists.txt
+        # here: just take enable_testing() closes to project root
+        sorted_testing_path = self.sort_testing_path(analyzer.get_enable_testing_path())
+        if len(sorted_testing_path) == 0:
+            logging.error(f"[{self.repo.full_name}:{self.sha}] path to enable_testing() was not found in {self.root}: {sorted_testing_path}")
+            return None
+        
+        if len(sorted_testing_path) > 1:
+            logging.warning(f"[{self.repo.full_name}:{self.sha}] multiple paths to enable_testing() was found in {self.root}. For testing: {sorted_testing_path[0]}")
+
+        test_path = sorted_testing_path[0]
+        if test_path.name == "CMakeLists.txt":
+            test_path = test_path.parent
+        enable_testing_path = test_path.relative_to(self.root) if self.root else Path()
+        logging.info(f"[{self.repo.full_name}:{self.sha}] path to enable_testing(): '{enable_testing_path}'")
+        try:
+            return self.build_collect_test(process, enable_testing_path, flags, container_name, docker_image, cpuset_cpus, msg)
+        except Exception as e:
+            logging.exception(f"[{self.repo.full_name}:{self.sha}] Unexpected error during process run: {e}")
+            return None
+        
+    def build_collect_test(
+        self, 
+        process: CMakeProcess, 
+        enable_testing_path: Path, 
+        flags: list[str],
+        container_name: str,
+        docker_image: str,
+        cpuset_cpus: str,
+        msg: str
+    ) -> Optional[CMakeProcess]:
+        
+        process.set_enable_testing(enable_testing_path)
+        process.set_flags(flags)
+        new = not docker_image
+        if self.config.testdocker or self.config.testpatch:
+            process.docker_image = self.config.docker_image
+            process.set_docker(container_name, new)
+            process.docker.start_docker_container(container_name, cpuset_cpus)
+            process.container = process.docker.container
+        else:
+            process.docker_image = self.config.docker_image
+            process.start_docker_image(container_name, new, cpuset_cpus)
+
+        if new and self.config.diff and not process.diff():
+            logging.error(f"[{self.repo.full_name}:{self.sha}] diff application to old (original) commit failed")
+            process.docker.stop_container(self.repo.full_name)
+            return None
+        
+        if not process.build():
+            logging.error(f"[{self.repo.full_name}:{self.sha}] {msg} build failed")
+            process.docker.stop_container(self.repo.full_name)
+            return None
+        
+        if not process.collect_tests():
+            logging.error(f"[{self.repo.full_name}:{self.sha}] {msg} generating test commands failed")
+            process.docker.stop_container(self.repo.full_name)
+            return None
+        
+        logging.info(f"[{self.repo.full_name}:{self.sha}] {msg} build successful")
+        return process
+        
+    def test_run(self, msg: str, command: list[str], process: CMakeProcess, has_list_args: bool) -> bool:
+        try:
+            if not process.test(command, has_list_args):
+                logging.error(f"[{self.repo.full_name}:{self.sha}] {msg} test failed")
+                process.docker.stop_container(self.repo.full_name)
+                return False
+            
+            logging.debug(f"[{self.repo.full_name}:{self.sha}] {msg} build and test successful")
+            return True
+        except Exception as e:
+            logging.exception(f"[{self.repo.full_name}:{self.sha}] Unexpected error during process run: {e}")
+            return False
+
+    def _sort_key(self, y: Path) -> tuple[int, int]:
+        valid_test_dirs = [Path(x) for x in self.config.valid_test_dirs]
+        try:
+            priority = 0 if any(y.is_relative_to(x) for x in valid_test_dirs) else 1
+        except:
+            priority = 0
+        return (priority, len(y.parts))
+    
+    def sort_testing_path(self, paths: list[Path]) -> list[Path]: 
+        return sorted(paths, key=self._sort_key)
diff --git a/src/core/filter/structure_filter.py b/src/core/filter/structure_filter.py
old mode 100644
new mode 100755
index 281358b..2db7543
--- a/src/core/filter/structure_filter.py
+++ b/src/core/filter/structure_filter.py
@@ -1,171 +1,171 @@
-import logging, tempfile, os, time
-from collections import Counter
-from github.GitTreeElement import GitTreeElement
-from github.Repository import Repository
-from typing import Optional
-from pathlib import Path
-from github.ContentFile import ContentFile
-from github.GithubException import GithubException, RateLimitExceededException
-
-from src.config.config import Config
-from src.cmake.analyzer import CMakeAnalyzer
-from src.cmake.process import CMakeProcess
-from src.gh.clone import GitHandler
-from src.utils.stats import RepoStats
-
-class StructureFilter:
-    """
-    This class analyses and filters the structure of a repository.
-    """
-    def __init__(self, repo: Repository, config: Config, root: Optional[Path] = None, sha: str = ""):
-        self.config = config
-        self.repo = repo
-        self.root = root
-        self.sha = sha if sha else self.repo.get_commits()[0].sha
-
-        self.cmake_tree, self.tree_paths, self.tree = self._get_repo_tree()
-        self.root_files = {item.path for item in self.tree if item.type == "blob"}
-        self.stats = RepoStats()
-        self.testing_flags: dict = {}
-        self.process: Optional[CMakeProcess] = None
-
-    def is_valid(self, without_pkg_manager: bool = True) -> bool:
-        vcpkg = self._has_root_vcpkg()
-        conan = self._has_root_conan()
-
-        if not self._has_root_cmake() or not (without_pkg_manager or vcpkg or conan):
-            logging.warning(f"[{self.repo.full_name}] no CMakeLists.txt at root found")
-            return False
-        
-        logging.info(f"[{self.repo.full_name}] CMakeLists.txt at root found")
-
-        with tempfile.TemporaryDirectory(dir=Path.cwd()) as tmpdir:
-            tmpdir = Path(tmpdir)
-            self._get_cmake_lists(tmpdir)
-            analyzer = CMakeAnalyzer(tmpdir)
-            analyzer.reset()
-
-            if not analyzer.has_testing(nolist=self.config.testing.no_list_testing):
-                logging.warning(f"[{self.repo.full_name}] invalid ctest")
-                return False
-            
-            test_dirs = self._extract_test_dirs()
-            if test_dirs:
-                logging.debug(f"[{self.repo.full_name}] test directories: {test_dirs}")
-                for d in test_dirs:
-                    self.stats.test_dirs[d] += 1
-                conv_test_dir = test_dirs & self.config.valid_test_dirs
-
-                if conv_test_dir:
-                    logging.debug(f"[{self.repo.full_name}] conventional test directories {conv_test_dir}")
-                    self.testing_flags = analyzer.has_build_testing_flag()
-                    self.test_flags = Counter(self.testing_flags.keys())
-            
-            logging.info(f"[{self.repo.full_name}] ctest is defined")
-            return True
-                
-    def is_valid_commit(self, root: Path, sha: str, docker_test_dir: str) -> bool:
-        vcpkg = self._has_root_vcpkg()
-        conan = self._has_root_conan()
-
-        if not self._has_root_cmake():
-            logging.error(f"[{self.repo.full_name}] no CMakeLists.txt at root found")
-            return False
-        
-        logging.info(f"[{self.repo.full_name}] CMakeLists.txt at root found")
-        analyzer = CMakeAnalyzer(root)
-        self.process = CMakeProcess(self.config, root, None, [], analyzer, "", jobs=self.config.resources.jobs, docker_test_dir=docker_test_dir)
-        
-        if not GitHandler().clone_repo(self.repo.full_name, root, sha=sha):
-            logging.error(f"[{self.repo.full_name}] git cloning failed")
-            return False
-
-        self.process.analyzer.reset()
-        if not self.process.analyzer.has_testing(nolist=self.config.testing.no_list_testing):
-            logging.error(f"[{self.repo.full_name}] invalid ctest")
-            return False
-
-        logging.info(f"[{self.repo.full_name}] ctest is defined")
-        return True
-    
-    def _has_root_cmake(self) -> bool:
-        return self._has_root_file("CMakeLists.txt")
-    
-    def _has_root_vcpkg(self) -> bool:
-        return self._has_root_file("vcpkg.json")
-    
-    def _has_root_conan(self) -> bool:
-        return self._has_root_file("conanfile.txt") or self._has_root_file("conanfile.py")
-    
-    def _has_root_bazel(self) -> bool:
-        return self._has_root_file("WORKSPACE") or self._has_root_file("MODULE.bazel")
-
-    def _has_root_meson(self) -> bool:
-        return self._has_root_file("meson.build")
-    
-    def _has_root_file(self, filename: str) -> bool:
-        return filename in self.root_files
-
-    def _get_repo_tree(self) -> tuple[list[GitTreeElement], list[str], list[GitTreeElement]]:
-        tree = self.repo.get_git_tree(self.sha, recursive=True).tree
-        tree_paths = [item.path for item in tree]
-        cmake_tree = [item for item in tree if item.type == "blob" and item.path.endswith("CMakeLists.txt")]
-        return cmake_tree, tree_paths, tree
-    
-    def _get_cmake_lists(self, dest: Path) -> list[str]:
-        """
-        Fetch only CMakeLists.txt files from a GitHub repo using requests, preserving folder structure.
-        """
-        os.makedirs(dest, exist_ok=True)
-        result_paths = []
-        for item in self.cmake_tree:
-            try: 
-                content_file = self._attempts(item.path) #.repo.get_contents(item.path, ref=self.sha)
-                target_path = dest / item.path
-                os.makedirs(target_path.parent, exist_ok=True)
-                if isinstance(content_file, ContentFile):
-                    with open(target_path, "wb") as f:
-                        f.write(content_file.decoded_content)
-                    result_paths.append(str(target_path))
-                time.sleep(0.2)
-            except Exception as e:
-                logging.warning(f"[{self.repo.full_name}] Failed to fetch/write {item.path}: {e}")
-
-        return result_paths
-    
-    def _attempts(self, path: str):
-        for attempt in range(5):
-            try:
-                return self.repo.get_contents(path, ref=self.sha)
-            except RateLimitExceededException:
-                reset = self.config.git_client.get_rate_limit().rate.reset.timestamp()
-                sleep_for = max(0, reset - time.time()) + 5
-                logging.warning(f"Rate limit exceeded. Sleeping for {sleep_for:.0f} seconds...")
-                time.sleep(sleep_for)
-                continue
-            except GithubException as e:
-                if e.status in (500, 502, 503, 504):
-                    logging.warning(f"Server error {e.status}, retrying...")
-                    time.sleep(5 * (attempt + 1))
-                else:
-                    raise e
-
-    def _extract_test_dirs(self) -> set[str]:
-        """Extracts all directories that look like test directories from a PyGithub GitTree."""
-        test_dirs: set[str] = set()
-
-        for element in self.tree: 
-            path = element.path.lower()
-            parts = path.split("/")
-
-            for i, part in enumerate(parts[:-1]):
-                if any(keyword in part for keyword in self.config.test_keywords):
-                    test_dir = "/".join(parts[:i+1])
-                    test_dirs.add(test_dir)
-
-        top_level_dirs: set[str] = set()
-        for dir_path in test_dirs:
-            if not any(dir_path != other and dir_path.startswith(other + "/") for other in test_dirs):
-                top_level_dirs.add(dir_path)
-
+import logging, tempfile, os, time
+from collections import Counter
+from github.GitTreeElement import GitTreeElement
+from github.Repository import Repository
+from typing import Optional
+from pathlib import Path
+from github.ContentFile import ContentFile
+from github.GithubException import GithubException, RateLimitExceededException
+from src.config.config import Config
+from src.cmake.analyzer import CMakeAnalyzer
+from src.cmake.process import CMakeProcess
+from src.gh.clone import GitHandler
+from src.utils.stats import RepoStats
+
+class StructureFilter:
+    """
+    This class analyses and filters the structure of a repository.
+    """
+    def __init__(self, repo: Repository, config: Config, root: Optional[Path] = None, sha: str = ""):
+        self.config = config
+        self.repo = repo
+        self.root = root
+        self.sha = sha if sha else self.repo.get_commits()[0].sha
+
+        if root:
+            self.analyzer = CMakeAnalyzer(root)
+
+        self.stats = RepoStats()
+        self.testing_flags: dict = {}
+
+    def is_valid(self, without_pkg_manager: bool = True) -> bool:
+        self.cmake_tree, self.tree_paths, self.tree = self._get_repo_tree()
+        self.root_files = {item.path for item in self.tree if item.type == "blob"}
+
+        vcpkg = self._has_root_vcpkg()
+        conan = self._has_root_conan()
+
+        if not self._has_root_cmake() or not (without_pkg_manager or vcpkg or conan):
+            logging.warning(f"[{self.repo.full_name}] no CMakeLists.txt at root found")
+            return False
+        
+        logging.info(f"[{self.repo.full_name}] CMakeLists.txt at root found")
+
+        with tempfile.TemporaryDirectory(dir=Path.cwd()) as tmpdir:
+            tmpdir = Path(tmpdir)
+            self._get_cmake_lists(tmpdir)
+            analyzer = CMakeAnalyzer(tmpdir)
+            analyzer.reset()
+
+            if not analyzer.has_testing(nolist=self.config.testing.no_list_testing):
+                logging.warning(f"[{self.repo.full_name}] invalid ctest")
+                return False
+            
+            test_dirs = self._extract_test_dirs()
+            if test_dirs:
+                logging.debug(f"[{self.repo.full_name}] test directories: {test_dirs}")
+                for d in test_dirs:
+                    self.stats.test_dirs[d] += 1
+                conv_test_dir = test_dirs & self.config.valid_test_dirs
+
+                if conv_test_dir:
+                    logging.debug(f"[{self.repo.full_name}] conventional test directories {conv_test_dir}")
+                    self.testing_flags = analyzer.extract_build_testing_flag()
+                    self.test_flags = Counter(self.testing_flags.keys())
+            
+            logging.info(f"[{self.repo.full_name}] ctest is defined")
+            return True
+                
+    def is_valid_commit(self, root: Path) -> bool:
+        self.cmake_tree, self.tree_paths, self.tree = self._get_repo_tree()
+        self.root_files = {item.path for item in self.tree if item.type == "blob"}
+
+        vcpkg = self._has_root_vcpkg()
+        conan = self._has_root_conan()
+
+        if not self._has_root_cmake():
+            logging.error(f"[{self.repo.full_name}] no CMakeLists.txt at root found")
+            return False
+        
+        logging.info(f"[{self.repo.full_name}] CMakeLists.txt at root found")
+        
+        
+        self.analyzer.reset()
+        if not self.analyzer.has_testing(nolist=self.config.testing.no_list_testing):
+            logging.error(f"[{self.repo.full_name}] invalid ctest")
+            return False
+
+        logging.info(f"[{self.repo.full_name}] ctest is defined")
+        return True
+    
+    def _has_root_cmake(self) -> bool:
+        return self._has_root_file("CMakeLists.txt")
+    
+    def _has_root_vcpkg(self) -> bool:
+        return self._has_root_file("vcpkg.json")
+    
+    def _has_root_conan(self) -> bool:
+        return self._has_root_file("conanfile.txt") or self._has_root_file("conanfile.py")
+    
+    def _has_root_bazel(self) -> bool:
+        return self._has_root_file("WORKSPACE") or self._has_root_file("MODULE.bazel")
+
+    def _has_root_meson(self) -> bool:
+        return self._has_root_file("meson.build")
+    
+    def _has_root_file(self, filename: str) -> bool:
+        return filename in self.root_files
+
+    def _get_repo_tree(self) -> tuple[list[GitTreeElement], list[str], list[GitTreeElement]]:
+        tree = self.repo.get_git_tree(self.sha, recursive=True).tree
+        tree_paths = [item.path for item in tree]
+        cmake_tree = [item for item in tree if item.type == "blob" and item.path.endswith("CMakeLists.txt")]
+        return cmake_tree, tree_paths, tree
+    
+    def _get_cmake_lists(self, dest: Path) -> list[str]:
+        """
+        Fetch only CMakeLists.txt files from a GitHub repo using requests, preserving folder structure.
+        """
+        os.makedirs(dest, exist_ok=True)
+        result_paths = []
+        for item in self.cmake_tree:
+            try: 
+                content_file = self._attempts(item.path) #.repo.get_contents(item.path, ref=self.sha)
+                target_path = dest / item.path
+                os.makedirs(target_path.parent, exist_ok=True)
+                if isinstance(content_file, ContentFile):
+                    with open(target_path, "wb") as f:
+                        f.write(content_file.decoded_content)
+                    result_paths.append(str(target_path))
+                time.sleep(0.2)
+            except Exception as e:
+                logging.warning(f"[{self.repo.full_name}] Failed to fetch/write {item.path}: {e}")
+
+        return result_paths
+    
+    def _attempts(self, path: str):
+        for attempt in range(5):
+            try:
+                return self.repo.get_contents(path, ref=self.sha)
+            except RateLimitExceededException:
+                reset = self.config.git_client.get_rate_limit().rate.reset.timestamp()
+                sleep_for = max(0, reset - time.time()) + 5
+                logging.warning(f"Rate limit exceeded. Sleeping for {sleep_for:.0f} seconds...")
+                time.sleep(sleep_for)
+                continue
+            except GithubException as e:
+                if e.status in (500, 502, 503, 504):
+                    logging.warning(f"Server error {e.status}, retrying...")
+                    time.sleep(5 * (attempt + 1))
+                else:
+                    raise e
+
+    def _extract_test_dirs(self) -> set[str]:
+        """Extracts all directories that look like test directories from a PyGithub GitTree."""
+        test_dirs: set[str] = set()
+
+        for element in self.tree: 
+            path = element.path.lower()
+            parts = path.split("/")
+
+            for i, part in enumerate(parts[:-1]):
+                if any(keyword in part for keyword in self.config.test_keywords):
+                    test_dir = "/".join(parts[:i+1])
+                    test_dirs.add(test_dir)
+
+        top_level_dirs: set[str] = set()
+        for dir_path in test_dirs:
+            if not any(dir_path != other and dir_path.startswith(other + "/") for other in test_dirs):
+                top_level_dirs.add(dir_path)
+
         return top_level_dirs
\ No newline at end of file
diff --git a/src/core/pipelines/collector_pipeline.py b/src/core/pipelines/collector_pipeline.py
old mode 100644
new mode 100755
index 4dd95de..8103c9e
--- a/src/core/pipelines/collector_pipeline.py
+++ b/src/core/pipelines/collector_pipeline.py
@@ -1,20 +1,20 @@
-import logging
-from src.config.config import Config
-from src.gh.collector import RepositoryCollector
-from src.utils.writer import Writer
-from github.Repository import Repository
-
-class CollectionPipeline:
-    """
-    This class crawls popular GitHub repositories.
-    """
-    def __init__(self, config: Config):
-        self.config = config
-
-    def query_popular_repos(self) -> list[Repository]:
-        collector = RepositoryCollector(config=self.config)
-        repos = collector.query_popular_repos()
-        logging.info(f"Found {len(repos)} repositories from collector.")
-        for repo in repos:
-            Writer(repo.full_name, self.config.output_file or self.config.storage_paths['popular']).write_repo()
+import logging
+from src.config.config import Config
+from src.gh.collector import RepositoryCollector
+from src.utils.writer import Writer
+from github.Repository import Repository
+
+class CollectionPipeline:
+    """
+    This class collects popular GitHub repositories.
+    """
+    def __init__(self, config: Config):
+        self.config = config
+
+    def query_popular_repos(self) -> list[Repository]:
+        collector = RepositoryCollector(config=self.config)
+        repos = collector.query_popular_repos()
+        logging.info(f"Found {len(repos)} repositories from collector.")
+        for repo in repos:
+            Writer(repo.full_name, self.config.output_file or self.config.storage_paths['collect']).write_repo()
         return repos
\ No newline at end of file
diff --git a/src/core/pipelines/commit_pipeline.py b/src/core/pipelines/commit_pipeline.py
old mode 100644
new mode 100755
index 9acb835..9656cbb
--- a/src/core/pipelines/commit_pipeline.py
+++ b/src/core/pipelines/commit_pipeline.py
@@ -1,65 +1,134 @@
-import logging
-from tqdm import tqdm
-from src.config.config import Config
-from src.utils.writer import Writer
-from src.core.filter.commit_filter import CommitFilter
-from src.utils.stats import CommitStats
-from github.Repository import Repository
-
-class CommitPipeline:
-    """
-    This class filters and saves the commit history of a repository.
-    """
-    def __init__(self, repo_ids: list[str], config: Config):
-        self.config = config
-        self.repo_ids = repo_ids
-        #self.repo = repo
-        #self.repo_id = self.repo.full_name
-        self.stats = CommitStats()
-        self.filtered_commits: list[str] = []
-
-    # TODO: test
-    def filter_all_commits(self):
-        if self.config.sha and self.config.repo_id:
-            repo = self.config.git_client.get_repo(self.config.repo_id)
-            try:
-                self.commits = repo.get_commits(sha=self.config.sha) 
-            except Exception as e:
-                logging.exception(f"[{repo.full_name}] Error fetching commits: {e}")
-                self.commits = []
-            self._filter_commits(repo)
-            return
-        
-        for repo_id in self.repo_ids:
-            repo = self.config.git_client.get_repo(repo_id)
-
-            since = self.config.commits_time['since']
-            until = self.config.commits_time['until']
-            try:
-                self.commits = repo.get_commits(sha=repo.default_branch, since=since, until=until)
-            except Exception as e:
-                logging.exception(f"[{repo.full_name}] Error fetching commits: {e}")
-                self.commits = []
-            self._filter_commits(repo)
-
-    def _filter_commits(self, repo: Repository) -> None:
-        if not self.commits:
-            logging.warning(f"[{repo.full_name}] No commits found")
-            return
-        
-        stats = CommitStats()
-        filtered_commits: list[str] = []
-        for commit in tqdm(self.commits, desc=f"{repo.full_name} commits", position=1, leave=False):
-            stats.num_commits += 1
-            try:
-                if not CommitFilter(commit, self.config, repo).accept():
-                    continue
-            except Exception as e:
-                logging.exception(f"[{repo.full_name}] Error processing commit: {e}")
-            
-            writer = Writer(repo.full_name, self.config.output_file or self.config.storage_paths['clones'])
-            filtered_commits.append(writer.file or "")
-            stats.perf_commits += 1
-            stats += writer.write_commit(commit, self.config.separate, self.config.filter_type)
-
-        stats.write_final_log()
\ No newline at end of file
+import logging, ast
+from tqdm import tqdm
+from src.config.config import Config
+from src.utils.writer import Writer
+from src.core.filter.commit_filter import CommitFilter
+from src.utils.stats import CommitStats
+from github.Repository import Repository
+from github.Commit import Commit
+from pathlib import Path
+
+class CommitPipeline:
+    """
+    This class filters and saves the commit history of a repository.
+    """
+    def __init__(self, repo_ids: list[str], config: Config):
+        self.config = config
+        self.repo_ids = repo_ids
+        self.stats = CommitStats()
+        self.filtered_commits: list[Commit] = []
+
+    def filter_all_commits(self):
+        if self.config.sha and self.config.repo_id:
+            repo = self.config.git_client.get_repo(self.config.repo_id)
+            try:
+                self.commits = [repo.get_commit(sha=self.config.sha)]
+            except Exception as e:
+                logging.exception(f"[{repo.full_name}] Error fetching commits: {e}")
+                self.commits = []
+            self.filter_commits_from_repo(repo)
+            return
+        
+        for repo_id in self.repo_ids:
+            repo = self.config.git_client.get_repo(repo_id)
+
+            since = self.config.commits_time['since']
+            until = self.config.commits_time['until']
+            try:
+                self.commits = repo.get_commits(sha=repo.default_branch, since=since, until=until)
+            except Exception as e:
+                logging.exception(f"[{repo.full_name}] Error fetching commits: {e}")
+                self.commits = []
+            self.filter_commits_from_repo(repo)
+
+    def filter_commits_from_repo(self, repo: Repository) -> None:
+        if not self.commits:
+            logging.warning(f"[{repo.full_name}] No commits found")
+            return
+        
+        stats = CommitStats()
+        filtered_commits: list[str] = []
+        for commit in tqdm(self.commits, desc=f"{repo.full_name} commits", position=1, leave=False, mininterval=5):
+            stats.num_commits += 1
+            perf_improv_filter = CommitFilter(commit, self.config, repo)
+            if not perf_improv_filter.accept():
+                continue
+            
+            self.filtered_commits.append(commit)
+            writer = Writer(repo.full_name, self.config.output_file or self.config.storage_paths['commits'])
+            filtered_commits.append(writer.file or "")
+            stats.perf_commits += 1
+            stats += writer.write_pr_commit(repo, commit, perf_improv_filter.is_issue)
+
+        self._rewrite_commits()
+        stats.write_final_log()
+
+    def filter_commits(self, commits: list[Commit], repo: Repository) -> None:
+        if not commits:
+            logging.warning(f"[{repo.full_name}] No commits found")
+            return
+        
+        stats = CommitStats()
+        filtered_commits: list[str] = []
+        for commit in tqdm(commits, desc=f"{repo.full_name} commits", position=1, leave=False, mininterval=5):
+            stats.num_commits += 1
+            perf_improv_filter = CommitFilter(commit, self.config, repo)
+            if not perf_improv_filter.accept():
+                continue
+            
+            self.filtered_commits.append(commit)
+            writer = Writer(repo.full_name, self.config.output_file or self.config.storage_paths['clones'])
+            filtered_commits.append(writer.file or "")
+            stats.perf_commits += 1
+            stats += writer.write_pr_commit(repo, commit, perf_improv_filter.is_issue)
+
+        self._rewrite_commits()
+        stats.write_final_log()
+
+
+    def _read_commits(self) -> list[str]:
+        """
+        Merges multiple 'owner/repo | patched_sha | original_sha | new_shaN' into 
+        'owner/repo | patched_sha | original_sha | [new_sha1, ..., new_shaN]'
+        """
+        commits: dict[str, list[str]] = {}
+        path = self.config.output_file or self.config.storage_paths['clones']
+        file = "filtered.txt"
+        path = Path(path) / file
+        with open(path, "r") as f:
+            for line_no, line in enumerate(f, start=1):
+                line = line.strip()
+                if not line:
+                    continue
+
+                parts = [p.strip() for p in line.split("|") if p.strip()]
+                if len(parts) <= 2:
+                    logging.warning(f"Malformed commit line at {path}:{line_no} -> {line}")
+                    continue
+                msg = f"{parts[0]} | {parts[1]} | {parts[2]}"
+                
+                if len(parts) > 3:
+                    # repo_id | new_sha | old_sha | new_sha_not_pr
+                    extra_commits = ast.literal_eval(parts[3])
+                    commits.setdefault(msg, []).extend(extra_commits)
+                    continue
+                
+                commits.setdefault(msg, [])
+                    
+        output = [f"{k} | {v}" for k, v in commits.items()]
+        return output
+    
+    def _organize_commits(self) -> list[str]:
+        commits = self._read_commits()
+        commits = list(set(commits))
+        commits.sort(key=str.casefold)
+        return commits
+    
+    def _rewrite_commits(self) -> None:
+        commits = self._organize_commits()
+        path = self.config.output_file or self.config.storage_paths['clones']
+        file = "filtered.txt"
+        path = Path(path) / file
+        with open(path, "w") as f:
+            for line in commits:
+                f.write(line + "\n")
\ No newline at end of file
diff --git a/src/core/pipelines/commit_tester_pipeline.py b/src/core/pipelines/commit_tester_pipeline.py
old mode 100644
new mode 100755
index 14bb938..e81dc78
--- a/src/core/pipelines/commit_tester_pipeline.py
+++ b/src/core/pipelines/commit_tester_pipeline.py
@@ -1,46 +1,100 @@
-import logging
-from tqdm import tqdm
-from src.config.config import Config
-from src.utils.commit import Commit
-from src.core.docker.tester import DockerTester
-
-class CommitTesterPipeline:
-    """
-    This class runs the commits and evaluates its performance.
-    """
-    def __init__(self, config: Config):
-        self.config = config
-        self.commit = Commit(self.config.input_file or self.config.storage_paths['commits'], self.config.storage_paths['clones'])   
-        self.docker = DockerTester(self.config) 
-
-    def test_commit(self) -> None:
-        if self.config.input_file or self.config.repo_id:
-            self._input_tester()
-        else:
-            self._sha_tester()
-
-    def _input_tester(self) -> None:
-        commits = self.commit.get_commits()
-        for repo_id, new_sha, old_sha in tqdm(commits, total=len(commits), desc="Commits testing...", position=0):
-            file = self.commit.get_file_prefix(repo_id)
-            new_path, old_path = self.commit.get_paths(file, new_sha)
-            repo = self.config.git_client.get_repo(repo_id)
-            try:
-                self.docker.run_commit_pair(repo, new_sha, old_sha, new_path, old_path)
-            except Exception as e:
-                logging.exception(f"[{repo_id}] Error testing commits: {e}")
-
-    def _sha_tester(self):
-        if self.config.sha and self.config.repo_id:
-            repo = self.config.git_client.get_repo(self.config.repo_id)
-            commit = repo.get_commit(self.config.sha)
-            file_prefix = "_".join(repo.full_name.split("/"))
-            new_sha = self.config.sha
-            if not commit.parents:
-                logging.info(f"[{repo.full_name}] Commit {self.config.sha} has no parents (root commit).")
-                return
-            old_sha = commit.parents[0].sha
-            new_path, old_path = self.commit.get_paths(file_prefix, new_sha)
-            self.docker.run_commit_pair(repo, new_sha, old_sha, new_path, old_path)
-        else:
-            logging.error("Wrong sha input")
\ No newline at end of file
+import logging, os
+from tqdm import tqdm
+from src.config.config import Config
+from src.utils.commit import CommitHandler
+from src.core.docker.tester import DockerTester
+from concurrent.futures import ProcessPoolExecutor, as_completed
+from src.utils.image_handling import config_image
+from github.Commit import Commit
+
+def get_available_cpus() -> list[int]:
+    """Returns the list of CPUs the current process is allowed to run on."""
+    try:
+        cpus = sorted(os.sched_getaffinity(0))
+        if cpus:
+            return cpus
+    except Exception:
+        pass
+
+    count = os.cpu_count() or 1
+    return list(range(count))
+
+
+def generate_cpu_sets(cpus: list[int], cpus_per_job: int, max_jobs: int) -> list[str]:
+    cpu_sets = []
+    idx = 0
+
+    for _ in range(max_jobs):
+        chunk = cpus[idx : idx + cpus_per_job]
+        if len(chunk) < cpus_per_job:
+            break
+        cpu_sets.append(",".join(map(str, chunk)))
+        idx += cpus_per_job
+
+    if not cpu_sets:
+        raise RuntimeError("Not enough CPUs for requested parallel jobs")
+    return cpu_sets
+
+
+def run_one_commit(repo_id: str, new_sha: str, old_sha: str, config: Config, cpuset_cpus: str = ""):
+    try:
+        if not config_image(config, repo_id, new_sha):
+            return
+
+        commit = CommitHandler("", config.storage_paths["clones"])
+        file = commit.get_file_prefix(repo_id)
+
+        # commits are cloned into these paths
+        new_path, old_path = commit.get_paths(file, new_sha)
+
+        repo = config.git_client.get_repo(repo_id)
+        docker = DockerTester(repo, config)
+        docker.run_commit_pair(new_sha, old_sha, new_path, old_path, cpuset_cpus)
+
+    except Exception as e:
+        logging.exception(f"[{repo_id}] Error testing commits: {e}")
+
+class CommitTesterPipeline:
+    """
+    This class runs the commits and evaluates its performance.
+    """
+    def __init__(self, config: Config):
+        self.config = config
+        self.commit = CommitHandler(self.config.input_file or self.config.storage_paths['commits'], self.config.storage_paths['clones'])
+
+    def test_commit(self, commits_list: list[Commit] = []) -> None:
+        if self.config.docker_image:
+            owner, name, new_sha = tuple(self.config.docker_image.split("_"))
+            repo_id = f"{owner}/{name}"
+            old_sha = self.config.git_client.get_repo(repo_id).get_commit(new_sha).parents[0].sha
+            commits = [(repo_id, new_sha, old_sha)]
+        else:
+            commits = self.commit.get_commits(commits_list)
+        if len(commits) > 0:
+            logging.info(f"Commits found {len(commits)}")
+        tasks: list[tuple[str, str, str, str]] = []
+        available_cpus = get_available_cpus()
+
+        cpu_sets = generate_cpu_sets(
+            cpus=available_cpus,
+            cpus_per_job=2,
+            max_jobs=self.config.resources.max_parallel_jobs,
+        )
+
+        logging.info(f"Available CPUs: {available_cpus}")
+        logging.info(f"CPU pinning sets: {cpu_sets}")
+
+        for i, (repo_id, new_sha, old_sha) in enumerate(commits):
+            cpu_set = cpu_sets[i % len(cpu_sets)]
+            tasks.append((repo_id, new_sha, old_sha, cpu_set))
+
+        with ProcessPoolExecutor(max_workers=len(cpu_sets)) as executor:
+            futures = [
+                executor.submit(run_one_commit, repo_id, new_sha, old_sha, self.config, cpu_set)
+                for repo_id, new_sha, old_sha, cpu_set in tasks
+            ]
+
+            for future in tqdm(as_completed(futures), total=len(futures)):
+                future.result()
+
+        
\ No newline at end of file
diff --git a/src/core/pipelines/pipeline.py b/src/core/pipelines/pipeline.py
old mode 100644
new mode 100755
index 04849ab..acd0eb4
--- a/src/core/pipelines/pipeline.py
+++ b/src/core/pipelines/pipeline.py
@@ -1,15 +1,17 @@
-from src.core.pipelines.collector_pipeline import CollectionPipeline
-from src.core.pipelines.repository_pipeline import RepositoryPipeline
-from src.core.pipelines.commit_pipeline import CommitPipeline
-from src.core.pipelines.commit_tester_pipeline import CommitTesterPipeline
-from src.core.pipelines.tester_pipeline import TesterPipeline
-
-
-
-
-
-
-
-
-
-    
+from src.core.pipelines.collector_pipeline import CollectionPipeline
+from src.core.pipelines.repository_pipeline import RepositoryPipeline
+from src.core.pipelines.commit_pipeline import CommitPipeline
+from src.core.pipelines.commit_tester_pipeline import CommitTesterPipeline
+from src.core.pipelines.push_pipeline import PushPipeline
+from src.core.pipelines.patch_pipeline import PatchPipeline
+
+
+
+
+
+
+
+
+
+
+    
diff --git a/src/core/pipelines/repository_pipeline.py b/src/core/pipelines/repository_pipeline.py
old mode 100644
new mode 100755
index d9b628c..6c2a0e5
--- a/src/core/pipelines/repository_pipeline.py
+++ b/src/core/pipelines/repository_pipeline.py
@@ -1,67 +1,51 @@
-import logging
-from tqdm import tqdm
-from src.config.config import Config
-from src.utils.stats import RepoStats
-from github.Repository import Repository
-from src.gh.collector import RepositoryCollector
-from src.core.filter.structure_filter import StructureFilter
-from src.core.filter.process_filter import ProcessFilter
-from src.utils.writer import Writer
-
-class RepositoryPipeline:
-    """
-    This class takes a list of repositories, and tests and validates them.
-    """
-    def __init__(self, config: Config):
-        self.config = config
-        self.stats = RepoStats()
-        self.valid_repos: list[Repository] = []
-
-    def get_repos(self) -> list[str]:
-        collector = RepositoryCollector(config=self.config)
-        return collector.get_repos()
-
-    def test_repos(self) -> None:
-        collector = RepositoryCollector(self.config)
-        repo_ids = collector.get_repos()
-        if not repo_ids:
-            logging.warning("No repositories found.")
-            return
-
-        logging.info(f"Found {len(repo_ids)} repositories.")
-        for repo_id in tqdm(repo_ids, total=len(repo_ids), desc=f"Testing repositories..."):
-            repo = self.config.git_client.get_repo(repo_id)
-            structure = StructureFilter(repo, self.config)
-
-            try:
-                if structure.is_valid(): #and process.valid_run("_".join(repo.full_name.split("/"))):
-                    self.valid_repos.append(repo)
-                    if self.config.popular or self.config.output_file:
-                        Writer(repo_id, self.config.output_file or self.config.storage_paths['testcrawl']).write_repo()
-            
-                elif self.config.output_file:
-                    Writer(repo_id, self.config.storage_paths['fail']).write_repo()
-
-            except Exception as e:
-                logging.exception(f"[{repo_id}] Error processing repository: {e}")
-            
-    def analyze_repos(self) -> None:
-        collector = RepositoryCollector(config=self.config)
-        repo_ids = collector.get_repos()
-        if not repo_ids:
-            logging.warning("No repositories found.")
-            return
-        
-        logging.info(f"Found {len(repo_ids)} repositories.")
-        for repo_id in tqdm(repo_ids, total=len(repo_ids), desc=f"Analyzing repositories..."):
-            repo = self.config.git_client.get_repo(repo_id)
-            structure = StructureFilter(repo, self.config)
-            try:
-                if structure.is_valid() and (self.config.popular or self.config.output_file):
-                    Writer(repo_id, self.config.output_file).write_repo()
-                self.stats += structure.stats
-
-            except Exception as e:
-                logging.exception(f"[{repo_id}] Error analyzing: {e}")
-
-        self.stats.write_final_log()
+import logging
+from tqdm import tqdm
+from src.config.config import Config
+from src.utils.stats import RepoStats
+from github.Repository import Repository
+from src.gh.collector import RepositoryCollector
+from src.core.filter.structure_filter import StructureFilter
+from src.core.filter.process_filter import ProcessFilter
+from src.utils.writer import Writer
+
+class RepositoryPipeline:
+    """
+    This class takes a list of repositories, and tests and validates them.
+    """
+    def __init__(self, config: Config):
+        self.config = config
+        self.stats = RepoStats()
+        self.valid_repos: list[Repository] = []
+
+    def get_repos(self) -> list[str]:
+        collector = RepositoryCollector(config=self.config)
+        return collector.get_repos()
+
+    def test_repos(self, repos: list[Repository] = []) -> None:
+        if repos:
+            repo_ids = [repo.full_name for repo in repos]
+        else:
+            collector = RepositoryCollector(self.config)
+            repo_ids = collector.get_repos()
+
+        if not repo_ids:
+            logging.warning("No repositories found.")
+            return
+
+        logging.info(f"Found {len(repo_ids)} repositories.")
+        for repo_id in tqdm(repo_ids, total=len(repo_ids), desc=f"Testing repositories...", mininterval=5):
+            repo = self.config.git_client.get_repo(repo_id)
+            structure = StructureFilter(repo, self.config)
+            process = ProcessFilter(repo, self.config)
+
+            try:
+                if structure.is_valid() and process.valid_run("_".join(repo.full_name.split("/"))):
+                    self.valid_repos.append(repo)
+                    if self.config.collect or self.config.output_file:
+                        Writer(repo_id, self.config.output_file or self.config.storage_paths['testcollect']).write_repo()
+            
+                elif self.config.output_file:
+                    Writer(repo_id, self.config.storage_paths['fail']).write_repo()
+
+            except Exception as e:
+                logging.exception(f"[{repo_id}] Error processing repository: {e}")
diff --git a/src/gh/__init__.py b/src/gh/__init__.py
old mode 100644
new mode 100755
diff --git a/src/gh/clone.py b/src/gh/clone.py
old mode 100644
new mode 100755
index cb8e47e..e941c39
--- a/src/gh/clone.py
+++ b/src/gh/clone.py
@@ -1,102 +1,108 @@
-import os, stat, subprocess, shutil, logging
-from pathlib import Path
-
-class GitHandler:
-    def _get_default_branch(self, repo_url: str) -> str:
-        result = subprocess.run(
-            ["git", "ls-remote", "--symref", repo_url, "HEAD"],
-            capture_output=True, text=True
-        )
-
-        for line in result.stdout.splitlines():
-            if line.startswith("ref:"):
-                return line.split()[1].split("/")[-1]
-        return "main" 
-
-    def clone_repo(self, repo_id: str, repo_path: Path, branch: str = "main", sha: str = "") -> bool:
-        url = f"https://github.com/{repo_id}.git"
-        
-        if os.path.exists(repo_path):
-            shutil.rmtree(repo_path, onerror=self._on_rm_error)
-
-        repo_path.mkdir(parents=True, exist_ok=True)
-        
-        if sha:
-            logging.info(f"Cloning repository {url} for commit {sha} into {repo_path}")
-            try:
-                subprocess.run(
-                    ["git", "clone", url, repo_path],
-                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-                )
-
-                subprocess.run(
-                    ["git", "config", "--local", "safe.directory", str(repo_path)],
-                    cwd=repo_path,
-                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-                )
-
-                subprocess.run(
-                    ["git", "fetch", "origin", sha],
-                    cwd=repo_path,
-                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-                )
-
-                subprocess.run(
-                    ["git", "checkout", sha],
-                    cwd=repo_path,
-                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-                )
-
-                subprocess.run(
-                    ["git", "submodule", "update", "--init", "--recursive"],
-                    cwd=repo_path,
-                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-                )
-                self.set_permission(str(repo_path))
-                logging.info(f"Repository checked out to commit {sha} successfully")
-                return True
-                
-            except subprocess.CalledProcessError as e:
-                logging.error(f"Failed to clone/checkout commit {sha}", exc_info=True)
-                logging.error(f"Output (stdout):\n{e.stdout}")
-                logging.error(f"Error (stderr):\n{e.stderr}")
-                return False
-
-        if branch == "main":
-            branch = self._get_default_branch(url)
-        
-        logging.info(f"Cloning repository {url} (branch: {branch}) into {repo_path}")
-        try:
-            subprocess.run(
-                ["git", "clone", "--recurse-submodules", "--branch", branch, url, repo_path],
-                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-            )
-
-            subprocess.run(
-                ["git", "submodule", "update", "--init", "--recursive"],
-                cwd=repo_path,
-                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
-            )
-            self.set_permission(str(repo_path))
-            logging.info(f"Repository cloned successfully")
-            return True
-        except subprocess.CalledProcessError as e:
-            logging.error(f"Failed to clone repository {url} (branch: {branch})", exc_info=True)
-            logging.error(f"Output (stdout):\n{e.stdout}")
-            logging.error(f"Error (stderr):\n{e.stderr}")
-            return False
-        
-    def _on_rm_error(self, func, path, exc_info):
-        os.chmod(path, stat.S_IWRITE)
-        func(path)
-    
-    def set_permission(self, path: str):
-        try:
-            for root, dirs, files in os.walk(path):
-                os.chmod(root, 0o777)
-                for d in dirs:
-                    os.chmod(os.path.join(root, d), 0o777)
-                for f in files:
-                    os.chmod(os.path.join(root, f), 0o777)
-        except Exception as e:
-            logging.warning(f"Tried to set permission failed: {e}")
+import os, stat, subprocess, shutil, logging
+from pathlib import Path
+
+class GitHandler:
+    def _get_default_branch(self, repo_url: str) -> str:
+        result = subprocess.run(
+            ["git", "ls-remote", "--symref", repo_url, "HEAD"],
+            capture_output=True, text=True
+        )
+
+        for line in result.stdout.splitlines():
+            if line.startswith("ref:"):
+                return line.split()[1].split("/")[-1]
+        return "main" 
+
+    def clone_repo(self, repo_id: str, repo_path: Path, branch: str = "main", sha: str = "") -> bool:
+        url = f"https://github.com/{repo_id}.git"
+        
+        if os.path.exists(repo_path):
+            shutil.rmtree(repo_path, onerror=self._on_rm_error)
+
+        repo_path.mkdir(parents=True, exist_ok=True)
+        
+        if sha:
+            logging.info(f"Cloning repository {url} for commit {sha} into {repo_path}")
+            try:
+                subprocess.run(
+                    ["git", "clone", url, repo_path],
+                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+                )
+
+                subprocess.run(
+                    ["git", "config", "--local", "safe.directory", str(repo_path)],
+                    cwd=repo_path,
+                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+                )
+
+                subprocess.run(
+                    ["git", "fetch", "origin", sha],
+                    cwd=repo_path,
+                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+                )
+
+                subprocess.run(
+                    ["git", "checkout", sha],
+                    cwd=repo_path,
+                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+                )
+
+                subprocess.run(
+                    ["git", "submodule", "update", "--init", "--recursive"],
+                    cwd=repo_path,
+                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+                )
+
+                subprocess.run(
+                    ["git", "config", "core.fileMode", "false"], 
+                    cwd=repo_path, 
+                    check=True
+                )
+                self.set_permission(str(repo_path))
+                logging.info(f"Repository checked out to commit {sha} successfully")
+                return True
+                
+            except subprocess.CalledProcessError as e:
+                logging.error(f"Failed to clone/checkout commit {sha}", exc_info=True)
+                logging.error(f"Output (stdout):\n{e.stdout}")
+                logging.error(f"Error (stderr):\n{e.stderr}")
+                return False
+
+        if branch == "main":
+            branch = self._get_default_branch(url)
+        
+        logging.info(f"Cloning repository {url} (branch: {branch}) into {repo_path}")
+        try:
+            subprocess.run(
+                ["git", "clone", "--recurse-submodules", "--branch", branch, url, repo_path],
+                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+            )
+
+            subprocess.run(
+                ["git", "submodule", "update", "--init", "--recursive"],
+                cwd=repo_path,
+                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
+            )
+            self.set_permission(str(repo_path))
+            logging.info(f"Repository cloned successfully")
+            return True
+        except subprocess.CalledProcessError as e:
+            logging.error(f"Failed to clone repository {url} (branch: {branch})", exc_info=True)
+            logging.error(f"Output (stdout):\n{e.stdout}")
+            logging.error(f"Error (stderr):\n{e.stderr}")
+            return False
+        
+    def _on_rm_error(self, func, path, exc_info):
+        os.chmod(path, stat.S_IWRITE)
+        func(path)
+    
+    def set_permission(self, path: str):
+        try:
+            for root, dirs, files in os.walk(path):
+                os.chmod(root, 0o777)
+                for d in dirs:
+                    os.chmod(os.path.join(root, d), 0o777)
+                for f in files:
+                    os.chmod(os.path.join(root, f), 0o777)
+        except Exception as e:
+            logging.warning(f"Tried to set permission failed: {e}")
diff --git a/src/gh/collector.py b/src/gh/collector.py
old mode 100644
new mode 100755
index 6e8db7a..f271cd2
--- a/src/gh/collector.py
+++ b/src/gh/collector.py
@@ -1,221 +1,216 @@
-import logging, time
-from tqdm import tqdm
-from datetime import datetime, timezone, timedelta
-from pathlib import Path
-from src.config.config import Config
-from github.GithubException import GithubException, RateLimitExceededException
-from github.Repository import Repository
-
-class RepositoryCollector:
-
-    # languages considered acceptable alongside C++
-    ACCEPTABLE_LANGUAGES = {
-        "C++", "CMake", "Shell", "C", "Makefile", "Dockerfile",
-        "Meson", "Bazel", "Ninja", "QMake", "Gradle", "JSON", "YAML",
-        "TOML", "INI", "Batchfile", "PowerShell", "Markdown",
-        "HTML", "CSS", "TeX"
-    }
-    MAX_OTHER_LANGUAGE_RATIO = 0.05
-    STAR_REDUCTION_FACTOR = 0.95
-
-    def __init__(self, config: Config, language: str = "C++"):
-        self.language = language
-        self.config = config
-
-    def get_repos(self) -> list[str]:
-        """Get repository IDs from input file or default location."""
-        path = self.config.input_file or self.config.storage_paths["repos"]
-        logging.debug(f"Loading repos from: {path}")
-        return self._get_repo_ids(path)
-    
-    def query_popular_repos(self) -> list[Repository]:
-        """
-        Query GitHub for popular repositories matching criteria.
-        
-        Returns:
-            List of Repository objects that match language and composition criteria
-        """
-        # TODO: test
-        seen_repo_ids = set()
-        if self.config.input_file:
-            seen_repo_ids = set(self._get_repo_ids(self.config.input_file))
-            logging.info(f"Loaded {len(seen_repo_ids)} existing repositories to skip")
-
-        results: list[Repository] = []
-        #upper = self.config.stars 
-        #lower = upper
-        limit = self.config.limit
-        count = 0
-
-        logging.info(f"Starting GitHub query for popular {self.language} repos...")
-        logging.info(f"Target: {limit} repos") #with stars <= {upper}")
-
-        start_boundary = self.config.commits_time['since'] #datetime.strptime(, "%Y-%m-%d").replace(tzinfo=timezone.utc)
-        window_end = datetime.now(timezone.utc)
-        window_size = timedelta(days=1)
-            
-        with tqdm(desc="Discovering repos", unit="repo") as pbar:
-            while window_end > start_boundary and count < limit:
-            #while lower > 0 and count < limit:
-            #    upper = lower
-            #    lower = int(self.STAR_REDUCTION_FACTOR * upper)
-                window_start = max(start_boundary, window_end - window_size)
-                pushed_range = f"pushed:{window_start.date()}..{window_end.date()}"
-                #query = f"language:{self.language} stars:{lower}..{upper}"
-                query = f"{pushed_range} language:{self.language} archived:false"
-
-                if getattr(self.config, "stars", None):
-                    query += f" stars:<= {self.config.stars}"
-
-                logging.info(f"Query: {query}")
-
-                try:
-                    repos = self.config.git_client.search_repositories(
-                        query=query, sort="stars", order="desc"
-                    )
-                    for repo in repos:
-                        if repo.full_name in seen_repo_ids:
-                            logging.debug(f"Skipping {repo.full_name}: already in input list")
-                            continue
-                        
-                        if self._is_valid_repo(repo):
-                            results.append(repo)
-                            seen_repo_ids.add(repo.full_name)
-                            count += 1
-                            pbar.update(1)
-                            pbar.set_postfix({"matched": count})
-
-                            if count >= limit:
-                                break
-
-                        time.sleep(0.5)
-
-                except RateLimitExceededException:
-                    logging.warning("Rate limit exceeded. Waiting 60 seconds...")
-                    time.sleep(60)
-                    continue
-                except GithubException as e:
-                    logging.error(f"GitHub API error: {e}")
-                    time.sleep(5)
-                    continue
-
-                window_end = window_start
-            
-        logging.info(f"Collected {len(results)} repositories matching criteria")
-        return results
-    
-    def _is_valid_repo(self, repo: Repository) -> bool:
-        """
-        Check if repository meets language composition criteria.
-        
-        Args:
-            repo: GitHub Repository object to validate
-            
-        Returns:
-            True if repo has C++ and acceptable language composition
-        """
-        try:
-            languages = repo.get_languages()
-            cpp_bytes = languages.get("C++", 0)
-            total_bytes = sum(languages.values())
-
-            if total_bytes == 0 or cpp_bytes == 0:
-                return False
-            
-            for lang, size in languages.items():
-                if lang not in self.ACCEPTABLE_LANGUAGES:
-                    ratio = size / total_bytes
-                    if ratio > self.MAX_OTHER_LANGUAGE_RATIO:
-                        logging.debug(
-                            f"Rejecting {repo.full_name}: "
-                            f"{lang} comprises {ratio:.1%} (threshold: {self.MAX_OTHER_LANGUAGE_RATIO:.1%})"
-                        )
-                        return False
-            
-            return True
-
-        except GithubException as e:
-            logging.warning(f"Error checking languages for {repo.full_name}: {e}")
-            return False
-    
-    def _get_repo_ids(self, path: str) -> list[str]:
-        """
-        Extract repository IDs (owner/repo) from file or URL.
-        
-        Supports multiple formats:
-        - GitHub URLs: https://github.com/owner/repo
-        - Direct format: owner/repo
-        - CSV format: owner/repo,other,data
-        - Pipe-delimited tables
-        
-        Args:
-            path: File path or URL to parse
-            
-        Returns:
-            List of repository IDs in 'owner/repo' format
-        """
-        repo_ids: list[str] = []
-
-        if self.config.repo_id:
-            repo_id = self.config.repo_id
-            repo_ids.append(repo_id)
-            logging.info(f"Using single repository from URL: {repo_id}")
-            return repo_ids
-        
-        try:
-            file_path = Path(path)
-            if not file_path.exists():
-                logging.warning(f"Input file not found: {path}")
-                return repo_ids
-            
-            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
-                lines = f.readlines()
-
-            for i, line in enumerate(lines, 1):
-                line = line.strip()
-
-                if not line:
-                    continue
-
-                try:
-                    repo_id = self._parse_repo_line(line, path)
-                    if repo_id:
-                        repo_ids.append(repo_id)
-                except Exception as e:
-                    logging.warning(f"Error parsing line {i} in {path}: {e}")
-                    continue
-
-            logging.info(f"Loaded {len(repo_ids)} repository URLs from {path}")
-
-        except (OSError, IOError) as e:
-            logging.error(f"Failed to read repo list from {path}: {e}", exc_info=True)
-
-        return repo_ids
-    
-    def _parse_repo_line(self, line: str, filepath: str) -> str:
-        """
-        Parse a single line to extract repository ID.
-        
-        Args:
-            line: Line to parse
-            filepath: Source file path (used for pipe-delimited format)
-            
-        Returns:
-            Repository ID in 'owner/repo' format, or None if invalid
-        """
-        # Pipe-delimited table format (extract from filename)
-        if '|' in line:
-            # Extract owner/repo from filename pattern
-            filename = Path(filepath).stem
-            parts = filename.split('_')
-            if len(parts) >= 2:
-                return f"{parts[0]}/{parts[1]}"
-            return ""
-        
-        # CSV format
-        if ',' in line:
-            repo_url = line.split(',')[0].strip()
-            return repo_url.removeprefix("https://github.com/").strip()
-        
-        # Direct format or URL
+import logging, time
+from tqdm import tqdm
+from datetime import datetime, timezone, timedelta
+from pathlib import Path
+from src.config.config import Config
+from github.GithubException import GithubException, RateLimitExceededException
+from github.Repository import Repository
+
+class RepositoryCollector:
+
+    # languages considered acceptable alongside C++
+    ACCEPTABLE_LANGUAGES = {
+        "C++", "CMake", "Shell", "C", "Makefile", "Dockerfile",
+        "Meson", "Bazel", "Ninja", "QMake", "Gradle", "JSON", "YAML",
+        "TOML", "INI", "Batchfile", "PowerShell", "Markdown",
+        "HTML", "CSS", "TeX"
+    }
+    MAX_OTHER_LANGUAGE_RATIO = 0.05
+    STAR_REDUCTION_FACTOR = 0.95
+
+    def __init__(self, config: Config, language: str = "C++"):
+        self.language = language
+        self.config = config
+
+    def get_repos(self) -> list[str]:
+        """Get repository IDs from input file or default location."""
+        path = self.config.input_file or self.config.storage_paths["repos"]
+        logging.debug(f"Loading repos from: {path}")
+        return self._get_repo_ids(path)
+    
+    def query_popular_repos(self) -> list[Repository]:
+        """
+        Query GitHub for popular repositories matching criteria.
+        
+        Returns:
+            List of Repository objects that match language and composition criteria
+        """
+        seen_repo_ids = set()
+        path = Path(self.config.input_file)
+        if path.is_file:
+            seen_repo_ids = set(self._get_repo_ids(self.config.input_file))
+            logging.info(f"Loaded {len(seen_repo_ids)} existing repositories to skip")
+
+        results: list[Repository] = []
+        limit = self.config.limit
+        count = 0
+
+        logging.info(f"Starting GitHub query for popular {self.language} repos...")
+        logging.info(f"Target: {limit} repos")
+
+        start_boundary = self.config.commits_time['since']
+        window_end = datetime.now(timezone.utc)
+        window_size = timedelta(days=1)
+            
+        with tqdm(desc="Discovering repos", unit="repo", mininterval=5) as pbar:
+            while window_end > start_boundary and count < limit:
+                window_start = max(start_boundary, window_end - window_size)
+                pushed_range = f"pushed:{window_start.date()}..{window_end.date()}"
+                query = f"{pushed_range}, language:{self.language}, archived:false,"
+
+                if getattr(self.config, "stars", None):
+                    query += f" stars:<={self.config.stars}"
+                    query += f" stars:>={self.config.min_stars}"
+
+                logging.info(f"Query: {query}")
+
+                try:
+                    repos = self.config.git_client.search_repositories(
+                        query=query, sort="stars", order="desc"
+                    )
+                    for repo in repos:
+                        if repo.full_name in seen_repo_ids:
+                            logging.debug(f"Skipping {repo.full_name}: already in input list")
+                            continue
+                        
+                        if self._is_valid_repo(repo):
+                            results.append(repo)
+                            seen_repo_ids.add(repo.full_name)
+                            count += 1
+                            pbar.update(1)
+                            pbar.set_postfix({"matched": count})
+
+                            if count >= limit:
+                                break
+
+                        time.sleep(0.5)
+
+                except RateLimitExceededException:
+                    logging.warning("Rate limit exceeded. Waiting 60 seconds...")
+                    time.sleep(60)
+                    continue
+                except GithubException as e:
+                    logging.error(f"GitHub API error: {e}")
+                    time.sleep(5)
+                    continue
+
+                window_end = window_start
+            
+        logging.info(f"Collected {len(results)} repositories matching criteria")
+        return results
+    
+    def _is_valid_repo(self, repo: Repository) -> bool:
+        """
+        Check if repository meets language composition criteria.
+        
+        Args:
+            repo: GitHub Repository object to validate
+            
+        Returns:
+            True if repo has C++ and acceptable language composition
+        """
+        try:
+            languages = repo.get_languages()
+            cpp_bytes = languages.get("C++", 0)
+            total_bytes = sum(languages.values())
+
+            if total_bytes == 0 or cpp_bytes == 0:
+                return False
+            
+            for lang, size in languages.items():
+                if lang not in self.ACCEPTABLE_LANGUAGES:
+                    ratio = size / total_bytes
+                    if ratio > self.MAX_OTHER_LANGUAGE_RATIO:
+                        logging.debug(
+                            f"Rejecting {repo.full_name}: "
+                            f"{lang} comprises {ratio:.1%} (threshold: {self.MAX_OTHER_LANGUAGE_RATIO:.1%})"
+                        )
+                        return False
+            
+            return True
+
+        except GithubException as e:
+            logging.warning(f"Error checking languages for {repo.full_name}: {e}")
+            return False
+    
+    def _get_repo_ids(self, path: str) -> list[str]:
+        """
+        Extract repository IDs (owner/repo) from file or URL.
+        
+        Supports multiple formats:
+        - GitHub URLs: https://github.com/owner/repo
+        - Direct format: owner/repo
+        - CSV format: owner/repo,other,data
+        - Pipe-delimited tables
+        
+        Args:
+            path: File path or URL to parse
+            
+        Returns:
+            List of repository IDs in 'owner/repo' format
+        """
+        repo_ids: list[str] = []
+
+        if self.config.repo_id:
+            repo_id = self.config.repo_id
+            repo_ids.append(repo_id)
+            logging.info(f"Using single repository from URL: {repo_id}")
+            return repo_ids
+        
+        try:
+            file_path = Path(path)
+            if not file_path.exists():
+                logging.warning(f"Input file not found: {path}")
+                return repo_ids
+            
+            if not file_path.is_file:
+                logging.warning(f"Input path is not a file: {path}")
+                return repo_ids
+            
+            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
+                lines = f.readlines()
+
+            for i, line in enumerate(lines, 1):
+                line = line.strip()
+
+                if not line:
+                    continue
+
+                try:
+                    repo_id = self._parse_repo_line(line, path)
+                    if repo_id:
+                        repo_ids.append(repo_id)
+                except Exception as e:
+                    logging.warning(f"Error parsing line {i} in {path}: {e}")
+                    continue
+
+            logging.info(f"Loaded {len(repo_ids)} repository URLs from {path}")
+
+        except (OSError, IOError) as e:
+            logging.error(f"Failed to read repo list from {path}: {e}", exc_info=True)
+
+        return repo_ids
+    
+    def _parse_repo_line(self, line: str, filepath: str) -> str:
+        """
+        Parse a single line to extract repository ID.
+        
+        Args:
+            line: Line to parse
+            filepath: Source file path (used for pipe-delimited format)
+            
+        Returns:
+            Repository ID in 'owner/repo' format, or None if invalid
+        """
+        if '|' in line:
+            parts = line.split('|')
+            if len(parts) >= 1 and '/' in parts[0]:
+                return parts[0]
+            return ""
+        
+        # CSV format
+        if ',' in line:
+            repo_url = line.split(',')[0].strip()
+            return repo_url.removeprefix("https://github.com/").strip()
+
         return line.removeprefix("https://github.com/").strip()
\ No newline at end of file
diff --git a/src/input.py b/src/input.py
old mode 100644
new mode 100755
index 7930460..aa4af0b
--- a/src/input.py
+++ b/src/input.py
@@ -1,85 +1,98 @@
-import argparse
-from src.core.controller import Controller
-from src.config.config import Config
-
-def setup_parser() -> argparse.ArgumentParser:
-    parser = argparse.ArgumentParser(
-        description="GitHub automation tool: crawl repos, gather commits, and run tests.",
-        formatter_class=argparse.RawDescriptionHelpFormatter,
-        epilog="""
-        Examples:
-        python main.py --popular --stars=1000 --limit=10 --output=data/crawl.txt
-        python main.py --testcrawl --input=data/crawl.txt --output=data/test.txt
-        python main.py --commits --repo=gabime/spdlog
-        python main.py --testcommits --input data/test.txt
-        python main.py --test --mount data/repo --docker cpp-base
-        """
-    )
-
-    # === Mode selection ===
-    mode = parser.add_mutually_exclusive_group(required=True)
-    mode.add_argument("--popular", action="store_true",
-                      help="Crawl GitHub for popular repositories.")
-    mode.add_argument("--testcrawl", action="store_true",
-                      help="Test and validate crawled GitHub repositories.")
-    mode.add_argument("--commits", action="store_true",
-                      help="Gather and filter commits from a repository.")
-    mode.add_argument("--testcommits", action="store_true",
-                      help="Test commits between two versions or commit file.")
-    mode.add_argument("--testdocker", action="store_true",
-                      help="Test Docker images or compare mounted and old commit.")
-
-    # === Input / Output ===
-    io_group = parser.add_argument_group("Input / Output Options")
-    io_group.add_argument("--input", type=str,
-                          help="Path to input file (e.g., crawl.txt).")
-    io_group.add_argument("--output", type=str, default="data/results.txt",
-                          help="Output file path (default: data/results.txt).")
-    io_group.add_argument("--repo", type=str,
-                          help="Repository URL or slug (e.g., owner/repo).")
-
-    # === Commit options ===
-    commit_group = parser.add_argument_group("Commit Options")
-    commit_group.add_argument("--sha", type=str,
-                              help="SHA for testing.")
-    commit_group.add_argument("--separate", action="store_true",
-                              help="Save each filtered commit separately with commit message and diff.")
-
-    # === Filtering / Analysis ===
-    filter_group = parser.add_argument_group("Filtering and Analysis Options")
-    filter_group.add_argument("--limit", type=int, default=10,
-                              help="Limit number of repositories or commits (default: 10).")
-    filter_group.add_argument("--stars", type=int, default=1000,
-                              help="Minimum star count for popular repos (default: 1000).")
-    filter_group.add_argument("--filter", type=str, choices=["simple", "llm", "issue"],
-                              default="simple", help="Filter strategy to use (default: simple).")
-    filter_group.add_argument("--analyze", action="store_true",
-                              help="Analyze the given repositories.")
-
-    # === Docker / Testing ===
-    docker_group = parser.add_argument_group("Docker and Testing Options")
-    docker_group.add_argument("--docker", type=str,
-                              help="Docker image to build and test repositories or commits.")
-    docker_group.add_argument("--mount", type=str,
-                              help="Mount directory to Docker, build, test, and evaluate against old commit.")
-
-    return parser
-
-
-def create_config(args: argparse.Namespace) -> Config:
-    """Create a Config object from argparse arguments."""
-    cfg = Config(**vars(args))
-    return cfg
-
-
-def start() -> None:
-    parser = setup_parser()
-    args = parser.parse_args()
-
-    config = create_config(args)
-    pipeline = Controller(config=config)
-    pipeline.run()
-
-
-if __name__ == "__main__":
-    start()
+import argparse
+from src.core.controller import Controller
+from src.config.config import Config
+
+def setup_parser() -> argparse.ArgumentParser:
+    parser = argparse.ArgumentParser(
+        description="GitHub automation tool: crawl repos, gather commits, and run tests.",
+        formatter_class=argparse.RawDescriptionHelpFormatter
+    )
+
+    # === Mode selection ===
+    mode = parser.add_argument_group()
+    mode.add_argument("--collect", action="store_true",
+                      help="Collect C++ Repositories from GitHub. Set with --limit and --stars flags.")
+    mode.add_argument("--testcollect", action="store_true",
+                      help="Test and validate collected C++ Repositories from GitHub.")
+    mode.add_argument("--commits", action="store_true",
+                      help="Gather and filter commits from C++ Repositories.")
+    mode.add_argument("--testcommits", action="store_true",
+                      help="Test commits between two versions and generates a commit file.")
+    mode.add_argument("--test", action="store_true",
+                      help="Call --testcollect on --collect output, or --testcommits on --commits output.")
+    mode.add_argument("--genimages", action="store_true",
+                      help="Given a folder of json files generated via the --testcommits flag, " \
+                      "generate and save docker images (no test is run here) of each json file.")
+    mode.add_argument("--pushimages", action="store_true",
+                      help="Given a folder of json files generated via the --testcommits flag, " \
+                      "push the image to Dockerhub.")
+    mode.add_argument("--testdocker", action="store_true",
+                      help="Build and test docker images. " \
+                      "Given a file of docker images 'owner_repo_newsha' with commits in " \
+                      "'/test_workspace/workspace/new' and '/test_workspace/workspace/old' " \
+                      "Docker images should be named 'owner_repo_newsha' or 'dockerhub_user/dockerhub_repo:owner_repo_newsha'")
+    mode.add_argument("--patch", action="store_true",
+                      help="Given owner/repo (repo_id), a commit SHA value and a prompt " \
+                      "use OpenHands to generate a patch. Generates a diff file.")
+    mode.add_argument("--testpatch", action="store_true",
+                      help="Build and test docker images." \
+                      "Given a file of docker images (or a docker image tar files) with a " \
+                      "commit at '/test_workspace/workspace/old' and its patch in '/test_workspace/workspace/patch'. " \
+                      "Docker images should be named 'owner_repo_newsha' or 'dockerhub_user/dockerhub_repo:owner_repo_newsha'")
+    
+    
+
+    # === Input / Output ===
+    io_group = parser.add_argument_group("Input / Output Options")
+    io_group.add_argument("--input", type=str,
+                          help="Path to input file (e.g., crawl.txt).")
+    io_group.add_argument("--output", type=str, default="data/results.txt",
+                          help="Output file path (default: data/results.txt).")
+    io_group.add_argument("--repo", type=str,
+                          help="Repository URL or repo full name (e.g., owner/repo).")
+    io_group.add_argument("--prompt", type=str,
+                          help="Prompt for OpenHands")
+
+    # === Commit options ===
+    commit_group = parser.add_argument_group("Commit Options")
+    commit_group.add_argument("--sha", type=str,
+                              help="SHA for testing.")
+
+    # === Filtering / Analysis ===
+    filter_group = parser.add_argument_group("Filtering and Analysis Options")
+    filter_group.add_argument("--limit", type=int, default=10,
+                              help="Limit number of repositories or commits (default: 10).")
+    filter_group.add_argument("--stars", type=int, default=1000,
+                              help="Minimum star count for popular repos (default: 1000).")
+    filter_group.add_argument("--filter", type=str, choices=["simple", "llm", "issue"],
+                              default="llm", help="Filter strategy to use (default: llm).")
+
+    # === Docker / Testing ===
+    docker_group = parser.add_argument_group("Docker and Testing Options")
+    docker_group.add_argument("--tar", type=str,
+                              help="Saves the docker image as a tar file.")
+    docker_group.add_argument("--docker", type=str,
+                              help="Docker image to create a docker container that builds and tests the commits.")
+    docker_group.add_argument("--mount", type=str,
+                              help="Mounts a folder to the docker container.")
+    docker_group.add_argument("--diff", type=str,
+                              help="Applies the diff patch to the old (original) commit in the docker container.")
+    return parser
+
+
+def create_config(args: argparse.Namespace) -> Config:
+    """Create a Config object from argparse arguments."""
+    cfg = Config(**vars(args))
+    return cfg
+
+
+def start() -> None:
+    parser = setup_parser()
+    args = parser.parse_args()
+    config = create_config(args)
+    pipeline = Controller(config=config)
+    pipeline.run()
+
+
+if __name__ == "__main__":
+    start()
diff --git a/src/llm/__init__.py b/src/llm/__init__.py
old mode 100644
new mode 100755
diff --git a/src/llm/llmadapter.py b/src/llm/llmadapter.py
old mode 100644
new mode 100755
index f0dcb97..e224c9e
--- a/src/llm/llmadapter.py
+++ b/src/llm/llmadapter.py
@@ -1,22 +1,22 @@
-from openai import OpenAI
-from src.llm.prompt import Prompt
-from src.config.config import Config
-
-class LLMAdapter():
-    def __init__(self, config: Config, model: str, read_from_cache: bool = False, save_to_cache: bool = False):
-        self.config = config
-        self.read_from_cache = read_from_cache
-        self.save_to_cache = save_to_cache
-        if self.config.llm.base:
-            self.client = OpenAI(base_url=self.config.llm.base_url, api_key=self.config.llm.api_key)
-        else:
-            self.client = OpenAI(api_key=self.config.llm.api_key)
-        self.model = model
-
-    def _send_request(self, prompt: Prompt):
-        raise NotImplementedError
-
-    def generate(self, prompt: Prompt) -> str:
-        return self._send_request(prompt)
-
+from openai import OpenAI
+from src.llm.prompt import Prompt
+from src.config.config import Config
+
+class LLMAdapter():
+    def __init__(self, config: Config, model: str, read_from_cache: bool = False, save_to_cache: bool = False):
+        self.config = config
+        self.read_from_cache = read_from_cache
+        self.save_to_cache = save_to_cache
+        if self.config.llm.base:
+            self.client = OpenAI(base_url=self.config.llm.base_url, api_key=self.config.llm.api_key)
+        else:
+            self.client = OpenAI(api_key=self.config.llm.api_key)
+        self.model = model
+
+    def _send_request(self, prompt: Prompt):
+        raise NotImplementedError
+
+    def generate(self, prompt: Prompt) -> str:
+        return self._send_request(prompt)
+
     
\ No newline at end of file
diff --git a/src/llm/ollama.py b/src/llm/ollama.py
old mode 100644
new mode 100755
index 45a251e..b9e6adc
--- a/src/llm/ollama.py
+++ b/src/llm/ollama.py
@@ -1,18 +1,24 @@
-from src.config.settings import LLMSettings
-from src.llm.llmadapter import LLMAdapter
-from src.llm.prompt import Prompt
-import requests
-from src.config.config import Config
-
-class OllamaLLM(LLMAdapter):
-    def __init__(self, config: Config, model: str):
-        super().__init__(config, model)
-
-    def _send_request(self, prompt: Prompt) -> str:
-        full_prompt = "\n".join([m.content for m in prompt.messages]).strip()
-        response = requests.post(self.config.llm.ollama_url, json={
-            "model": self.model,
-            "prompt": full_prompt,
-            "stream": False
-        })
+from src.config.settings import LLMSettings
+from src.llm.llmadapter import LLMAdapter
+from src.llm.prompt import Prompt
+import requests
+from src.config.config import Config
+
+class OllamaLLM(LLMAdapter):
+    def __init__(self, config: Config, model: str):
+        super().__init__(config, model)
+
+    def _send_request(self, prompt: Prompt) -> str:
+        full_prompt = "\n".join([m.content for m in prompt.messages]).strip()
+
+        response = requests.post(self.config.llm.ollama_url, json={
+            "model": self.model,
+            "prompt": full_prompt,
+            "stream": False
+        })
+        
+        data = response.json()
+        if "error" in data:
+            raise RuntimeError(f"Ollama error: {data['error']}")
+        
         return response.json()["response"]
\ No newline at end of file
diff --git a/src/llm/openai.py b/src/llm/openai.py
old mode 100644
new mode 100755
index 5cc669e..d30791f
--- a/src/llm/openai.py
+++ b/src/llm/openai.py
@@ -1,16 +1,16 @@
-from src.llm.llmadapter import LLMAdapter 
-from src.llm.prompt import Prompt
-from typing import Union
-from src.config.config import Config
-
-class OpenRouterLLM(LLMAdapter):
-    def __init__(self, config: Config, model: str):
-        super().__init__(config, model)
-
-    def _send_request(self, prompt: Prompt) -> Union[str, None]:
-        completion = self.client.chat.completions.create(
-            model=f"{self.model}",
-            messages=[m.__dict__ for m in prompt.messages] # type: ignore
-        )
-        return completion.choices[0].message.content
+from src.llm.llmadapter import LLMAdapter 
+from src.llm.prompt import Prompt
+from typing import Union
+from src.config.config import Config
+
+class OpenRouterLLM(LLMAdapter):
+    def __init__(self, config: Config, model: str):
+        super().__init__(config, model)
+
+    def _send_request(self, prompt: Prompt) -> Union[str, None]:
+        completion = self.client.chat.completions.create(
+            model=f"{self.model}",
+            messages=[m.__dict__ for m in prompt.messages] # type: ignore
+        )
+        return completion.choices[0].message.content
         
\ No newline at end of file
diff --git a/src/llm/prompt.py b/src/llm/prompt.py
old mode 100644
new mode 100755
index 5b186f8..e1de380
--- a/src/llm/prompt.py
+++ b/src/llm/prompt.py
@@ -1,9 +1,9 @@
-class Prompt():
-    class Message():
-        def __init__(self, role: str, content: str):
-            self.role = role
-            self.content = content
-
-    def __init__(self, messages: list[Message]):
-        self.messages = messages
-
+class Prompt():
+    class Message():
+        def __init__(self, role: str, content: str):
+            self.role = role
+            self.content = content
+
+    def __init__(self, messages: list[Message]):
+        self.messages = messages
+
diff --git a/src/utils/__init__.py b/src/utils/__init__.py
old mode 100644
new mode 100755
diff --git a/src/utils/commit.py b/src/utils/commit.py
old mode 100644
new mode 100755
index ea2d498..3b41616
--- a/src/utils/commit.py
+++ b/src/utils/commit.py
@@ -1,80 +1,100 @@
-import logging
-from pathlib import Path
-
-class Commit:
-    def __init__(self, input_file: str, output_path: str):
-        self.input_file = input_file
-        self.output_path = output_path
-        #Path(self.input_file).mkdir(parents=True, exist_ok=True)
-        #Path(self.output_path).mkdir(parents=True, exist_ok=True)
-    
-    # repo_id: str
-    def get_commits(self) -> list[tuple[str, str, str]]:
-        """Return list of (repo_id, new_sha, old_sha) pairs."""
-        #try:
-        #    owner, name = repo_id.strip().split("/", 1)
-        #except ValueError:
-        #    raise ValueError(f"Invalid repo ID format: '{repo_id}'. Expected '<owner>/<repo>'.")
-        
-        #file_prefix = f"{owner}_{name}"
-        #filename = f"{file_prefix}_filtered.txt"
-        file_path = Path(self.input_file) #/ filename
-
-        commits = self._get_filtered_commits(file_path)
-        if not commits:
-            logging.warning(f"No valid commit pairs found in {file_path}")
-
-        return commits
-    
-    def get_paths(self, file_prefix: str, sha: str) -> tuple[Path, Path]:
-        """
-        Returns paths for {old,new} commit directories to be tested.
-        Example: data/commits/<file_prefix>_<sha>/{old,new}
-        """
-        output = Path(self.output_path)
-        output.chmod(0o777)
-        commit_root = output / f"{file_prefix}_{sha}"
-        old_path = commit_root / "old"
-        new_path = commit_root / "new"
-        return new_path, old_path
-
-    def _get_filtered_commits(self, path: Path) -> list[tuple[str, str, str]]:
-        """
-        Extract commit pairs from a text file.
-        Expected line format:
-            <new_sha> | <old_sha> | ...
-        """
-        commits_info: list[tuple[str, str, str]] = []
-
-        if not path.exists():
-            logging.warning(f"Commit file not found: {path}")
-            return commits_info
-        
-        try:
-            with open(path, 'r', errors='ignore') as f:
-                for line_no, line in enumerate(f, start=1):
-                    stripped = line.strip()
-                    if not stripped or stripped.startswith("#"):
-                        continue
-
-                    parts = [p.strip() for p in stripped.split("|") if p.strip()]
-                    if len(parts) <= 2:
-                        logging.warning(f"Malformed commit line at {path}:{line_no} -> {stripped}")
-                        continue
-
-                    if len(parts) > 2:
-                        # repo_id | new_sha | old_sha
-                        commits_info.append((parts[0].strip(), parts[1].strip(), parts[2].strip()))
-
-        except (OSError, IOError) as e:
-            logging.error(f"Failed to read commits from {path}: {e}", exc_info=True)
-
-        return commits_info
-
-    def get_file_prefix(self, repo_id: str) -> str:
-        try:
-            owner, name = repo_id.strip().split("/", 1)
-        except ValueError:
-            raise ValueError(f"Invalid repo ID format: '{repo_id}'. Expected '<owner>/<repo>'.")
-        
+import logging, ast, json
+from pathlib import Path
+from src.config.config import Config
+from github.Commit import Commit
+
+class CommitHandler:
+    def __init__(self, input_file: str, output_path: str):
+        self.input_file = input_file
+        self.output_path = output_path
+
+    def _get_commits_from_json_files(self) -> list[tuple[str, str, str]]:
+        """
+        Return list of (repo_id, new_sha, old_sha) pairs from
+        json files generated from '--testcommits' flag. 
+        """
+        commits_info: list[tuple[str, str, str]] = []
+        json_folder = Path(self.input_file)
+        for json_file in json_folder.glob("*.json"):
+            with open(json_file, 'r', errors='ignore') as f:
+                res = json.load(f)
+
+            metadata = res['metadata']
+            commit_info = res['commit_info']
+            repo_id = metadata['repository_name']
+            new_sha = commit_info['new_sha']
+            old_sha = commit_info['old_sha']
+            commits_info.append((repo_id, new_sha.strip(), old_sha.strip()))
+        
+        return commits_info
+    
+    def get_commits(self, commits_list: list[Commit] = []) -> list[tuple[str, str, str]]:
+        """Return list of (repo_id, new_sha, old_sha) pairs."""
+        file_path = Path(self.input_file)
+
+        if commits_list:
+            all_commits = [(commit.repository.full_name, commit.sha, commit.parents[0].sha) for commit in commits_list]
+        elif file_path.is_file():
+            all_commits = self._get_filtered_commits(file_path)
+        elif file_path.is_dir():
+            all_commits = self._get_commits_from_json_files()
+
+        if not all_commits:
+            logging.warning(f"No valid commit pairs found in {file_path}")
+
+        return all_commits
+    
+    def get_paths(self, file_prefix: str, sha: str) -> tuple[Path, Path]:
+        """
+        Returns paths for {old,new|patch} commit directories to be tested.
+        Example: data/commits/<file_prefix>_<sha>/{old,new}
+        """
+        output = Path(self.output_path)
+        output.chmod(0o777)
+        commit_root = output / f"{file_prefix}_{sha}"
+        old_path = commit_root / "old"
+        new_path = commit_root / "new"
+        return new_path, old_path
+
+    def _get_filtered_commits(self, path: Path) -> list[tuple[str, str, str]]:
+        """
+        Extract commit pairs from a text file.
+        Expected line format:
+            <repo_id> | <new_sha> | <old_sha>
+        """
+        commits_info: list[tuple[str, str, str]] = []
+
+        if not path.exists():
+            logging.warning(f"Commit file not found: {path}")
+            return commits_info
+        
+        try:
+            with open(path, 'r', errors='ignore') as f:
+                for line_no, line in enumerate(f, start=1):
+                    stripped = line.strip()
+                    if not stripped or stripped.startswith("#"):
+                        continue
+
+                    parts = [p.strip() for p in stripped.split("|") if p.strip()]
+                    if len(parts) < 3:
+                        logging.warning(f"Malformed commit line at {path}:{line_no} -> {stripped}")
+                        continue
+
+                    repo_id = parts[0]
+                    new_sha = parts[1]
+                    old_sha = parts[2]
+                    
+                    commits_info.append((repo_id, new_sha, old_sha))
+
+        except (OSError, IOError) as e:
+            logging.error(f"Failed to read commits from {path}: {e}", exc_info=True)
+
+        return commits_info
+
+    def get_file_prefix(self, repo_id: str) -> str:
+        try:
+            owner, name = repo_id.strip().split("/", 1)
+        except ValueError:
+            raise ValueError(f"Invalid repo ID format: '{repo_id}'. Expected '<owner>/<repo>'.")
+        
         return f"{owner}_{name}"
\ No newline at end of file
diff --git a/src/utils/exceptions.py b/src/utils/exceptions.py
old mode 100644
new mode 100755
index 229902a..cee34c1
--- a/src/utils/exceptions.py
+++ b/src/utils/exceptions.py
@@ -1,2 +1,7 @@
-class TestFailed(Exception):
+class TestFailed(Exception):
+    """Raised when """
+    pass
+
+class UndefinedStructureFilter(Exception):
+    """Raised when a StructureFilter or its CMakeProcess is undefined."""
     pass
\ No newline at end of file
diff --git a/src/utils/logging.py b/src/utils/logging.py
old mode 100644
new mode 100755
index b865edc..a8fecff
--- a/src/utils/logging.py
+++ b/src/utils/logging.py
@@ -1,19 +1,20 @@
-import logging, sys
-from datetime import datetime
-from pathlib import Path
-
-def logging_setup() -> None:
-    LOG_DIR = Path("logs")
-    LOG_DIR.mkdir(exist_ok=True)
-
-    log_filename = LOG_DIR / f"logging_{datetime.now():%Y-%m-%d-%H-%M}.log"
-
-    logging.basicConfig(
-        filename=log_filename,
-        filemode='a',
-        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
-        datefmt='%H:%M:%S',
-        level=logging.INFO
-    )
-
+import logging, sys
+from datetime import datetime
+from pathlib import Path
+
+def logging_setup() -> None:
+    LOG_DIR = Path("logs")
+    LOG_DIR.mkdir(exist_ok=True)
+
+    log_filename = LOG_DIR / f"logging_{datetime.now():%Y-%m-%d-%H-%M}.log"
+
+    logging.basicConfig(
+        filename=log_filename,
+        filemode='a',
+        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
+        datefmt='%H:%M:%S',
+        level=logging.INFO,
+        force=True
+    )
+
     logging.info(f"Command run: {' '.join(sys.argv)}")
\ No newline at end of file
diff --git a/src/utils/parser.py b/src/utils/parser.py
old mode 100644
new mode 100755
index 81cf749..381fc5a
--- a/src/utils/parser.py
+++ b/src/utils/parser.py
@@ -1,144 +1,161 @@
-import re
-from typing import Union
-from collections import defaultdict
-
-def parse_framework_output(output: str, framework: str, test_name: str) -> float:
-    if framework == "gtest":
-        pattern = rf"\[\s*OK\s*\].*{re.escape(test_name)}.*\((\d+)\s*ms\)"
-        match = re.search(pattern, output)
-        if match:
-            ms = int(match.group(1))
-            return ms / 1000.0
-        return -1.0
-    elif framework == "catch":
-        pattern = rf"(\d+\.\d+)\s+s:\s+{re.escape(test_name)}"
-        match = re.search(pattern, output)
-        if match:
-            return float(match.group(1))
-        return -1.0
-    elif framework == "doctest":
-        #pattern = rf"\[{re.escape(test_name)}\]\s+passed\s+in\s+(\d+\.\d+)s"
-        #pattern = rf"{re.escape(test_name)}.*\((\d+\.\d+)s\)"
-        #match = re.search(pattern, output)
-        #if match:
-        #    return float(match.group(1))
-        if "Status:" in output and "SUCCESS" in output:
-            return 0.0
-        else:
-            return -1.0
-    elif framework == "boost":
-        pattern = rf"{re.escape(test_name)}.*passed in (\d+\.\d+) sec"
-        match = re.search(pattern, output)
-        if match:
-            return float(match.group(1))
-        return -1.0
-    elif framework == "qt":
-        pattern = rf"PASS\s*:\s*{re.escape(test_name)}\s*\((\d+\.\d+)\s*seconds\)"
-        match = re.search(pattern, output)
-        if match:
-            return float(match.group(1))
-        return -1.0
-    else:
-        raise ValueError(f"Unknown framework {framework}")
-
-
-def parse_ctest_output(output: str) -> dict[str, Union[int, float]]:
-    """Parse CTest output text to extract key test statistics."""
-    stats: dict[str, Union[int, float]] = {
-        "passed": 0,
-        "failed": 0,
-        "skipped": 0,
-        "total": 0,
-        "total_time_sec": 0.0,
-        "pass_rate": 0,
-    }
-
-    m = re.search(r"(\d+)% tests passed,\s*(\d+)\s*tests failed out of\s*(\d+)", output)
-    if m:
-        pass_rate, failed, total = map(int, m.groups())
-        passed = total - failed
-        stats.update({
-            "passed": passed,
-            "failed": failed,
-            "total": total,
-            "pass_rate": pass_rate
-        })
-
-    m = re.search(r"(\d+)\s*tests skipped", output)
-    if m:
-        stats["skipped"] = int(m.group(1))
-
-    m = re.search(r"Total Test time \(real\)\s*=\s*([\d\.]+)\s*sec", output)
-    if m:
-        stats["total_time_sec"] = float(m.group(1))
-
-    return stats
-
-def parse_single_ctest_output(output: str, previous_results: dict[str, dict[str, list[float]]] = {}) -> dict[str, dict[str, list[float]]]:
-    """
-    Parse ctest output into a dictionary mapping test names to a list of runtimes.
-
-    Args:
-        output (str): The raw ctest output as a string.
-        previous_results (dict, optional): A previous dictionary to merge with.
-
-    Returns:
-        dict: {test_name: [times]}
-    """
-
-    if not previous_results:
-        results: dict[str, dict[str, list[float]]] = defaultdict(dict)
-    else:
-        results: dict[str, dict[str, list[float]]] = defaultdict(dict, {k: v for k, v in previous_results.items()})
-
-    patterns = [
-        # pattern 1: [OK] test_name (time ms)
-        r"\[\s*OK\s*\]\s*(.+?)\s*\((\d+)\s*ms\)",
-        
-        # pattern 2: time s: test_name
-        r"(\d+\.\d+)\s+s:\s+(.+)",
-
-        # pattern 3: [test_name] passed in time s
-        r"\[(.+?)\]\s+passed\s+in\s+(\d+\.\d+)s",
-
-        # pattern 4: test_name passed in time sec
-        r"(.+?)\s+passed\s+in\s+(\d+\.\d+)\s+sec",
-
-        # pattern 5: PASS : test_name (time seconds)
-        r"PASS\s*:\s*(.+?)\s*\((\d+\.\d+)\s*seconds\)",
-
-        # pattern 6: Test #number: test_name ... Passed time sec
-        r"Test\s+#\d+:\s+([\w\.\-]+)\s+.*Passed\s+([\d\.]+)\s+sec",
-    ]
-
-    for pattern in patterns:
-        for match in re.finditer(pattern, output):
-            if pattern == patterns[1]:
-                # pattern 2 swaps order: time, then test name
-                time = float(match.group(1))
-                test_name = match.group(2)
-            else:
-                test_name = match.group(1)
-                time = match.group(2)
-
-            if not results[test_name]:
-                results[test_name] = {'parsed': [], 'time': []}
-
-            # Normalize time to seconds (convert ms to seconds if needed)
-            if "ms" in pattern:
-                time = int(time) / 1000.0
-            else:
-                time = float(time)
-            
-            results[test_name]['parsed'].append(time)
-            results[test_name]['time'].append(time)
-
-    return dict(results)
-
-def parse_usr_bin_time(output: str) -> float:
-    match = re.search(r'(\d+)\s*ms', output, re.MULTILINE)
-    if match:
-        real_ms = int(match.group(1))
-        real_seconds = real_ms / 1000.0
-        return real_seconds
-    return 0.0
\ No newline at end of file
+import re
+from pathlib import Path
+from typing import Union
+from collections import defaultdict
+
+def parse_framework_output(output: str, framework: str, test_name: str) -> float:
+    if framework == "gtest":
+        pattern = r"ran\. \((\d+)\s*ms total\)"
+        match = re.search(pattern, output)
+        if match:
+            ms = int(match.group(1))
+            return ms / 1000.0
+        return -1.0
+    elif framework == "catch":
+        pattern = rf"(\d+\.\d+)\s+s:\s+{re.escape(test_name)}"
+        match = re.search(pattern, output)
+        if match:
+            return float(match.group(1))
+        return -1.0
+    elif framework == "doctest":
+        #pattern = rf"\[{re.escape(test_name)}\]\s+passed\s+in\s+(\d+\.\d+)s"
+        #pattern = rf"{re.escape(test_name)}.*\((\d+\.\d+)s\)"
+        #match = re.search(pattern, output)
+        #if match:
+        #    return float(match.group(1))
+        if "Status:" in output and "SUCCESS" in output:
+            return 0.0
+        else:
+            return -1.0
+    elif framework == "boost":
+        pattern = rf"{re.escape(test_name)}.*passed in (\d+\.\d+) sec"
+        match = re.search(pattern, output)
+        if match:
+            return float(match.group(1))
+        return -1.0
+    elif framework == "qt":
+        pattern = rf"PASS\s*:\s*{re.escape(test_name)}\s*\((\d+\.\d+)\s*seconds\)"
+        match = re.search(pattern, output)
+        if match:
+            return float(match.group(1))
+        return -1.0
+    else:
+        raise ValueError(f"Unknown framework {framework}")
+
+
+def parse_ctest_output(output: str) -> dict[str, Union[int, float]]:
+    """Parse CTest output text to extract key test statistics."""
+    stats: dict[str, Union[int, float]] = {
+        "passed": 0,
+        "failed": 0,
+        "skipped": 0,
+        "total": 0,
+        "total_time_sec": 0.0,
+        "pass_rate": 0,
+    }
+
+    m = re.search(r"(\d+)% tests passed,\s*(\d+)\s*tests failed out of\s*(\d+)", output)
+    if m:
+        pass_rate, failed, total = map(int, m.groups())
+        passed = total - failed
+        stats.update({
+            "passed": passed,
+            "failed": failed,
+            "total": total,
+            "pass_rate": pass_rate
+        })
+
+    m = re.search(r"(\d+)\s*tests skipped", output)
+    if m:
+        stats["skipped"] = int(m.group(1))
+
+    m = re.search(r"Total Test time \(real\)\s*=\s*([\d\.]+)\s*sec", output)
+    if m:
+        stats["total_time_sec"] = float(m.group(1))
+
+    return stats
+
+def parse_single_ctest_output(output: str, previous_results: dict[str, dict[str, list[float]]] = {}) -> dict[str, dict[str, list[float]]]:
+    """
+    Parse ctest output into a dictionary mapping test names to a list of runtimes.
+
+    Args:
+        output (str): The raw ctest output as a string.
+        previous_results (dict, optional): A previous dictionary to merge with.
+
+    Returns:
+        dict: {test_name: [times]}
+    """
+
+    if not previous_results:
+        results: dict[str, dict[str, list[float]]] = defaultdict(dict)
+    else:
+        results: dict[str, dict[str, list[float]]] = defaultdict(dict, {k: v for k, v in previous_results.items()})
+
+    patterns = [
+        # pattern 1: [OK] test_name (time ms)
+        r"\[\s*OK\s*\]\s*(.+?)\s*\((\d+)\s*ms\)",
+        
+        # pattern 2: time s: test_name
+        r"(\d+\.\d+)\s+s:\s+(.+)",
+
+        # pattern 3: [test_name] passed in time s
+        r"\[(.+?)\]\s+passed\s+in\s+(\d+\.\d+)s",
+
+        # pattern 4: test_name passed in time sec
+        r"(.+?)\s+passed\s+in\s+(\d+\.\d+)\s+sec",
+
+        # pattern 5: PASS : test_name (time seconds)
+        r"PASS\s*:\s*(.+?)\s*\((\d+\.\d+)\s*seconds\)",
+
+        # pattern 6: Test #number: test_name ... Passed time sec
+        r"Test\s+#\d+:\s+([\w\.\-]+)\s+.*Passed\s+([\d\.]+)\s+sec",
+    ]
+
+    for pattern in patterns:
+        for match in re.finditer(pattern, output):
+            if pattern == patterns[1]:
+                # pattern 2 swaps order: time, then test name
+                time = float(match.group(1))
+                test_name = match.group(2)
+            else:
+                test_name = match.group(1)
+                time = match.group(2)
+
+            if not results[test_name]:
+                results[test_name] = {'parsed': [], 'time': []}
+
+            if "ms" in pattern:
+                time = int(time) / 1000.0
+            else:
+                time = float(time)
+            
+            results[test_name]['parsed'].append(time)
+            results[test_name]['time'].append(time)
+
+    return dict(results)
+
+def parse_usr_bin_time(output: str) -> float:
+    match = re.search(r'(\d+)\s*ms', output, re.MULTILINE)
+    if match:
+        real_ms = int(match.group(1))
+        real_seconds = real_ms / 1000.0
+        return real_seconds
+    return 0.0
+
+
+def remove_exclude_from_all(repo_root: Path) -> None:
+    cmake_files = list(repo_root.rglob("CMakeLists.txt"))
+
+    pattern = re.compile(r'\bEXCLUDE_FROM_ALL\b')
+
+    for cmake_file in cmake_files:
+        original = cmake_file.read_text(encoding="utf-8")
+
+        if "EXCLUDE_FROM_ALL" not in original:
+            continue
+
+        modified = pattern.sub("", original)
+
+        if modified != original:
+            cmake_file.write_text(modified, encoding="utf-8")
diff --git a/src/utils/permission.py b/src/utils/permission.py
old mode 100644
new mode 100755
index b9d65b2..b62b7bb
--- a/src/utils/permission.py
+++ b/src/utils/permission.py
@@ -1,35 +1,35 @@
-import stat, logging
-from pathlib import Path
-
-def check_and_fix_path_permissions(target_path: Path) -> bool:
-    parts = target_path.parts
-    current_path = Path(parts[0])
-    
-    for part in parts[1:]:
-        current_path = current_path / part
-        
-        try:
-            st = current_path.stat()
-        except FileNotFoundError:
-            logging.error(f"Path does not exist: {current_path}")
-            return False
-        except PermissionError:
-            logging.error(f"Permission denied: {current_path}")
-            return False
-        
-        mode = stat.S_IMODE(st.st_mode)
-        
-        # Check if permissions are at least 711 (rwx--x--x)
-        owner_ok = (mode & stat.S_IRWXU) == stat.S_IRWXU
-        group_exec = (mode & stat.S_IXGRP) != 0
-        others_exec = (mode & stat.S_IXOTH) != 0
-        
-        if not (owner_ok and group_exec and others_exec):
-            logging.info(f"Fixing permissions on {current_path} from {oct(mode)} to 0o711")
-            try:
-                current_path.chmod(0o711)
-            except PermissionError:
-                logging.error(f"Failed to chmod {current_path}: permission denied")
-                return False
-            
+import stat, logging
+from pathlib import Path
+
+def check_and_fix_path_permissions(target_path: Path) -> bool:
+    parts = target_path.parts
+    current_path = Path(parts[0])
+    
+    for part in parts[1:]:
+        current_path = current_path / part
+        
+        try:
+            st = current_path.stat()
+        except FileNotFoundError:
+            logging.error(f"Path does not exist: {current_path}")
+            return False
+        except PermissionError:
+            logging.error(f"Permission denied: {current_path}")
+            return False
+        
+        mode = stat.S_IMODE(st.st_mode)
+        
+        # Check if permissions are at least 711 (rwx--x--x)
+        owner_ok = (mode & stat.S_IRWXU) == stat.S_IRWXU
+        group_exec = (mode & stat.S_IXGRP) != 0
+        others_exec = (mode & stat.S_IXOTH) != 0
+        
+        if not (owner_ok and group_exec and others_exec):
+            logging.info(f"Fixing permissions on {current_path} from {oct(mode)} to 0o711")
+            try:
+                current_path.chmod(0o711)
+            except PermissionError:
+                logging.error(f"Failed to chmod {current_path}: permission denied")
+                return False
+            
     return True
\ No newline at end of file
diff --git a/src/utils/stats.py b/src/utils/stats.py
old mode 100644
new mode 100755
index 678528c..30220fc
--- a/src/utils/stats.py
+++ b/src/utils/stats.py
@@ -1,58 +1,58 @@
-import logging
-from collections import Counter
-
-class RepoStats:
-    def __init__(self):
-        self.test_dirs = Counter()
-        self.test_flags = Counter()
-        self.pack_manager = Counter()
-        self.pack_files = Counter()
-        self.dependencies = Counter()
-        self.total_repos: int = 0
-        self.valid_repos: int = 0
-
-    def __iadd__(self, other: 'RepoStats') -> 'RepoStats':
-        self.test_dirs += other.test_dirs
-        self.test_flags += other.test_flags
-        self.pack_manager += other.pack_manager
-        self.pack_files += other.pack_files
-        self.dependencies += other.dependencies
-        self.total_repos += other.total_repos
-        self.valid_repos += other.valid_repos
-        return self
-
-    def write_final_log(self) -> None:
-        logging.info(f"Repositories analyzed: {self.total_repos}")
-        logging.info(f"Valid repositories: {self.valid_repos}")
-        logging.info(f"Final Counter: {self.test_dirs}")
-        logging.info(f"Final Flags: {self.test_flags}")
-        logging.info(f"Final Package Managers: {self.pack_manager}")
-        logging.info(f"Final Package Files: {self.pack_files}")
-        logging.info(f"Final Dependencies: {self.dependencies}")
-
-class CommitStats:
-    def __init__(self):
-        self.num_commits = 0
-        self.perf_commits = 0
-        self.lines_added = 0
-        self.lines_deleted = 0
-
-    def __iadd__(self, other: 'CommitStats') -> 'CommitStats':
-        self.num_commits += other.num_commits
-        self.perf_commits += other.perf_commits
-        self.lines_added += other.lines_added
-        self.lines_deleted += other.lines_deleted
-        return self
-
-    def write_final_log(self) -> None:
-        logging.info(f"Total commits analyzed: {self.num_commits}")
-        logging.info(f"Performance-related commits: {self.perf_commits}")
-        if self.num_commits > 0:
-            logging.info(f"Optimization ratio: {self.perf_commits / self.num_commits}")
-        logging.info(f"Lines added: {self.lines_added}")
-        logging.info(f"Lines deleted: {self.lines_deleted}")
-        logging.info(f"Total lines changed: {self.lines_added + self.lines_deleted}")
-        if self.perf_commits > 0:
-            logging.info(f"Average lines added per perf commit: {self.lines_added / self.perf_commits}")
-            logging.info(f"Average lines deleted per perf commit: {self.lines_deleted / self.perf_commits}")
-            logging.info(f"Average lines changed per perf commit: {(self.lines_added + self.lines_deleted) / self.perf_commits}")
+import logging
+from collections import Counter
+
+class RepoStats:
+    def __init__(self):
+        self.test_dirs = Counter()
+        self.test_flags = Counter()
+        self.pack_manager = Counter()
+        self.pack_files = Counter()
+        self.dependencies = Counter()
+        self.total_repos: int = 0
+        self.valid_repos: int = 0
+
+    def __iadd__(self, other: 'RepoStats') -> 'RepoStats':
+        self.test_dirs += other.test_dirs
+        self.test_flags += other.test_flags
+        self.pack_manager += other.pack_manager
+        self.pack_files += other.pack_files
+        self.dependencies += other.dependencies
+        self.total_repos += other.total_repos
+        self.valid_repos += other.valid_repos
+        return self
+
+    def write_final_log(self) -> None:
+        logging.info(f"Repositories analyzed: {self.total_repos}")
+        logging.info(f"Valid repositories: {self.valid_repos}")
+        logging.info(f"Final Counter: {self.test_dirs}")
+        logging.info(f"Final Flags: {self.test_flags}")
+        logging.info(f"Final Package Managers: {self.pack_manager}")
+        logging.info(f"Final Package Files: {self.pack_files}")
+        logging.info(f"Final Dependencies: {self.dependencies}")
+
+class CommitStats:
+    def __init__(self):
+        self.num_commits = 0
+        self.perf_commits = 0
+        self.lines_added = 0
+        self.lines_deleted = 0
+
+    def __iadd__(self, other: 'CommitStats') -> 'CommitStats':
+        self.num_commits += other.num_commits
+        self.perf_commits += other.perf_commits
+        self.lines_added += other.lines_added
+        self.lines_deleted += other.lines_deleted
+        return self
+
+    def write_final_log(self) -> None:
+        logging.info(f"Total commits analyzed: {self.num_commits}")
+        logging.info(f"Performance-related commits: {self.perf_commits}")
+        if self.num_commits > 0:
+            logging.info(f"Optimization ratio: {self.perf_commits / self.num_commits}")
+        logging.info(f"Lines added: {self.lines_added}")
+        logging.info(f"Lines deleted: {self.lines_deleted}")
+        logging.info(f"Total lines changed: {self.lines_added + self.lines_deleted}")
+        if self.perf_commits > 0:
+            logging.info(f"Average lines added per perf commit: {self.lines_added / self.perf_commits}")
+            logging.info(f"Average lines deleted per perf commit: {self.lines_deleted / self.perf_commits}")
+            logging.info(f"Average lines changed per perf commit: {(self.lines_added + self.lines_deleted) / self.perf_commits}")
diff --git a/src/utils/test_analyzer.py b/src/utils/test_analyzer.py
old mode 100644
new mode 100755
index 9aa1e36..475c23e
--- a/src/utils/test_analyzer.py
+++ b/src/utils/test_analyzer.py
@@ -1,321 +1,474 @@
-import logging, math
-from datetime import datetime
-from src.utils.parser import parse_single_ctest_output
-import numpy as np
-from scipy import stats
-from github.Commit import Commit
-from github.Repository import Repository
-from src.core.filter.commit_filter import CommitFilter
-from src.config.config import Config
-
-def safe_float(x):
-    if x is None or (isinstance(x, float) and math.isnan(x)):
-        return "NaN"
-    return float(x)
-
-class TestAnalyzer:
-    def __init__(self, config: Config, new_single_tests: dict[str, list[float]], old_single_tests: dict[str, list[float]]):
-        self.config = config
-        self.new_single_tests = new_single_tests
-        self.old_single_tests = old_single_tests
-
-        self.warmup = self.config.testing.warmup
-        self.commit_test_times = self.config.testing.commit_test_times
-        self.min_exec_time_improvement: float = self.config.commits_time['min-exec-time-improvement']
-        self.min_p_value: float = self.config.commits_time['min-p-value']
-
-    def relative_improvement(self, old_times: list[float], new_times: list[float]):
-        """relative improvement of new_times to old_times"""
-        if sum(old_times) > 0.0:
-            return (sum(old_times) - sum(new_times)) / sum(old_times)
-        return 0.0
-    
-    def get_overall_change(self) -> float:
-        total_old = 0.0
-        total_new = 0.0
-
-        for test_name in self.new_single_tests.keys():
-            new_times = self.new_single_tests[test_name][self.warmup:]
-            old_times = self.old_single_tests[test_name][self.warmup:]
-            total_new += sum(new_times)
-            total_old += sum(old_times)
-
-        if total_old == 0.0:
-            return 0.0
-        return (total_old - total_new) / total_old
-
-    
-    def get_improvement_p_value(
-        self,
-        old_times: list[float],
-        new_times: list[float]
-    ) -> float:
-        if len(old_times) != len(new_times):
-            raise ValueError("v1_times and v2_times must have the same length")
-        old = np.asarray(old_times, dtype=float)
-        new = np.asarray(new_times, dtype=float)
-
-        c = 1.0 - self.min_exec_time_improvement # we test 1 < c * 2
-        old_scaled = c * old
-
-        # Welch's t-test, one-sided: H1: mean(v1) < mean(v2_scaled)
-        res = stats.ttest_ind(new, old_scaled, equal_var=False, alternative='less')
-        logging.debug(f"T-test result: {res.pvalue} (pvalue)") # type: ignore
-        return float(res.pvalue) # type: ignore
-    
-    
-    def get_significant_test_time_changes(self) -> dict[str, list[str]]:
-        significant_test_time_changes = {'old_outperforms_new': [], 'new_outperforms_old': []}
-
-        for test_name in self.new_single_tests.keys():
-            new_times = self.new_single_tests[test_name][self.warmup:]
-            old_times = self.old_single_tests[test_name][self.warmup:]
-            if self.get_improvement_p_value(old_times, new_times) < self.min_p_value:
-                significant_test_time_changes['new_outperforms_old'].append(test_name)
-                logging.debug(f"new_outperforms_old improvement: {self.relative_improvement(old_times, old_times)*100}%")
-            if self.get_improvement_p_value(new_times, old_times) < self.min_p_value:
-                significant_test_time_changes['old_outperforms_new'].append(test_name)
-                logging.debug(f"old_outperforms_new improvement: {self.relative_improvement(old_times, new_times)*100}%")
-            
-        logging.debug(significant_test_time_changes)
-        return significant_test_time_changes
-        
-    def create_test_log(self, commit: Commit, repo: Repository, old_sha: str, new_sha: str, 
-                        old_full_times: list[float], new_full_times: list[float],
-                        old_commands: list[str], new_commands: list[str]) -> dict:
-        message = commit.commit.message
-        patches = self.get_diff(commit)
-
-        commit_filter = CommitFilter(commit, self.config, repo)
-        extracted_refs = commit_filter.extract_fixed_issues()
-        gh_refs = [
-            (ref_type, number, issue.title, issue.body, issue) 
-            for number, ref_type in extracted_refs.items()
-            if (issue := repo.get_issue(number))
-        ]
-
-        metadata = {
-            "collection_date": datetime.now().isoformat(),
-            "repository": f"https://github.com/{repo.full_name}",
-            "repository_name": repo.full_name
-        }
-        commit_info = {
-            "old_sha": old_sha,
-            "new_sha": new_sha,
-            "commit_message": message,
-            "commit_date": commit.commit.author.date.isoformat(),
-            "patch": patches,
-            "files_changed": [
-                {
-                    "filename": f.filename,
-                    "status": f.status,
-                    "additions": f.additions,
-                    "deletions": f.deletions,
-                    "changes": f.changes,
-                    "patch": f.patch
-                }
-                for f in commit.files or []
-            ],
-            "lines_added": commit.stats.additions,
-            "lines_removed": commit.stats.deletions, 
-        }
-        build_info = {
-            "old_build_script": "#!/bin/bash\n" + "\n".join(old_commands[0:2]),
-            "new_build_script": "#!/bin/bash\n" + "\n".join(old_commands[0:2]),
-            "old_test_script": "#!/bin/bash\n" + "\n".join(old_commands[2:]),
-            "new_test_script": "#!/bin/bash\n" + "\n".join(old_commands[2:]),
-            "build_system": "cmake"
-        }
-
-        pvalue = self.get_improvement_p_value(old_full_times[self.warmup:], new_full_times[self.warmup:], ) 
-        old = np.asarray(old_full_times[self.warmup:], float)
-        new = np.asarray(new_full_times[self.warmup:], float)
-        performance_analysis = {
-            "is_significant": pvalue < self.min_p_value,
-            "p_value": safe_float(pvalue),
-            "relative_improvement": safe_float(self.relative_improvement(old_full_times[self.warmup:], new_full_times[self.warmup:])),
-            "absolute_improvement_ms": safe_float((np.mean(old) - np.mean(new)) * 1000),
-            "old_mean_ms": safe_float(np.mean(old) * 1000),
-            "new_mean_ms": safe_float(np.mean(new) * 1000),
-            "old_std_ms": safe_float(np.std(old, ddof=1) * 1000),
-            "new_std_ms": safe_float(np.std(new, ddof=1) * 1000),
-            "effect_size_cohens_d": safe_float(self.cohens_d(old, new)),
-            "old_ci95_ms": self.ci95(old_full_times[self.warmup:]),
-            "new_ci95_ms": self.ci95(new_full_times[self.warmup:]),
-            "old_ci99_ms": self.ci99(old_full_times[self.warmup:]),
-            "new_ci99_ms": self.ci99(new_full_times[self.warmup:]),
-            "old_times_s": old_full_times,
-            "new_times_s": new_full_times
-        }
-
-        significant_test_time_changes = self.get_significant_test_time_changes()
-        tests = {
-            "total_tests": len(self.new_single_tests.keys()),
-            "significant_improvements": len(significant_test_time_changes['new_outperforms_old']),
-            "significant_improvements_tests": significant_test_time_changes['new_outperforms_old'],
-            "significant_regressions": len(significant_test_time_changes['old_outperforms_new']),
-            "significant_regressions_tests": significant_test_time_changes['old_outperforms_new'],
-            "tests": []
-        }
-        for test_name in self.new_single_tests.keys():
-            new_times = self.new_single_tests[test_name][self.warmup:]
-            old_times = self.old_single_tests[test_name][self.warmup:]
-            pvalue = self.get_improvement_p_value(old_times, new_times) 
-            tests["tests"].append({
-                "test_name": test_name,
-                "is_significant": pvalue < self.min_p_value,
-                "p_value": safe_float(pvalue),
-                "relative_improvement": safe_float(self.relative_improvement(old_times, new_times)),
-                "absolute_improvement_ms": safe_float((np.mean(old_times) - np.mean(new_times)) * 1000),
-                "old_mean_ms": safe_float(np.mean(old_times) * 1000),
-                "new_mean_ms": safe_float(np.mean(new_times) * 1000),
-                "old_std_ms": safe_float(np.std(old_times, ddof=1) * 1000),
-                "new_std_ms": safe_float(np.std(new_times, ddof=1) * 1000),
-                "effect_size_cohens_d": safe_float(self.cohens_d(old_times, new_times)),
-                "old_ci95_ms": self.ci95(old_times),
-                "new_ci95_ms": self.ci95(new_times),
-                "old_ci99_ms": self.ci99(old_times),
-                "new_ci99_ms": self.ci99(new_times),
-                "new_times": new_times,
-                "old_times": old_times
-            })
-
-        issues = [
-            {
-                "number": number,
-                "url": f"https://github.com/{repo.full_name}/issues/{number}",
-                "title": title,
-                "body": body,
-                "created_at": issue.created_at.isoformat()
-            }
-            for ref in gh_refs
-            if (ref_type := ref[0]) == "issue" and (number := ref[1]) and (title := ref[2]) and (body := ref[3]) and (issue := ref[4])
-        ]
-
-        pull_requests = [
-            {
-                "number": number,
-                "url": f"https://github.com/{repo.full_name}/pull/{number}",
-                "title": title,
-                "body": body,
-                "merged_at": issue.pull_request.merged_at.isoformat() if issue.pull_request and issue.pull_request.merged_at else None
-            }
-            for ref in gh_refs
-            if (ref_type := ref[0]) == "pull_request" and (number := ref[1]) and (title := ref[2]) and (body := ref[3]) and (issue := ref[4])
-        ]
-
-        results = {
-            "metadata": metadata,
-            "commit_info": commit_info,
-            "issues": issues, 
-            "pull_requests": pull_requests,
-            "build_info": build_info,
-            "performance_analysis": performance_analysis,
-            "tests": tests,
-            "logs": {
-                "full_log_path": "/logs/full.log",
-                "config_log_path": "/logs/config.log",
-                "build_log_path": "/logs/build.log",
-                "test_log_path": "/logs/test.log",
-                "build_success": True,
-                "test_success": True
-            },
-            "raw_timing_data": {
-                "warmup_runs": self.config.testing.warmup,
-                "measurement_runs": self.config.testing.commit_test_times,
-                "min_exec_time_improvement": self.min_exec_time_improvement,
-                "min_p_value": self.min_p_value
-            }
-
-        }
-        return results
-
-    def get_diff(self, commit: Commit) -> str:
-        diff_lines: list[str] = []
-        for f in commit.files:
-            if f.patch:
-                diff_lines.append(f"--- {f.filename}")
-                patch = f.patch
-                diff_lines.append(patch)
-        return "\n".join(diff_lines)
-    
-    def cohens_d(self, old, new) -> float:
-        """
-        Compute Cohen's d effect size between old and new execution times.
-
-        What it measures:
-            - The magnitude of the performance improvement/regression.
-            - It is scale-free and independent of units (s, ms, etc).
-            - It shows *how big* the difference is, not just whether it is statistically significant.
-
-        Interpretation:
-            d = 0.2  -> small effect
-            d = 0.5  -> medium effect
-            d = 0.8  -> large effect
-            d > 1.0  -> very large effect
-
-        Sign convention:
-            Positive d  -> new is faster (mean(new) < mean(old))
-            Negative d  -> new is slower (regression)
-
-        Formula:
-            d = (mean(old) - mean(new)) / pooled_std
-            where pooled_std is the pooled standard deviation of both samples.
-
-        Why we use it:
-            - p-values alone do not tell how *big* the improvement is.
-            - Cohen's d tells how meaningful the difference is in practical terms.
-        """
-        old = np.asarray(old, float)
-        new = np.asarray(new, float)
-        n1, n2 = len(old), len(new)
-        s1, s2 = np.var(old, ddof=1), np.var(new, ddof=1)
-        pooled = np.sqrt(((n1 - 1)*s1 + (n2 - 1)*s2) / (n1 + n2 - 2))
-        return (np.mean(old) - np.mean(new)) / pooled
-
-    def ci95(self, xs: list[float]) -> tuple[float, float]:
-        """
-        Compute a 95% confidence interval for the mean execution time.
-
-        What it measures:
-            - A range where the true mean execution time is expected to lie
-            with 95% probability.
-
-        Interpretation:
-            If CI95 for old is (100ms, 110ms)
-            and CI95 for new is (80ms, 90ms),
-            the intervals do not overlap -> strong indication of improvement.
-
-        Why we use confidence intervals:
-            - They show the stability and variability of the benchmark.
-            - They are a more intuitive alternative to p-values.
-            - Narrow CI -> stable performance
-            - Wide CI -> noisy benchmark, possible measurement issues.
-
-        Returns:
-            (ci_low_ms, ci_high_ms)
-        """
-        arr = np.asarray(xs, dtype=float)
-        n = len(arr)
-        if n < 2:
-            return (0.0, 0.0)
-        se = np.std(arr, ddof=1) / math.sqrt(n)
-        h = se * stats.t.ppf(0.975, n-1)
-        mean = np.mean(arr)
-        return (float(mean - h)*1000, float(mean + h)*1000)
-    
-    def ci99(self, xs: list[float]) -> tuple[float, float]:
-        """
-        Return the 99% confidence interval (lower_ms, upper_ms) for the mean.
-        """
-        arr = np.asarray(xs, dtype=float)
-        n = len(arr)
-        if n < 2:
-            return (0.0, 0.0)
-
-        se = np.std(arr, ddof=1) / math.sqrt(n)
-        h = se * stats.t.ppf(0.995, n - 1)  # 99%: use 0.995
-        mean = np.mean(arr)
-
-        # return in milliseconds
+import logging, math
+from datetime import datetime
+import numpy as np
+from scipy import stats
+from github.Commit import Commit
+from github.Repository import Repository
+from github.Issue import Issue
+from src.core.filter.commit_filter import CommitFilter
+from src.config.config import Config
+
+def safe_float(x):
+    if x is None or (isinstance(x, float) and math.isnan(x)):
+        return "NaN"
+    return float(x)
+
+class TestAnalyzer:
+    def __init__(self, config: Config, new_single_tests: dict[str, list[float]], old_single_tests: dict[str, list[float]]):
+        self.config = config
+        self.new_single_tests = new_single_tests
+        self.old_single_tests = old_single_tests
+
+        self.warmup = self.config.testing.warmup
+        self.commit_test_times = self.config.testing.commit_test_times
+        self.min_exec_time_improvement = self.config.min_exec_time_improvement
+        self.min_p_value = self.config.min_p_value
+
+    def relative_improvement(self, old_times: list[float], new_times: list[float]):
+        """relative improvement of new_times to old_times"""
+        if sum(old_times) > 0.0:
+            return (sum(old_times) - sum(new_times)) / sum(old_times)
+        return 0.0
+    
+    def get_overall_change(self) -> float:
+        total_old = 0.0
+        total_new = 0.0
+
+        for test_name in self.new_single_tests.keys():
+            new_times = self.new_single_tests[test_name][self.warmup:]
+            old_times = self.old_single_tests[test_name][self.warmup:]
+            total_new += sum(new_times)
+            total_old += sum(old_times)
+
+        if total_old == 0.0:
+            return 0.0
+        return (total_old - total_new) / total_old
+
+    
+    def get_improvement_p_value(
+        self,
+        old_times: list[float],
+        new_times: list[float]
+    ) -> float:
+        if len(old_times) != len(new_times):
+            raise ValueError("v1_times and v2_times must have the same length")
+        old = np.asarray(old_times, dtype=float)
+        new = np.asarray(new_times, dtype=float)
+
+        c = 1.0 - self.min_exec_time_improvement # we test 1 < c * 2
+        old_scaled = c * old
+
+        # Welch's t-test, one-sided: H1: mean(v1) < mean(v2_scaled)
+        res = stats.ttest_ind(new, old_scaled, equal_var=False, alternative='less')
+        logging.debug(f"T-test result: {res.pvalue} (pvalue)") # type: ignore
+        return float(res.pvalue) # type: ignore
+    
+    def get_pair_improvement_p_value(
+        self,
+        old_times: list[float],
+        new_times: list[float]
+    ) -> float:
+        old = np.asarray(old_times, dtype=float)
+        new = np.asarray(new_times, dtype=float)
+        if old.shape != new.shape:
+            raise ValueError("old_times and new_times must have same length")
+
+        # Differences: positive = old slower than new (i.e., improvement)
+        diff = old - new
+
+        # Convert relative threshold to absolute delta (use mean(old) as baseline)
+        delta = self.min_exec_time_improvement * old.mean()
+
+        # Adjust diffs by delta so test is H0: mean(diff) <= delta  => H1: mean(diff) > delta
+        adjusted = diff - delta
+
+        # One-sample t-test (one-sided greater)
+        res = stats.ttest_1samp(adjusted, popmean=0.0, alternative='greater')
+        return res.pvalue # type: ignore
+    
+    def get_wilcoxon_pvalue(
+        self,
+        old_times: list[float],
+        new_times: list[float]
+    ) -> float:
+        old = np.asarray(old_times, dtype=float)
+        new = np.asarray(new_times, dtype=float)
+        diff = old - new
+        delta = self.min_exec_time_improvement * old.mean()
+        adjusted = diff - delta
+        # SciPy supports alternative='greater'
+        res = stats.wilcoxon(adjusted, alternative='greater', zero_method='wilcox')
+        return float(res.pvalue) # type: ignore
+    
+    def get_mannwhitney_pvalue(
+        self,
+        old_times: list[float],
+        new_times: list[float]
+    ) -> float:
+        old = np.asarray(old_times, dtype=float)
+        new = np.asarray(new_times, dtype=float)
+        if len(old) != len(new):
+            raise ValueError("old_times and new_times must have same length")
+
+        # One-sided: H1: new < old (new is faster)
+        res = stats.mannwhitneyu(new, old, alternative='less')
+        return float(res.pvalue)
+    
+    def is_mannwhitney_significant(
+        self,
+        old_times: list[float],
+        new_times: list[float],
+    ) -> bool:
+        p_mwu = self.get_mannwhitney_pvalue(old_times, new_times)
+        delta  = self.relative_improvement(old_times, new_times)
+        return bool(p_mwu < self.min_p_value) and bool(delta > self.config.min_exec_time_improvement)
+    
+    def get_binom_improvement_p_value(
+        self,
+        old_times: list[float],
+        new_times: list[float]
+    ) -> float:
+        """
+        Test if the median of v2 is significantly less than min-exec-time-improvement * the median of v1.
+
+        Args:
+            v1: List of values
+            v2: List of values
+
+        Returns:
+            p-value
+        """
+        old = np.asarray(old_times, dtype=float)
+        new = np.asarray(new_times, dtype=float)
+        if old.shape != new.shape:
+            raise ValueError("old_times and new_times must have same length")
+
+        c = 1.0 - self.min_exec_time_improvement  # we test 2 < c * 1
+        old_scaled = c * old
+        diff = new - old_scaled
+
+        wins = np.sum(diff < 0)   # V2 achieves at least 'margin' speedup
+        losses = np.sum(diff > 0) # V2 fails to achieve the margin
+        n = wins + losses
+
+        if n == 0:
+            logging.error("All pairs are ties or NaN after applying the margin; cannot perform sign test.")
+            return np.nan
+        
+        # Exact one-sided binomial test: H1 is 'wins' > 0.5
+        res = stats.binomtest(k=wins, n=n, p=0.5, alternative="greater")
+
+        return float(res.pvalue)
+
+    def get_significant_test_time_changes(
+            self, f
+        ) -> dict[str, list[str]]:
+        significant_test_time_changes = {'old_outperforms_new': [], 'new_outperforms_old': []}
+
+        for test_name in self.new_single_tests.keys():
+            new_times = self.new_single_tests[test_name][self.warmup:]
+            old_times = self.old_single_tests[test_name][self.warmup:]
+            if any(t <= 0.005 for t in new_times) or any(t <= 0.005 for t in old_times):
+                continue
+            if f(old_times, new_times) < self.min_p_value:
+                significant_test_time_changes['new_outperforms_old'].append(test_name)
+                logging.debug(f"new_outperforms_old improvement: {self.relative_improvement(old_times, old_times)*100}%")
+            if f(new_times, old_times) < self.min_p_value:
+                significant_test_time_changes['old_outperforms_new'].append(test_name)
+                logging.debug(f"old_outperforms_new improvement: {self.relative_improvement(old_times, new_times)*100}%")
+            
+        logging.debug(significant_test_time_changes)
+        return significant_test_time_changes
+        
+    def create_test_log(self, commit: Commit, repo: Repository, old_sha: str, new_sha: str,
+                        old_full_times: list[float], new_full_times: list[float],
+                        new_build_cmd: list[str], old_build_cmd: list[str], 
+                        new_test_cmd: list[str], old_test_cmd: list[str],) -> dict:
+        
+        gh_refs: list[tuple[str, int, str, str, Issue]] = []
+        messages: list[str] = []
+        patches: list[str] = []
+
+        messages = [commit.commit.message]
+        patches = [self.get_diff(commit)]
+        
+        commit_filter = CommitFilter(commit, self.config, repo)
+        extracted_refs = commit_filter.extract_fixed_issues()
+        gh_refs = [
+            (ref_type, number, issue.title, issue.body, issue) 
+            for number, ref_type in extracted_refs.items()
+            if (issue := repo.get_issue(number))
+        ]
+
+        metadata = {
+            "collection_date": datetime.now().isoformat(),
+            "repository": f"https://github.com/{repo.full_name}",
+            "repository_name": repo.full_name
+        }
+
+        commit_info = {
+            "old_sha": old_sha,
+            "new_sha": new_sha,
+            "commit_message": messages,
+            "commit_date": commit.commit.author.date.isoformat(),
+            "patch": patches,
+            "files_changed": [
+                {
+                    "filename": f.filename,
+                    "status": f.status,
+                    "additions": f.additions,
+                    "deletions": f.deletions,
+                    "changes": f.changes,
+                    "patch": f.patch
+                }
+                for f in commit.files or []
+            ],
+            "lines_added": commit.stats.additions,
+            "lines_removed": commit.stats.deletions, 
+        }
+        build_info = {
+            "old_build_script": old_build_cmd,
+            "new_build_script": new_build_cmd,
+            "old_test_script": old_test_cmd,
+            "new_test_script": new_test_cmd,
+            "build_system": "cmake"
+        }
+
+        pvalue = self.get_improvement_p_value(old_full_times[self.warmup:], new_full_times[self.warmup:]) 
+        pair_pvalue = self.get_pair_improvement_p_value(old_full_times[self.warmup:], new_full_times[self.warmup:])
+        binom_pvalue = self.get_binom_improvement_p_value(old_full_times[self.warmup:], new_full_times[self.warmup:])
+        wilcoxon_pvalue = self.get_wilcoxon_pvalue(old_full_times[self.warmup:], new_full_times[self.warmup:])
+        mannwhitney_pvalue = self.get_mannwhitney_pvalue(old_full_times[self.warmup:], new_full_times[self.warmup:])
+        old = np.asarray(old_full_times[self.warmup:], float)
+        new = np.asarray(new_full_times[self.warmup:], float)
+        performance_analysis = {
+            "is_significant": bool(pvalue < self.min_p_value),
+            "p_value": safe_float(pvalue),
+            "is_pair_significant": bool(pair_pvalue < self.min_p_value),
+            "pair_p_value": safe_float(pair_pvalue),
+            "is_binom_significant": bool(binom_pvalue < self.min_p_value),
+            "binom_p_value": safe_float(binom_pvalue),
+            "is_wilcoxon_significant": bool(wilcoxon_pvalue < self.min_p_value),
+            "wilcoxon_p_value": safe_float(wilcoxon_pvalue), 
+            "is_mannwhitney_significant": bool(self.is_mannwhitney_significant(old_full_times[self.warmup:], new_full_times[self.warmup:])),
+            "mannwhitney_p_value": safe_float(mannwhitney_pvalue),
+            "relative_improvement": safe_float(self.relative_improvement(old_full_times[self.warmup:], new_full_times[self.warmup:])),
+            "absolute_improvement_ms": safe_float((np.mean(old) - np.mean(new)) * 1000),
+            "old_mean_ms": safe_float(np.mean(old) * 1000),
+            "new_mean_ms": safe_float(np.mean(new) * 1000),
+            "old_std_ms": safe_float(np.std(old, ddof=1) * 1000),
+            "new_std_ms": safe_float(np.std(new, ddof=1) * 1000),
+            "effect_size_cohens_d": safe_float(self.cohens_d(old, new)),
+            "old_ci95_ms": self.ci95(old_full_times[self.warmup:]),
+            "new_ci95_ms": self.ci95(new_full_times[self.warmup:]),
+            "old_ci99_ms": self.ci99(old_full_times[self.warmup:]),
+            "new_ci99_ms": self.ci99(new_full_times[self.warmup:]),
+            "new_times_s": new_full_times,
+            "old_times_s": old_full_times
+        }
+
+        significant_test_time_changes = self.get_significant_test_time_changes(self.get_improvement_p_value)
+        significant_pair_test_time_changes = self.get_significant_test_time_changes(self.get_pair_improvement_p_value)
+        significant_binom_test_time_changes = self.get_significant_test_time_changes(self.get_binom_improvement_p_value)
+        significant_wilcoxon_test_time_changes = self.get_significant_test_time_changes(self.get_wilcoxon_pvalue)
+        significant_mannwhitney_test_time_changes = self.get_significant_test_time_changes(self.get_mannwhitney_pvalue)
+        tests = {
+            "total_tests": len(self.new_single_tests.keys()),
+            "significant_improvements": len(significant_test_time_changes['new_outperforms_old']),
+            "significant_improvements_tests": significant_test_time_changes['new_outperforms_old'],
+            "significant_regressions": len(significant_test_time_changes['old_outperforms_new']),
+            "significant_regressions_tests": significant_test_time_changes['old_outperforms_new'],
+            "significant_pair_improvements": len(significant_pair_test_time_changes['new_outperforms_old']),
+            "significant_pair_improvements_tests": significant_pair_test_time_changes['new_outperforms_old'],
+            "significant_pair_regressions": len(significant_pair_test_time_changes['old_outperforms_new']),
+            "significant_pair_regressions_tests": significant_pair_test_time_changes['old_outperforms_new'],
+            "significant_binom_improvements": len(significant_binom_test_time_changes['new_outperforms_old']),
+            "significant_binom_improvements_tests": significant_binom_test_time_changes['new_outperforms_old'],
+            "significant_binom_regressions": len(significant_binom_test_time_changes['old_outperforms_new']),
+            "significant_binom_regressions_tests": significant_binom_test_time_changes['old_outperforms_new'],
+            "significant_wilcoxon_improvements": len(significant_wilcoxon_test_time_changes['new_outperforms_old']),
+            "significant_wilcoxon_improvements_tests": significant_wilcoxon_test_time_changes['new_outperforms_old'],
+            "significant_wilcoxon_regressions": len(significant_wilcoxon_test_time_changes['old_outperforms_new']),
+            "significant_wilcoxon_regressions_tests": significant_wilcoxon_test_time_changes['old_outperforms_new'],
+            "significant_mannwhitney_improvements": len(significant_mannwhitney_test_time_changes['new_outperforms_old']),
+            "significant_mannwhitney_improvements_tests": significant_mannwhitney_test_time_changes['new_outperforms_old'],
+            "significant_mannwhitney_regressions": len(significant_mannwhitney_test_time_changes['old_outperforms_new']),
+            "significant_mannwhitney_regressions_tests": significant_mannwhitney_test_time_changes['old_outperforms_new'],
+            "tests": []
+        }
+        for test_name in self.new_single_tests.keys():
+            new_times = self.new_single_tests[test_name][self.warmup:]
+            old_times = self.old_single_tests[test_name][self.warmup:]
+            if any(t <= 0.005 for t in new_times) or any(t <= 0.005 for t in old_times): #0.0 in new_times or 0.0 in old_times:
+                continue
+            pvalue = self.get_improvement_p_value(old_times, new_times) 
+            pair_pvalue = self.get_pair_improvement_p_value(old_times, new_times)
+            binom_pvalue = self.get_binom_improvement_p_value(old_times, new_times)
+            wilcoxon_pvalue = self.get_wilcoxon_pvalue(old_times, new_times)
+            mannwhitney_pvalue = self.get_mannwhitney_pvalue(old_times, new_times)
+            tests["tests"].append({
+                "test_name": test_name,
+                "is_significant": bool(pvalue < self.min_p_value),
+                "p_value": safe_float(pvalue),
+                "is_pair_significant": bool(pair_pvalue < self.min_p_value),
+                "pair_p_value": safe_float(pair_pvalue),
+                "is_binom_significant": bool(binom_pvalue < self.min_p_value),
+                "binom_p_value": safe_float(binom_pvalue),
+                "is_wilcoxon_significant": bool(wilcoxon_pvalue < self.min_p_value),
+                "wilcoxon_p_value": safe_float(wilcoxon_pvalue), 
+                "is_mannwhitney_significant": bool(self.is_mannwhitney_significant(old_times, new_times)),
+                "mannwhitney_p_value": safe_float(mannwhitney_pvalue),
+                "relative_improvement": safe_float(self.relative_improvement(old_times, new_times)),
+                "absolute_improvement_ms": safe_float((np.mean(old_times) - np.mean(new_times)) * 1000),
+                "old_mean_ms": safe_float(np.mean(old_times) * 1000),
+                "new_mean_ms": safe_float(np.mean(new_times) * 1000),
+                "old_std_ms": safe_float(np.std(old_times, ddof=1) * 1000),
+                "new_std_ms": safe_float(np.std(new_times, ddof=1) * 1000),
+                "effect_size_cohens_d": safe_float(self.cohens_d(old_times, new_times)),
+                "old_ci95_ms": self.ci95(old_times),
+                "new_ci95_ms": self.ci95(new_times),
+                "old_ci99_ms": self.ci99(old_times),
+                "new_ci99_ms": self.ci99(new_times),
+                "new_times": new_times,
+                "old_times": old_times
+            })
+
+        issues = [
+            {
+                "number": number,
+                "url": f"https://github.com/{repo.full_name}/issues/{number}",
+                "title": title,
+                "body": body,
+                "created_at": issue.created_at.isoformat()
+            }
+            for ref in gh_refs
+            if (ref_type := ref[0]) == "issue" and (number := ref[1]) and (title := ref[2]) and (body := ref[3]) and (issue := ref[4])
+        ]
+
+        pull_requests = [
+            {
+                "number": number,
+                "url": f"https://github.com/{repo.full_name}/pull/{number}",
+                "title": title,
+                "body": body,
+                "merged_at": issue.pull_request.merged_at.isoformat() if issue.pull_request and issue.pull_request.merged_at else None
+            }
+            for ref in gh_refs
+            if (ref_type := ref[0]) == "pull_request" and (number := ref[1]) and (title := ref[2]) and (body := ref[3]) and (issue := ref[4])
+        ]
+
+        results = {
+            "metadata": metadata,
+            "commit_info": commit_info,
+            "issues": issues, 
+            "pull_requests": pull_requests,
+            "build_info": build_info,
+            "performance_analysis": performance_analysis,
+            "tests": tests,
+            "logs": {
+                "full_log_path": "/logs/full.log",
+                "config_log_path": "/logs/config.log",
+                "build_log_path": "/logs/build.log",
+                "test_log_path": "/logs/test.log",
+                "build_success": True,
+                "test_success": True
+            },
+            "raw_timing_data": {
+                "warmup_runs": self.config.testing.warmup,
+                "measurement_runs": self.config.testing.commit_test_times,
+                "min_exec_time_improvement": self.min_exec_time_improvement,
+                "min_p_value": self.min_p_value
+            }
+
+        }
+        return results
+
+    def get_diff(self, commit: Commit) -> str:
+        diff_lines: list[str] = []
+        for f in commit.files:
+            if f.patch:
+                diff_lines.append(f"--- {f.filename}")
+                patch = f.patch
+                diff_lines.append(patch)
+        return "\n".join(diff_lines)
+    
+    def cohens_d(self, old, new) -> float:
+        """
+        Compute Cohen's d effect size between old and new execution times.
+
+        What it measures:
+            - The magnitude of the performance improvement/regression.
+            - It is scale-free and independent of units (s, ms, etc).
+            - It shows *how big* the difference is, not just whether it is statistically significant.
+
+        Interpretation:
+            d = 0.2  -> small effect
+            d = 0.5  -> medium effect
+            d = 0.8  -> large effect
+            d > 1.0  -> very large effect
+
+        Sign convention:
+            Positive d  -> new is faster (mean(new) < mean(old))
+            Negative d  -> new is slower (regression)
+
+        Formula:
+            d = (mean(old) - mean(new)) / pooled_std
+            where pooled_std is the pooled standard deviation of both samples.
+
+        Why we use it:
+            - p-values alone do not tell how *big* the improvement is.
+            - Cohen's d tells how meaningful the difference is in practical terms.
+        """
+        old = np.asarray(old, float)
+        new = np.asarray(new, float)
+        n1, n2 = len(old), len(new)
+        s1, s2 = np.var(old, ddof=1), np.var(new, ddof=1)
+        pooled = np.sqrt(((n1 - 1)*s1 + (n2 - 1)*s2) / (n1 + n2 - 2))
+        return (np.mean(old) - np.mean(new)) / pooled
+
+    def ci95(self, xs: list[float]) -> tuple[float, float]:
+        """
+        Compute a 95% confidence interval for the mean execution time.
+
+        What it measures:
+            - A range where the true mean execution time is expected to lie
+            with 95% probability.
+
+        Interpretation:
+            If CI95 for old is (100ms, 110ms)
+            and CI95 for new is (80ms, 90ms),
+            the intervals do not overlap -> strong indication of improvement.
+
+        Why we use confidence intervals:
+            - They show the stability and variability of the benchmark.
+            - They are a more intuitive alternative to p-values.
+            - Narrow CI -> stable performance
+            - Wide CI -> noisy benchmark, possible measurement issues.
+
+        Returns:
+            (ci_low_ms, ci_high_ms)
+        """
+        arr = np.asarray(xs, dtype=float)
+        n = len(arr)
+        if n < 2:
+            return (0.0, 0.0)
+        se = np.std(arr, ddof=1) / math.sqrt(n)
+        h = se * stats.t.ppf(0.975, n-1)
+        mean = np.mean(arr)
+        return (float(mean - h)*1000, float(mean + h)*1000)
+    
+    def ci99(self, xs: list[float]) -> tuple[float, float]:
+        """
+        Return the 99% confidence interval (lower_ms, upper_ms) for the mean.
+        """
+        arr = np.asarray(xs, dtype=float)
+        n = len(arr)
+        if n < 2:
+            return (0.0, 0.0)
+
+        se = np.std(arr, ddof=1) / math.sqrt(n)
+        h = se * stats.t.ppf(0.995, n - 1)  # 99%: use 0.995
+        mean = np.mean(arr)
+
+        # return in milliseconds
         return (float((mean - h) * 1000), float((mean + h) * 1000))
\ No newline at end of file
diff --git a/src/utils/writer.py b/src/utils/writer.py
old mode 100644
new mode 100755
index c24dabd..4da62bf
--- a/src/utils/writer.py
+++ b/src/utils/writer.py
@@ -1,81 +1,101 @@
-import logging, json
-from github.Commit import Commit
-from src.utils.stats import CommitStats
-from typing import Optional
-from pathlib import Path
-
-class Writer:
-    def __init__(self, repo_id: str, output_path: str):
-        self.repo_id = repo_id
-        try:
-            self.owner, self.name = self.repo_id.split("/", 1)
-        except ValueError:
-            raise ValueError(f"Invalid repo name format: '{repo_id}'. Expected '<owner>/<repo>'.")
-        
-        self.output_path = output_path
-        self.file: Optional[str] = None
-
-    def write_repo(self, m: list[str] = []) -> None:
-        msg = " | ".join([f"{self.owner}/{self.name}"] + m)
-        msg += f"\n"
-        path = Path(self.output_path)
-        self._write(path, msg)
-
-    def write_commit(self, commit: Commit, separate: bool, filter: str) -> CommitStats:
-        stats = CommitStats()
-
-        stats.perf_commits += 1
-        total_add = sum(f.additions for f in commit.files)
-        total_del = sum(f.deletions for f in commit.files)
-
-        stats.lines_added += total_add
-        stats.lines_deleted += total_del
-
-        current_sha = commit.sha
-        parent_sha = commit.parents[0].sha if commit.parents else "None"
-
-        self.file = "filtered.txt"
-        msg = f"{self.repo_id} | {current_sha} | {parent_sha} | {filter}\n" 
-        path = Path(self.output_path) / self.file
-        self._write(path, msg)
-        
-        # saves each commit version to file with patch information
-        if separate:
-            file = f"{self.owner}_{self.name}_{current_sha}.txt"
-            msg = f"{current_sha} | {parent_sha}"
-            final_msg: list[str] = [msg, commit.commit.message]
-            for f in commit.files:
-                final_msg.append(f.patch)
-            path = Path(self.output_path) / file
-            self._write(path, "\n".join(final_msg))
-
-        return stats
-
-    def write_improve(self, results: dict) -> None:
-        self.file = f"improved.txt"
-        path = Path(self.output_path) / self.file
-        new_sha: str = results['commit_info']["new_sha"]
-        old_sha: str = results['commit_info']["old_sha"]
-        repo_id: str = results['metadata']['repository_name']
-        p_value: float = results['performance_analysis']['p_value']
-        rel_improv: float = results['performance_analysis']['relative_improvement']
-        msg = f"{repo_id} | {new_sha} | {old_sha} | {p_value} | {rel_improv}\n"
-        self._write(path, msg)
-
-    def write_results(self, results: dict) -> None:
-        new_sha: str = results['commit_info']["new_sha"]
-        file = f"{self.owner}_{self.name}_{new_sha}.json"
-        path = Path(self.output_path) / file
-        path.parent.mkdir(parents=True, exist_ok=True)
-        with open(path, "w", encoding="utf-8") as f:
-            json.dump(results, f, indent=2)
-        logging.info(f"[{self.owner}/{self.name}] Wrote results to {path}")
-
-    def _write(self, path: Path, msg: str) -> None:
-        try:
-            path.parent.mkdir(parents=True, exist_ok=True)
-            with path.open("a", encoding="utf-8", errors="ignore") as f:
-                f.write(msg)
-            logging.info(f"[{self.owner}/{self.name}] Wrote data to {path}")
-        except (OSError, IOError) as e:
-            logging.error(f"[{self.owner}/{self.name}] Failed to write to {path}: {e}", exc_info=True)
\ No newline at end of file
+import logging, json, fcntl, os
+from github.Repository import Repository
+from github.Commit import Commit
+from src.utils.stats import CommitStats
+from typing import Optional
+from pathlib import Path
+from src.utils.pull_request_handler import get_pr_chain_msg
+
+class Writer:
+    def __init__(self, repo_id: str, output_path: str):
+        self.repo_id = repo_id
+        try:
+            self.owner, self.name = self.repo_id.split("/", 1)
+        except ValueError:
+            raise ValueError(f"Invalid repo name format: '{repo_id}'. Expected '<owner>/<repo>'.")
+        
+        self.output_path = output_path
+        self.file: Optional[str] = None
+
+    def write_repo(self, m: list[str] = []) -> None:
+        msg = " | ".join([f"{self.owner}/{self.name}"] + m)
+        msg += f"\n"
+        path = Path(self.output_path)
+        self._write(path, msg)
+
+    def write_commit(self, commit: Commit, filter: str) -> CommitStats:
+        stats = CommitStats()
+
+        stats.perf_commits += 1
+        total_add = sum(f.additions for f in commit.files)
+        total_del = sum(f.deletions for f in commit.files)
+
+        stats.lines_added += total_add
+        stats.lines_deleted += total_del
+
+        current_sha = commit.sha
+        parent_sha = commit.parents[0].sha if commit.parents else "None"
+
+        self.file = "filtered.txt"
+        msg = f"{self.repo_id} | {current_sha} | {parent_sha}\n" 
+        path = Path(self.output_path) / self.file
+        self._write(path, msg)
+
+        return stats
+    
+    def write_pr_commit(self, repo: Repository, commit: Commit, is_issue: bool):
+        stats = CommitStats()
+
+        stats.perf_commits += 1
+        total_add = sum(f.additions for f in commit.files)
+        total_del = sum(f.deletions for f in commit.files)
+
+        stats.lines_added += total_add
+        stats.lines_deleted += total_del
+
+        msg = get_pr_chain_msg(repo, commit, is_issue)
+        path = Path(self.output_path)
+        self._write(path, msg)
+
+        return stats
+
+    def write_improve(self, results: dict) -> None:
+        self.file = f"improved.txt"
+        path = Path(self.output_path) / self.file
+        new_sha: str = results['commit_info']["new_sha"]
+        old_sha: str = results['commit_info']["old_sha"]
+        repo_id: str = results['metadata']['repository_name']
+        p_value: float = results['performance_analysis']['p_value']
+        rel_improv: float = results['performance_analysis']['relative_improvement']
+        msg = f"{repo_id} | {new_sha} | {old_sha} | {p_value} | {rel_improv}\n"
+        self._write(path, msg)
+
+    def write_results(self, results: dict) -> None:
+        new_sha: str = results['commit_info']["new_sha"]
+        file = f"{self.owner}_{self.name}_{new_sha}.json"
+        path = Path(self.output_path) / file
+        path.parent.mkdir(parents=True, exist_ok=True)
+        with open(path, "w", encoding="utf-8") as f:
+            json.dump(results, f, indent=2)
+        logging.info(f"[{self.owner}/{self.name}] Wrote results to {path}")
+
+    
+    def _write(self, path: Path, msg: str) -> None:
+        path.parent.mkdir(parents=True, exist_ok=True)
+
+        with open(path, "a", encoding="utf-8", errors="ignore") as f:
+            fcntl.flock(f, fcntl.LOCK_EX)
+            f.write(msg)
+            f.flush()
+            os.fsync(f.fileno())
+            fcntl.flock(f, fcntl.LOCK_UN)
+    """
+    def _write(self, path: Path, msg: str) -> None:
+        try:
+            path.parent.mkdir(parents=True, exist_ok=True)
+            with path.open("a", encoding="utf-8", errors="ignore") as f:
+                f.write(msg)
+            logging.info(f"[{self.owner}/{self.name}] Wrote data to {path}")
+        except (OSError, IOError) as e:
+            logging.error(f"[{self.owner}/{self.name}] Failed to write to {path}: {e}", exc_info=True)
+    """
\ No newline at end of file
