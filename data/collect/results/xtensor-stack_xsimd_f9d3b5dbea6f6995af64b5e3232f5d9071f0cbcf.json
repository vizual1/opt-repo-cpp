{
  "metadata": {
    "collection_date": "2026-01-11T17:55:22.133149",
    "repository": "https://github.com/xtensor-stack/xsimd",
    "repository_name": "xtensor-stack/xsimd"
  },
  "commit_info": {
    "old_sha": "b103d2885d0daf00d9059ff86487d060add72731",
    "new_sha": "f9d3b5dbea6f6995af64b5e3232f5d9071f0cbcf",
    "commit_message": [
      "Mark all functions inline\n\n- it helps the optimizer\n- it prevents regression like #617"
    ],
    "commit_date": "2021-10-29T21:00:06+00:00",
    "patch": [
      "--- include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp\n@@ -25,61 +25,69 @@ namespace xsimd {\n \n     // bitwise_lshift\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x << y;}, self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x >> y;}, self, other);\n     }\n \n     // div\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x / y;}, self, other);\n     }\n \n     // fma\n-    template<class A, class T> batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return x * y + z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n       auto res_i = fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fms\n-    template<class A, class T> batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return x * y - z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n       auto res_i = fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fnma\n-    template<class A, class T> batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return -x * y + z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = - fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n       auto res_i = - fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fnms\n-    template<class A, class T> batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return -x * y - z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = - fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n       auto res_i = - fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n@@ -89,7 +97,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x * y;}, self, other);\n     }\n \n--- include/xsimd/arch/generic/xsimd_generic_complex.hpp\n@@ -24,47 +24,47 @@ namespace xsimd {\n \n     // real\n     template <class A, class T>\n-    batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n \n     template <class A, class T>\n-    batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return self.real();\n     }\n \n     // imag\n     template <class A, class T>\n-    batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {\n+    inline batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {\n       return batch<T, A>(T(0));\n     }\n \n     template <class A, class T>\n-    batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return self.imag();\n     }\n-    \n+\n     // arg\n     template<class A, class T>\n-    real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {\n+    inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {\n       return atan2(imag(self), real(self));\n     }\n \n     // conj\n     template<class A, class T>\n-    complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {\n+    inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {\n       return {real(self), - imag(self)};\n     }\n \n     // norm\n     template<class A, class T>\n-    real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {\n+    inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {\n       return {fma(real(self), real(self), imag(self) * imag(self))};\n     }\n \n     // proj\n     template<class A, class T>\n-    complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {\n+    inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = complex_batch_type_t<batch<T, A>>;\n       using real_batch = typename batch_type::real_batch;\n       using real_value_type = typename real_batch::value_type;\n@@ -76,11 +76,10 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return batch_bool<T, A>(isnan(self.real()) || isnan(self.imag()));\n     }\n   }\n }\n \n #endif\n-\n--- include/xsimd/arch/generic/xsimd_generic_details.hpp\n@@ -105,7 +105,7 @@ namespace xsimd {\n \n     namespace detail {\n       template<class F, class A, class T, class... Batches>\n-      batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {\n+      inline batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {\n         constexpr std::size_t size = batch<T, A>::size;\n         alignas(A::alignment()) T self_buffer[size];\n         alignas(A::alignment()) T other_buffer[size];\n@@ -207,8 +207,6 @@ namespace xsimd {\n         }\n     }\n \n-\n-\n   }\n \n }\n--- include/xsimd/arch/generic/xsimd_generic_logical.hpp\n@@ -22,82 +22,92 @@ namespace xsimd {\n     using namespace types;\n \n     // ge\n-    template<class A, class T> batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return other <= self;\n     }\n \n     // gt\n-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return other < self;\n     }\n \n     // is_even\n-    template<class A, class T> batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {\n       return is_flint(self * T(0.5));\n     }\n \n     // is_flint\n-    template<class A, class T> batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {\n       auto frac = select(isnan(self - self), constants::nan<batch<T, A>>(), self - trunc(self));\n       return frac == T(0.);\n     }\n \n     // is_odd\n-    template<class A, class T> batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {\n       return is_even(self - T(1.));\n     }\n \n     // isinf\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isinf(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isinf(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(false);\n     }\n-    template<class A> batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {\n       return abs(self) == std::numeric_limits<float>::infinity();\n     }\n-    template<class A> batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {\n       return abs(self) == std::numeric_limits<double>::infinity();\n     }\n \n     // isfinite\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isfinite(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isfinite(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(true);\n     }\n-    template<class A> batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {\n       return (self - self) == 0;\n     }\n-    template<class A> batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {\n       return (self - self) == 0;\n     }\n \n     // isnan\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isnan(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isnan(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(false);\n     }\n \n     // le\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return (self < other) || (self == other);\n     }\n \n \n     // neq\n-    template<class A, class T> batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return !(other == self);\n     }\n \n     // logical_and\n     template <class A, class T>\n-    batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x && y;}, self, other);\n     }\n \n     // logical_or\n     template <class A, class T>\n-    batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x || y;}, self, other);\n     }\n   }\n--- include/xsimd/arch/generic/xsimd_generic_math.hpp\n@@ -26,7 +26,7 @@ namespace xsimd {\n     using namespace types;\n     // abs\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>)\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>)\n     {\n       if(std::is_unsigned<T>::value)\n         return self;\n@@ -38,22 +38,23 @@ namespace xsimd {\n     }\n \n     template<class A, class T>\n-    batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    inline batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return hypot(z.real(), z.imag());\n     }\n \n     // batch_cast\n-    template<class A, class T> batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {\n       return self;\n     }\n \n     namespace detail {\n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {\n       return fast_cast(self, out, A{});\n     }\n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be no conversion for this type combination\");\n       using batch_type_in = batch<T_in, A>;\n       using batch_type_out = batch<T_out, A>;\n@@ -68,23 +69,26 @@ namespace xsimd {\n     }\n \n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {\n       return detail::batch_cast(self, out, A{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // bitofsign\n-    template<class A, class T> batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {\n       static_assert(std::is_integral<T>::value, \"int type implementation\");\n       if(std::is_unsigned<T>::value)\n         return batch<T, A>(0);\n       else\n         return self >> (T)(8 * sizeof(T) - 1);\n     }\n \n-    template<class A> batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {\n       return self & constants::minuszero<batch<float, A>>();\n     }\n-    template<class A> batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {\n       return self & constants::minuszero<batch<double, A>>();\n     }\n \n@@ -99,7 +103,8 @@ namespace xsimd {\n          * (See copy at http://boost.org/LICENSE_1_0.txt)\n          * ====================================================\n          */\n-    template<class A> batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type z = abs(self);\n #ifndef XSIMD_NO_DENORMALS\n@@ -143,7 +148,9 @@ namespace xsimd {\n                 return select(self == batch_type(0.), self, x);\n #endif\n     }\n-    template<class A> batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type z = abs(self);\n #ifndef XSIMD_NO_DENORMALS\n@@ -190,13 +197,15 @@ namespace xsimd {\n     }\n \n     // clip\n-    template<class A, class T> batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {\n       return min(hi, max(self, lo));\n     }\n \n \n     // copysign\n-    template<class A, class T> batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return abs(self) | bitofsign(other);\n     }\n \n@@ -375,7 +384,7 @@ namespace xsimd {\n          */\n \n     template<class A>\n-    batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {\n+    inline batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 batch_type r1(0.);\n@@ -395,8 +404,10 @@ namespace xsimd {\n                 r1 = select(xsimd::isinf(self), sign(self), r1);\n #endif\n                 return r1;\n-            }\n-    template<class A> batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {\n+    }\n+\n+    template<class A>\n+    inline batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 batch_type xx = x * x;\n@@ -430,7 +441,8 @@ namespace xsimd {\n     }\n \n     // erfc\n-    template<class A> batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 auto test0 = self < batch_type(0.);\n@@ -451,7 +463,9 @@ namespace xsimd {\n #endif\n                 return select(test0, batch_type(2.) - r1, r1);\n     }\n-    template<class A> batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 batch_type xx = x * x;\n@@ -547,8 +561,9 @@ namespace xsimd {\n             }\n         };\n     }\n+\n     template <class T, class A, uint64_t... Coefs>\n-    batch<T, A> estrin(const batch<T, A>& self) {\n+    inline batch<T, A> estrin(const batch<T, A>& self) {\n       using batch_type = batch<T, A>;\n       return detail::estrin<batch_type>{self}(detail::coef<batch_type, Coefs>()...);\n     }\n@@ -786,7 +801,8 @@ namespace xsimd {\n             }\n         };\n \n-      template<exp_reduction_tag Tag, class A> batch<float, A> exp(batch<float, A> const& self) {\n+      template<exp_reduction_tag Tag, class A>\n+      inline batch<float, A> exp(batch<float, A> const& self) {\n         using batch_type = batch<float, A>;\n         using reducer_t = exp_reduction<float, A, Tag>;\n         batch_type x;\n@@ -796,7 +812,9 @@ namespace xsimd {\n         x = select(self >= reducer_t::maxlog(), constants::infinity<batch_type>(), x);\n         return x;\n       }\n-      template<exp_reduction_tag Tag, class A> batch<double, A> exp(batch<double, A> const& self) {\n+\n+      template<exp_reduction_tag Tag, class A>\n+      inline batch<double, A> exp(batch<double, A> const& self) {\n         using batch_type = batch<double, A>;\n         using reducer_t = exp_reduction<double, A, Tag>;\n         batch_type hi, lo, x;\n@@ -809,23 +827,27 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A, class T> batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp_tag>(self);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<std::complex<T>, A>;\n       auto isincos = sincos(self.imag());\n       return exp(self.real()) * batch_type(std::get<1>(isincos), std::get<0>(isincos));\n     }\n \n     // exp10\n-    template<class A, class T> batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp10_tag>(self);\n     }\n \n     // exp2\n-    template<class A, class T> batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp2_tag>(self);\n     }\n \n@@ -895,7 +917,8 @@ namespace xsimd {\n \n     }\n \n-    template<class A, class T> batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       return select(self < constants::logeps<batch_type>(),\n                     batch_type(-1.),\n@@ -905,7 +928,7 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    inline batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n     {\n         using batch_type = batch<std::complex<T>, A>;\n         using real_batch = typename batch_type::real_batch;\n@@ -917,12 +940,14 @@ namespace xsimd {\n     }\n \n     // fdim\n-    template<class A, class T> batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return fmax(batch<T, A>(0), self - other);\n     }\n \n     // fmod\n-    template<class A, class T> batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return fnma(trunc(self / other), other, self);\n     }\n \n@@ -937,7 +962,7 @@ namespace xsimd {\n      * ====================================================\n      */\n     template <class A, class T>\n-    batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {\n+    inline batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         using i_type = batch<as_integer_t<T>, A>;\n         i_type m1f = constants::mask1frexp<batch_type>();\n@@ -950,28 +975,31 @@ namespace xsimd {\n \n     // from bool\n     template<class A, class T>\n-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {\n       return batch<T, A>(self.data) & batch<T,A>(1);\n     }\n \n     // hadd\n-    template<class A, class T> std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return {hadd(self.real()), hadd(self.imag())};\n     }\n \n     // horner\n     template <class T, class A, uint64_t... Coefs>\n-    batch<T, A> horner(const batch<T, A>& self) {\n+    inline batch<T, A> horner(const batch<T, A>& self) {\n       return detail::horner<batch<T, A>, Coefs...>(self);\n     }\n \n     // hypot\n-    template<class A, class T> batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return sqrt(fma(self, self, other * other));\n     }\n \n     // ipow\n-    template<class A, class T, class ITy> batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {\n+    template<class A, class T, class ITy>\n+    inline batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {\n       return ::xsimd::detail::ipow(self, other);\n     }\n \n@@ -987,7 +1015,7 @@ namespace xsimd {\n       * ====================================================\n       */\n     template <class A, class T>\n-    batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {\n+    inline batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         using itype = as_integer_t<batch_type>;\n         itype ik = other + constants::maxexponent<T>();\n@@ -996,7 +1024,8 @@ namespace xsimd {\n     }\n \n     // lgamma\n-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);\n+    template<class A, class T>\n+    inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);\n \n     namespace detail {\n     /* origin: boost/simd/arch/common/detail/generic/gammaln_kernel.hpp */\n@@ -1009,77 +1038,79 @@ namespace xsimd {\n      * ====================================================\n      */\n     template<class A>\n-            static inline batch<float, A> gammalnB(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0x3ed87730,  //    4.227843421859038E-001\n-                              0x3ea51a64,  //    3.224669577325661E-001,\n-                              0xbd89f07e,  //   -6.735323259371034E-002,\n-                              0x3ca89ed8,  //    2.058355474821512E-002,\n-                              0xbbf164fd,  //   -7.366775108654962E-003,\n-                              0x3b3ba883,  //    2.863437556468661E-003,\n-                              0xbaabeab1,  //   -1.311620815545743E-003,\n-                              0x3a1ebb94  //    6.055172732649237E-004\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammalnB(const batch<float, A>& x)\n+    {\n+            return horner<batch<float, A>,\n+                          0x3ed87730,  //    4.227843421859038E-001\n+                          0x3ea51a64,  //    3.224669577325661E-001,\n+                          0xbd89f07e,  //   -6.735323259371034E-002,\n+                          0x3ca89ed8,  //    2.058355474821512E-002,\n+                          0xbbf164fd,  //   -7.366775108654962E-003,\n+                          0x3b3ba883,  //    2.863437556468661E-003,\n+                          0xbaabeab1,  //   -1.311620815545743E-003,\n+                          0x3a1ebb94  //    6.055172732649237E-004\n+                          >(x);\n+     }\n \n     template<class A>\n-            static inline batch<float, A> gammalnC(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0xbf13c468,  //   -5.772156501719101E-001\n-                              0x3f528d34,  //    8.224670749082976E-001,\n-                              0xbecd27a8,  //   -4.006931650563372E-001,\n-                              0x3e8a898b,  //    2.705806208275915E-001,\n-                              0xbe53c04f,  //   -2.067882815621965E-001,\n-                              0x3e2d4dab,  //    1.692415923504637E-001,\n-                              0xbe22d329,  //   -1.590086327657347E-001,\n-                              0x3e0c3c4f  //    1.369488127325832E-001\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammalnC(const batch<float, A>& x)\n+    {\n+        return horner<batch<float, A>,\n+                      0xbf13c468,  //   -5.772156501719101E-001\n+                      0x3f528d34,  //    8.224670749082976E-001,\n+                      0xbecd27a8,  //   -4.006931650563372E-001,\n+                      0x3e8a898b,  //    2.705806208275915E-001,\n+                      0xbe53c04f,  //   -2.067882815621965E-001,\n+                      0x3e2d4dab,  //    1.692415923504637E-001,\n+                      0xbe22d329,  //   -1.590086327657347E-001,\n+                      0x3e0c3c4f  //    1.369488127325832E-001\n+                      >(x);\n+    }\n \n     template<class A>\n-            static inline batch<float, A> gammaln2(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0x3daaaa94,  //   8.333316229807355E-002f\n-                              0xbb358701,  //  -2.769887652139868E-003f,\n-                              0x3a31fd69  //   6.789774945028216E-004f\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammaln2(const batch<float, A>& x)\n+    {\n+        return horner<batch<float, A>,\n+                      0x3daaaa94,  //   8.333316229807355E-002f\n+                      0xbb358701,  //  -2.769887652139868E-003f,\n+                      0x3a31fd69  //   6.789774945028216E-004f\n+                      >(x);\n+    }\n+\n     template<class A>\n-            static inline batch<double, A> gammaln1(const batch<double, A>& x)\n-            {\n-                return horner<batch<double, A>,\n-                              0xc12a0c675418055eull,  //  -8.53555664245765465627E5\n-                              0xc13a45890219f20bull,  //  -1.72173700820839662146E6,\n-                              0xc131bc82f994db51ull,  //  -1.16237097492762307383E6,\n-                              0xc1143d73f89089e5ull,  //  -3.31612992738871184744E5,\n-                              0xc0e2f234355bb93eull,  //  -3.88016315134637840924E4,\n-                              0xc09589018ff36761ull  //  -1.37825152569120859100E3\n-                              >(x) /\n-                    horner<batch<double, A>,\n-                           0xc13ece4b6a11e14aull,  //  -2.01889141433532773231E6\n-                           0xc1435255892ff34cull,  //  -2.53252307177582951285E6,\n-                           0xc131628671950043ull,  //  -1.13933444367982507207E6,\n-                           0xc10aeb84b9744c9bull,  //  -2.20528590553854454839E5,\n-                           0xc0d0aa0d7b89d757ull,  //  -1.70642106651881159223E4,\n-                           0xc075fd0d1cf312b2ull,  //  -3.51815701436523470549E2,\n-                           0x3ff0000000000000ull  //   1.00000000000000000000E0\n-                           >(x);\n-            }\n+    static inline batch<double, A> gammaln1(const batch<double, A>& x)\n+    {\n+        return horner<batch<double, A>,\n+                      0xc12a0c675418055eull,  //  -8.53555664245765465627E5\n+                      0xc13a45890219f20bull,  //  -1.72173700820839662146E6,\n+                      0xc131bc82f994db51ull,  //  -1.16237097492762307383E6,\n+                      0xc1143d73f89089e5ull,  //  -3.31612992738871184744E5,\n+                      0xc0e2f234355bb93eull,  //  -3.88016315134637840924E4,\n+                      0xc09589018ff36761ull  //  -1.37825152569120859100E3\n+                      >(x) /\n+            horner<batch<double, A>,\n+                   0xc13ece4b6a11e14aull,  //  -2.01889141433532773231E6\n+                   0xc1435255892ff34cull,  //  -2.53252307177582951285E6,\n+                   0xc131628671950043ull,  //  -1.13933444367982507207E6,\n+                   0xc10aeb84b9744c9bull,  //  -2.20528590553854454839E5,\n+                   0xc0d0aa0d7b89d757ull,  //  -1.70642106651881159223E4,\n+                   0xc075fd0d1cf312b2ull,  //  -3.51815701436523470549E2,\n+                   0x3ff0000000000000ull  //   1.00000000000000000000E0\n+                   >(x);\n+    }\n \n     template<class A>\n-            static inline batch<double, A> gammalnA(const batch<double, A>& x)\n-            {\n-                return horner<batch<double, A>,\n-                              0x3fb555555555554bull,  //    8.33333333333331927722E-2\n-                              0xbf66c16c16b0a5a1ull,  //   -2.77777777730099687205E-3,\n-                              0x3f4a019f20dc5ebbull,  //    7.93650340457716943945E-4,\n-                              0xbf437fbdb580e943ull,  //   -5.95061904284301438324E-4,\n-                              0x3f4a985027336661ull  //    8.11614167470508450300E-4\n-                              >(x);\n-            }\n+    static inline batch<double, A> gammalnA(const batch<double, A>& x)\n+    {\n+        return horner<batch<double, A>,\n+                      0x3fb555555555554bull,  //    8.33333333333331927722E-2\n+                      0xbf66c16c16b0a5a1ull,  //   -2.77777777730099687205E-3,\n+                      0x3f4a019f20dc5ebbull,  //    7.93650340457716943945E-4,\n+                      0xbf437fbdb580e943ull,  //   -5.95061904284301438324E-4,\n+                      0x3f4a985027336661ull  //    8.11614167470508450300E-4\n+                      >(x);\n+    }\n+\n     /* origin: boost/simd/arch/common/simd/function/gammaln.hpp */\n     /*\n      * ====================================================\n@@ -1089,129 +1120,129 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-        template <class B>\n-        struct lgamma_impl;\n+      template <class B>\n+      struct lgamma_impl;\n \n-        template <class A>\n-        struct lgamma_impl<batch<float, A>>\n-        {\n-          using batch_type = batch<float, A>;\n-            static inline batch_type compute(const batch_type& a)\n-            {\n-                auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n-                batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n-                batch_type q = abs(x);\n+      template <class A>\n+      struct lgamma_impl<batch<float, A>>\n+      {\n+        using batch_type = batch<float, A>;\n+          static inline batch_type compute(const batch_type& a)\n+          {\n+              auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n+              batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n+              batch_type q = abs(x);\n #ifndef XSIMD_NO_INFINITIES\n-                inf_result = (x == constants::infinity<batch_type>()) || inf_result;\n+              inf_result = (x == constants::infinity<batch_type>()) || inf_result;\n #endif\n-                auto ltza = a < batch_type(0.);\n-                batch_type r;\n-                batch_type r1 = other(q);\n-                if (any(ltza))\n-                {\n-                    r = select(inf_result, constants::infinity<batch_type>(), negative(q, r1));\n-                    if (all(ltza))\n-                        return r;\n-                }\n-                batch_type r2 = select(ltza, r, r1);\n-                return select(a == constants::minusinfinity<batch_type>(), constants::nan<batch_type>(), select(inf_result, constants::infinity<batch_type>(), r2));\n-            }\n-\n-        private:\n-\n-            static inline batch_type negative(const batch_type& q, const batch_type& w)\n-            {\n-                batch_type p = floor(q);\n-                batch_type z = q - p;\n-                auto test2 = z < batch_type(0.5);\n-                z = select(test2, z - batch_type(1.), z);\n-                z = q * sin(z, trigo_pi_tag());\n-                return -log(constants::invpi<batch_type>() * abs(z)) - w;\n-            }\n-\n-            static inline batch_type other(const batch_type& x)\n-            {\n-                auto xlt650 = (x < batch_type(6.5));\n-                batch_type r0x = x;\n-                batch_type r0z = x;\n-                batch_type r0s = batch_type(1.);\n-                batch_type r1 = batch_type(0.);\n-                batch_type p = constants::nan<batch_type>();\n-                if (any(xlt650))\n-                {\n-                    batch_type z = batch_type(1.);\n-                    batch_type tx = select(xlt650, x, batch_type(0.));\n-                    batch_type nx = batch_type(0.);\n-                    const batch_type _075 = batch_type(0.75);\n-                    const batch_type _150 = batch_type(1.50);\n-                    const batch_type _125 = batch_type(1.25);\n-                    const batch_type _250 = batch_type(2.50);\n-                    auto xge150 = (x >= _150);\n-                    auto txgt250 = (tx > _250);\n-\n-                    // x >= 1.5\n-                    while (any(xge150 && txgt250))\n-                    {\n-                        nx = select(txgt250, nx - batch_type(1.), nx);\n-                        tx = select(txgt250, x + nx, tx);\n-                        z = select(txgt250, z * tx, z);\n-                        txgt250 = (tx > _250);\n-                    }\n-                    r0x = select(xge150, x + nx - batch_type(2.), x);\n-                    r0z = select(xge150, z, r0z);\n-                    r0s = select(xge150, batch_type(1.), r0s);\n-\n-                    // x >= 1.25 && x < 1.5\n-                    auto xge125 = (x >= _125);\n-                    auto xge125t = xge125 && !xge150;\n-                    if (any(xge125))\n-                    {\n-                        r0x = select(xge125t, x - batch_type(1.), r0x);\n-                        r0z = select(xge125t, z * x, r0z);\n-                        r0s = select(xge125t, batch_type(-1.), r0s);\n-                    }\n-\n-                    // x >= 0.75 && x < 1.5\n-                    batch_bool<float, A> kernelC(false);\n-                    auto xge075 = (x >= _075);\n-                    auto xge075t = xge075 && !xge125;\n-                    if (any(xge075t))\n-                    {\n-                        kernelC = xge075t;\n-                        r0x = select(xge075t, x - batch_type(1.), x);\n-                        r0z = select(xge075t, batch_type(1.), r0z);\n-                        r0s = select(xge075t, batch_type(-1.), r0s);\n-                        p = gammalnC(r0x);\n-                    }\n-\n-                    // tx < 1.5 && x < 0.75\n-                    auto txlt150 = (tx < _150) && !xge075;\n-                    if (any(txlt150))\n-                    {\n-                        auto orig = txlt150;\n-                        while (any(txlt150))\n-                        {\n-                            z = select(txlt150, z * tx, z);\n-                            nx = select(txlt150, nx + batch_type(1.), nx);\n-                            tx = select(txlt150, x + nx, tx);\n-                            txlt150 = (tx < _150) && !xge075;\n-                        }\n-                        r0x = select(orig, r0x + nx - batch_type(2.), r0x);\n-                        r0z = select(orig, z, r0z);\n-                        r0s = select(orig, batch_type(-1.), r0s);\n-                    }\n-                    p = select(kernelC, p, gammalnB(r0x));\n-                    if (all(xlt650))\n-                        return fma(r0x, p, r0s * log(abs(r0z)));\n-                }\n-                r0z = select(xlt650, abs(r0z), x);\n-                batch_type m = log(r0z);\n-                r1 = fma(r0x, p, r0s * m);\n-                batch_type r2 = fma(x - batch_type(0.5), m, constants::logsqrt2pi<batch_type>() - x);\n-                r2 += gammaln2(batch_type(1.) / (x * x)) / x;\n-                return select(xlt650, r1, r2);\n-            }\n-        };\n+              auto ltza = a < batch_type(0.);\n+              batch_type r;\n+              batch_type r1 = other(q);\n+              if (any(ltza))\n+              {\n+                  r = select(inf_result, constants::infinity<batch_type>(), negative(q, r1));\n+                  if (all(ltza))\n+                      return r;\n+              }\n+              batch_type r2 = select(ltza, r, r1);\n+              return select(a == constants::minusinfinity<batch_type>(), constants::nan<batch_type>(), select(inf_result, constants::infinity<batch_type>(), r2));\n+          }\n+\n+      private:\n+\n+          static inline batch_type negative(const batch_type& q, const batch_type& w)\n+          {\n+              batch_type p = floor(q);\n+              batch_type z = q - p;\n+              auto test2 = z < batch_type(0.5);\n+              z = select(test2, z - batch_type(1.), z);\n+              z = q * sin(z, trigo_pi_tag());\n+              return -log(constants::invpi<batch_type>() * abs(z)) - w;\n+          }\n+\n+          static inline batch_type other(const batch_type& x)\n+          {\n+              auto xlt650 = (x < batch_type(6.5));\n+              batch_type r0x = x;\n+              batch_type r0z = x;\n+              batch_type r0s = batch_type(1.);\n+              batch_type r1 = batch_type(0.);\n+              batch_type p = constants::nan<batch_type>();\n+              if (any(xlt650))\n+              {\n+                  batch_type z = batch_type(1.);\n+                  batch_type tx = select(xlt650, x, batch_type(0.));\n+                  batch_type nx = batch_type(0.);\n+                  const batch_type _075 = batch_type(0.75);\n+                  const batch_type _150 = batch_type(1.50);\n+                  const batch_type _125 = batch_type(1.25);\n+                  const batch_type _250 = batch_type(2.50);\n+                  auto xge150 = (x >= _150);\n+                  auto txgt250 = (tx > _250);\n+\n+                  // x >= 1.5\n+                  while (any(xge150 && txgt250))\n+                  {\n+                      nx = select(txgt250, nx - batch_type(1.), nx);\n+                      tx = select(txgt250, x + nx, tx);\n+                      z = select(txgt250, z * tx, z);\n+                      txgt250 = (tx > _250);\n+                  }\n+                  r0x = select(xge150, x + nx - batch_type(2.), x);\n+                  r0z = select(xge150, z, r0z);\n+                  r0s = select(xge150, batch_type(1.), r0s);\n+\n+                  // x >= 1.25 && x < 1.5\n+                  auto xge125 = (x >= _125);\n+                  auto xge125t = xge125 && !xge150;\n+                  if (any(xge125))\n+                  {\n+                      r0x = select(xge125t, x - batch_type(1.), r0x);\n+                      r0z = select(xge125t, z * x, r0z);\n+                      r0s = select(xge125t, batch_type(-1.), r0s);\n+                  }\n+\n+                  // x >= 0.75 && x < 1.5\n+                  batch_bool<float, A> kernelC(false);\n+                  auto xge075 = (x >= _075);\n+                  auto xge075t = xge075 && !xge125;\n+                  if (any(xge075t))\n+                  {\n+                      kernelC = xge075t;\n+                      r0x = select(xge075t, x - batch_type(1.), x);\n+                      r0z = select(xge075t, batch_type(1.), r0z);\n+                      r0s = select(xge075t, batch_type(-1.), r0s);\n+                      p = gammalnC(r0x);\n+                  }\n+\n+                  // tx < 1.5 && x < 0.75\n+                  auto txlt150 = (tx < _150) && !xge075;\n+                  if (any(txlt150))\n+                  {\n+                      auto orig = txlt150;\n+                      while (any(txlt150))\n+                      {\n+                          z = select(txlt150, z * tx, z);\n+                          nx = select(txlt150, nx + batch_type(1.), nx);\n+                          tx = select(txlt150, x + nx, tx);\n+                          txlt150 = (tx < _150) && !xge075;\n+                      }\n+                      r0x = select(orig, r0x + nx - batch_type(2.), r0x);\n+                      r0z = select(orig, z, r0z);\n+                      r0s = select(orig, batch_type(-1.), r0s);\n+                  }\n+                  p = select(kernelC, p, gammalnB(r0x));\n+                  if (all(xlt650))\n+                      return fma(r0x, p, r0s * log(abs(r0z)));\n+              }\n+              r0z = select(xlt650, abs(r0z), x);\n+              batch_type m = log(r0z);\n+              r1 = fma(r0x, p, r0s * m);\n+              batch_type r2 = fma(x - batch_type(0.5), m, constants::logsqrt2pi<batch_type>() - x);\n+              r2 += gammaln2(batch_type(1.) / (x * x)) / x;\n+              return select(xlt650, r1, r2);\n+          }\n+      };\n \n         template <class A>\n         struct lgamma_impl<batch<double, A>>\n@@ -1295,7 +1326,8 @@ namespace xsimd {\n         };\n     }\n \n-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::lgamma_impl<batch<T, A>>::compute(self);\n     }\n \n@@ -1310,7 +1342,8 @@ namespace xsimd {\n              * (See copy at http://boost.org/LICENSE_1_0.txt)\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n       using i_type = as_integer_t<batch_type>;\n       batch_type x = self;\n@@ -1347,7 +1380,8 @@ namespace xsimd {\n       return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-    template<class A> batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {\n                using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n \n@@ -1395,13 +1429,14 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    inline batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n     {\n         return batch<std::complex<T>, A>(log(abs(z)), atan2(z.imag(), z.real()));\n     }\n \n     // log2\n-    template<class A> batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 batch_type x = self;\n@@ -1437,7 +1472,9 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 batch_type x = self;\n@@ -1488,6 +1525,7 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n+\n     namespace detail {\n         template <class T, class A>\n         inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base)\n@@ -1498,7 +1536,8 @@ namespace xsimd {\n         }\n   }\n \n-    template<class A, class T> batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return detail::logN_complex_impl(self, std::log(2));\n     }\n \n@@ -1514,7 +1553,8 @@ namespace xsimd {\n              * is preserved.\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 const batch_type\n                     ivln10hi(4.3432617188e-01f),\n@@ -1561,7 +1601,9 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 const batch_type\n                     ivln10hi(4.34294481878168880939e-01),\n@@ -1619,8 +1661,8 @@ namespace xsimd {\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-        template <class A, class T>\n-            batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    template <class A, class T>\n+    inline batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n             {\n                 return detail::logN_complex_impl(z, std::log(10));\n             }\n@@ -1635,7 +1677,8 @@ namespace xsimd {\n              * (See copy at http://boost.org/LICENSE_1_0.txt)\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 const batch_type uf = self + batch_type(1.);\n@@ -1663,7 +1706,9 @@ namespace xsimd {\n #endif\n                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 const batch_type uf = self + batch_type(1.);\n@@ -1700,7 +1745,8 @@ namespace xsimd {\n                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n         using batch_type = batch<std::complex<T>, A>;\n         using real_batch = typename batch_type::real_batch;\n         batch_type u = 1 + self;\n@@ -1715,18 +1761,19 @@ namespace xsimd {\n \n     // mod\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x % y;}, self, other);\n     }\n \n \n     // nearbyint\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n     namespace detail {\n-      template<class A, class T> batch<T, A> nearbyintf(batch<T, A> const& self) {\n+      template<class A, class T>\n+      inline batch<T, A> nearbyintf(batch<T, A> const& self) {\n         using batch_type = batch<T, A>;\n         batch_type s = bitofsign(self);\n         batch_type v = self ^ s;\n@@ -1744,10 +1791,12 @@ namespace xsimd {\n         return s ^ select(v < t2n, d, v);\n       }\n     }\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::nearbyintf(self);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::nearbyintf(self);\n     }\n \n@@ -1805,7 +1854,8 @@ namespace xsimd {\n             }\n         };\n     }\n-    template<class A, class T> batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {\n       using kernel = detail::nextafter_kernel<T, A>;\n       return select(from == to, from,\n                     select(to > from, kernel::next(from), kernel::prev(from)));\n@@ -1822,7 +1872,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         auto negx = self < batch_type(0.);\n         batch_type z = exp(other * log(abs(self)));\n@@ -1831,55 +1882,57 @@ namespace xsimd {\n         return select(invalid, constants::nan<batch_type>(), z);\n     }\n \n-        template <class A, class T>\n-        inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)\n-        {\n-            using cplx_batch = batch<std::complex<T>, A>;\n-            using real_batch = typename cplx_batch::real_batch;\n-            real_batch absa = abs(a);\n-            real_batch arga = arg(a);\n-            real_batch x = z.real();\n-            real_batch y = z.imag();\n-            real_batch r = pow(absa, x);\n-            real_batch theta = x * arga;\n-            real_batch ze(0);\n-            auto cond = (y == ze);\n-            r = select(cond, r, r * exp(-y * arga));\n-            theta = select(cond, theta, theta + y * log(absa));\n-            return select(absa == ze, cplx_batch(ze), cplx_batch(r * cos(theta), r * sin(theta)));\n-        }\n+    template <class A, class T>\n+    inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    {\n+        using cplx_batch = batch<std::complex<T>, A>;\n+        using real_batch = typename cplx_batch::real_batch;\n+        real_batch absa = abs(a);\n+        real_batch arga = arg(a);\n+        real_batch x = z.real();\n+        real_batch y = z.imag();\n+        real_batch r = pow(absa, x);\n+        real_batch theta = x * arga;\n+        real_batch ze(0);\n+        auto cond = (y == ze);\n+        r = select(cond, r, r * exp(-y * arga));\n+        theta = select(cond, theta, theta + y * log(absa));\n+        return select(absa == ze, cplx_batch(ze), cplx_batch(r * cos(theta), r * sin(theta)));\n+    }\n \n \n     // remainder\n     template<class A>\n-    batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {\n+    inline batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {\n       return fnma(nearbyint(self / other), other, self);\n     }\n     template<class A>\n-    batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {\n+    inline batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {\n       return fnma(nearbyint(self / other), other, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       auto mod = self % other;\n       return select(mod <= other / 2, mod, mod - other);\n     }\n \n     // select\n     template<class A, class T>\n-    batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {\n+    inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {\n       return {select(cond, true_br.real(), false_br.real()), select(cond, true_br.imag(), false_br.imag())};\n     }\n \n     // sign\n-    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type> batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n+    inline batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       batch_type res = select(self > batch_type(0), batch_type(1), batch_type(0)) - select(self < batch_type(0), batch_type(1), batch_type(0));\n       return res;\n     }\n \n     namespace detail {\n-    template<class T, class A> batch<T, A> signf(batch<T, A> const& self) {\n+    template<class T, class A>\n+    inline batch<T, A> signf(batch<T, A> const& self) {\n       using batch_type = batch<T, A>;\n       batch_type res = select(self > batch_type(0.f), batch_type(1.f), batch_type(0.f)) - select(self < batch_type(0.f), batch_type(1.f), batch_type(0.f));\n #ifdef XSIMD_NO_NANS\n@@ -1890,10 +1943,12 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A> batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::signf(self);\n     }\n-    template<class A> batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::signf(self);\n     }\n         template <class A, class T>\n@@ -1909,13 +1964,15 @@ namespace xsimd {\n         }\n \n     // signnz\n-    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type> batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n+    inline batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       return (self >> (sizeof(T) * 8 - 1)) | batch_type(1.);\n     }\n \n     namespace detail {\n-    template<class T, class A> batch<T, A> signnzf(batch<T, A> const& self) {\n+    template<class T, class A>\n+    inline batch<T, A> signnzf(batch<T, A> const& self) {\n       using batch_type = batch<T, A>;\n #ifndef XSIMD_NO_NANS\n                 return select(isnan(self), constants::nan<batch_type>(), batch_type(1.) | (constants::signmask<batch_type>() & self));\n@@ -1925,15 +1982,18 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A> batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::signnzf(self);\n     }\n-    template<class A> batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::signnzf(self);\n     }\n \n     // sqrt\n-    template<class A, class T> batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n \n                 constexpr T csqrt_scale_factor = std::is_same<T, float>::value?6.7108864e7f:1.8014398509481984e16;\n                 constexpr T csqrt_scale = std::is_same<T, float>::value?1.220703125e-4f:7.450580596923828125e-9;\n@@ -2134,7 +2194,7 @@ namespace xsimd {\n      * ====================================================\n      */\n         template <class B>\n-        B tgamma_large_negative(const B& a)\n+        inline B tgamma_large_negative(const B& a)\n         {\n             B st = stirling(a);\n             B p = floor(a);\n@@ -2148,7 +2208,7 @@ namespace xsimd {\n         }\n \n         template <class B, class BB>\n-        B tgamma_other(const B& a, const BB& test)\n+        inline B tgamma_other(const B& a, const BB& test)\n         {\n             B x = select(test, B(2.), a);\n #ifndef XSIMD_NO_INFINITIES\n@@ -2186,7 +2246,8 @@ namespace xsimd {\n         }\n     }\n \n-    template<class A, class T> batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n             auto nan_result = (self < batch_type(0.) && is_flint(self));\n #ifndef XSIMD_NO_INVALIDS\n--- include/xsimd/arch/generic/xsimd_generic_memory.hpp\n@@ -26,7 +26,8 @@ namespace xsimd {\n     using namespace types;\n \n     // extract_pair\n-    template<class A, class T> batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {\n       constexpr std::size_t size = batch<T, A>::size;\n       assert(0<= i && i< size && \"index in bounds\");\n \n@@ -52,13 +53,13 @@ namespace xsimd {\n     // load_aligned\n     namespace detail {\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n+      inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n         using batch_type_in = batch<T_in, A>;\n         using batch_type_out = batch<T_out, A>;\n         return fast_cast(batch_type_in::load_aligned(mem), batch_type_out(), A{});\n       }\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {\n+      inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {\n         static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n         using batch_type_out = batch<T_out, A>;\n         alignas(A::alignment()) T_out buffer[batch_type_out::size];\n@@ -67,33 +68,33 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T_in, class T_out>\n-    batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n+    inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n       return detail::load_aligned<A>(mem, cvt, A{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // load_unaligned\n     namespace detail {\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n+      inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n         using batch_type_in = batch<T_in, A>;\n         using batch_type_out = batch<T_out, A>;\n         return fast_cast(batch_type_in::load_unaligned(mem), batch_type_out(), A{});\n       }\n \n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {\n+      inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {\n         static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n         return load_aligned<A>(mem, cvt, generic{}, with_slow_conversion{});\n       }\n     }\n     template<class A, class T_in, class T_out>\n-    batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n+    inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n       return detail::load_unaligned<A>(mem, cvt, generic{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // store\n     template<class T, class A>\n-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {\n+    inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       constexpr auto size = batch_bool<T, A>::size;\n       alignas(A::alignment()) T buffer[size];\n@@ -104,43 +105,45 @@ namespace xsimd {\n \n \n     // store_aligned\n-    template<class A, class T_in, class T_out> void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n+    template<class A, class T_in, class T_out>\n+    inline void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n       alignas(A::alignment()) T_in buffer[batch<T_in, A>::size];\n       store_aligned(&buffer[0], self);\n       std::copy(std::begin(buffer), std::end(buffer), mem);\n     }\n \n     // store_unaligned\n-    template<class A, class T_in, class T_out> void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n+    template<class A, class T_in, class T_out>\n+    inline void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n       return store_aligned<A>(mem, self, generic{});\n     }\n \n     namespace detail\n     {\n         template <class A, class T>\n-        batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"load_complex not implemented for the required architecture\");\n         }\n \n         template <class A, class T>\n-        batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n+        inline batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"complex_high not implemented for the required architecture\");\n         }\n \n         template <class A, class T>\n-        batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n+        inline batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"complex_low not implemented for the required architecture\");\n         }\n     }\n \n     // load_complex_aligned\n     template <class A, class T_out, class T_in>\n-    batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {\n+    inline batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {\n       using real_batch = batch<T_out, A>;\n       T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n       real_batch hi = real_batch::load_aligned(buffer),\n@@ -150,7 +153,7 @@ namespace xsimd {\n \n     // load_complex_unaligned\n     template <class A, class T_out, class T_in>\n-    batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {\n+    inline batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {\n       using real_batch = batch<T_out, A>;\n       T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n       real_batch hi = real_batch::load_unaligned(buffer),\n@@ -160,7 +163,7 @@ namespace xsimd {\n \n     // store_complex_aligned\n     template <class A, class T_out, class T_in>\n-    void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n+    inline void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n         using real_batch = batch<T_in, A>;\n         real_batch hi = detail::complex_high(src, A{});\n         real_batch lo = detail::complex_low(src, A{});\n@@ -171,7 +174,7 @@ namespace xsimd {\n \n     // store_compelx_unaligned\n     template <class A, class T_out, class T_in>\n-    void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n+    inline void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n         using real_batch = batch<T_in, A>;\n         real_batch hi = detail::complex_high(src, A{});\n         real_batch lo = detail::complex_low(src, A{});\n--- include/xsimd/arch/generic/xsimd_generic_rounding.hpp\n@@ -23,20 +23,23 @@ namespace xsimd {\n     using namespace types;\n \n     // ceil\n-    template<class A, class T> batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {\n       batch<T, A> truncated_self = trunc(self);\n       return select(truncated_self < self, truncated_self + 1, truncated_self);\n     }\n \n \n     // floor\n-    template<class A, class T> batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {\n       batch<T, A> truncated_self = trunc(self);\n       return select(truncated_self > self, truncated_self - 1, truncated_self);\n     }\n \n     // round\n-    template<class A, class T> batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {\n       auto v = abs(self);\n       auto c = ceil(v);\n       auto cp = select(c - 0.5 > v, c - 1, c);\n@@ -45,13 +48,15 @@ namespace xsimd {\n \n     // trunc\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {\n       return select(abs(self) < constants::maxflint<batch<float, A>>(), to_float(to_int(self)), self);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {\n       return select(abs(self) < constants::maxflint<batch<double, A>>(), to_float(to_int(self)), self);\n     }\n \n--- include/xsimd/arch/generic/xsimd_generic_trigo.hpp\n@@ -30,7 +30,8 @@ namespace xsimd {\n     using namespace types;\n \n     // acos\n-    template<class A, class T> batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n         batch_type x = abs(self);\n         auto x_larger_05 = x > batch_type(0.5);\n@@ -41,7 +42,7 @@ namespace xsimd {\n         return select(x_larger_05, x, constants::pio2<batch_type>() - x);\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -59,7 +60,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = self - batch_type(1.);\n                 auto test = x > constants::oneotwoeps<batch_type>();\n@@ -77,7 +79,8 @@ namespace xsimd {\n         }\n \n     // asin\n-    template<class A> batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 batch_type sign = bitofsign(self);\n@@ -94,7 +97,8 @@ namespace xsimd {\n                 z = select(x_larger_05, constants::pio2<batch_type>() - (z1 + z1), z1);\n                 return z ^ sign;\n     }\n-    template<class A> batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 auto small_cond = x < constants::sqrteps<batch_type>();\n@@ -137,7 +141,7 @@ namespace xsimd {\n                                   bitofsign(self));\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -168,29 +172,30 @@ namespace xsimd {\n          */\n     namespace detail {\n         template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-          batch<T, A>\n+          inline batch<T, A>\n             average(const batch<T, A>& x1, const batch<T, A>& x2)\n             {\n                 return (x1 & x2) + ((x1 ^ x2) >> 1);\n             }\n \n         template <class A, class T>\n-          batch<T, A>\n+          inline batch<T, A>\n             averagef(const batch<T, A>& x1, const batch<T, A>& x2)\n             {\n               using batch_type = batch<T, A>;\n                 return fma(x1, batch_type(0.5), x2 * batch_type(0.5));\n             }\n         template<class A>\n-          batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {\n+          inline batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {\n             return averagef(x1, x2);\n           }\n         template<class A>\n-          batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {\n+          inline batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {\n             return averagef(x1, x2);\n           }\n     }\n-    template<class A> batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 auto lthalf = x < batch_type(0.5);\n@@ -216,7 +221,8 @@ namespace xsimd {\n                 return select(lthalf, z, log(tmp) + constants::log_2<batch_type>()) ^ bts;\n #endif\n     }\n-    template<class A> batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 auto test = x > constants::oneosqrteps<batch_type>();\n@@ -289,14 +295,15 @@ namespace xsimd {\n                 return yy + z;\n             }\n     }\n-    template<class A, class T> batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type absa = abs(self);\n                 const batch_type x = detail::kernel_atan(absa, batch_type(1.) / absa);\n                 return x ^ bitofsign(self);\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -326,7 +333,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = abs(self);\n                 batch_type t = x + x;\n@@ -345,7 +353,8 @@ namespace xsimd {\n         }\n \n     // atan2\n-    template<class A, class T> batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type q = abs(self / other);\n                 const batch_type z = detail::kernel_atan(q, batch_type(1.) / q);\n@@ -357,17 +366,17 @@ namespace xsimd {\n     namespace detail\n     {\n         template <class T, class A>\n-        batch<T, A> quadrant(const batch<T, A>& x) {\n+        inline batch<T, A> quadrant(const batch<T, A>& x) {\n           return x & batch<T, A>(3);\n         }\n \n         template <class A>\n-        batch<float, A> quadrant(const batch<float, A>& x) {\n+        inline batch<float, A> quadrant(const batch<float, A>& x) {\n           return to_float(quadrant(to_int(x)));\n         }\n \n         template <class A>\n-        batch<double, A> quadrant(const batch<double, A>& x) {\n+        inline batch<double, A> quadrant(const batch<double, A>& x) {\n           using batch_type = batch<double, A>;\n                 batch_type a = x * batch_type(0.25);\n                 return (a - floor(a)) * batch_type(4.);\n@@ -610,7 +619,8 @@ namespace xsimd {\n         };\n \n     }\n-    template<class A, class T> batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -625,7 +635,8 @@ namespace xsimd {\n                 return z1 ^ sign_bit;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return {cos(z.real()) * cosh(z.imag()), -sin(z.real()) * sinh(z.imag())};\n     }\n \n@@ -641,7 +652,8 @@ namespace xsimd {\n       * ====================================================\n       */\n \n-    template<class A, class T> batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = abs(self);\n                 auto test1 = x > (constants::maxlog<batch_type>() - constants::log_2<batch_type>());\n@@ -661,7 +673,8 @@ namespace xsimd {\n \n     // sin\n     namespace detail {\n-    template<class A, class T, class Tag=trigo_radian_tag> batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {\n+    template<class A, class T, class Tag=trigo_radian_tag>\n+    inline batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -677,16 +690,19 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A, class T> batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::sin(self);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return {sin(z.real()) * cosh(z.imag()), cos(z.real()) * sinh(z.imag())};\n     }\n \n     // sincos\n-    template<class A, class T> std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -704,7 +720,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T>\n-    std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n+    inline std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n     sincos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -726,7 +742,8 @@ namespace xsimd {\n          * (See copy at http://boost.org/LICENSE_1_0.txt)\n          * ====================================================\n          */\n-    template<class A> batch<float, A> sinh_kernel(batch<float, A> const& self) {\n+    template<class A>\n+    inline batch<float, A> sinh_kernel(batch<float, A> const& self) {\n       using batch_type = batch<float, A>;\n                 batch_type sqr_self = self * self;\n                 return detail::horner<batch_type,\n@@ -738,7 +755,8 @@ namespace xsimd {\n                     self;\n     }\n \n-    template<class A> batch<double, A> sinh_kernel(batch<double, A> const& self) {\n+    template<class A>\n+    inline batch<double, A> sinh_kernel(batch<double, A> const& self) {\n       using batch_type = batch<double, A>;\n                 batch_type sqrself = self * self;\n                 return fma(self, (detail::horner<batch_type,\n@@ -765,7 +783,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {\n                 using batch_type = batch<T, A>;\n                 batch_type half(0.5);\n                 batch_type x = abs(a);\n@@ -794,7 +813,8 @@ namespace xsimd {\n         }\n \n     // tan\n-    template<class A, class T> batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -805,7 +825,8 @@ namespace xsimd {\n                 const batch_type y = detail::tan_eval(xr, test);\n                 return y ^ bitofsign(self);\n     }\n-    template<class A, class T> batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n             real_batch d = cos(2 * z.real()) + cosh(2 * z.imag());\n@@ -901,7 +922,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type one(1.);\n                 batch_type x = abs(self);\n--- include/xsimd/arch/xsimd_avx.hpp\n@@ -32,15 +32,15 @@ namespace xsimd {\n         return _mm256_insertf128_si256(_mm256_castsi128_si256(low), high, 1);\n       }\n       template <class F>\n-      __m256i fwd_to_sse(F f, __m256i self) {\n+      inline __m256i fwd_to_sse(F f, __m256i self) {\n         __m128i self_low, self_high;\n         split_avx(self, self_low, self_high);\n         __m128i res_low = f(self_low);\n         __m128i res_high = f(self_high);\n         return merge_sse(res_low, res_high);\n       }\n       template<class F>\n-      __m256i fwd_to_sse(F f, __m256i self, __m256i other) {\n+      inline __m256i fwd_to_sse(F f, __m256i self, __m256i other) {\n         __m128i self_low, self_high, other_low, other_high;\n         split_avx(self, self_low, self_high);\n         split_avx(other, other_low, other_high);\n@@ -49,7 +49,7 @@ namespace xsimd {\n         return merge_sse(res_low, res_high);\n       }\n       template<class F>\n-      __m256i fwd_to_sse(F f, __m256i self, int32_t other) {\n+      inline __m256i fwd_to_sse(F f, __m256i self, int32_t other) {\n         __m128i self_low, self_high;\n         split_avx(self, self_low, self_high);\n         __m128i res_low = f(self_low, other);\n@@ -59,230 +59,260 @@ namespace xsimd {\n     }\n \n     // abs\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {\n       __m256 sign_mask = _mm256_set1_ps(-0.f);  // -0.f = 1 << 31\n       return _mm256_andnot_ps(sign_mask, self);\n     }\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {\n       __m256d sign_mask = _mm256_set1_pd(-0.f);  // -0.f = 1 << 31\n       return _mm256_andnot_pd(sign_mask, self);\n     }\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, __m128i o) { return add(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_add_ps(self, other);\n     }\n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_add_pd(self, other);\n     }\n \n     // all\n-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool all(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return _mm256_testc_ps(self, batch_bool<float, A>(true)) != 0;\n     }\n-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool all(batch_bool<double, A> const& self, requires_arch<avx>) {\n       return _mm256_testc_pd(self, batch_bool<double, A>(true)) != 0;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool all(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<avx>) {\n                 return _mm256_testc_si256(self, batch_bool<T, A>(true)) != 0;\n     }\n \n     // any\n-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool any(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return !_mm256_testz_ps(self, self);\n     }\n-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool any(batch_bool<double, A> const& self, requires_arch<avx>) {\n       return !_mm256_testz_pd(self, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<avx>) {\n                 return !_mm256_testz_si256(self, self);\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_and_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_and_pd(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_and_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_and_pd(self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_pd(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_pd(self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, int32_t o) { return bitwise_lshift(batch<T, sse4_2>(s), o, sse4_2{}); },self, other);\n     }\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s) { return bitwise_not(batch<T, sse4_2>(s), sse4_2{}); }, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s) { return bitwise_not(batch_bool<T, sse4_2>(s), sse4_2{}); }, self);\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_or_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_or_pd(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_or_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_or_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_or(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_or(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, int32_t o) { return bitwise_rshift(batch<T, sse4_2>(s), o, sse4_2{}); }, self, other);\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_xor(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2{}); },\n                                 self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_xor(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o), sse4_2{}); },\n                                 self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n       return _mm256_castsi256_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n       return _mm256_castsi256_pd(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n       return _mm256_castps_pd(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n       return _mm256_castps_si256(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n       return _mm256_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n       return _mm256_castpd_si256(self);\n     }\n \n     // bitwise_not\n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n     }\n-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {\n+    inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {\n         return _mm256_castps_si256(self);\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {\n         return _mm256_castsi256_ps(self);\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {\n         return _mm256_castpd_si256(self);\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {\n         return _mm256_castsi256_pd(self);\n     }\n \n     // broadcast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<avx>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_set1_epi8(val);\n         case 2: return _mm256_set1_epi16(val);\n@@ -291,18 +321,22 @@ namespace xsimd {\n         default: assert(false && \"unsupported\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> broadcast(float val, requires_arch<avx>) {\n       return _mm256_set1_ps(val);\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> broadcast(double val, requires_arch<avx>) {\n       return _mm256_set1_pd(val);\n     }\n \n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_ceil_ps(self);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_ceil_pd(self);\n     }\n \n@@ -336,98 +370,117 @@ namespace xsimd {\n         }\n \n         // complex_low\n-        template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_f<0>(self.real(), self.imag());\n         }\n-        template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_d<0>(self.real(), self.imag());\n         }\n \n         // complex_high\n-        template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_f<1>(self.real(), self.imag());\n         }\n-        template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_d<1>(self.real(), self.imag());\n         }\n     }\n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {\n       return _mm256_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {\n       return _mm256_cvttps_epi32(self);\n     }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_div_pd(self, other);\n     }\n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return  _mm256_castsi256_ps(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); },\n                                   _mm256_castps_si256(self), _mm256_castps_si256(other)));\n     }\n-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return  _mm256_castsi256_pd(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); }, _mm256_castpd_si256(self), _mm256_castpd_si256(other)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2{}); },self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return eq(batch<T, A>(self.data), batch<T, A>(other.data));\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_floor_ps(self);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_floor_pd(self);\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_GE_OQ);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_GE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return ge(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // gt\n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_GT_OQ);\n     }\n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_GT_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return gt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& rhs, requires_arch<avx>) {\n                 // Warning about _mm256_hadd_ps:\n                 // _mm256_hadd_ps(a,b) gives\n                 // (a0+a1,a2+a3,b0+b1,b2+b3,a4+a5,a6+a7,b4+b5,b6+b7). Hence we can't\n@@ -445,7 +498,7 @@ namespace xsimd {\n \n     }\n     template <class A>\n-    double hadd(batch<double, A> const &rhs, requires_arch<avx>) {\n+    inline double hadd(batch<double, A> const &rhs, requires_arch<avx>) {\n                 // rhs = (x0, x1, x2, x3)\n                 // tmp = (x2, x3, x0, x1)\n                 __m256d tmp = _mm256_permute2f128_pd(rhs, rhs, 1);\n@@ -456,15 +509,16 @@ namespace xsimd {\n                 return _mm_cvtsd_f64(_mm256_extractf128_pd(tmp, 0));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx>) {\n       __m128i low, high;\n       detail::split_avx(self, low, high);\n       batch<T, sse4_2> blow(low), bhigh(high);\n       return hadd(blow) + hadd(bhigh);\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {\n                 // row = (a,b,c,d,e,f,g,h)\n                 // tmp0 = (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)\n                 __m256 tmp0 = _mm256_hadd_ps(row[0], row[1]);\n@@ -489,7 +543,7 @@ namespace xsimd {\n                 return _mm256_add_ps(tmp0, tmp1);\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {\n                 // row = (a,b,c,d)\n                 // tmp0 = (a0+a1, b0+b1, a2+a3, b2+b3)\n                 __m256d tmp0 = _mm256_hadd_pd(row[0], row[1]);\n@@ -503,37 +557,44 @@ namespace xsimd {\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {\n                 return _mm256_cmp_ps(self, self, _CMP_UNORD_Q);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {\n                 return _mm256_cmp_pd(self, self, _CMP_UNORD_Q);\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_LE_OQ);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_LE_OQ);\n     }\n \n     // load_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) {\n       return _mm256_load_si256((__m256i const*)mem);\n     }\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {\n       return _mm256_load_ps(mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {\n       return _mm256_load_pd(mem);\n     }\n \n     namespace detail\n     {\n     // load_complex\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {\n             using batch_type = batch<float, A>;\n             __m128 tmp0 = _mm256_extractf128_ps(hi, 0);\n             __m128 tmp1 = _mm256_extractf128_ps(hi, 1);\n@@ -551,7 +612,8 @@ namespace xsimd {\n             imag = _mm256_insertf128_ps(imag, tmp_imag, 1);\n             return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {\n             using batch_type = batch<double, A>;\n             __m128d tmp0 = _mm256_extractf128_pd(hi, 0);\n             __m128d tmp1 = _mm256_extractf128_pd(hi, 1);\n@@ -570,115 +632,133 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) {\n       return _mm256_loadu_si256((__m256i const*)mem);\n     }\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){\n       return _mm256_loadu_ps(mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){\n       return _mm256_loadu_pd(mem);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_LT_OQ);\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_LT_OQ);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return lt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_max_ps(self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_max_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return select(self > other, self, other);\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_min_ps(self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_min_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return select(self <= other, self, other);\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_mul_pd(self, other);\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n \n     // neg\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) {\n       return 0 - self;\n     }\n     template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)));\n     }\n     template <class A>\n-    batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {\n+    inline batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)));\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_NEQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_NEQ_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return ~(self == other);\n     }\n \n \n-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n         return ~(self == other);\n     }\n \n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       if(std::is_signed<T>::value) {\n         auto mask = (other >> (8 * sizeof(T) - 1));\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -693,14 +773,16 @@ namespace xsimd {\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {\n                 return _mm256_blendv_ps(false_br, true_br, cond);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {\n                 return _mm256_blendv_pd(false_br, true_br, cond);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n       __m128i cond_low, cond_hi;\n       detail::split_avx(cond, cond_low, cond_hi);\n \n@@ -715,75 +797,79 @@ namespace xsimd {\n       return detail::merge_sse(res_low, res_hi);\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, avx2{});\n     }\n \n \n     // set\n     template<class A, class... Values>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n       return _mm256_setr_ps(values...);\n     }\n \n     template<class A, class... Values>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n       return _mm256_setr_pd(values...);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) {\n       return _mm256_set_epi64x(v3, v2, v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm256_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm256_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n       T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31  ) {\n       return _mm256_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n     }\n \n     template<class A, class T, class... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) {\n       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;\n     }\n \n     template<class A, class... Values>\n-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n       return _mm256_castsi256_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     template<class A, class... Values>\n-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n       return _mm256_castsi256_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {\n       return _mm256_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {\n       return _mm256_sqrt_pd(val);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -795,55 +881,61 @@ namespace xsimd {\n \n     // store_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n       return _mm256_store_si256((__m256i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n       return _mm256_store_si256((__m256i *)mem, self);\n     }\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_store_ps(mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_store_pd(mem, self);\n     }\n \n     // store_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_si256((__m256i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_si256((__m256i *)mem, self);\n     }\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_ps(mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_pd(mem, self);\n     }\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, __m128i o) { return sub(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_ps(self, other);\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {\n       return _mm256_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -852,29 +944,31 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n       return {(int64_t)buffer[0], (int64_t)buffer[1], (int64_t)buffer[2], (int64_t)buffer[3]};\n     }\n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_round_ps(self, _MM_FROUND_TO_ZERO);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_round_pd(self, _MM_FROUND_TO_ZERO);\n     }\n \n     // zip_hi\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_unpackhi_epi8(self, other);\n         case 2: return _mm256_unpackhi_epi16(self, other);\n@@ -883,16 +977,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_unpackhi_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_unpacklo_epi8(self, other);\n         case 2: return _mm256_unpacklo_epi16(self, other);\n@@ -901,10 +997,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_unpacklo_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_unpacklo_pd(self, other);\n     }\n \n--- include/xsimd/arch/xsimd_avx2.hpp\n@@ -25,7 +25,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_abs_epi8(self);\n@@ -39,7 +39,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_add_epi8(self, other);\n         case 2: return _mm256_add_epi16(self, other);\n@@ -51,37 +51,37 @@ namespace xsimd {\n \n     // bitwise_and\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_and_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_and_si256(self, other);\n     }\n \n     // bitwise_andnot\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_andnot_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_andnot_si256(self, other);\n     }\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 2: return _mm256_slli_epi16(self, other);\n         case 4: return _mm256_slli_epi32(self, other);\n@@ -91,7 +91,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 4: return _mm256_sllv_epi32(self, other);\n         case 8: return _mm256_sllv_epi64(self, other);\n@@ -101,17 +101,17 @@ namespace xsimd {\n \n     // bitwise_or\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_or_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_or_si256(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: {\n@@ -139,7 +139,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm256_srav_epi32(self, other);\n@@ -157,31 +157,33 @@ namespace xsimd {\n \n     // bitwise_xor\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, other);\n     }\n \n     // complex_low\n-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n+    template<class A> batch<double, A>\n+    inline complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 1, 1, 0));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(1, 2, 0, 0));\n             return _mm256_blend_pd(tmp0, tmp1, 10);\n     }\n \n     // complex_high\n-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n+    template<class A> batch<double, A>\n+    inline complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 3, 1, 2));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(3, 2, 2, 0));\n             return _mm256_blend_pd(tmp0, tmp1, 10);\n     }\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_cmpeq_epi8(self, other);\n         case 2: return _mm256_cmpeq_epi16(self, other);\n@@ -193,7 +195,7 @@ namespace xsimd {\n \n     // gt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_cmpgt_epi8(self, other);\n@@ -210,7 +212,7 @@ namespace xsimd {\n \n     // hadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 4:\n           {\n@@ -240,7 +242,8 @@ namespace xsimd {\n       }\n     }\n     // load_complex\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {\n+    template<class A> batch<std::complex<float>, A>\n+    inline load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {\n             using batch_type = batch<float, A>;\n             batch_type real = _mm256_castpd_ps(\n                          _mm256_permute4x64_pd(\n@@ -252,7 +255,8 @@ namespace xsimd {\n                              _MM_SHUFFLE(3, 1, 2, 0)));\n             return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {\n             using batch_type = batch<double, A>;\n             batch_type real = _mm256_permute4x64_pd(_mm256_unpacklo_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n             batch_type imag = _mm256_permute4x64_pd(_mm256_unpackhi_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n@@ -261,7 +265,7 @@ namespace xsimd {\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_max_epi8(self, other);\n@@ -282,7 +286,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1:  return _mm256_min_epi8(self, other);\n@@ -303,7 +307,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 2: return _mm256_mullo_epi16(self, other);\n         case 4: return _mm256_mullo_epi32(self, other);\n@@ -313,7 +317,7 @@ namespace xsimd {\n \n     // sadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_adds_epi8(self, other);\n@@ -332,7 +336,7 @@ namespace xsimd {\n \n     // select\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_blendv_epi8(false_br, true_br, cond);\n         case 2: return _mm256_blendv_epi8(false_br, true_br, cond);\n@@ -342,7 +346,7 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n       constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();\n       switch(sizeof(T)) {\n         // FIXME: for some reason mask here is not considered as an immediate,\n@@ -359,7 +363,7 @@ namespace xsimd {\n \n     // ssub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_subs_epi8(self, other);\n@@ -378,7 +382,7 @@ namespace xsimd {\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_sub_epi8(self, other);\n         case 2: return _mm256_sub_epi16(self, other);\n@@ -387,9 +391,6 @@ namespace xsimd {\n         default: return sub(self, other, avx{});\n       }\n     }\n-\n-\n-\n   }\n \n }\n--- include/xsimd/arch/xsimd_avx512bw.hpp\n@@ -23,7 +23,7 @@ namespace xsimd {\n \n     namespace detail {\n     template<class A, class T,  int Cmp>\n-    batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {\n+    inline batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n@@ -46,7 +46,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) {\n       if(std::is_unsigned<T>::value)\n         return self;\n \n@@ -59,7 +59,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_add_epi8(self, other);\n         case 2: return _mm512_add_epi16(self, other);\n@@ -69,7 +69,7 @@ namespace xsimd {\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n         case 2: return _mm512_sllv_epi16(self, _mm512_set1_epi16(other));\n@@ -82,7 +82,7 @@ namespace xsimd {\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1:\n@@ -120,38 +120,38 @@ namespace xsimd {\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_EQ>(self, other);\n     }\n \n     // ge\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GE>(self, other);\n     }\n \n     // gt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GT>(self, other);\n     }\n \n \n     // le\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LE>(self, other);\n     }\n \n     // lt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LT>(self, other);\n     }\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_max_epi8(self, other);\n@@ -170,7 +170,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_min_epi8(self, other);\n@@ -190,7 +190,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: {\n                 __m512i upper = _mm512_and_si512(_mm512_mullo_epi16(self, other), _mm512_srli_epi16(_mm512_set1_epi16(-1), 8));\n@@ -205,13 +205,13 @@ namespace xsimd {\n \n     // neq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_NE>(self, other);\n     }\n \n     // sadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_adds_epi8(self, other);\n@@ -230,7 +230,7 @@ namespace xsimd {\n \n     // select\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_mask_blend_epi8(cond, false_br, true_br);\n         case 2: return _mm512_mask_blend_epi16(cond, false_br, true_br);\n@@ -241,7 +241,7 @@ namespace xsimd {\n \n     // ssub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_subs_epi8(self, other);\n@@ -261,7 +261,7 @@ namespace xsimd {\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_sub_epi8(self, other);\n         case 2: return _mm512_sub_epi16(self, other);\n--- include/xsimd/arch/xsimd_avx512dq.hpp\n@@ -20,51 +20,60 @@ namespace xsimd {\n     using namespace types;\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_and_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_and_pd(self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_andnot_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_andnot_pd(self, other);\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_or_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_or_pd(self, other);\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_xor_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_xor_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {\n       return _mm512_cvtepi64_pd(self);\n     }\n \n     // to_int\n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {\n       return _mm512_cvttpd_epi64(self);\n     }\n \n--- include/xsimd/arch/xsimd_avx512f.hpp\n@@ -75,39 +75,39 @@ namespace xsimd {\n \n     inline uint32_t morton(uint16_t x, uint16_t y) {\n \n-      static const unsigned short MortonTable256[256] = \n+      static const unsigned short MortonTable256[256] =\n       {\n-        0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015, \n-        0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055, \n-        0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115, \n-        0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155, \n-        0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415, \n-        0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455, \n-        0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515, \n-        0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555, \n-        0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015, \n-        0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055, \n-        0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115, \n-        0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155, \n-        0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415, \n-        0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455, \n-        0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515, \n-        0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555, \n-        0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015, \n-        0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055, \n-        0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115, \n-        0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155, \n-        0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415, \n-        0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455, \n-        0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515, \n-        0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555, \n-        0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015, \n-        0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055, \n-        0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115, \n-        0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155, \n-        0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415, \n-        0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455, \n-        0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515, \n+        0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015,\n+        0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055,\n+        0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115,\n+        0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155,\n+        0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415,\n+        0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455,\n+        0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515,\n+        0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555,\n+        0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015,\n+        0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055,\n+        0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115,\n+        0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155,\n+        0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415,\n+        0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455,\n+        0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515,\n+        0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555,\n+        0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015,\n+        0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055,\n+        0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115,\n+        0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155,\n+        0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415,\n+        0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455,\n+        0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515,\n+        0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555,\n+        0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015,\n+        0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055,\n+        0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115,\n+        0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155,\n+        0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415,\n+        0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455,\n+        0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515,\n         0x5540, 0x5541, 0x5544, 0x5545, 0x5550, 0x5551, 0x5554, 0x5555\n       };\n \n@@ -119,7 +119,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T,  int Cmp>\n-    batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {\n+    inline batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n@@ -189,21 +189,23 @@ namespace xsimd {\n     }\n \n     // abs\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {\n       __m512 self_asf = (__m512)self;\n       __m512i self_asi = *reinterpret_cast<__m512i *>(&self_asf);\n       __m512i res_asi = _mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF), self_asi);\n       return *reinterpret_cast<__m512*>(&res_asi);\n     }\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {\n                 __m512d self_asd = (__m512d)self;\n                 __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asd);\n                 __m512i res_asi = _mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),\n                                                    self_asi);\n                 return *reinterpret_cast<__m512d*>(&res_asi);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) {\n       if(std::is_unsigned<T>::value)\n         return self;\n \n@@ -218,7 +220,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return detail::fwd_to_avx([](__m256i s, __m256i o) { return add(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n         case 2: return detail::fwd_to_avx([](__m256i s, __m256i o) { return add(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n@@ -227,68 +229,74 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_add_ps(self, other);\n     }\n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_add_pd(self, other);\n     }\n \n     // all\n     template<class A, class T>\n-    bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return self.data == register_type(-1);\n     }\n \n     // any\n     template<class A, class T>\n-    bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return self.data != register_type(0);\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_and_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_and_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_and_si512(self, other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data & other.data);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_andnot_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_andnot_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_andnot_si512(self, other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data & ~other.data);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n@@ -312,44 +320,48 @@ namespace xsimd {\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_xor_si512(self, _mm512_set1_epi32(-1));\n     }\n     template<class A, class T>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(~self.data);\n     }\n \n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_xor_ps(self, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {\n       return _mm512_xor_pd(self, _mm512_castsi512_pd(_mm512_set1_epi32(-1)));\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_or_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_or_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_or_si512(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n@@ -386,71 +398,78 @@ namespace xsimd {\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_xor_si512(self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx512f>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx512f>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n       return _mm512_castps_pd(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n       return _mm512_castps_si512(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n       return _mm512_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n       return _mm512_castpd_si512(self);\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n \n \n     // broadcast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<avx512f>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_set1_epi8(val);\n         case 2: return _mm512_set1_epi16(val);\n@@ -459,146 +478,168 @@ namespace xsimd {\n         default: assert(false && \"unsupported\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> broadcast(float val, requires_arch<avx512f>) {\n       return _mm512_set1_ps(val);\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx512f>) {\n+    template<class A> batch<double, A>\n+    inline broadcast(double val, requires_arch<avx512f>) {\n       return _mm512_set1_pd(val);\n     }\n \n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_ps(self, _MM_FROUND_TO_POS_INF);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_pd(self, _MM_FROUND_TO_POS_INF);\n     }\n \n \n     namespace detail\n     {\n     // complex_low\n-    template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi32(0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);\n         return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n     }\n-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi64(0, 8, 1, 9, 2, 10, 3, 11);\n         return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n     }\n \n     // complex_high\n-    template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi32(8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);\n         return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n     }\n-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi64(4, 12, 5, 13, 6, 14, 7, 15);\n         return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n     }\n     }\n \n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {\n       return _mm512_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {\n       return _mm512_cvttps_epi32(self);\n     }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_div_pd(self, other);\n     }\n \n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_EQ_OQ);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_EQ>(self, other);\n     }\n     template<class A, class T>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(~self.data ^ other.data);\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_ps(self, _MM_FROUND_TO_NEG_INF);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_pd(self, _MM_FROUND_TO_NEG_INF);\n     }\n \n     // from bool\n     template<class A, class T>\n-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return select(self, batch<T, A>(1), batch<T, A>(0));\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_GE_OQ);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_GE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_GE>(self, other);\n     }\n \n     // gt\n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_GT_OQ);\n     }\n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_GT_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_GT>(self, other);\n     }\n \n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {\n                 __m256 tmp1 = _mm512_extractf32x8_ps(rhs, 1);\n                 __m256 tmp2 = _mm512_extractf32x8_ps(rhs, 0);\n                 __m256 res1 = _mm256_add_ps(tmp1, tmp2);\n                 return hadd(batch<float, avx2>(res1), avx2{});\n \n     }\n     template <class A>\n-    double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {\n+    inline double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {\n                 __m256d tmp1 = _mm512_extractf64x4_pd(rhs, 1);\n                 __m256d tmp2 = _mm512_extractf64x4_pd(rhs, 0);\n                 __m256d res1 = _mm256_add_pd(tmp1, tmp2);\n                 return hadd(batch<double, avx2>(res1), avx2{});\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx512f>) {\n       __m256i low, high;\n       detail::split_avx512(self, low, high);\n       batch<T, avx2> blow(low), bhigh(high);\n       return hadd(blow, avx2{}) + hadd(bhigh, avx2{});\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {\n                 // The following folds over the vector once:\n                 // tmp1 = [a0..8, b0..8]\n                 // tmp2 = [a8..f, b8..f]\n@@ -656,8 +697,9 @@ namespace xsimd {\n                 concat = _mm512_insertf32x8(concat, halfx1, 1);\n                 return concat;\n     }\n+\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {\n #define step1(I, a, b)                                                   \\\n         batch<double, avx512f> res ## I;                                           \\\n         {                                                                    \\\n@@ -690,49 +732,57 @@ namespace xsimd {\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {\n                 return _mm512_cmp_ps_mask(self, self, _CMP_UNORD_Q);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {\n                 return _mm512_cmp_pd_mask(self, self, _CMP_UNORD_Q);\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_LE_OQ);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_LE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_LE>(self, other);\n     }\n \n \n     // load_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n       return _mm512_load_si512((__m512i const*)mem);\n     }\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {\n       return _mm512_load_ps(mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {\n       return _mm512_load_pd(mem);\n     }\n \n     // load_complex\n     namespace detail\n     {\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {\n         __m512i real_idx = _mm512_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);\n         __m512i imag_idx = _mm512_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);\n         auto real = _mm512_permutex2var_ps(hi, real_idx, lo);\n         auto imag = _mm512_permutex2var_ps(hi, imag_idx, lo);\n         return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {\n         __m512i real_idx = _mm512_setr_epi64(0, 2, 4, 6, 8, 10, 12, 14);\n         __m512i imag_idx = _mm512_setr_epi64(1, 3, 5, 7, 9, 11, 13, 15);\n         auto real = _mm512_permutex2var_pd(hi, real_idx, lo);\n@@ -743,39 +793,44 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n       return _mm512_loadu_si512((__m512i const*)mem);\n     }\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){\n       return _mm512_loadu_ps(mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){\n       return _mm512_loadu_pd(mem);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_LT_OQ);\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_LT_OQ);\n     }\n \n-\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_LT>(self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_max_ps(self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_max_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_max_epi32(self, other);\n@@ -793,14 +848,16 @@ namespace xsimd {\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_min_ps(self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_min_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_min_epi32(self, other);\n@@ -818,14 +875,16 @@ namespace xsimd {\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_mul_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_mullo_epi32(self, other);\n           case 8: return _mm512_mullo_epi64(self, other);\n@@ -834,46 +893,52 @@ namespace xsimd {\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n     }\n \n     // neg\n     template<class A, class T>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {\n       return 0 - self;\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_NEQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_NEQ_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n         return ~(self == other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data ^ other.data);\n     }\n \n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         auto mask = other < 0;\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -888,15 +953,17 @@ namespace xsimd {\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {\n                 return _mm512_mask_blend_ps(cond, false_br, true_br);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {\n                 return _mm512_mask_blend_pd(cond, false_br, true_br);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: {\n           alignas(avx2::alignment()) uint8_t buffer[64];\n@@ -935,8 +1002,9 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/type combination\"); return {};\n       };\n     }\n+\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, avx512f{});\n     }\n \n@@ -954,25 +1022,25 @@ namespace xsimd {\n \n     // set\n     template<class A>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {\n       return _mm512_setr_ps(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n \n     template<class A>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {\n       return _mm512_setr_pd(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm512_set_epi64(v7, v6, v5, v4, v3, v2, v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm512_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n     template<class A, class T, detail::enable_signed_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) {\n@@ -987,8 +1055,9 @@ namespace xsimd {\n                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n #endif\n     }\n+\n     template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) {\n@@ -1003,8 +1072,9 @@ namespace xsimd {\n                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n #endif\n     }\n+\n     template<class A, class T, detail::enable_signed_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n@@ -1030,7 +1100,7 @@ namespace xsimd {\n #endif\n     }\n     template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n@@ -1055,8 +1125,9 @@ namespace xsimd {\n           v48, v49, v50, v51, v52, v53, v54, v55, v56, v57, v58, v59, v60, v61, v62, v63);\n #endif\n     }\n+\n     template<class A, class T, class... Values>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<T, A>::size, \"consistent init\");\n       using register_type = typename batch_bool<T, A>::register_type;\n       register_type r = 0;\n@@ -1066,22 +1137,26 @@ namespace xsimd {\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {\n       return _mm512_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {\n       return _mm512_sqrt_pd(val);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -1093,7 +1168,7 @@ namespace xsimd {\n \n     // store\n     template<class T, class A>\n-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {\n+    inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       constexpr auto size = batch_bool<T, A>::size;\n       for(std::size_t i = 0; i < size; ++i)\n@@ -1102,39 +1177,43 @@ namespace xsimd {\n \n     // store_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_si512((__m512i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_si512((__m512i *)mem, self);\n     }\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_ps(mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_pd(mem, self);\n     }\n \n     // store_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_si512((__m512i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_si512((__m512i *)mem, self);\n     }\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_ps(mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_pd(mem, self);\n     }\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return detail::fwd_to_avx([](__m256i s, __m256i o) { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n         case 2: return detail::fwd_to_avx([](__m256i s, __m256i o) { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n@@ -1143,20 +1222,22 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_ps(self, other);\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {\n       return _mm512_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -1166,12 +1247,12 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -1180,16 +1261,18 @@ namespace xsimd {\n     }\n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n     }\n \n     // zip_hi\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_unpackhi_epi8(self, other);\n         case 2: return _mm512_unpackhi_epi16(self, other);\n@@ -1198,16 +1281,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpackhi_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_unpacklo_epi8(self, other);\n         case 2: return _mm512_unpacklo_epi16(self, other);\n@@ -1216,10 +1301,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpacklo_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpacklo_pd(self, other);\n     }\n \n--- include/xsimd/arch/xsimd_constants.hpp\n@@ -160,7 +160,7 @@ namespace constants {\n     }\n \n     template <class T>\n-    constexpr T allbits() noexcept\n+    inline constexpr T allbits() noexcept\n     {\n         return T(detail::allbits_impl<typename T::value_type>::get_value());\n     }\n@@ -170,19 +170,19 @@ namespace constants {\n      *****************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> mask1frexp() noexcept\n+    inline constexpr as_integer_t<T> mask1frexp() noexcept\n     {\n         return as_integer_t<T>(mask1frexp<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t mask1frexp<float>() noexcept\n+    inline constexpr int32_t mask1frexp<float>() noexcept\n     {\n         return 0x7f800000;\n     }\n \n     template <>\n-    constexpr int64_t mask1frexp<double>() noexcept\n+    inline constexpr int64_t mask1frexp<double>() noexcept\n     {\n         return 0x7ff0000000000000;\n     }\n@@ -192,19 +192,19 @@ namespace constants {\n      *****************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> mask2frexp() noexcept\n+    inline constexpr as_integer_t<T> mask2frexp() noexcept\n     {\n         return as_integer_t<T>(mask2frexp<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t mask2frexp<float>() noexcept\n+    inline constexpr int32_t mask2frexp<float>() noexcept\n     {\n         return 0x3f000000;\n     }\n \n     template <>\n-    constexpr int64_t mask2frexp<double>() noexcept\n+    inline constexpr int64_t mask2frexp<double>() noexcept\n     {\n         return 0x3fe0000000000000;\n     }\n@@ -214,19 +214,19 @@ namespace constants {\n      ******************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> maxexponent() noexcept\n+    inline constexpr as_integer_t<T> maxexponent() noexcept\n     {\n         return as_integer_t<T>(maxexponent<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t maxexponent<float>() noexcept\n+    inline constexpr int32_t maxexponent<float>() noexcept\n     {\n         return 127;\n     }\n \n     template <>\n-    constexpr int64_t maxexponent<double>() noexcept\n+    inline constexpr int64_t maxexponent<double>() noexcept\n     {\n         return 1023;\n     }\n@@ -236,19 +236,19 @@ namespace constants {\n      ******************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> maxexponentm1() noexcept\n+    inline constexpr as_integer_t<T> maxexponentm1() noexcept\n     {\n         return as_integer_t<T>(maxexponentm1<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t maxexponentm1<float>() noexcept\n+    inline constexpr int32_t maxexponentm1<float>() noexcept\n     {\n         return 126;\n     }\n \n     template <>\n-    constexpr int64_t maxexponentm1<double>() noexcept\n+    inline constexpr int64_t maxexponentm1<double>() noexcept\n     {\n         return 1022;\n     }\n@@ -258,19 +258,19 @@ namespace constants {\n      **********************/\n \n     template <class T>\n-    constexpr int32_t nmb() noexcept\n+    inline constexpr int32_t nmb() noexcept\n     {\n         return nmb<typename T::value_type>();\n     }\n \n     template <>\n-    constexpr int32_t nmb<float>() noexcept\n+    inline constexpr int32_t nmb<float>() noexcept\n     {\n         return 23;\n     }\n \n     template <>\n-    constexpr int32_t nmb<double>() noexcept\n+    inline constexpr int32_t nmb<double>() noexcept\n     {\n         return 52;\n     }\n@@ -280,7 +280,7 @@ namespace constants {\n      ***********************/\n \n     template <class T>\n-    constexpr T zero() noexcept\n+    inline constexpr T zero() noexcept\n     {\n         return T(typename T::value_type(0));\n     }\n@@ -346,7 +346,7 @@ namespace constants {\n     }\n \n     template <class T>\n-    constexpr T minvalue() noexcept\n+    inline constexpr T minvalue() noexcept\n     {\n         return T(detail::minvalue_impl<typename T::value_type>::get_value());\n     }\n@@ -356,7 +356,7 @@ namespace constants {\n      ***************************/\n \n     template <class T>\n-    constexpr T maxvalue() noexcept\n+    inline constexpr T maxvalue() noexcept\n     {\n         return T(std::numeric_limits<typename T::value_type>::max());\n     }\n@@ -365,4 +365,3 @@ namespace constants {\n }\n \n #endif\n-\n--- include/xsimd/arch/xsimd_fma3.hpp\n@@ -20,38 +20,46 @@ namespace xsimd {\n   namespace kernel {\n     using namespace types;\n     // fnma\n-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmadd_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fmadd_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fmsub_pd(x, y, z);\n     }\n \n--- include/xsimd/arch/xsimd_fma4.hpp\n@@ -21,38 +21,46 @@ namespace xsimd {\n     using namespace types;\n \n     // fnma\n-    template<class A> batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_nmacc_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_nmacc_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_nmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_nmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_macc_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_macc_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_msub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_msub_pd(x, y, z);\n     }\n   }\n--- include/xsimd/arch/xsimd_fma5.hpp\n@@ -21,38 +21,46 @@ namespace xsimd {\n     using namespace types;\n \n     // fnma\n-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmadd_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmadd_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmsub_pd(x, y, z);\n     }\n \n--- include/xsimd/arch/xsimd_generic_fwd.hpp\n@@ -19,14 +19,15 @@ namespace xsimd {\n   namespace kernel {\n     // forward declaration\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>);\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    template<class A, class T>\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n \n   }\n }\n--- include/xsimd/arch/xsimd_neon.hpp\n@@ -124,7 +124,7 @@ namespace xsimd\n \n             template <class T>\n             using identity_return_type = T;\n-            \n+\n             template <class... T>\n             struct neon_dispatcher_impl : neon_dispatcher_base<identity_return_type, T...>\n             {\n@@ -196,7 +196,7 @@ namespace xsimd\n             {\n                 using type = uint64x2_t;\n             };\n-            \n+\n             template <>\n             struct comp_return_type_impl<float32x4_t>\n             {\n@@ -254,55 +254,55 @@ namespace xsimd\n          *************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u8(uint8_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s8(int8_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u16(uint16_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s16(int16_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u32(uint32_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s32(int32_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u64(uint64_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s64(int64_t(val));\n         }\n \n         template <class A>\n-        batch<float, A> broadcast(float val, requires_arch<neon>)\n+        inline batch<float, A> broadcast(float val, requires_arch<neon>)\n         {\n             return vdupq_n_f32(val);\n         }\n@@ -312,27 +312,27 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>\n-        batch<T, A> set(batch<T, A> const&, requires_arch<neon>, Args... args)\n+        inline batch<T, A> set(batch<T, A> const&, requires_arch<neon>, Args... args)\n         {\n             return xsimd::types::detail::neon_vector_type<T>{args...};\n         }\n \n         template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>\n-        batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<neon>, Args... args)\n+        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<neon>, Args... args)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<T>;\n             return register_type{static_cast<unsigned_type>(args ? -1LL : 0LL)...};\n         }\n \n         template <class A>\n-        batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)\n+        inline batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)\n         {\n             return float32x4_t{f0, f1, f2, f3};\n         }\n \n         template <class A, class... Args>\n-        batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)\n+        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)\n         {\n             using register_type = typename batch_bool<float, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<float>;\n@@ -344,55 +344,55 @@ namespace xsimd\n          *************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u8(arg, vdupq_n_u8(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s8(reinterpret_cast<int8x16_t>(arg.data), vdupq_n_s8(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u16(arg, vdupq_n_u16(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s16(reinterpret_cast<int16x8_t>(arg.data), vdupq_n_s16(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u32(arg, vdupq_n_u32(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s32(reinterpret_cast<int32x4_t>(arg.data), vdupq_n_s32(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u64(arg, vdupq_n_u64(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s64(reinterpret_cast<int64x2_t>(arg.data), vdupq_n_s64(1));\n         }\n \n         template <class A>\n-        batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)\n+        inline batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)\n         {\n             return vreinterpretq_f32_u32(vandq_u32(arg, vreinterpretq_u32_f32(vdupq_n_f32(1.f))));\n         }\n@@ -402,56 +402,56 @@ namespace xsimd\n          ********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u8(src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s8(src);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u16(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s16(src);\n         }\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u32(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s32(src);\n         }\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u64(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s64(src);\n         }\n \n         template <class A>\n-        batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)\n+        inline batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)\n         {\n             return vld1q_f32(src);\n         }\n \n         template <class A, class T>\n-        batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return load_aligned<A>(src, convert<T>(), A{});\n         }\n@@ -461,61 +461,61 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u8(dst, src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s8(dst, src);\n         }\n-        \n+\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u16(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s16(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u32(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s32(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u64(dst, src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s64(dst, src);\n         }\n \n         template <class A>\n-        void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)\n         {\n             vst1q_f32(dst, src);\n         }\n \n         template <class A, class T>\n-        void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             store_aligned<A>(dst, src, A{});\n         }\n@@ -525,7 +525,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)\n+        inline batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)\n         {\n             using real_batch = batch<float, A>;\n             const float* buf = reinterpret_cast<const float*>(mem);\n@@ -536,7 +536,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)\n+        inline batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)\n         {\n             return load_complex_aligned<A>(mem, cvt, A{});\n         }\n@@ -546,7 +546,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n+        inline void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n         {\n             float32x4x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -556,7 +556,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n+        inline void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n         {\n             store_complex_aligned(dst, src, A{});\n         }\n@@ -566,55 +566,55 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u8_s8(vnegq_s8(vreinterpretq_s8_u8(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s8(rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u16_s16(vnegq_s16(vreinterpretq_s16_u16(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s16(rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u32_s32(vnegq_s32(vreinterpretq_s32_u32(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s32(rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch<T, A>({-rhs.get(0), -rhs.get(1)});\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch<T, A>({-rhs.get(0), -rhs.get(1)});\n         }\n         \n         template <class A>\n-        batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_f32(rhs);\n         }\n@@ -627,7 +627,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vaddq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -646,7 +646,7 @@ namespace xsimd\n         WRAP_BINARY_INT(vqaddq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -666,7 +666,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vsubq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -685,7 +685,7 @@ namespace xsimd\n         WRAP_BINARY_INT(vqsubq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -706,7 +706,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vmulq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher =\n@@ -723,20 +723,20 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T,  detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcvtq_s32_f32(vcvtq_f32_s32(lhs) / vcvtq_f32_s32(rhs));\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcvtq_u32_f32(vcvtq_f32_u32(lhs) / vcvtq_f32_u32(rhs));\n         }\n #endif\n \n         template <class A>\n-        batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             // from stackoverflow & https://projectne10.github.io/Ne10/doc/NE10__divc_8neon_8c_source.html\n             // get an initial estimate of 1/b.\n@@ -760,7 +760,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vceqq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -772,7 +772,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             using dispatcher_type = detail::neon_comp_dispatcher_impl<uint8x16_t, uint16x8_t, uint32x4_t>::binary;\n@@ -784,13 +784,13 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});\n         }\n@@ -803,7 +803,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcltq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -815,7 +815,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) < rhs.get(0), lhs.get(1) < rhs.get(1)});\n         }\n@@ -828,7 +828,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcleq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -840,7 +840,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) <= rhs.get(0), lhs.get(1) <= rhs.get(1)});\n         }\n@@ -853,7 +853,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcgtq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -865,7 +865,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) > rhs.get(0), lhs.get(1) > rhs.get(1)});\n         }\n@@ -878,7 +878,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcgeq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -890,7 +890,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) >= rhs.get(0), lhs.get(1) >= rhs.get(1)});\n         }\n@@ -923,14 +923,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_and_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_and_neon(register_type(lhs), register_type(rhs));\n@@ -951,7 +951,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_or_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_or_neon(V const& lhs, V const& rhs)\n             {\n                 const neon_dispatcher::binary dispatcher =\n                 {\n@@ -964,14 +964,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_or_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_or_neon(register_type(lhs), register_type(rhs));\n@@ -992,7 +992,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_xor_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_xor_neon(V const& lhs, V const& rhs)\n             {\n                 const neon_dispatcher::binary dispatcher =\n                 {\n@@ -1005,14 +1005,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_xor_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_xor_neon(register_type(lhs), register_type(rhs));\n@@ -1023,7 +1023,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T>\n-        batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             return bitwise_xor(lhs, rhs, A{});\n         }\n@@ -1052,7 +1052,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_not_neon(V const& arg)\n+            inline V bitwise_not_neon(V const& arg)\n             {\n                 const neon_dispatcher::unary dispatcher =\n                 {\n@@ -1066,14 +1066,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_not_neon(register_type(arg));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_not_neon(register_type(arg));\n@@ -1093,7 +1093,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_andnot_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_andnot_neon(V const& lhs, V const& rhs)\n             {\n                 const detail::neon_dispatcher::binary dispatcher =\n                 {\n@@ -1106,14 +1106,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_andnot_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_andnot_neon(register_type(lhs), register_type(rhs));\n@@ -1127,7 +1127,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vminq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher = \n@@ -1139,7 +1139,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return { std::min(lhs.get(0), rhs.get(0)), std::min(lhs.get(1), rhs.get(1)) };\n         }\n@@ -1152,7 +1152,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vmaxq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher = \n@@ -1164,7 +1164,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return { std::max(lhs.get(0), rhs.get(0)), std::max(lhs.get(1), rhs.get(1)) };\n         }\n@@ -1199,7 +1199,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> abs(batch<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::unary dispatcher = \n@@ -1215,7 +1215,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)\n+        inline batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             batch<float, A> sqrt_reciprocal = vrsqrteq_f32(arg);\n             // one iter\n@@ -1231,13 +1231,13 @@ namespace xsimd\n \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n+        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n         {\n             return vfmaq_f32(z, x, y);\n         }\n \n         template <class A>\n-        batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n+        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n         {\n             return vfmaq_f32(-z, x, y);\n         }\n@@ -1250,7 +1250,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A, class V>\n-            T sum_batch(V const& arg)\n+            inline T sum_batch(V const& arg)\n             {\n                 T res = T(0);\n                 for (std::size_t i = 0; i < batch<T, A>::size; ++i)\n@@ -1262,57 +1262,57 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vpadd_u8(vget_low_u8(arg), vget_high_u8(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int8x8_t tmp = vpadd_s8(vget_low_s8(arg), vget_high_s8(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vpadd_u16(vget_low_u16(arg), vget_high_u16(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int16x4_t tmp = vpadd_s16(vget_low_s16(arg), vget_high_s16(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vpadd_u32(vget_low_u32(arg), vget_high_u32(arg));\n             tmp = vpadd_u32(tmp, tmp);\n             return vget_lane_u32(tmp, 0);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int32x2_t tmp = vpadd_s32(vget_low_s32(arg), vget_high_s32(arg));\n             tmp = vpadd_s32(tmp, tmp);\n             return vget_lane_s32(tmp, 0);\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             return arg.get(0) + arg.get(1);\n         }\n \n         template <class A>\n-        float hadd(batch<float, A> const& arg, requires_arch<neon>)\n+        inline float hadd(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             float32x2_t tmp = vpadd_f32(vget_low_f32(arg), vget_high_f32(arg));\n             tmp = vpadd_f32(tmp, tmp);\n@@ -1324,7 +1324,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)\n+        inline batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)\n         {\n             // row = (a,b,c,d)\n             float32x2_t tmp1, tmp2, tmp3;\n@@ -1385,7 +1385,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<neon>)\n+        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<neon>)\n         {\n             using bool_register_type = typename batch_bool<T, A>::register_type;\n             using register_type = typename batch<T, A>::register_type;\n@@ -1399,7 +1399,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, bool... b, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> select(batch_bool_constant<batch<T, A>, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<neon>)\n+        inline batch<T, A> select(batch_bool_constant<batch<T, A>, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<neon>)\n         {\n             return select(batch_bool<T, A>{b...}, true_br, false_br, neon{});\n         }\n@@ -1409,61 +1409,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint8x8x2_t tmp = vzip_u8(vget_low_u8(lhs), vget_low_u8(rhs));\n             return vcombine_u8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int8x8x2_t tmp = vzip_s8(vget_low_s8(lhs), vget_low_s8(rhs));\n             return vcombine_s8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint16x4x2_t tmp = vzip_u16(vget_low_u16(lhs), vget_low_u16(rhs));\n             return vcombine_u16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int16x4x2_t tmp = vzip_s16(vget_low_s16(lhs), vget_low_s16(rhs));\n             return vcombine_s16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint32x2x2_t tmp = vzip_u32(vget_low_u32(lhs), vget_low_u32(rhs));\n             return vcombine_u32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int32x2x2_t tmp = vzip_s32(vget_low_s32(lhs), vget_low_s32(rhs));\n             return vcombine_s32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_u64(vget_low_u64(lhs), vget_low_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_s64(vget_low_s64(lhs), vget_low_s64(rhs));\n         }\n \n         template <class A>\n-        batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             float32x2x2_t tmp = vzip_f32(vget_low_f32(lhs), vget_low_f32(rhs));\n             return vcombine_f32(tmp.val[0], tmp.val[1]);\n@@ -1474,61 +1474,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint8x8x2_t tmp = vzip_u8(vget_high_u8(lhs), vget_high_u8(rhs));\n             return vcombine_u8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int8x8x2_t tmp = vzip_s8(vget_high_s8(lhs), vget_high_s8(rhs));\n             return vcombine_s8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint16x4x2_t tmp = vzip_u16(vget_high_u16(lhs), vget_high_u16(rhs));\n             return vcombine_u16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int16x4x2_t tmp = vzip_s16(vget_high_s16(lhs), vget_high_s16(rhs));\n             return vcombine_s16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint32x2x2_t tmp = vzip_u32(vget_high_u32(lhs), vget_high_u32(rhs));\n             return vcombine_u32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int32x2x2_t tmp = vzip_s32(vget_high_s32(lhs), vget_high_s32(rhs));\n             return vcombine_s32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_u64(vget_high_u64(lhs), vget_high_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_s64(vget_high_s64(lhs), vget_high_s64(rhs));\n         }\n \n         template <class A>\n-        batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             float32x2x2_t tmp = vzip_f32(vget_high_f32(lhs), vget_high_f32(rhs));\n             return vcombine_f32(tmp.val[0], tmp.val[1]);\n@@ -1541,14 +1541,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)\n+            inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)\n             {\n                 assert(false && \"extract_pair out of bounds\");\n                 return  batch<T, A>{};\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1561,7 +1561,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1574,7 +1574,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1587,7 +1587,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1600,7 +1600,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1613,7 +1613,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1626,7 +1626,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1639,7 +1639,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1652,7 +1652,7 @@ namespace xsimd\n             }\n \n             template <class A, size_t I, size_t... Is>\n-            batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1665,7 +1665,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t... Is>\n-            batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)\n+            inline batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -1679,7 +1679,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)\n+        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(0<= n && n< size && \"index in bounds\");\n@@ -1693,14 +1693,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n             {\n                 assert(false && \"bitwise_lshift out of bounds\");\n                 return batch<T, A>{};\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1713,7 +1713,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1726,7 +1726,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1739,7 +1739,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1752,7 +1752,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1765,7 +1765,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1778,7 +1778,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1791,7 +1791,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1804,7 +1804,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int... Is>\n-            batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n+            inline batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -1818,57 +1818,57 @@ namespace xsimd\n         }\n         \n         template <class A, class T>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0<= n && n< size && \"index in bounds\");\n             return detail::bitwise_lshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s64(lhs, rhs);\n         }\n@@ -1880,14 +1880,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n             {\n                 assert(false && \"bitwise_rshift out of bounds\");\n                 return batch<T, A>{};\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1900,7 +1900,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1913,7 +1913,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1926,7 +1926,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1939,7 +1939,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1952,7 +1952,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1965,7 +1965,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1978,7 +1978,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1991,7 +1991,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int... Is>\n-            batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n+            inline batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -2005,45 +2005,45 @@ namespace xsimd\n         }\n         \n         template <class A, class T>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0<= n && n< size && \"index in bounds\");\n             return detail::bitwise_rshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u8(lhs, vnegq_s8(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s8(lhs, vnegq_s8(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u16(lhs, vnegq_s16(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s16(lhs, vnegq_s16(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u32(lhs, vnegq_s32(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s32(lhs, vnegq_s32(rhs));\n         }\n@@ -2055,7 +2055,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vand_u8(vget_low_u8(arg), vget_high_u8(arg));\n             tmp = vpmin_u8(tmp, tmp);\n@@ -2065,7 +2065,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vand_u16(vget_low_u16(arg), vget_high_u16(arg));\n             tmp = vpmin_u16(tmp, tmp);\n@@ -2074,14 +2074,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vand_u32(vget_low_u32(arg), vget_high_u32(arg));\n             return vget_lane_u32(vpmin_u32(tmp, tmp), 0) != 0;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint64x1_t tmp = vand_u64(vget_low_u64(arg), vget_high_u64(arg));\n             return vget_lane_u64(tmp, 0) != 0;\n@@ -2092,7 +2092,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vorr_u8(vget_low_u8(arg), vget_high_u8(arg));\n             tmp = vpmax_u8(tmp, tmp);\n@@ -2102,7 +2102,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vorr_u16(vget_low_u16(arg), vget_high_u16(arg));\n             tmp = vpmax_u16(tmp, tmp);\n@@ -2111,14 +2111,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vorr_u32(vget_low_u32(arg), vget_high_u32(arg));\n             return vget_lane_u32(vpmax_u32(tmp, tmp), 0);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint64x1_t tmp = vorr_u64(vget_low_u64(arg), vget_high_u64(arg));\n             return bool(vget_lane_u64(tmp, 0));\n@@ -2171,7 +2171,7 @@ namespace xsimd\n             };\n \n             template <class R, class... T>\n-            const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))\n+            inline const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))\n             {\n                 return {std::make_tuple(arg...)};\n             }\n@@ -2208,7 +2208,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class R>\n-        batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)\n+        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)\n         {\n             const detail::neon_bitwise_caster caster = {\n                 std::make_tuple(\n@@ -2250,14 +2250,14 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<int32_t, A>::register_type;\n             return register_type(arg);\n         }\n \n         template <class A>\n-        batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<float, A>::register_type;\n             return register_type(arg);\n@@ -2268,7 +2268,7 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)\n+        inline batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)\n         {\n             return vcvtq_s32_f32(x);\n         }\n@@ -2278,7 +2278,7 @@ namespace xsimd\n          ************/\n \n         template <class A>\n-        batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)\n+        inline batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)\n         {\n             return vcvtq_f32_s32(x);\n         }\n@@ -2290,7 +2290,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class Tin, class Tout, class A>\n-            batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)\n+            inline batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)\n             {\n                 return bitwise_cast(in, out, A{});\n             }\n@@ -2301,7 +2301,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             return !(arg == arg);\n         }\n--- include/xsimd/arch/xsimd_neon64.hpp\n@@ -29,25 +29,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});\n         }\n@@ -57,25 +57,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});\n         }\n@@ -86,13 +86,13 @@ namespace xsimd\n \n         // Required to avoid ambiguous call\n         template <class A, class T>\n-        batch<T, A> broadcast(T val, requires_arch<neon64>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon64>)\n         {\n             return broadcast<neon64>(val, neon{});\n         }\n \n         template <class A>\n-        batch<double, A> broadcast(double val, requires_arch<neon64>)\n+        inline batch<double, A> broadcast(double val, requires_arch<neon64>)\n         {\n             return vdupq_n_f64(val);\n         }\n@@ -102,13 +102,13 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)\n+        inline batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)\n         {\n             return float64x2_t{d0, d1};\n         }\n \n         template <class A>\n-        batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)\n+        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<double>;\n@@ -121,7 +121,7 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)\n+        inline batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vandq_u64(arg, vreinterpretq_u64_f64(vdupq_n_f64(1.))));\n         }\n@@ -131,13 +131,13 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)\n+        inline batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)\n         {\n             return vld1q_f64(src);\n         }\n \n         template <class A>\n-        batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)\n+        inline batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)\n         {\n             return load_aligned<A>(src, convert<double>(), A{});\n         }\n@@ -147,13 +147,13 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n+        inline void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n         {\n             vst1q_f64(dst, src);\n         }\n \n         template <class A>\n-        void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n+        inline void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n         {\n             return store_aligned<A>(dst, src, A{});\n         }\n@@ -163,7 +163,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)\n+        inline batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)\n         {\n             using real_batch = batch<double, A>;\n             const double* buf = reinterpret_cast<const double*>(mem);\n@@ -174,7 +174,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)\n+        inline batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)\n         {\n             return load_complex_aligned<A>(mem, cvt, A{});\n         }\n@@ -184,7 +184,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)\n+        inline void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)\n         {\n             float64x2x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -194,7 +194,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)\n+        inline void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)\n         {\n             store_complex_aligned(dst, src, A{});\n         }\n@@ -204,19 +204,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_u64_s64(vnegq_s64(vreinterpretq_s64_u64(rhs)));\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vnegq_s64(rhs);\n         }\n \n         template <class A>\n-        batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vnegq_f64(rhs);\n         }\n@@ -226,7 +226,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vaddq_f64(lhs, rhs);\n         }\n@@ -236,7 +236,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return add(lhs, rhs, neon64{});\n         }\n@@ -246,7 +246,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vsubq_f64(lhs, rhs);\n         }\n@@ -256,7 +256,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return sub(lhs, rhs, neon64{});\n         }\n@@ -266,7 +266,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vmulq_f64(lhs, rhs);\n         }\n@@ -277,19 +277,19 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcvtq_u64_f64(vcvtq_f64_u64(lhs) / vcvtq_f64_u64(rhs));\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcvtq_s64_f64(vcvtq_f64_s64(lhs) / vcvtq_f64_s64(rhs));\n         }\n #endif\n         template <class A>\n-        batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vdivq_f64(lhs, rhs);\n         }\n@@ -299,37 +299,37 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_f64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n@@ -339,41 +339,41 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_f64(lhs, rhs);\n         }\n \n         /******\n          * le *\n          ******/\n-        \n+\n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_f64(lhs, rhs);\n         }\n@@ -383,19 +383,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_f64(lhs, rhs);\n         }\n@@ -405,19 +405,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_f64(lhs, rhs);\n         }\n@@ -427,14 +427,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vandq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vandq_u64(lhs, rhs);\n         }\n@@ -444,14 +444,14 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vorrq_u64(lhs, rhs);\n         }\n@@ -461,14 +461,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(veorq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return veorq_u64(lhs, rhs);\n         }\n@@ -478,7 +478,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return bitwise_xor(lhs, rhs, A{});\n         }\n@@ -488,13 +488,13 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u32(vmvnq_u32(vreinterpretq_u32_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return detail::bitwise_not_u64(rhs);\n         }\n@@ -504,14 +504,14 @@ namespace xsimd\n          ******************/\n \n         template <class A>\n-        batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vbicq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n         \n         template <class A>\n-        batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vbicq_u64(lhs, rhs);\n         }\n@@ -521,7 +521,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vminq_f64(lhs, rhs);\n         }\n@@ -531,7 +531,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vmaxq_f64(lhs, rhs);\n         }\n@@ -541,19 +541,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return rhs;\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vabsq_s64(rhs);\n         }\n \n         template <class A>\n-        batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vabsq_f64(rhs);\n         }\n@@ -563,7 +563,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vsqrtq_f64(rhs);\n         }\n@@ -574,13 +574,13 @@ namespace xsimd\n         \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n+        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n         {\n             return vfmaq_f64(z, x, y);\n         }\n \n         template <class A>\n-        batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n+        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n         {\n             return vfmaq_f64(-z, x, y);\n         }\n@@ -591,55 +591,55 @@ namespace xsimd\n          ********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u64(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s64(arg);\n         }\n \n         template <class A>\n-        double hadd(batch<double, A> const& arg, requires_arch<neon64>)\n+        inline double hadd(batch<double, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_f64(arg);\n         }\n@@ -649,7 +649,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)\n+        inline batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)\n         {\n             return vpaddq_f64(row[0], row[1]);\n         }\n@@ -659,13 +659,13 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)\n+        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)\n         {\n             return vbslq_f64(cond, a, b);\n         }\n \n         template <class A, bool... b>\n-        batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,\n+        inline batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,\n                                 batch<double, A> const& true_br,\n                                 batch<double, A> const& false_br,\n                                 requires_arch<neon64>)\n@@ -677,19 +677,19 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_f64(lhs, rhs);\n         }\n@@ -699,19 +699,19 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_f64(lhs, rhs);\n         }\n@@ -723,7 +723,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t I, size_t... Is>\n-            batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n+            inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n                                           ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n@@ -738,7 +738,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)\n+        inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)\n         {\n             constexpr std::size_t size = batch<double, A>::size;\n             assert(0<= n && n< size && \"index in bounds\");\n@@ -750,25 +750,25 @@ namespace xsimd\n          ******************/\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n         {\n             return bitwise_rshift<A>(lhs, n, neon{}); \n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>)\n         {\n             return vshlq_u64(lhs, vnegq_s64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n         {\n             return bitwise_rshift<A>(lhs, n, neon{}); \n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vshlq_s64(lhs, vnegq_s64(rhs));\n         }\n@@ -796,7 +796,7 @@ namespace xsimd\n         #undef WRAP_CAST\n \n         template <class A, class T>\n-        batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n         {\n             using caster_type = detail::bitwise_caster_impl<float64x2_t,\n                                                             uint8x16_t, int8x16_t,\n@@ -832,7 +832,7 @@ namespace xsimd\n         }\n \n         template <class A, class R>\n-        batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)\n+        inline batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)\n         {\n             using caster_type = detail::bitwise_caster_neon64<float64x2_t,\n                                                               uint8x16_t, int8x16_t,\n@@ -851,7 +851,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n         {\n             return arg;\n         }\n@@ -861,14 +861,14 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)\n         {\n             using register_type = typename batch_bool<int64_t, A>::register_type;\n             return register_type(arg);\n         }\n \n         template <class A>\n-        batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             return register_type(arg);\n@@ -879,7 +879,7 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)\n+        inline batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)\n         {\n             return vcvtq_s64_f64(x);\n         }\n@@ -889,7 +889,7 @@ namespace xsimd\n          ************/\n \n         template <class A>\n-        batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)\n+        inline batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)\n         {\n             return vcvtq_f64_s64(x);\n         }\n@@ -899,7 +899,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)\n         {\n             return !(arg == arg);\n         }\n--- include/xsimd/arch/xsimd_scalar.hpp\n@@ -76,42 +76,42 @@ namespace xsimd\n #else\n     // Windows defines catch all templates\n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isfinite(T var)\n     {\n         return std::isfinite(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isfinite(T var)\n     {\n         return isfinite(double(var));\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isinf(T var)\n     {\n         return std::isinf(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isinf(T var)\n     {\n         return isinf(double(var));\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isnan(T var)\n     {\n         return std::isnan(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isnan(T var)\n     {\n         return isnan(double(var));\n@@ -120,13 +120,13 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_NUMPY_COMPLEX\n     template <class T>\n-    bool isnan(std::complex<T> var)\n+    inline bool isnan(std::complex<T> var)\n     {\n         return std::isnan(std::real(var)) || std::isnan(std::imag(var));\n     }\n \n     template <class T>\n-    bool isinf(std::complex<T> var)\n+    inline bool isinf(std::complex<T> var)\n     {\n         return std::isinf(std::real(var)) || std::isinf(std::imag(var));\n     }\n@@ -228,13 +228,13 @@ namespace xsimd\n     }\n \n     template <class T>\n-    std::complex<T> log2(const std::complex<T>& val)\n+    inline std::complex<T> log2(const std::complex<T>& val)\n     {\n         return log(val) / std::log(T(2));\n     }\n \n     template<typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    T sadd(const T& lhs, const T& rhs)\n+    inline T sadd(const T& lhs, const T& rhs)\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -265,7 +265,7 @@ namespace xsimd\n     }\n \n     template<typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    T ssub(const T& lhs, const T& rhs)\n+    inline T ssub(const T& lhs, const T& rhs)\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n--- include/xsimd/arch/xsimd_sse2.hpp\n@@ -24,18 +24,20 @@ namespace xsimd {\n     using namespace types;\n \n     // abs\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {\n       __m128d sign_mask = _mm_set1_pd(-0.f);  // -0.f = 1 << 31\n       return _mm_andnot_pd(sign_mask, self);\n     }\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {\n       __m128 sign_mask = _mm_set1_ps(-0.f);  // -0.f = 1 << 31\n       return _mm_andnot_ps(sign_mask, self);\n     }\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_add_epi8(self, other);\n         case 2: return _mm_add_epi16(self, other);\n@@ -45,90 +47,104 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_add_ps(self, other);\n     }\n \n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_add_pd(self, other);\n     }\n \n     // all\n-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_ps(self) == 0x0F;\n     }\n-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_pd(self) == 0x03;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool all(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_epi8(self) == 0xFFFF;\n     }\n \n     // any\n-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_ps(self) != 0;\n     }\n-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_pd(self) != 0;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_epi8(self) != 0;\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_and_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_and_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_and_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_and_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A> batch<double, A>\n+    inline bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_and_pd(self, other);\n     }\n \n-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_and_pd(self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_ps(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_pd(self, other);\n     }\n-    \n-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_pd(self, other);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_and_si128(_mm_set1_epi8(0xFF << other), _mm_slli_epi32(self, other));\n         case 2: return _mm_slli_epi16(self, other);\n@@ -139,56 +155,62 @@ namespace xsimd {\n     }\n \n     // bitwise_not\n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n     }\n-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_si128(self, _mm_set1_epi32(-1));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_si128(self, _mm_set1_epi32(-1));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {\n+    inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_or_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_or_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_or_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_or_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_or_pd(self, other);\n     }\n \n-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_or_pd(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: {\n@@ -222,77 +244,86 @@ namespace xsimd {\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_si128(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_si128(self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n       return _mm_castsi128_ps(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<sse2>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<sse2>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n       return _mm_castps_si128(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n       return _mm_castsi128_pd(self);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n       return _mm_castps_pd(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n       return _mm_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n       return _mm_castpd_si128(self);\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<int32_t, A>\n+    inline bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {\n         return _mm_castps_si128(self);\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<float, A>\n+    inline bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {\n         return _mm_castsi128_ps(self);\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<int64_t, A>\n+    inline bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {\n         return _mm_castpd_si128(self);\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<double, A>\n+    inline bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {\n         return _mm_castsi128_pd(self);\n     }\n \n     // broadcast\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<sse2>) {\n+    template<class A> batch<float, A>\n+    inline broadcast(float val, requires_arch<sse2>) {\n       return _mm_set1_ps(val);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<sse2>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_set1_epi8(val);\n         case 2: return _mm_set1_epi16(val);\n@@ -301,7 +332,8 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> broadcast(double val, requires_arch<sse2>) {\n       return _mm_set1_pd(val);\n     }\n \n@@ -310,48 +342,58 @@ namespace xsimd {\n     {\n       // Override these methods in SSE-based archs, no need to override store_aligned / store_unaligned\n       // complex_low\n-      template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpacklo_ps(self.real(), self.imag());\n       }\n       // complex_high\n-      template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpackhi_ps(self.real(), self.imag());\n       }\n-      template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpacklo_pd(self.real(), self.imag());\n       }\n-      template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpackhi_pd(self.real(), self.imag());\n       }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_div_pd(self, other);\n     }\n \n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {\n       return _mm_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {\n       return _mm_cvttps_epi32(self);\n     }\n     }\n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpeq_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return  _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_castps_si128(self), _mm_castps_si128(other)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_cmpeq_epi8(self, other);\n         case 2: return _mm_cmpeq_epi16(self, other);\n@@ -367,30 +409,35 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return eq(batch<T, A>(self.data), batch<T, A>(other.data));\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpeq_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return  _mm_castsi128_pd(_mm_cmpeq_epi32(_mm_castpd_si128(self), _mm_castpd_si128(other)));\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpge_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpge_pd(self, other);\n     }\n-    // gt\n \n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    // gt\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpgt_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_cmpgt_epi8(self, other);\n@@ -404,12 +451,14 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpgt_pd(self, other);\n     }\n-    \n+\n     // hadd\n-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& self, requires_arch<sse2>) {\n       __m128 tmp0 = _mm_add_ps(self, _mm_movehl_ps(self, self));\n       __m128 tmp1 = _mm_add_ss(tmp0, _mm_shuffle_ps(tmp0, tmp0, 1));\n       return _mm_cvtss_f32(tmp1);\n@@ -418,7 +467,7 @@ namespace xsimd {\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd_default(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline T hadd_default(batch<T, A> const& self, requires_arch<sse2>) {\n         alignas(A::alignment()) T buffer[batch<T, A>::size];\n         self.store_aligned(buffer);\n         T res = 0;\n@@ -430,7 +479,7 @@ namespace xsimd {\n     }\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 4: {\n                 __m128i tmp1 = _mm_shuffle_epi32(self, 0x0E);\n@@ -456,12 +505,13 @@ namespace xsimd {\n       }\n     }\n     template <class A>\n-    double hadd(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline double hadd(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_cvtsd_f64(_mm_add_sd(self, _mm_unpackhi_pd(self, self)));\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {\n       __m128 tmp0 = _mm_unpacklo_ps(row[0], row[1]);\n       __m128 tmp1 = _mm_unpackhi_ps(row[0], row[1]);\n       __m128 tmp2 = _mm_unpackhi_ps(row[2], row[3]);\n@@ -473,69 +523,80 @@ namespace xsimd {\n       return _mm_add_ps(tmp0, tmp2);\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {\n       return _mm_add_pd(_mm_unpacklo_pd(row[0], row[1]),\n           _mm_unpackhi_pd(row[0], row[1]));\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_cmpunord_ps(self, self);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_cmpunord_pd(self, self);\n     }\n \n     // load_aligned\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {\n       return _mm_load_ps(mem);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) {\n       return _mm_load_si128((__m128i const*)mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {\n       return _mm_load_pd(mem);\n     }\n \n     // load_unaligned\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){\n       return _mm_loadu_ps(mem);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) {\n       return _mm_loadu_si128((__m128i const*)mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){\n       return _mm_loadu_pd(mem);\n     }\n \n     // load_complex\n     namespace detail\n     {\n       // Redefine these methods in the SSE-based archs if required\n-      template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {\n         return {_mm_shuffle_ps(hi, lo, _MM_SHUFFLE(2, 0, 2, 0)), _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(3, 1, 3, 1))};\n       }\n-        template<class A> batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {\n-            return {_mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1))};\n-        }\n+      template<class A>\n+      inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {\n+          return {_mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1))};\n+      }\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmple_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmple_pd(self, other);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmplt_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_cmplt_epi8(self, other);\n@@ -573,116 +634,134 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmplt_pd(self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_max_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return select(self > other, self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_max_pd(self, other);\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_min_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return select(self <= other, self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_min_pd(self, other);\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_mul_pd(self, other);\n     }\n \n     // neg\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) {\n       return 0 - self;\n     }\n-    template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(0x80000000)));\n     }\n \n \n     template <class A>\n-    batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(\n           self, _mm_castsi128_pd(_mm_setr_epi32(0, 0x80000000, 0, 0x80000000)));\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n         return ~(self == other);\n     }\n-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n         return ~(self == other);\n     }\n \n \n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_pd(self, other);\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_ps(_mm_and_ps(cond, true_br), _mm_andnot_ps(cond, false_br));\n     }\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_si128(_mm_and_si128(cond, true_br), _mm_andnot_si128(cond, false_br));\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, sse2{});\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_pd(_mm_and_pd(cond, true_br), _mm_andnot_pd(cond, false_br));\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {\n       return _mm_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {\n       return _mm_sqrt_pd(val);\n     }\n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_add_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n     // TODO: move this in xsimd_generic\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sadd_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         auto mask = (other >> (8 * sizeof(T) - 1));\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -699,7 +778,7 @@ namespace xsimd {\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_adds_epi8(self, other);\n@@ -715,67 +794,69 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_add_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n \n     // set\n     template<class A, class... Values>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n       return _mm_setr_ps(values...);\n     }\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) {\n       return _mm_set_epi64x(v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) {\n       return _mm_setr_epi32(v0, v1, v2, v3);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n \n     template<class A, class... Values>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n       return _mm_setr_pd(values...);\n     }\n \n     template<class A, class T, class... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) {\n       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;\n     }\n \n     template<class A, class... Values>\n-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n       return _mm_castsi128_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     template<class A, class... Values>\n-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n       return _mm_castsi128_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n     // TODO: move this in xsimd_generic\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> ssub_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -788,7 +869,7 @@ namespace xsimd {\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_subs_epi8(self, other);\n@@ -804,48 +885,55 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n \n     // store_aligned\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_store_ps(mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_store_si128((__m128i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_store_si128((__m128i *)mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_store_pd(mem, self);\n     }\n \n     // store_unaligned\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_ps(mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_si128((__m128i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_si128((__m128i *)mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_pd(mem, self);\n     }\n \n     // sub\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_sub_epi8(self, other);\n         case 2: return _mm_sub_epi16(self, other);\n@@ -854,17 +942,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {\n       return _mm_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -873,24 +962,25 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n       return {(int64_t)buffer[0], (int64_t)buffer[1]};\n     }\n \n     // zip_hi\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_unpackhi_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_unpackhi_epi8(self, other);\n         case 2: return _mm_unpackhi_epi16(self, other);\n@@ -899,16 +989,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_unpacklo_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_unpacklo_epi8(self, other);\n         case 2: return _mm_unpacklo_epi16(self, other);\n@@ -917,13 +1009,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_unpacklo_pd(self, other);\n     }\n   }\n \n }\n \n #endif\n-\n-\n--- include/xsimd/arch/xsimd_sse3.hpp\n@@ -22,29 +22,31 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) {\n       return _mm_lddqu_si128((__m128i const*)mem);\n     }\n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse3>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& self, requires_arch<sse3>) {\n       __m128 tmp0 = _mm_hadd_ps(self, self);\n       __m128 tmp1 = _mm_hadd_ps(tmp0, tmp0);\n       return _mm_cvtss_f32(tmp1);\n     }\n     template <class A>\n-    double hadd(batch<double, A> const &self, requires_arch<sse3>) {\n+    inline double hadd(batch<double, A> const &self, requires_arch<sse3>) {\n       __m128d tmp0 = _mm_hadd_pd(self, self);\n       return _mm_cvtsd_f64(tmp0);\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {\n       return _mm_hadd_ps(_mm_hadd_ps(row[0], row[1]),\n                               _mm_hadd_ps(row[2], row[3]));\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {\n       return _mm_hadd_pd(row[0], row[1]);\n     }\n \n--- include/xsimd/arch/xsimd_sse4_1.hpp\n@@ -22,37 +22,41 @@ namespace xsimd {\n     using namespace types;\n     // any\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch<T, A> const& self, requires_arch<sse4_1>) {\n+    inline bool any(batch<T, A> const& self, requires_arch<sse4_1>) {\n       return !_mm_testz_si128(self, self);\n     }\n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_ceil_ps(self);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_ceil_pd(self);\n     }\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       switch(sizeof(T)) {\n         case 8: return _mm_cmpeq_epi64(self, other);\n         default: return eq(self, other, ssse3{});\n       }\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_floor_ps(self);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_floor_pd(self);\n     }\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_max_epi8(self, other);\n@@ -73,7 +77,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_min_epi8(self, other);\n@@ -94,7 +98,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_or_si128(\n                             _mm_and_si128(_mm_mullo_epi16(self, other), _mm_srli_epi16(_mm_cmpeq_epi8(self, self), 8)),\n@@ -115,35 +119,39 @@ namespace xsimd {\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n \n     // select\n     namespace detail {\n       template<class T>\n-      constexpr T interleave(T const &cond) {\n+      inline constexpr T interleave(T const &cond) {\n         return (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 49) & 0x5555) |\n                (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 48) & 0xAAAA);\n       }\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_epi8(false_br, true_br, cond);\n     }\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_ps(false_br, true_br, cond);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_pd(false_br, true_br, cond);\n     }\n \n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();\n       switch(sizeof(T)) {\n         case 2: return _mm_blend_epi16(false_br, true_br, mask);\n@@ -159,21 +167,25 @@ namespace xsimd {\n         default: return select(batch_bool_constant<batch<T, A>, Values...>(), true_br, false_br, ssse3{});\n       }\n     }\n-    template<class A, bool... Values> batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A, bool... Values>\n+    inline batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<float, A>, Values...>::mask();\n       return _mm_blend_ps(false_br, true_br, mask);\n     }\n-    template<class A, bool... Values> batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A, bool... Values>\n+    inline batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<double, A>, Values...>::mask();\n       return _mm_blend_pd(false_br, true_br, mask);\n     }\n \n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_ps(self, _MM_FROUND_TO_ZERO);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_pd(self, _MM_FROUND_TO_ZERO);\n     }\n \n--- include/xsimd/arch/xsimd_sse4_2.hpp\n@@ -23,11 +23,11 @@ namespace xsimd {\n \n     // lt\n     template<class A>\n-    batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {\n+    inline batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {\n       return _mm_cmpgt_epi64(other, self);\n     }\n     template<class A>\n-    batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {\n+    inline batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {\n       auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n       auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n       return _mm_cmpgt_epi64(xother, xself);\n--- include/xsimd/arch/xsimd_ssse3.hpp\n@@ -24,7 +24,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_abs_epi8(self);\n         case 2: return _mm_abs_epi16(self);\n@@ -38,12 +38,12 @@ namespace xsimd {\n     namespace detail {\n \n       template<class T, class A>\n-      batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {\n+      inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {\n         return other;\n       }\n \n       template<class T, class A, std::size_t I, std::size_t... Is>\n-      batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {\n+      inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {\n         if(i == I) {\n           return _mm_alignr_epi8(self, other, sizeof(T) * I);\n         }\n@@ -53,15 +53,15 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class _ = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) {\n+    inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) {\n       constexpr std::size_t size = batch<T, A>::size;\n       assert(0<= i && i< size && \"index in bounds\");\n       return detail::extract_pair(self, other, i, ::xsimd::detail::make_index_sequence<size>());\n     }\n \n     // hadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<ssse3>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<ssse3>) {\n       switch(sizeof(T)) {\n         case 2: {\n                 __m128i tmp1 = _mm_hadd_epi16(self, self);\n--- include/xsimd/types/xsimd_api.hpp\n@@ -53,7 +53,7 @@ namespace xsimd {\n  * @return the absolute values of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> abs(batch<T, A> const& x) {\n+inline batch<T, A> abs(batch<T, A> const& x) {\n   return kernel::abs<A>(x, A{});\n }\n \n@@ -65,7 +65,7 @@ batch<T, A> abs(batch<T, A> const& x) {\n  * @return the absolute values of \\c z.\n  */\n template<class T, class A>\n-batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n+inline batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n   return kernel::abs<A>(z, A{});\n }\n \n@@ -78,7 +78,7 @@ batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n  * @return the sum of \\c x and \\c y\n  */\n template<class T, class Tp>\n-auto add(T const& x, Tp const& y) -> decltype(x + y){\n+inline auto add(T const& x, Tp const& y) -> decltype(x + y){\n   return x + y;\n }\n \n@@ -90,7 +90,7 @@ auto add(T const& x, Tp const& y) -> decltype(x + y){\n  * @return the arc cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> acos(batch<T, A> const& x) {\n+inline batch<T, A> acos(batch<T, A> const& x) {\n   return kernel::acos<A>(x, A{});\n }\n \n@@ -102,7 +102,7 @@ batch<T, A> acos(batch<T, A> const& x) {\n  * @return the inverse hyperbolic cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> acosh(batch<T, A> const& x) {\n+inline batch<T, A> acosh(batch<T, A> const& x) {\n   return kernel::acosh<A>(x, A{});\n }\n \n@@ -114,7 +114,7 @@ batch<T, A> acosh(batch<T, A> const& x) {\n  * @return the argument of \\c z.\n  */\n template<class T, class A>\n-real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n+inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n   return kernel::arg<A>(z, A{});\n }\n \n@@ -126,7 +126,7 @@ real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n  * @return the arc sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> asin(batch<T, A> const& x) {\n+inline batch<T, A> asin(batch<T, A> const& x) {\n   return kernel::asin<A>(x, A{});\n }\n \n@@ -138,7 +138,7 @@ batch<T, A> asin(batch<T, A> const& x) {\n  * @return the inverse hyperbolic sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> asinh(batch<T, A> const& x) {\n+inline batch<T, A> asinh(batch<T, A> const& x) {\n   return kernel::asinh<A>(x, A{});\n }\n \n@@ -150,7 +150,7 @@ batch<T, A> asinh(batch<T, A> const& x) {\n  * @return the arc tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> atan(batch<T, A> const& x) {\n+inline batch<T, A> atan(batch<T, A> const& x) {\n   return kernel::atan<A>(x, A{});\n }\n \n@@ -164,7 +164,7 @@ batch<T, A> atan(batch<T, A> const& x) {\n  * @return the arc tangent of \\c x/y.\n  */\n template<class T, class A>\n-batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::atan2<A>(x, y, A{});\n }\n \n@@ -176,7 +176,7 @@ batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the inverse hyperbolic tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> atanh(batch<T, A> const& x) {\n+inline batch<T, A> atanh(batch<T, A> const& x) {\n   return kernel::atanh<A>(x, A{});\n }\n \n@@ -188,7 +188,7 @@ batch<T, A> atanh(batch<T, A> const& x) {\n  * @return \\c x casted to \\c T_out\n  */\n template<class T_out, class T_in, class A>\n-batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n+inline batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n   return kernel::batch_cast<A>(x, batch<T_out, A>{}, A{});\n }\n \n@@ -200,7 +200,7 @@ batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n  * @return bit of sign of \\c x\n  */\n template<class T, class A>\n-batch<T, A> bitofsign(batch<T, A> const& x) {\n+inline batch<T, A> bitofsign(batch<T, A> const& x) {\n   return kernel::bitofsign<A>(x, A{});\n }\n \n@@ -213,7 +213,7 @@ batch<T, A> bitofsign(batch<T, A> const& x) {\n  * @return the result of the bitwise and.\n  */\n template<class T, class Tp>\n-auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n+inline auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n   return x & y;\n }\n \n@@ -226,7 +226,7 @@ auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n  * @return the result of the bitwise and not.\n  */\n template<class T, class A>\n-batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::bitwise_andnot<A>(x, y, A{});\n }\n \n@@ -240,7 +240,7 @@ batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the bitwise and not.\n  */\n template<class T, class A>\n-batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {\n+inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {\n   return kernel::bitwise_andnot<A>(x, y, A{});\n }\n \n@@ -252,7 +252,7 @@ batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> cons\n  * @return \\c x reinterpreted as \\c T_out\n  */\n template<class B, class T, class A>\n-B bitwise_cast(batch<T, A> const& x) {\n+inline B bitwise_cast(batch<T, A> const& x) {\n   return kernel::bitwise_cast<A>(x, B{}, A{});\n }\n \n@@ -264,7 +264,7 @@ B bitwise_cast(batch<T, A> const& x) {\n  * @return the result of the bitwise not.\n  */\n template<class T, class A>\n-batch<T, A> bitwise_not(batch<T, A> const& x) {\n+inline batch<T, A> bitwise_not(batch<T, A> const& x) {\n   return kernel::bitwise_not<A>(x, A{});\n }\n \n@@ -277,7 +277,7 @@ batch<T, A> bitwise_not(batch<T, A> const& x) {\n  * @return the result of the bitwise or.\n  */\n template<class T, class Tp>\n-auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n+inline auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n   return x | y;\n }\n \n@@ -290,25 +290,25 @@ auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n  * @return the result of the bitwise xor.\n  */\n template<class T, class Tp>\n-auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){\n+inline auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){\n   return x ^ y;\n }\n \n // FIXME: check if these need to be exposed, or removed (?)\n template<class A>\n-batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {\n+inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {\n+inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {\n+inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n+inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n \n@@ -320,7 +320,7 @@ batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n  * @return a new batch instance\n  */\n template<class T, class A=default_arch>\n-batch<T, A> broadcast(T v) {\n+inline batch<T, A> broadcast(T v) {\n   return kernel::broadcast<A>(v, A{});\n }\n \n@@ -333,7 +333,7 @@ batch<T, A> broadcast(T v) {\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> broadcast_as(From v) {\n+inline simd_return_type<From, To> broadcast_as(From v) {\n     using batch_value_type = typename simd_return_type<From, To>::value_type;\n     using value_type = typename std::conditional<std::is_same<From, bool>::value,\n                                                  bool,\n@@ -349,7 +349,7 @@ simd_return_type<From, To> broadcast_as(From v) {\n  * @return the cubic root of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cbrt(batch<T, A> const& x) {\n+inline batch<T, A> cbrt(batch<T, A> const& x) {\n   return kernel::cbrt<A>(x, A{});\n }\n \n@@ -362,7 +362,7 @@ batch<T, A> cbrt(batch<T, A> const& x) {\n  * @return the batch of smallest integer values not less than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> ceil(batch<T, A> const& x) {\n+inline batch<T, A> ceil(batch<T, A> const& x) {\n   return kernel::ceil<A>(x, A{});\n }\n \n@@ -377,7 +377,7 @@ batch<T, A> ceil(batch<T, A> const& x) {\n  * @return the result of the clipping.\n  */\n template<class A, class T>\n-batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {\n+inline batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {\n   return kernel::clip(x, lo, hi, A{});\n }\n \n@@ -389,7 +389,7 @@ batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const&\n  * @return the argument of \\c z.\n  */\n template<class A, class T>\n-complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n+inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n   return kernel::conj(z, A{});\n }\n \n@@ -404,7 +404,7 @@ complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n  * matches that of \\c y.\n  */\n template<class A, class T>\n-batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::copysign<A>(x, y, A{});\n }\n \n@@ -416,7 +416,7 @@ batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cos(batch<T, A> const& x) {\n+inline batch<T, A> cos(batch<T, A> const& x) {\n   return kernel::cos<A>(x, A{});\n }\n \n@@ -428,7 +428,7 @@ batch<T, A> cos(batch<T, A> const& x) {\n  * @return the hyperbolic cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cosh(batch<T, A> const& x) {\n+inline batch<T, A> cosh(batch<T, A> const& x) {\n   return kernel::cosh<A>(x, A{});\n }\n \n@@ -441,7 +441,7 @@ batch<T, A> cosh(batch<T, A> const& x) {\n  * @return the result of the division.\n  */\n template<class T, class Tp>\n-auto div(T const& x, Tp const& y) -> decltype(x / y){\n+inline auto div(T const& x, Tp const& y) -> decltype(x / y){\n   return x / y;\n }\n \n@@ -454,7 +454,7 @@ auto div(T const& x, Tp const& y) -> decltype(x / y){\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n   return x == y;\n }\n \n@@ -466,7 +466,7 @@ batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the natural exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp(batch<T, A> const& x) {\n+inline batch<T, A> exp(batch<T, A> const& x) {\n   return kernel::exp<A>(x, A{});\n }\n \n@@ -478,7 +478,7 @@ batch<T, A> exp(batch<T, A> const& x) {\n  * @return the base 10 exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp10(batch<T, A> const& x) {\n+inline batch<T, A> exp10(batch<T, A> const& x) {\n   return kernel::exp10<A>(x, A{});\n }\n \n@@ -490,7 +490,7 @@ batch<T, A> exp10(batch<T, A> const& x) {\n  * @return the base 2 exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp2(batch<T, A> const& x) {\n+inline batch<T, A> exp2(batch<T, A> const& x) {\n   return kernel::exp2<A>(x, A{});\n }\n \n@@ -502,7 +502,7 @@ batch<T, A> exp2(batch<T, A> const& x) {\n  * @return the natural exponential of \\c x, minus one.\n  */\n template<class T, class A>\n-batch<T, A> expm1(batch<T, A> const& x) {\n+inline batch<T, A> expm1(batch<T, A> const& x) {\n   return kernel::expm1<A>(x, A{});\n }\n \n@@ -514,7 +514,7 @@ batch<T, A> expm1(batch<T, A> const& x) {\n  * @return the error function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> erf(batch<T, A> const& x) {\n+inline batch<T, A> erf(batch<T, A> const& x) {\n   return kernel::erf<A>(x, A{});\n }\n \n@@ -526,7 +526,7 @@ batch<T, A> erf(batch<T, A> const& x) {\n  * @return the error function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> erfc(batch<T, A> const& x) {\n+inline batch<T, A> erfc(batch<T, A> const& x) {\n   return kernel::erfc<A>(x, A{});\n }\n \n@@ -539,7 +539,7 @@ batch<T, A> erfc(batch<T, A> const& x) {\n  * @return the evaluation ofpolynomial with coefficient \\c Coefs on point \\c x.\n  */\n template <class T, class A, uint64_t... Coefs>\n-batch<T, A> estrin(const batch<T, A>& x) {\n+inline batch<T, A> estrin(const batch<T, A>& x) {\n   return kernel::estrin<T, A, Coefs...>(x);\n }\n \n@@ -554,7 +554,7 @@ batch<T, A> estrin(const batch<T, A>& x) {\n  * @return.\n  */\n template <class T, class A>\n-batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {\n+inline batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {\n   return kernel::extract_pair<A>(x, y, i, A{});\n }\n \n@@ -566,7 +566,7 @@ batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_\n  * @return the asbolute values of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> fabs(batch<T, A> const& x) {\n+inline batch<T, A> fabs(batch<T, A> const& x) {\n   return kernel::abs<A>(x, A{});\n }\n \n@@ -580,7 +580,7 @@ batch<T, A> fabs(batch<T, A> const& x) {\n  * @return the positive difference.\n  */\n template<class T, class A>\n-batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::fdim<A>(x, y, A{});\n }\n \n@@ -593,7 +593,7 @@ batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the batch of largest integer values not greater than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> floor(batch<T, A> const& x) {\n+inline batch<T, A> floor(batch<T, A> const& x) {\n   return kernel::floor<A>(x, A{});\n }\n \n@@ -607,7 +607,7 @@ batch<T, A> floor(batch<T, A> const& x) {\n  * @return the result of the fused multiply-add operation.\n  */\n template<class T, class A>\n-batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fma<A>(x, y, z, A{});\n }\n \n@@ -621,7 +621,7 @@ batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::max<A>(x, y, A{});\n }\n \n@@ -635,7 +635,7 @@ batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::min<A>(x, y, A{});\n }\n \n@@ -648,7 +648,7 @@ batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the modulo.\n  */\n template<class T, class A>\n-batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::fmod<A>(x, y, A{});\n }\n \n@@ -662,7 +662,7 @@ batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the fused multiply-sub operation.\n  */\n template<class T, class A>\n-batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fms<A>(x, y, z, A{});\n }\n \n@@ -676,7 +676,7 @@ batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z\n  * @return the result of the fused negated multiply-add operation.\n  */\n template<class T, class A>\n-batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fnma<A>(x, y, z, A{});\n }\n \n@@ -690,7 +690,7 @@ batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&\n  * @return the result of the fused negated multiply-sub operation.\n  */\n template<class T, class A>\n-batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fnms<A>(x, y, z, A{});\n }\n \n@@ -703,7 +703,7 @@ batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&\n  * @return the normalized fraction of x\n  */\n template <class T, class A>\n-batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n+inline batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n   return kernel::frexp<A>(x, y, A{});\n }\n \n@@ -717,7 +717,7 @@ batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n   return x >= y;\n }\n \n@@ -731,7 +731,7 @@ batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n   return x > y;\n }\n \n@@ -743,7 +743,7 @@ batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the reduction.\n  */\n template<class T, class A>\n-T hadd(batch<T, A> const& x) {\n+inline T hadd(batch<T, A> const& x) {\n   return kernel::hadd<A>(x, A{});\n }\n \n@@ -757,7 +757,7 @@ T hadd(batch<T, A> const& x) {\n  * @return the result of the reduction.\n  */\n template<class T, class A>\n-batch<T, A> haddp(batch<T, A> const* row) {\n+inline batch<T, A> haddp(batch<T, A> const* row) {\n   return kernel::haddp<A>(row, A{});\n }\n \n@@ -771,7 +771,7 @@ batch<T, A> haddp(batch<T, A> const* row) {\n  * @return the evaluation ofpolynomial with coefficient \\c Coefs on point \\c x.\n  */\n template <class T, class A, uint64_t... Coefs>\n-batch<T, A> horner(const batch<T, A>& x) {\n+inline batch<T, A> horner(const batch<T, A>& x) {\n   return kernel::horner<T, A, Coefs...>(x);\n }\n \n@@ -785,7 +785,7 @@ batch<T, A> horner(const batch<T, A>& x) {\n  * @return the square root of the sum of the squares of \\c x and \\c y.\n  */\n template<class T, class A>\n-batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::hypot<A>(x, y, A{});\n }\n \n@@ -797,7 +797,7 @@ batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the argument of \\c z.\n  */\n template <class T, class A>\n-real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {\n   return kernel::imag<A>(x, A{});\n }\n \n@@ -821,7 +821,7 @@ B infinity() {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_even(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_even(batch<T, A> const& x) {\n   return kernel::is_even<A>(x, A{});\n }\n \n@@ -833,7 +833,7 @@ batch_bool<T, A> is_even(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_flint(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_flint(batch<T, A> const& x) {\n   return kernel::is_flint<A>(x, A{});\n }\n \n@@ -845,7 +845,7 @@ batch_bool<T, A> is_flint(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_odd(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_odd(batch<T, A> const& x) {\n   return kernel::is_odd<A>(x, A{});\n }\n \n@@ -858,7 +858,7 @@ batch_bool<T, A> is_odd(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> isinf(batch<T, A> const& x) {\n+inline batch_bool<T, A> isinf(batch<T, A> const& x) {\n   return kernel::isinf<A>(x, A{});\n }\n \n@@ -871,7 +871,7 @@ batch_bool<T, A> isinf(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> isfinite(batch<T, A> const& x) {\n+inline batch_bool<T, A> isfinite(batch<T, A> const& x) {\n   return kernel::isfinite<A>(x, A{});\n }\n \n@@ -883,7 +883,7 @@ batch_bool<T, A> isfinite(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n+inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n   return kernel::isnan<A>(x, A{});\n }\n \n@@ -896,7 +896,7 @@ typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n  * @return the natural logarithm of the gamma function of \\c x.\n  */\n template <class T, class A>\n-batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n+inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n   return kernel::ldexp<A>(x, y, A{});\n }\n \n@@ -909,7 +909,7 @@ batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n   return x <= y;\n }\n \n@@ -921,7 +921,7 @@ batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the natural logarithm of the gamma function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> lgamma(batch<T, A> const& x) {\n+inline batch<T, A> lgamma(batch<T, A> const& x) {\n   return kernel::lgamma<A>(x, A{});\n }\n \n@@ -934,18 +934,18 @@ batch<T, A> lgamma(batch<T, A> const& x) {\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {\n+inline simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {\n   using batch_value_type = typename simd_return_type<From, To>::value_type;\n   return kernel::load_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n }\n \n template <class To, class A = default_arch>\n-simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {\n+inline simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {\n   return simd_return_type<bool, To>::load_aligned(ptr);\n }\n \n template <class To, class A=default_arch, class From>\n-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)\n+inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)\n {\n   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;\n   return kernel::load_complex_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n@@ -960,18 +960,18 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {\n+inline simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {\n   using batch_value_type = typename simd_return_type<From, To>::value_type;\n   return kernel::load_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n }\n \n template <class To, class A = default_arch>\n-simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {\n+inline simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {\n   return simd_return_type<bool, To>::load_unaligned(ptr);\n }\n \n template <class To, class A=default_arch, class From>\n-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)\n+inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)\n {\n   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;\n   return kernel::load_complex_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n@@ -986,7 +986,7 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load(From const* ptr, aligned_mode= {}) {\n+inline batch<From, A> load(From const* ptr, aligned_mode= {}) {\n   return load_as<From, A>(ptr, aligned_mode{});\n }\n \n@@ -999,7 +999,7 @@ batch<From, A> load(From const* ptr, aligned_mode= {}) {\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load(From const* ptr, unaligned_mode) {\n+inline batch<From, A> load(From const* ptr, unaligned_mode) {\n   return load_as<From, A>(ptr, unaligned_mode{});\n }\n \n@@ -1012,7 +1012,7 @@ batch<From, A> load(From const* ptr, unaligned_mode) {\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load_aligned(From const* ptr) {\n+inline batch<From, A> load_aligned(From const* ptr) {\n   return load_as<From, A>(ptr, aligned_mode{});\n }\n \n@@ -1025,7 +1025,7 @@ batch<From, A> load_aligned(From const* ptr) {\n  * @return a new batch instance\n  */\n template <class A=default_arch, class From>\n-batch<From, A> load_unaligned(From const* ptr) {\n+inline batch<From, A> load_unaligned(From const* ptr) {\n   return load_as<From, A>(ptr, unaligned_mode{});\n }\n \n@@ -1037,7 +1037,7 @@ batch<From, A> load_unaligned(From const* ptr) {\n  * @return the natural logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log(batch<T, A> const& x) {\n+inline batch<T, A> log(batch<T, A> const& x) {\n   return kernel::log<A>(x, A{});\n }\n \n@@ -1048,7 +1048,7 @@ batch<T, A> log(batch<T, A> const& x) {\n  * @return the base 2 logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log2(batch<T, A> const& x) {\n+inline batch<T, A> log2(batch<T, A> const& x) {\n   return kernel::log2<A>(x, A{});\n }\n \n@@ -1059,7 +1059,7 @@ batch<T, A> log2(batch<T, A> const& x) {\n  * @return the base 10 logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log10(batch<T, A> const& x) {\n+inline batch<T, A> log10(batch<T, A> const& x) {\n   return kernel::log10<A>(x, A{});\n }\n \n@@ -1070,7 +1070,7 @@ batch<T, A> log10(batch<T, A> const& x) {\n  * @return the natural logarithm of one plus \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log1p(batch<T, A> const& x) {\n+inline batch<T, A> log1p(batch<T, A> const& x) {\n   return kernel::log1p<A>(x, A{});\n }\n \n@@ -1083,7 +1083,7 @@ batch<T, A> log1p(batch<T, A> const& x) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n   return x < y;\n }\n \n@@ -1096,7 +1096,7 @@ batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::max<A>(x, y, A{});\n }\n \n@@ -1109,7 +1109,7 @@ batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the smaller values.\n  */\n template<class T, class A>\n-batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::min<A>(x, y, A{});\n }\n \n@@ -1120,7 +1120,7 @@ batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of positive infinity\n  */\n template<class B>\n-B minusinfinity() {\n+inline B minusinfinity() {\n   using T = typename B::value_type;\n   return B(-std::numeric_limits<T>::infinity());\n }\n@@ -1134,7 +1134,7 @@ B minusinfinity() {\n  * @return the result of the modulo.\n  */\n template<class T, class Tp>\n-auto mod(T const& x, Tp const& y) -> decltype(x % y){\n+inline auto mod(T const& x, Tp const& y) -> decltype(x % y){\n   return x % y;\n }\n \n@@ -1148,7 +1148,7 @@ auto mod(T const& x, Tp const& y) -> decltype(x % y){\n  * @return the result of the product.\n  */\n template<class T, class Tp>\n-auto mul(T const& x, Tp const& y) -> decltype(x * y){\n+inline auto mul(T const& x, Tp const& y) -> decltype(x * y){\n   return x * y;\n }\n \n@@ -1161,7 +1161,7 @@ auto mul(T const& x, Tp const& y) -> decltype(x * y){\n  * @return the batch of nearest integer values.\n  */\n template<class T, class A>\n-batch<T, A> nearbyint(batch<T, A> const& x) {\n+inline batch<T, A> nearbyint(batch<T, A> const& x) {\n   return kernel::nearbyint<A>(x, A{});\n }\n \n@@ -1174,7 +1174,7 @@ batch<T, A> nearbyint(batch<T, A> const& x) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n   return x != y;\n }\n \n@@ -1187,7 +1187,7 @@ batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the opposite of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> neg(batch<T, A> const& x) {\n+inline batch<T, A> neg(batch<T, A> const& x) {\n   return -x;\n }\n \n@@ -1201,7 +1201,7 @@ batch<T, A> neg(batch<T, A> const& x) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class A>\n-batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::nextafter<A>(x, y, A{});\n }\n \n@@ -1213,7 +1213,7 @@ batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the norm of \\c x.\n  */\n template<class A, class T>\n-real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n   return kernel::norm(x, A{});\n }\n \n@@ -1225,7 +1225,7 @@ real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n  * @return \\c x.\n  */\n template<class T, class A>\n-batch<T, A> pos(batch<T, A> const& x) {\n+inline batch<T, A> pos(batch<T, A> const& x) {\n   return +x;\n }\n \n@@ -1239,7 +1239,7 @@ batch<T, A> pos(batch<T, A> const& x) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class A>\n-batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::pow<A>(x, y, A{});\n }\n \n@@ -1253,7 +1253,7 @@ batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class ITy, class A, class=typename std::enable_if<std::is_integral<ITy>::value, void>::type>\n-batch<T, A> pow(batch<T, A> const& x, ITy y) {\n+inline batch<T, A> pow(batch<T, A> const& x, ITy y) {\n   return kernel::ipow<A>(x, y, A{});\n }\n \n@@ -1265,7 +1265,7 @@ batch<T, A> pow(batch<T, A> const& x, ITy y) {\n  * @return the projection of \\c x.\n  */\n template<class A, class T>\n-complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n+inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n   return kernel::proj(x, A{});\n }\n \n@@ -1277,7 +1277,7 @@ complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n  * @return the argument of \\c z.\n  */\n template <class T, class A>\n-real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n   return kernel::real<A>(x, A{});\n }\n \n@@ -1290,7 +1290,7 @@ real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n  * @return the result of the addition.\n  */\n template<class T, class A>\n-batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::remainder<A>(x, y, A{});\n }\n \n@@ -1303,7 +1303,7 @@ batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the batch of rounded values.\n  */\n template<class T, class A>\n-batch<T, A> rint(batch<T, A> const& x) {\n+inline batch<T, A> rint(batch<T, A> const& x) {\n   return nearbyint(x);\n }\n \n@@ -1314,10 +1314,10 @@ batch<T, A> rint(batch<T, A> const& x) {\n  * floating point format), rounding halfway cases away from zero, regardless\n  * of the current rounding mode.\n  * @param x batch of flaoting point values.\n- * @return the batch of nearest integer values. \n+ * @return the batch of nearest integer values.\n  */\n template<class T, class A>\n-batch<T, A> round(batch<T, A> const& x) {\n+inline batch<T, A> round(batch<T, A> const& x) {\n   return kernel::round<A>(x, A{});\n }\n \n@@ -1332,7 +1332,7 @@ batch<T, A> round(batch<T, A> const& x) {\n  * @return the result of the saturated addition.\n  */\n template<class T, class Tp>\n-auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n+inline auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n   using B = decltype(x + y);\n   using A = typename B::arch_type;\n   return kernel::sadd<A>(B(x), B(y), A{});\n@@ -1353,7 +1353,7 @@ auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n  * @return the result of the selection.\n  */\n template<class T, class A>\n-batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n+inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1372,7 +1372,7 @@ batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, bat\n  * @return the result of the selection.\n  */\n template<class T, class A>\n-batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {\n+inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1391,7 +1391,7 @@ batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::comple\n  * @return the result of the selection.\n  */\n template<class T, class A, bool... Values>\n-batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n+inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1403,7 +1403,7 @@ batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batc\n  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n  */\n template<class T, class A>\n-batch<T, A> sign(batch<T, A> const& x) {\n+inline batch<T, A> sign(batch<T, A> const& x) {\n   return kernel::sign<A>(x, A{});\n }\n \n@@ -1415,7 +1415,7 @@ batch<T, A> sign(batch<T, A> const& x) {\n  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n  */\n template<class T, class A>\n-batch<T, A> signnz(batch<T, A> const& x) {\n+inline batch<T, A> signnz(batch<T, A> const& x) {\n   return kernel::signnz<A>(x, A{});\n }\n \n@@ -1427,7 +1427,7 @@ batch<T, A> signnz(batch<T, A> const& x) {\n  * @return the sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sin(batch<T, A> const& x) {\n+inline batch<T, A> sin(batch<T, A> const& x) {\n   return kernel::sin<A>(x, A{});\n }\n \n@@ -1439,7 +1439,7 @@ batch<T, A> sin(batch<T, A> const& x) {\n  * @return the hyperbolic sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sinh(batch<T, A> const& x) {\n+inline batch<T, A> sinh(batch<T, A> const& x) {\n   return kernel::sinh<A>(x, A{});\n }\n \n@@ -1452,7 +1452,7 @@ batch<T, A> sinh(batch<T, A> const& x) {\n  * @return a pair containing the sine then the cosine of  batch \\c x\n  */\n template<class T, class A>\n-std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n+inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n   return kernel::sincos<A>(x, A{});\n }\n \n@@ -1464,7 +1464,7 @@ std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n  * @return the square root of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sqrt(batch<T, A> const& x) {\n+inline batch<T, A> sqrt(batch<T, A> const& x) {\n   return kernel::sqrt<A>(x, A{});\n }\n \n@@ -1479,7 +1479,7 @@ batch<T, A> sqrt(batch<T, A> const& x) {\n  * @return the result of the saturated difference.\n  */\n template<class T, class Tp>\n-auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n+inline auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n   using B = decltype(x + y);\n   using A = typename B::arch_type;\n   return kernel::ssub<A>(B(x), B(y), A{});\n@@ -1494,17 +1494,17 @@ auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n  * @param val the batch to copy\n  */\n template <class To, class A=default_arch, class From>\n-void store_as(To* dst, batch<From, A> const& src, aligned_mode) {\n+inline void store_as(To* dst, batch<From, A> const& src, aligned_mode) {\n   kernel::store_aligned(dst, src, A{});\n }\n \n template <class A=default_arch, class From>\n-void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {\n+inline void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {\n   kernel::store(src, dst, A{});\n }\n \n template <class To, class A=default_arch, class From>\n-void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {\n+inline void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {\n   kernel::store_complex_aligned(dst, src, A{});\n }\n \n@@ -1517,17 +1517,17 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, ali\n  * @param val the batch to copy\n  */\n template <class To, class A=default_arch, class From>\n-void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {\n+inline void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {\n   kernel::store_unaligned(dst, src, A{});\n }\n \n template <class A=default_arch, class From>\n-void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {\n+inline void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {\n   kernel::store(src, dst, A{});\n }\n \n template <class To, class A=default_arch, class From>\n-void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {\n+inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {\n   kernel::store_complex_unaligned(dst, src, A{});\n }\n \n@@ -1540,7 +1540,7 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, un\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n+inline void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n   store_as<T, A>(mem, val, aligned_mode{});\n }\n \n@@ -1553,7 +1553,7 @@ void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n+inline void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n   store_as<T, A>(mem, val, unaligned_mode{});\n }\n \n@@ -1566,7 +1566,7 @@ void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store_aligned(T* mem, batch<T, A> const& val) {\n+inline void store_aligned(T* mem, batch<T, A> const& val) {\n   store_as<T, A>(mem, val, aligned_mode{});\n }\n \n@@ -1579,7 +1579,7 @@ void store_aligned(T* mem, batch<T, A> const& val) {\n  * @param val the batch to copy\n  */\n template<class A, class T>\n-void store_unaligned(T* mem, batch<T, A> const& val) {\n+inline void store_unaligned(T* mem, batch<T, A> const& val) {\n   store_as<T, A>(mem, val, unaligned_mode{});\n }\n \n@@ -1593,7 +1593,7 @@ void store_unaligned(T* mem, batch<T, A> const& val) {\n  * @return the difference between \\c x and \\c y\n  */\n template<class T, class Tp>\n-auto sub(T const& x, Tp const& y) -> decltype(x - y){\n+inline auto sub(T const& x, Tp const& y) -> decltype(x - y){\n   return x - y;\n }\n \n@@ -1605,7 +1605,7 @@ auto sub(T const& x, Tp const& y) -> decltype(x - y){\n  * @return the tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tan(batch<T, A> const& x) {\n+inline batch<T, A> tan(batch<T, A> const& x) {\n   return kernel::tan<A>(x, A{});\n }\n \n@@ -1617,7 +1617,7 @@ batch<T, A> tan(batch<T, A> const& x) {\n  * @return the hyperbolic tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tanh(batch<T, A> const& x) {\n+inline batch<T, A> tanh(batch<T, A> const& x) {\n   return kernel::tanh<A>(x, A{});\n }\n \n@@ -1629,7 +1629,7 @@ batch<T, A> tanh(batch<T, A> const& x) {\n  * @return the gamma function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tgamma(batch<T, A> const& x) {\n+inline batch<T, A> tgamma(batch<T, A> const& x) {\n   return kernel::tgamma<A>(x, A{});\n }\n \n@@ -1641,7 +1641,7 @@ batch<T, A> tgamma(batch<T, A> const& x) {\n  * @return \\c i converted to a value of an floating point type of the same size as \\c T\n  */\n template<class T, class A>\n-batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n+inline batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n   return kernel::to_float<A>(i, A{});\n }\n \n@@ -1653,7 +1653,7 @@ batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n  * @return \\c x converted to a value of an integer type of the same size as \\c T\n  */\n template<class T, class A>\n-batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n+inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n   return kernel::to_int<A>(x, A{});\n }\n \n@@ -1666,7 +1666,7 @@ batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n  * @return the batch of nearest integer values not greater in magnitude than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> trunc(batch<T, A> const& x) {\n+inline batch<T, A> trunc(batch<T, A> const& x) {\n   return kernel::trunc<A>(x, A{});\n }\n \n@@ -1680,7 +1680,7 @@ batch<T, A> trunc(batch<T, A> const& x) {\n  * @return a batch of the high part of shuffled values.\n  */\n template<class T, class A>\n-batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::zip_hi<A>(x, y, A{});\n }\n \n@@ -1694,26 +1694,26 @@ batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the low part of shuffled values.\n  */\n template<class T, class A>\n-batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::zip_lo<A>(x, y, A{});\n }\n \n // bitwise_cast\n template <class A, class T, typename std::enable_if<std::is_integral<T>::value, int>::type = 3>\n-batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n+inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n {\n   T z(0);\n   return select(self, batch<T, A>(T(~z)), batch<T, A>(z));\n }\n-    \n+\n template <class A, class T, typename std::enable_if<std::is_floating_point<T>::value, int>::type = 3>\n-batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n+inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n {\n     T z0(0), z1(0);\n     using int_type = as_unsigned_integer_t<T>;\n     int_type value(~int_type(0));\n     std::memcpy(&z1, &value, sizeof(int_type));\n-    return select(self, batch<T, A>(z1), batch<T, A>(z0)); \n+    return select(self, batch<T, A>(z1), batch<T, A>(z0));\n }\n \n /**\n@@ -1725,7 +1725,7 @@ batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n  * @return a boolean scalar.\n  */\n template<class T, class A>\n-bool all(batch_bool<T, A> const& x) {\n+inline bool all(batch_bool<T, A> const& x) {\n   return kernel::all<A>(x, A{});\n }\n \n@@ -1738,7 +1738,7 @@ bool all(batch_bool<T, A> const& x) {\n  * @return a boolean scalar.\n  */\n template<class T, class A>\n-bool any(batch_bool<T, A> const& x) {\n+inline bool any(batch_bool<T, A> const& x) {\n   return kernel::any<A>(x, A{});\n }\n \n@@ -1751,7 +1751,7 @@ bool any(batch_bool<T, A> const& x) {\n  * @return a reference to \\c o\n  */\n template<class T, class A>\n-std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n+inline std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n   constexpr auto size = batch<T, A>::size;\n   alignas(A::alignment()) T buffer[size];\n   x.store_aligned(&buffer[0]);\n@@ -1763,4 +1763,3 @@ std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n }\n \n #endif\n-\n--- include/xsimd/types/xsimd_batch.hpp\n@@ -181,7 +181,7 @@ namespace xsimd\n             return batch(self).logical_or(other);\n         }\n     private:\n-  \n+\n         template<size_t... Is>\n         batch(T const* data, detail::index_sequence<Is...>);\n \n@@ -358,7 +358,7 @@ namespace xsimd\n         {\n             return batch(self) -= other;\n         }\n-        \n+\n         friend batch operator*(batch const& self, batch const& other)\n         {\n             return batch(self) *= other;\n@@ -430,40 +430,40 @@ namespace xsimd\n      **********************/\n \n     template<class T, class A>\n-    batch<T, A>::batch(T val)\n+    inline batch<T, A>::batch(T val)\n         : types::simd_register<T, A>(kernel::broadcast<A>(val, A{}))\n     {\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(std::initializer_list<T> data)\n+    inline batch<T, A>::batch(std::initializer_list<T> data)\n         : batch(data.begin(), detail::make_index_sequence<size>())\n     {\n         assert(data.size() == size && \"consistent initialization\");\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(batch_bool<T, A> const &b)\n+    inline batch<T, A>::batch(batch_bool<T, A> const &b)\n         : batch(kernel::from_bool(b, A{}))\n     {\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(register_type reg)\n+    inline batch<T, A>::batch(register_type reg)\n         : types::simd_register<T, A>({reg})\n     {\n     }\n \n     template<class T, class A>\n     template<size_t... Is>\n-    batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)\n+    inline batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)\n         : batch(kernel::set<A>(batch{}, A{}, data[Is]...))\n     {\n     }\n \n     template <class T, class A>\n     template<class U>\n-    XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)\n+    inline XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)\n     {\n         return batch(static_cast<T>(val));\n     }\n@@ -479,7 +479,7 @@ namespace xsimd\n     */\n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store_aligned(U* mem) const\n+    inline void batch<T, A>::store_aligned(U* mem) const\n     {\n         kernel::store_aligned<A>(mem, *this, A{});\n     }\n@@ -491,21 +491,21 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store_unaligned(U* mem) const\n+    inline void batch<T, A>::store_unaligned(U* mem) const\n     {\n         kernel::store_unaligned<A>(mem, *this, A{});\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store(U * mem, aligned_mode) const\n+    inline void batch<T, A>::store(U * mem, aligned_mode) const\n     {\n         return store_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store(U * mem, unaligned_mode) const\n+    inline void batch<T, A>::store(U * mem, unaligned_mode) const\n     {\n         return store_unaligned(mem);\n     }\n@@ -519,7 +519,7 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load_aligned(U const* mem)\n+    inline batch<T, A> batch<T, A>::load_aligned(U const* mem)\n     {\n         return kernel::load_aligned<A>(mem, kernel::convert<T>{}, A{});\n     }\n@@ -533,27 +533,27 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load_unaligned(U const* mem)\n+    inline batch<T, A> batch<T, A>::load_unaligned(U const* mem)\n     {\n         return kernel::load_unaligned<A>(mem, kernel::convert<T>{}, A{});\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)\n+    inline batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)\n+    inline batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)\n     {\n         return load_unaligned(mem);\n     }\n \n     template <class T, class A>\n-    T batch<T, A>::get(std::size_t i) const\n+    inline T batch<T, A>::get(std::size_t i) const\n     {\n         alignas(A::alignment()) T buffer[size];\n         store_aligned(&buffer[0]);\n@@ -565,114 +565,114 @@ namespace xsimd\n      ******************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const\n     {\n         return kernel::eq<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const\n     {\n         return kernel::neq<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const\n     {\n         return kernel::ge<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const\n+    {\n         return kernel::le<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const\n+    {\n         return kernel::gt<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const\n+    {\n         return kernel::lt<A>(*this, other, A{});\n     }\n-    \n+\n     /**************************\n      * batch update operators *\n      **************************/\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)\n     {\n         return *this = kernel::add<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)\n     {\n         return *this = kernel::sub<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)\n     {\n         return *this = kernel::mul<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)\n     {\n         return *this = kernel::div<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)\n     {\n         return *this = kernel::mod<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_and<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_or<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_xor<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_rshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)\n+    {\n         return *this = kernel::bitwise_lshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator>>=(int32_t other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator>>=(int32_t other)\n+    {\n         return *this = kernel::bitwise_rshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator<<=(int32_t other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator<<=(int32_t other)\n+    {\n         return *this = kernel::bitwise_lshift<A>(*this, other, A{});\n     }\n \n@@ -681,28 +681,28 @@ namespace xsimd\n      *****************************/\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator++()\n-    { \n+    inline batch<T, A>& batch<T, A>::operator++()\n+    {\n         return operator+=(1);\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator--()\n-    { \n+    inline batch<T, A>& batch<T, A>::operator--()\n+    {\n         return operator-=(1);\n     }\n-    \n+\n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator++(int)\n+    inline batch<T, A> batch<T, A>::operator++(int)\n     {\n         batch<T, A> copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator--(int)\n-    { \n+    inline batch<T, A> batch<T, A>::operator--(int)\n+    {\n         batch copy(*this);\n         operator-=(1);\n         return copy;\n@@ -713,25 +713,25 @@ namespace xsimd\n      *************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator!() const\n+    inline batch_bool<T, A> batch<T, A>::operator!() const\n     {\n         return kernel::eq<A>(*this, batch(0), A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator~() const\n+    inline batch<T, A> batch<T, A>::operator~() const\n     {\n         return kernel::bitwise_not<A>(*this, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator-() const\n-    { \n+    inline batch<T, A> batch<T, A>::operator-() const\n+    {\n         return kernel::neg<A>(*this, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator+() const\n+    inline batch<T, A> batch<T, A>::operator+() const\n     {\n         return *this;\n     }\n@@ -741,13 +741,13 @@ namespace xsimd\n      ************************/\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const\n+    inline batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const\n     {\n         return kernel::logical_and<A>(*this, other, A());\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const\n+    inline batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const\n     {\n         return kernel::logical_or<A>(*this, other, A());\n     }\n@@ -758,42 +758,41 @@ namespace xsimd\n \n     template<class T, class A>\n     template<size_t... Is>\n-    batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)\n+    inline batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)\n         : batch_bool(kernel::set<A>(batch_bool{}, A{}, data[Is]...))\n     {\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(register_type reg)\n+    inline batch_bool<T, A>::batch_bool(register_type reg)\n         : types::get_bool_simd_register_t<T, A>({reg})\n     {\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)\n+    inline batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)\n         : batch_bool(data.begin(), detail::make_index_sequence<size>())\n     {\n-        assert(data.size() == size && \"consistent initialization\");\n     }\n \n     /*******************************\n      * batch_bool memory operators *\n      *******************************/\n \n     template<class T, class A>\n-    void batch_bool<T, A>::store_aligned(bool* mem) const\n+    inline void batch_bool<T, A>::store_aligned(bool* mem) const\n     {\n         kernel::store(*this, mem, A{});\n     }\n \n     template<class T, class A>\n-    void batch_bool<T, A>::store_unaligned(bool* mem) const\n+    inline void batch_bool<T, A>::store_unaligned(bool* mem) const\n     {\n         store_aligned(mem);\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)\n+    inline batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)\n     {\n         batch_type ref(0);\n         alignas(A::alignment()) T buffer[size];\n@@ -803,13 +802,13 @@ namespace xsimd\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)\n+    inline batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n-    bool batch_bool<T, A>::get(std::size_t i) const\n+    inline bool batch_bool<T, A>::get(std::size_t i) const\n     {\n         alignas(A::alignment()) bool buffer[size];\n         store_aligned(&buffer[0]);\n@@ -821,13 +820,13 @@ namespace xsimd\n      ***********************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const\n     {\n         return kernel::eq<A>(*this, other, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const\n     {\n         return kernel::neq<A>(*this, other, A{}).data;\n     }\n@@ -837,37 +836,37 @@ namespace xsimd\n      ********************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator~() const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator~() const\n     {\n         return kernel::bitwise_not<A>(*this, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator!() const\n-    { \n+    inline batch_bool<T, A> batch_bool<T, A>::operator!() const\n+    {\n         return operator==(batch_bool(false));\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const\n     {\n         return kernel::bitwise_and<A>(*this, other, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const\n     {\n         return kernel::bitwise_or<A>(*this, other, A{}).data;\n     }\n-        \n+\n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const\n     {\n         return operator&(other);\n     }\n-    \n+\n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const\n     {\n         return operator|(other);\n     }\n@@ -877,21 +876,21 @@ namespace xsimd\n      ******************************/\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(bool val)\n+    inline batch_bool<T, A>::batch_bool(bool val)\n         : base_type{make_register(detail::make_index_sequence<size-1>(), val)}\n     {\n     }\n \n     template <class T, class A>\n     template <class U, class... V, size_t I, size_t... Is>\n-    auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type\n+    inline auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type\n     {\n         return make_register(detail::index_sequence<Is...>(), u, u, v...);\n     }\n \n     template <class T, class A>\n     template <class... V>\n-    auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type\n+    inline auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type\n     {\n         return kernel::set<A>(batch_bool<T, A>(), A{}, v...).data;\n     }\n@@ -901,38 +900,37 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(value_type const& val)\n+    inline batch<std::complex<T>, A>::batch(value_type const& val)\n         : m_real(val.real()), m_imag(val.imag())\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)\n+    inline batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)\n         : m_real(real), m_imag(imag)\n     {\n     }\n-        \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(real_batch const& real)\n+    inline batch<std::complex<T>, A>::batch(real_batch const& real)\n         : m_real(real), m_imag(0)\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(T val)\n+    inline batch<std::complex<T>, A>::batch(T val)\n         : m_real(val), m_imag(0)\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)\n-    { \n-        assert(data.size() == size && \"consistent initialization\");\n+    inline batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)\n+    {\n         *this = load_unaligned(data.begin());\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(batch_bool_type const& b)\n+    inline batch<std::complex<T>, A>::batch(batch_bool_type const& b)\n         : m_real(b), m_imag(0)\n     {\n     }\n@@ -942,96 +940,96 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)\n     {\n         return {batch<T, A>::load_aligned(real_src), imag_src ? batch<T, A>::load_aligned(imag_src) : batch<T, A>(0)};\n     }\n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)\n     {\n         return {batch<T, A>::load_unaligned(real_src), imag_src?batch<T, A>::load_unaligned(imag_src):batch<T, A>(0)};\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)\n     {\n         return kernel::load_complex_aligned<A>(src, kernel::convert<value_type>{}, A{});\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)\n     {\n         return kernel::load_complex_unaligned<A>(src, kernel::convert<value_type>{}, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_aligned(value_type* dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(value_type* dst) const\n     {\n         return kernel::store_complex_aligned(dst, *this, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const\n     {\n         return kernel::store_complex_unaligned(dst, *this, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const\n     {\n         m_real.store_aligned(real_dst);\n         m_imag.store_aligned(imag_dst);\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const\n     {\n         m_real.store_unaligned(real_dst);\n         m_imag.store_unaligned(imag_dst);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)\n+    {\n         return load_unaligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const\n+    inline void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const\n     {\n         return store_aligned(mem);\n     }\n-    \n+\n     template<class T, class A>\n     template<class U>\n-    void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const\n-    { \n+    inline void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const\n+    {\n         return store_unaligned(mem);\n     }\n \n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::real() const -> real_batch\n+    inline auto batch<std::complex<T>, A>::real() const -> real_batch\n     {\n-        return m_real; \n+        return m_real;\n     }\n-    \n+\n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::imag() const -> real_batch \n+    inline auto batch<std::complex<T>, A>::imag() const -> real_batch\n     {\n         return m_imag;\n     }\n \n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type\n+    inline auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type\n     {\n         alignas(A::alignment()) value_type buffer[size];\n         store_aligned(&buffer[0]);\n@@ -1046,16 +1044,15 @@ namespace xsimd\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)\n+    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)\n         : m_real(val.real()), m_imag(val.imag())\n     {\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)\n+    inline batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)\n     {\n-        assert(data.size() == size && \"consistent initialization\");\n         *this = load_unaligned(data.begin());\n     }\n \n@@ -1065,28 +1062,28 @@ namespace xsimd\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)\n     {\n         return load_aligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)\n     {\n         return load_unaligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const\n     {\n         store_aligned(reinterpret_cast<std::complex<T> *>(dst));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const\n     {\n         store_unaligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n@@ -1098,14 +1095,14 @@ namespace xsimd\n      ***************************************/\n \n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const\n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const\n     {\n         return m_real == other.m_real && m_imag == other.m_imag;\n     }\n-    \n+\n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const\n-    { \n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const\n+    {\n         return m_real != other.m_real || m_imag != other.m_imag;\n     }\n \n@@ -1114,23 +1111,23 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)\n     {\n         m_real += other.m_real;\n         m_imag += other.m_imag;\n         return *this;\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)\n     {\n         m_real -= other.m_real;\n         m_imag -= other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)\n     {\n         real_batch new_real = real() * other.real() - imag() * other.imag();\n         real_batch new_imag = real() * other.imag() + imag() * other.real();\n@@ -1140,7 +1137,7 @@ namespace xsimd\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)\n     {\n         real_batch a = real();\n         real_batch b = imag();\n@@ -1157,27 +1154,27 @@ namespace xsimd\n      **************************************/\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()\n-    { \n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()\n+    {\n         return operator+=(1);\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()\n-    { \n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()\n+    {\n         return operator-=(1);\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)\n     {\n         batch copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)\n     {\n         batch copy(*this);\n         operator-=(1);\n@@ -1189,26 +1186,26 @@ namespace xsimd\n      **********************************/\n \n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator!() const\n-    { \n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!() const\n+    {\n         return operator==(batch(0));\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const\n+    {\n         return {~m_real, ~m_imag};\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const\n+    {\n         return {-m_real, -m_imag};\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const\n+    {\n         return {+m_real, +m_imag};\n     }\n }\n--- include/xsimd/types/xsimd_batch_constant.hpp\n@@ -65,13 +65,13 @@ namespace xsimd\n     namespace detail\n     {\n         template <class batch_type, class G, std::size_t... Is>\n-        constexpr auto make_batch_constant(detail::index_sequence<Is...>)\n+        inline constexpr auto make_batch_constant(detail::index_sequence<Is...>)\n             -> batch_constant<batch_type, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n         }\n         template <class batch_type, class G, std::size_t... Is>\n-        constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)\n+        inline constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)\n             -> batch_bool_constant<batch_type, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n@@ -80,14 +80,14 @@ namespace xsimd\n     } // namespace detail\n \n     template <class batch_type, class G>\n-    constexpr auto make_batch_constant() -> decltype(\n+    inline constexpr auto make_batch_constant() -> decltype(\n         detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>()))\n     {\n         return detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>());\n     }\n \n     template <class batch_type, class G>\n-    constexpr auto make_batch_bool_constant()\n+    inline constexpr auto make_batch_bool_constant()\n         -> decltype(detail::make_batch_bool_constant<batch_type, G>(\n             detail::make_index_sequence<batch_type::size>()))\n     {\n--- include/xsimd/types/xsimd_utils.hpp\n@@ -178,7 +178,7 @@ namespace xsimd\n      ********************/\n \n     template<class To, class From>\n-    To bit_cast(From val) {\n+    inline To bit_cast(From val) {\n       static_assert(sizeof(From) == sizeof(To), \"casting between compatible layout\");\n       // FIXME: Some old version of GCC don't support that trait\n       //static_assert(std::is_trivially_copyable<From>::value, \"input type is trivially copyable\");\n@@ -269,20 +269,20 @@ namespace xsimd\n     namespace detail\n     {\n         template <class T, class... Types, size_t I, size_t... Is>\n-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)\n+        inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)\n         {\n             return std::get<I>(t);\n         }\n \n         template <class T, class U, class... Types, size_t I, size_t... Is>\n-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)\n+        inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)\n         {\n             using tuple_elem = typename std::tuple_element<I+1, std::tuple<Types...>>::type;\n             return get_impl<T>(t, std::is_same<T, tuple_elem>(), index_sequence<Is...>());\n         }\n \n         template <class T, class... Types>\n-        const T& get(const std::tuple<Types...>& t)\n+        inline const T& get(const std::tuple<Types...>& t)\n         {\n             using tuple_elem = typename std::tuple_element<0, std::tuple<Types...>>::type;\n             return get_impl<T>(t, std::is_same<T, tuple_elem>(), make_index_sequence<sizeof...(Types)>());\n@@ -329,7 +329,7 @@ namespace xsimd\n     {\n         // std::array constructor from scalar value (\"broadcast\")\n         template <typename T, std::size_t... Is>\n-        constexpr std::array<T, sizeof...(Is)>\n+        inline constexpr std::array<T, sizeof...(Is)>\n         array_from_scalar_impl(const T& scalar, index_sequence<Is...>)\n         {\n             // You can safely ignore this silly ternary, the \"scalar\" is all\n@@ -338,22 +338,22 @@ namespace xsimd\n         }\n \n         template <typename T, std::size_t N>\n-        constexpr std::array<T, N>\n+        inline constexpr std::array<T, N>\n         array_from_scalar(const T& scalar)\n         {\n             return array_from_scalar_impl(scalar, make_index_sequence<N>());\n         }\n \n         // std::array constructor from C-style pointer (handled as an array)\n         template <typename T, std::size_t... Is>\n-        constexpr std::array<T, sizeof...(Is)>\n+        inline constexpr std::array<T, sizeof...(Is)>\n         array_from_pointer_impl(const T* c_array, index_sequence<Is...>)\n         {\n             return std::array<T, sizeof...(Is)>{ c_array[Is]... };\n         }\n \n         template <typename T, std::size_t N>\n-        constexpr std::array<T, N>\n+        inline constexpr std::array<T, N>\n         array_from_pointer(const T* c_array)\n         {\n             return array_from_pointer_impl(c_array, make_index_sequence<N>());"
    ],
    "files_changed": [
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp",
        "status": "modified",
        "additions": 20,
        "deletions": 12,
        "changes": 32,
        "patch": "@@ -25,61 +25,69 @@ namespace xsimd {\n \n     // bitwise_lshift\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x << y;}, self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x >> y;}, self, other);\n     }\n \n     // div\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x / y;}, self, other);\n     }\n \n     // fma\n-    template<class A, class T> batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return x * y + z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n       auto res_i = fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fms\n-    template<class A, class T> batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return x * y - z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n       auto res_i = fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fnma\n-    template<class A, class T> batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return -x * y + z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = - fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n       auto res_i = - fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n     }\n \n     // fnms\n-    template<class A, class T> batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) {\n       return -x * y - z;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       auto res_r = - fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n       auto res_i = - fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n       return {res_r, res_i};\n@@ -89,7 +97,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x * y;}, self, other);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_complex.hpp",
        "status": "modified",
        "additions": 10,
        "deletions": 11,
        "changes": 21,
        "patch": "@@ -24,47 +24,47 @@ namespace xsimd {\n \n     // real\n     template <class A, class T>\n-    batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n \n     template <class A, class T>\n-    batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return self.real();\n     }\n \n     // imag\n     template <class A, class T>\n-    batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {\n+    inline batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) {\n       return batch<T, A>(T(0));\n     }\n \n     template <class A, class T>\n-    batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return self.imag();\n     }\n-    \n+\n     // arg\n     template<class A, class T>\n-    real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {\n+    inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) {\n       return atan2(imag(self), real(self));\n     }\n \n     // conj\n     template<class A, class T>\n-    complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {\n+    inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) {\n       return {real(self), - imag(self)};\n     }\n \n     // norm\n     template<class A, class T>\n-    real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {\n+    inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) {\n       return {fma(real(self), real(self), imag(self) * imag(self))};\n     }\n \n     // proj\n     template<class A, class T>\n-    complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {\n+    inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = complex_batch_type_t<batch<T, A>>;\n       using real_batch = typename batch_type::real_batch;\n       using real_value_type = typename real_batch::value_type;\n@@ -76,11 +76,10 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    inline batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return batch_bool<T, A>(isnan(self.real()) || isnan(self.imag()));\n     }\n   }\n }\n \n #endif\n-"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_details.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 3,
        "changes": 4,
        "patch": "@@ -105,7 +105,7 @@ namespace xsimd {\n \n     namespace detail {\n       template<class F, class A, class T, class... Batches>\n-      batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {\n+      inline batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) {\n         constexpr std::size_t size = batch<T, A>::size;\n         alignas(A::alignment()) T self_buffer[size];\n         alignas(A::alignment()) T other_buffer[size];\n@@ -207,8 +207,6 @@ namespace xsimd {\n         }\n     }\n \n-\n-\n   }\n \n }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_logical.hpp",
        "status": "modified",
        "additions": 26,
        "deletions": 16,
        "changes": 42,
        "patch": "@@ -22,82 +22,92 @@ namespace xsimd {\n     using namespace types;\n \n     // ge\n-    template<class A, class T> batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return other <= self;\n     }\n \n     // gt\n-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return other < self;\n     }\n \n     // is_even\n-    template<class A, class T> batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) {\n       return is_flint(self * T(0.5));\n     }\n \n     // is_flint\n-    template<class A, class T> batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) {\n       auto frac = select(isnan(self - self), constants::nan<batch<T, A>>(), self - trunc(self));\n       return frac == T(0.);\n     }\n \n     // is_odd\n-    template<class A, class T> batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) {\n       return is_even(self - T(1.));\n     }\n \n     // isinf\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isinf(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isinf(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(false);\n     }\n-    template<class A> batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) {\n       return abs(self) == std::numeric_limits<float>::infinity();\n     }\n-    template<class A> batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) {\n       return abs(self) == std::numeric_limits<double>::infinity();\n     }\n \n     // isfinite\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isfinite(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isfinite(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(true);\n     }\n-    template<class A> batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) {\n       return (self - self) == 0;\n     }\n-    template<class A> batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) {\n       return (self - self) == 0;\n     }\n \n     // isnan\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> isnan(batch<T, A> const& , requires_arch<generic>) {\n+    inline batch_bool<T, A> isnan(batch<T, A> const& , requires_arch<generic>) {\n       return batch_bool<T, A>(false);\n     }\n \n     // le\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return (self < other) || (self == other);\n     }\n \n \n     // neq\n-    template<class A, class T> batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return !(other == self);\n     }\n \n     // logical_and\n     template <class A, class T>\n-    batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x && y;}, self, other);\n     }\n \n     // logical_or\n     template <class A, class T>\n-    batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) { return x || y;}, self, other);\n     }\n   }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_math.hpp",
        "status": "modified",
        "additions": 336,
        "deletions": 275,
        "changes": 611,
        "patch": "@@ -26,7 +26,7 @@ namespace xsimd {\n     using namespace types;\n     // abs\n     template<class A, class T, class/*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>)\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>)\n     {\n       if(std::is_unsigned<T>::value)\n         return self;\n@@ -38,22 +38,23 @@ namespace xsimd {\n     }\n \n     template<class A, class T>\n-    batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    inline batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return hypot(z.real(), z.imag());\n     }\n \n     // batch_cast\n-    template<class A, class T> batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) {\n       return self;\n     }\n \n     namespace detail {\n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) {\n       return fast_cast(self, out, A{});\n     }\n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be no conversion for this type combination\");\n       using batch_type_in = batch<T_in, A>;\n       using batch_type_out = batch<T_out, A>;\n@@ -68,23 +69,26 @@ namespace xsimd {\n     }\n \n     template<class A, class T_out, class T_in>\n-    batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {\n+    inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) {\n       return detail::batch_cast(self, out, A{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // bitofsign\n-    template<class A, class T> batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) {\n       static_assert(std::is_integral<T>::value, \"int type implementation\");\n       if(std::is_unsigned<T>::value)\n         return batch<T, A>(0);\n       else\n         return self >> (T)(8 * sizeof(T) - 1);\n     }\n \n-    template<class A> batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) {\n       return self & constants::minuszero<batch<float, A>>();\n     }\n-    template<class A> batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) {\n       return self & constants::minuszero<batch<double, A>>();\n     }\n \n@@ -99,7 +103,8 @@ namespace xsimd {\n          * (See copy at http://boost.org/LICENSE_1_0.txt)\n          * ====================================================\n          */\n-    template<class A> batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type z = abs(self);\n #ifndef XSIMD_NO_DENORMALS\n@@ -143,7 +148,9 @@ namespace xsimd {\n                 return select(self == batch_type(0.), self, x);\n #endif\n     }\n-    template<class A> batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type z = abs(self);\n #ifndef XSIMD_NO_DENORMALS\n@@ -190,13 +197,15 @@ namespace xsimd {\n     }\n \n     // clip\n-    template<class A, class T> batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) {\n       return min(hi, max(self, lo));\n     }\n \n \n     // copysign\n-    template<class A, class T> batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return abs(self) | bitofsign(other);\n     }\n \n@@ -375,7 +384,7 @@ namespace xsimd {\n          */\n \n     template<class A>\n-    batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {\n+    inline batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 batch_type r1(0.);\n@@ -395,8 +404,10 @@ namespace xsimd {\n                 r1 = select(xsimd::isinf(self), sign(self), r1);\n #endif\n                 return r1;\n-            }\n-    template<class A> batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {\n+    }\n+\n+    template<class A>\n+    inline batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 batch_type xx = x * x;\n@@ -430,7 +441,8 @@ namespace xsimd {\n     }\n \n     // erfc\n-    template<class A> batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 auto test0 = self < batch_type(0.);\n@@ -451,7 +463,9 @@ namespace xsimd {\n #endif\n                 return select(test0, batch_type(2.) - r1, r1);\n     }\n-    template<class A> batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 batch_type xx = x * x;\n@@ -547,8 +561,9 @@ namespace xsimd {\n             }\n         };\n     }\n+\n     template <class T, class A, uint64_t... Coefs>\n-    batch<T, A> estrin(const batch<T, A>& self) {\n+    inline batch<T, A> estrin(const batch<T, A>& self) {\n       using batch_type = batch<T, A>;\n       return detail::estrin<batch_type>{self}(detail::coef<batch_type, Coefs>()...);\n     }\n@@ -786,7 +801,8 @@ namespace xsimd {\n             }\n         };\n \n-      template<exp_reduction_tag Tag, class A> batch<float, A> exp(batch<float, A> const& self) {\n+      template<exp_reduction_tag Tag, class A>\n+      inline batch<float, A> exp(batch<float, A> const& self) {\n         using batch_type = batch<float, A>;\n         using reducer_t = exp_reduction<float, A, Tag>;\n         batch_type x;\n@@ -796,7 +812,9 @@ namespace xsimd {\n         x = select(self >= reducer_t::maxlog(), constants::infinity<batch_type>(), x);\n         return x;\n       }\n-      template<exp_reduction_tag Tag, class A> batch<double, A> exp(batch<double, A> const& self) {\n+\n+      template<exp_reduction_tag Tag, class A>\n+      inline batch<double, A> exp(batch<double, A> const& self) {\n         using batch_type = batch<double, A>;\n         using reducer_t = exp_reduction<double, A, Tag>;\n         batch_type hi, lo, x;\n@@ -809,23 +827,27 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A, class T> batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp_tag>(self);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<std::complex<T>, A>;\n       auto isincos = sincos(self.imag());\n       return exp(self.real()) * batch_type(std::get<1>(isincos), std::get<0>(isincos));\n     }\n \n     // exp10\n-    template<class A, class T> batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp10_tag>(self);\n     }\n \n     // exp2\n-    template<class A, class T> batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::exp<detail::exp2_tag>(self);\n     }\n \n@@ -895,7 +917,8 @@ namespace xsimd {\n \n     }\n \n-    template<class A, class T> batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       return select(self < constants::logeps<batch_type>(),\n                     batch_type(-1.),\n@@ -905,7 +928,7 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    inline batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n     {\n         using batch_type = batch<std::complex<T>, A>;\n         using real_batch = typename batch_type::real_batch;\n@@ -917,12 +940,14 @@ namespace xsimd {\n     }\n \n     // fdim\n-    template<class A, class T> batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return fmax(batch<T, A>(0), self - other);\n     }\n \n     // fmod\n-    template<class A, class T> batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return fnma(trunc(self / other), other, self);\n     }\n \n@@ -937,7 +962,7 @@ namespace xsimd {\n      * ====================================================\n      */\n     template <class A, class T>\n-    batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {\n+    inline batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         using i_type = batch<as_integer_t<T>, A>;\n         i_type m1f = constants::mask1frexp<batch_type>();\n@@ -950,28 +975,31 @@ namespace xsimd {\n \n     // from bool\n     template<class A, class T>\n-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) {\n       return batch<T, A>(self.data) & batch<T,A>(1);\n     }\n \n     // hadd\n-    template<class A, class T> std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline std::complex<T> hadd(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return {hadd(self.real()), hadd(self.imag())};\n     }\n \n     // horner\n     template <class T, class A, uint64_t... Coefs>\n-    batch<T, A> horner(const batch<T, A>& self) {\n+    inline batch<T, A> horner(const batch<T, A>& self) {\n       return detail::horner<batch<T, A>, Coefs...>(self);\n     }\n \n     // hypot\n-    template<class A, class T> batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return sqrt(fma(self, self, other * other));\n     }\n \n     // ipow\n-    template<class A, class T, class ITy> batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {\n+    template<class A, class T, class ITy>\n+    inline batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) {\n       return ::xsimd::detail::ipow(self, other);\n     }\n \n@@ -987,7 +1015,7 @@ namespace xsimd {\n       * ====================================================\n       */\n     template <class A, class T>\n-    batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {\n+    inline batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         using itype = as_integer_t<batch_type>;\n         itype ik = other + constants::maxexponent<T>();\n@@ -996,7 +1024,8 @@ namespace xsimd {\n     }\n \n     // lgamma\n-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);\n+    template<class A, class T>\n+    inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>);\n \n     namespace detail {\n     /* origin: boost/simd/arch/common/detail/generic/gammaln_kernel.hpp */\n@@ -1009,77 +1038,79 @@ namespace xsimd {\n      * ====================================================\n      */\n     template<class A>\n-            static inline batch<float, A> gammalnB(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0x3ed87730,  //    4.227843421859038E-001\n-                              0x3ea51a64,  //    3.224669577325661E-001,\n-                              0xbd89f07e,  //   -6.735323259371034E-002,\n-                              0x3ca89ed8,  //    2.058355474821512E-002,\n-                              0xbbf164fd,  //   -7.366775108654962E-003,\n-                              0x3b3ba883,  //    2.863437556468661E-003,\n-                              0xbaabeab1,  //   -1.311620815545743E-003,\n-                              0x3a1ebb94  //    6.055172732649237E-004\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammalnB(const batch<float, A>& x)\n+    {\n+            return horner<batch<float, A>,\n+                          0x3ed87730,  //    4.227843421859038E-001\n+                          0x3ea51a64,  //    3.224669577325661E-001,\n+                          0xbd89f07e,  //   -6.735323259371034E-002,\n+                          0x3ca89ed8,  //    2.058355474821512E-002,\n+                          0xbbf164fd,  //   -7.366775108654962E-003,\n+                          0x3b3ba883,  //    2.863437556468661E-003,\n+                          0xbaabeab1,  //   -1.311620815545743E-003,\n+                          0x3a1ebb94  //    6.055172732649237E-004\n+                          >(x);\n+     }\n \n     template<class A>\n-            static inline batch<float, A> gammalnC(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0xbf13c468,  //   -5.772156501719101E-001\n-                              0x3f528d34,  //    8.224670749082976E-001,\n-                              0xbecd27a8,  //   -4.006931650563372E-001,\n-                              0x3e8a898b,  //    2.705806208275915E-001,\n-                              0xbe53c04f,  //   -2.067882815621965E-001,\n-                              0x3e2d4dab,  //    1.692415923504637E-001,\n-                              0xbe22d329,  //   -1.590086327657347E-001,\n-                              0x3e0c3c4f  //    1.369488127325832E-001\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammalnC(const batch<float, A>& x)\n+    {\n+        return horner<batch<float, A>,\n+                      0xbf13c468,  //   -5.772156501719101E-001\n+                      0x3f528d34,  //    8.224670749082976E-001,\n+                      0xbecd27a8,  //   -4.006931650563372E-001,\n+                      0x3e8a898b,  //    2.705806208275915E-001,\n+                      0xbe53c04f,  //   -2.067882815621965E-001,\n+                      0x3e2d4dab,  //    1.692415923504637E-001,\n+                      0xbe22d329,  //   -1.590086327657347E-001,\n+                      0x3e0c3c4f  //    1.369488127325832E-001\n+                      >(x);\n+    }\n \n     template<class A>\n-            static inline batch<float, A> gammaln2(const batch<float, A>& x)\n-            {\n-                return horner<batch<float, A>,\n-                              0x3daaaa94,  //   8.333316229807355E-002f\n-                              0xbb358701,  //  -2.769887652139868E-003f,\n-                              0x3a31fd69  //   6.789774945028216E-004f\n-                              >(x);\n-            }\n+    static inline batch<float, A> gammaln2(const batch<float, A>& x)\n+    {\n+        return horner<batch<float, A>,\n+                      0x3daaaa94,  //   8.333316229807355E-002f\n+                      0xbb358701,  //  -2.769887652139868E-003f,\n+                      0x3a31fd69  //   6.789774945028216E-004f\n+                      >(x);\n+    }\n+\n     template<class A>\n-            static inline batch<double, A> gammaln1(const batch<double, A>& x)\n-            {\n-                return horner<batch<double, A>,\n-                              0xc12a0c675418055eull,  //  -8.53555664245765465627E5\n-                              0xc13a45890219f20bull,  //  -1.72173700820839662146E6,\n-                              0xc131bc82f994db51ull,  //  -1.16237097492762307383E6,\n-                              0xc1143d73f89089e5ull,  //  -3.31612992738871184744E5,\n-                              0xc0e2f234355bb93eull,  //  -3.88016315134637840924E4,\n-                              0xc09589018ff36761ull  //  -1.37825152569120859100E3\n-                              >(x) /\n-                    horner<batch<double, A>,\n-                           0xc13ece4b6a11e14aull,  //  -2.01889141433532773231E6\n-                           0xc1435255892ff34cull,  //  -2.53252307177582951285E6,\n-                           0xc131628671950043ull,  //  -1.13933444367982507207E6,\n-                           0xc10aeb84b9744c9bull,  //  -2.20528590553854454839E5,\n-                           0xc0d0aa0d7b89d757ull,  //  -1.70642106651881159223E4,\n-                           0xc075fd0d1cf312b2ull,  //  -3.51815701436523470549E2,\n-                           0x3ff0000000000000ull  //   1.00000000000000000000E0\n-                           >(x);\n-            }\n+    static inline batch<double, A> gammaln1(const batch<double, A>& x)\n+    {\n+        return horner<batch<double, A>,\n+                      0xc12a0c675418055eull,  //  -8.53555664245765465627E5\n+                      0xc13a45890219f20bull,  //  -1.72173700820839662146E6,\n+                      0xc131bc82f994db51ull,  //  -1.16237097492762307383E6,\n+                      0xc1143d73f89089e5ull,  //  -3.31612992738871184744E5,\n+                      0xc0e2f234355bb93eull,  //  -3.88016315134637840924E4,\n+                      0xc09589018ff36761ull  //  -1.37825152569120859100E3\n+                      >(x) /\n+            horner<batch<double, A>,\n+                   0xc13ece4b6a11e14aull,  //  -2.01889141433532773231E6\n+                   0xc1435255892ff34cull,  //  -2.53252307177582951285E6,\n+                   0xc131628671950043ull,  //  -1.13933444367982507207E6,\n+                   0xc10aeb84b9744c9bull,  //  -2.20528590553854454839E5,\n+                   0xc0d0aa0d7b89d757ull,  //  -1.70642106651881159223E4,\n+                   0xc075fd0d1cf312b2ull,  //  -3.51815701436523470549E2,\n+                   0x3ff0000000000000ull  //   1.00000000000000000000E0\n+                   >(x);\n+    }\n \n     template<class A>\n-            static inline batch<double, A> gammalnA(const batch<double, A>& x)\n-            {\n-                return horner<batch<double, A>,\n-                              0x3fb555555555554bull,  //    8.33333333333331927722E-2\n-                              0xbf66c16c16b0a5a1ull,  //   -2.77777777730099687205E-3,\n-                              0x3f4a019f20dc5ebbull,  //    7.93650340457716943945E-4,\n-                              0xbf437fbdb580e943ull,  //   -5.95061904284301438324E-4,\n-                              0x3f4a985027336661ull  //    8.11614167470508450300E-4\n-                              >(x);\n-            }\n+    static inline batch<double, A> gammalnA(const batch<double, A>& x)\n+    {\n+        return horner<batch<double, A>,\n+                      0x3fb555555555554bull,  //    8.33333333333331927722E-2\n+                      0xbf66c16c16b0a5a1ull,  //   -2.77777777730099687205E-3,\n+                      0x3f4a019f20dc5ebbull,  //    7.93650340457716943945E-4,\n+                      0xbf437fbdb580e943ull,  //   -5.95061904284301438324E-4,\n+                      0x3f4a985027336661ull  //    8.11614167470508450300E-4\n+                      >(x);\n+    }\n+\n     /* origin: boost/simd/arch/common/simd/function/gammaln.hpp */\n     /*\n      * ====================================================\n@@ -1089,129 +1120,129 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-        template <class B>\n-        struct lgamma_impl;\n+      template <class B>\n+      struct lgamma_impl;\n \n-        template <class A>\n-        struct lgamma_impl<batch<float, A>>\n-        {\n-          using batch_type = batch<float, A>;\n-            static inline batch_type compute(const batch_type& a)\n-            {\n-                auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n-                batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n-                batch_type q = abs(x);\n+      template <class A>\n+      struct lgamma_impl<batch<float, A>>\n+      {\n+        using batch_type = batch<float, A>;\n+          static inline batch_type compute(const batch_type& a)\n+          {\n+              auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n+              batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n+              batch_type q = abs(x);\n #ifndef XSIMD_NO_INFINITIES\n-                inf_result = (x == constants::infinity<batch_type>()) || inf_result;\n+              inf_result = (x == constants::infinity<batch_type>()) || inf_result;\n #endif\n-                auto ltza = a < batch_type(0.);\n-                batch_type r;\n-                batch_type r1 = other(q);\n-                if (any(ltza))\n-                {\n-                    r = select(inf_result, constants::infinity<batch_type>(), negative(q, r1));\n-                    if (all(ltza))\n-                        return r;\n-                }\n-                batch_type r2 = select(ltza, r, r1);\n-                return select(a == constants::minusinfinity<batch_type>(), constants::nan<batch_type>(), select(inf_result, constants::infinity<batch_type>(), r2));\n-            }\n-\n-        private:\n-\n-            static inline batch_type negative(const batch_type& q, const batch_type& w)\n-            {\n-                batch_type p = floor(q);\n-                batch_type z = q - p;\n-                auto test2 = z < batch_type(0.5);\n-                z = select(test2, z - batch_type(1.), z);\n-                z = q * sin(z, trigo_pi_tag());\n-                return -log(constants::invpi<batch_type>() * abs(z)) - w;\n-            }\n-\n-            static inline batch_type other(const batch_type& x)\n-            {\n-                auto xlt650 = (x < batch_type(6.5));\n-                batch_type r0x = x;\n-                batch_type r0z = x;\n-                batch_type r0s = batch_type(1.);\n-                batch_type r1 = batch_type(0.);\n-                batch_type p = constants::nan<batch_type>();\n-                if (any(xlt650))\n-                {\n-                    batch_type z = batch_type(1.);\n-                    batch_type tx = select(xlt650, x, batch_type(0.));\n-                    batch_type nx = batch_type(0.);\n-                    const batch_type _075 = batch_type(0.75);\n-                    const batch_type _150 = batch_type(1.50);\n-                    const batch_type _125 = batch_type(1.25);\n-                    const batch_type _250 = batch_type(2.50);\n-                    auto xge150 = (x >= _150);\n-                    auto txgt250 = (tx > _250);\n-\n-                    // x >= 1.5\n-                    while (any(xge150 && txgt250))\n-                    {\n-                        nx = select(txgt250, nx - batch_type(1.), nx);\n-                        tx = select(txgt250, x + nx, tx);\n-                        z = select(txgt250, z * tx, z);\n-                        txgt250 = (tx > _250);\n-                    }\n-                    r0x = select(xge150, x + nx - batch_type(2.), x);\n-                    r0z = select(xge150, z, r0z);\n-                    r0s = select(xge150, batch_type(1.), r0s);\n-\n-                    // x >= 1.25 && x < 1.5\n-                    auto xge125 = (x >= _125);\n-                    auto xge125t = xge125 && !xge150;\n-                    if (any(xge125))\n-                    {\n-                        r0x = select(xge125t, x - batch_type(1.), r0x);\n-                        r0z = select(xge125t, z * x, r0z);\n-                        r0s = select(xge125t, batch_type(-1.), r0s);\n-                    }\n-\n-                    // x >= 0.75 && x < 1.5\n-                    batch_bool<float, A> kernelC(false);\n-                    auto xge075 = (x >= _075);\n-                    auto xge075t = xge075 && !xge125;\n-                    if (any(xge075t))\n-                    {\n-                        kernelC = xge075t;\n-                        r0x = select(xge075t, x - batch_type(1.), x);\n-                        r0z = select(xge075t, batch_type(1.), r0z);\n-                        r0s = select(xge075t, batch_type(-1.), r0s);\n-                        p = gammalnC(r0x);\n-                    }\n-\n-                    // tx < 1.5 && x < 0.75\n-                    auto txlt150 = (tx < _150) && !xge075;\n-                    if (any(txlt150))\n-                    {\n-                        auto orig = txlt150;\n-                        while (any(txlt150))\n-                        {\n-                            z = select(txlt150, z * tx, z);\n-                            nx = select(txlt150, nx + batch_type(1.), nx);\n-                            tx = select(txlt150, x + nx, tx);\n-                            txlt150 = (tx < _150) && !xge075;\n-                        }\n-                        r0x = select(orig, r0x + nx - batch_type(2.), r0x);\n-                        r0z = select(orig, z, r0z);\n-                        r0s = select(orig, batch_type(-1.), r0s);\n-                    }\n-                    p = select(kernelC, p, gammalnB(r0x));\n-                    if (all(xlt650))\n-                        return fma(r0x, p, r0s * log(abs(r0z)));\n-                }\n-                r0z = select(xlt650, abs(r0z), x);\n-                batch_type m = log(r0z);\n-                r1 = fma(r0x, p, r0s * m);\n-                batch_type r2 = fma(x - batch_type(0.5), m, constants::logsqrt2pi<batch_type>() - x);\n-                r2 += gammaln2(batch_type(1.) / (x * x)) / x;\n-                return select(xlt650, r1, r2);\n-            }\n-        };\n+              auto ltza = a < batch_type(0.);\n+              batch_type r;\n+              batch_type r1 = other(q);\n+              if (any(ltza))\n+              {\n+                  r = select(inf_result, constants::infinity<batch_type>(), negative(q, r1));\n+                  if (all(ltza))\n+                      return r;\n+              }\n+              batch_type r2 = select(ltza, r, r1);\n+              return select(a == constants::minusinfinity<batch_type>(), constants::nan<batch_type>(), select(inf_result, constants::infinity<batch_type>(), r2));\n+          }\n+\n+      private:\n+\n+          static inline batch_type negative(const batch_type& q, const batch_type& w)\n+          {\n+              batch_type p = floor(q);\n+              batch_type z = q - p;\n+              auto test2 = z < batch_type(0.5);\n+              z = select(test2, z - batch_type(1.), z);\n+              z = q * sin(z, trigo_pi_tag());\n+              return -log(constants::invpi<batch_type>() * abs(z)) - w;\n+          }\n+\n+          static inline batch_type other(const batch_type& x)\n+          {\n+              auto xlt650 = (x < batch_type(6.5));\n+              batch_type r0x = x;\n+              batch_type r0z = x;\n+              batch_type r0s = batch_type(1.);\n+              batch_type r1 = batch_type(0.);\n+              batch_type p = constants::nan<batch_type>();\n+              if (any(xlt650))\n+              {\n+                  batch_type z = batch_type(1.);\n+                  batch_type tx = select(xlt650, x, batch_type(0.));\n+                  batch_type nx = batch_type(0.);\n+                  const batch_type _075 = batch_type(0.75);\n+                  const batch_type _150 = batch_type(1.50);\n+                  const batch_type _125 = batch_type(1.25);\n+                  const batch_type _250 = batch_type(2.50);\n+                  auto xge150 = (x >= _150);\n+                  auto txgt250 = (tx > _250);\n+\n+                  // x >= 1.5\n+                  while (any(xge150 && txgt250))\n+                  {\n+                      nx = select(txgt250, nx - batch_type(1.), nx);\n+                      tx = select(txgt250, x + nx, tx);\n+                      z = select(txgt250, z * tx, z);\n+                      txgt250 = (tx > _250);\n+                  }\n+                  r0x = select(xge150, x + nx - batch_type(2.), x);\n+                  r0z = select(xge150, z, r0z);\n+                  r0s = select(xge150, batch_type(1.), r0s);\n+\n+                  // x >= 1.25 && x < 1.5\n+                  auto xge125 = (x >= _125);\n+                  auto xge125t = xge125 && !xge150;\n+                  if (any(xge125))\n+                  {\n+                      r0x = select(xge125t, x - batch_type(1.), r0x);\n+                      r0z = select(xge125t, z * x, r0z);\n+                      r0s = select(xge125t, batch_type(-1.), r0s);\n+                  }\n+\n+                  // x >= 0.75 && x < 1.5\n+                  batch_bool<float, A> kernelC(false);\n+                  auto xge075 = (x >= _075);\n+                  auto xge075t = xge075 && !xge125;\n+                  if (any(xge075t))\n+                  {\n+                      kernelC = xge075t;\n+                      r0x = select(xge075t, x - batch_type(1.), x);\n+                      r0z = select(xge075t, batch_type(1.), r0z);\n+                      r0s = select(xge075t, batch_type(-1.), r0s);\n+                      p = gammalnC(r0x);\n+                  }\n+\n+                  // tx < 1.5 && x < 0.75\n+                  auto txlt150 = (tx < _150) && !xge075;\n+                  if (any(txlt150))\n+                  {\n+                      auto orig = txlt150;\n+                      while (any(txlt150))\n+                      {\n+                          z = select(txlt150, z * tx, z);\n+                          nx = select(txlt150, nx + batch_type(1.), nx);\n+                          tx = select(txlt150, x + nx, tx);\n+                          txlt150 = (tx < _150) && !xge075;\n+                      }\n+                      r0x = select(orig, r0x + nx - batch_type(2.), r0x);\n+                      r0z = select(orig, z, r0z);\n+                      r0s = select(orig, batch_type(-1.), r0s);\n+                  }\n+                  p = select(kernelC, p, gammalnB(r0x));\n+                  if (all(xlt650))\n+                      return fma(r0x, p, r0s * log(abs(r0z)));\n+              }\n+              r0z = select(xlt650, abs(r0z), x);\n+              batch_type m = log(r0z);\n+              r1 = fma(r0x, p, r0s * m);\n+              batch_type r2 = fma(x - batch_type(0.5), m, constants::logsqrt2pi<batch_type>() - x);\n+              r2 += gammaln2(batch_type(1.) / (x * x)) / x;\n+              return select(xlt650, r1, r2);\n+          }\n+      };\n \n         template <class A>\n         struct lgamma_impl<batch<double, A>>\n@@ -1295,7 +1326,8 @@ namespace xsimd {\n         };\n     }\n \n-    template<class A, class T> batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::lgamma_impl<batch<T, A>>::compute(self);\n     }\n \n@@ -1310,7 +1342,8 @@ namespace xsimd {\n              * (See copy at http://boost.org/LICENSE_1_0.txt)\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n       using i_type = as_integer_t<batch_type>;\n       batch_type x = self;\n@@ -1347,7 +1380,8 @@ namespace xsimd {\n       return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-    template<class A> batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) {\n                using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n \n@@ -1395,13 +1429,14 @@ namespace xsimd {\n     }\n \n     template <class A, class T>\n-    batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    inline batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n     {\n         return batch<std::complex<T>, A>(log(abs(z)), atan2(z.imag(), z.real()));\n     }\n \n     // log2\n-    template<class A> batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 batch_type x = self;\n@@ -1437,7 +1472,9 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 batch_type x = self;\n@@ -1488,6 +1525,7 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n+\n     namespace detail {\n         template <class T, class A>\n         inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base)\n@@ -1498,7 +1536,8 @@ namespace xsimd {\n         }\n   }\n \n-    template<class A, class T> batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n       return detail::logN_complex_impl(self, std::log(2));\n     }\n \n@@ -1514,7 +1553,8 @@ namespace xsimd {\n              * is preserved.\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 const batch_type\n                     ivln10hi(4.3432617188e-01f),\n@@ -1561,7 +1601,9 @@ namespace xsimd {\n #endif\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 const batch_type\n                     ivln10hi(4.34294481878168880939e-01),\n@@ -1619,8 +1661,8 @@ namespace xsimd {\n                 return select(!(self >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-        template <class A, class T>\n-            batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    template <class A, class T>\n+    inline batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n             {\n                 return detail::logN_complex_impl(z, std::log(10));\n             }\n@@ -1635,7 +1677,8 @@ namespace xsimd {\n              * (See copy at http://boost.org/LICENSE_1_0.txt)\n              * ====================================================\n              */\n-    template<class A> batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 const batch_type uf = self + batch_type(1.);\n@@ -1663,7 +1706,9 @@ namespace xsimd {\n #endif\n                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n-    template<class A> batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {\n+\n+    template<class A>\n+    inline batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 using i_type = as_integer_t<batch_type>;\n                 const batch_type uf = self + batch_type(1.);\n@@ -1700,7 +1745,8 @@ namespace xsimd {\n                 return select(!(uf >= batch_type(0.)), constants::nan<batch_type>(), zz);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) {\n         using batch_type = batch<std::complex<T>, A>;\n         using real_batch = typename batch_type::real_batch;\n         batch_type u = 1 + self;\n@@ -1715,18 +1761,19 @@ namespace xsimd {\n \n     // mod\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       return detail::apply([](T x, T y) -> T { return x % y;}, self, other);\n     }\n \n \n     // nearbyint\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n     namespace detail {\n-      template<class A, class T> batch<T, A> nearbyintf(batch<T, A> const& self) {\n+      template<class A, class T>\n+      inline batch<T, A> nearbyintf(batch<T, A> const& self) {\n         using batch_type = batch<T, A>;\n         batch_type s = bitofsign(self);\n         batch_type v = self ^ s;\n@@ -1744,10 +1791,12 @@ namespace xsimd {\n         return s ^ select(v < t2n, d, v);\n       }\n     }\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::nearbyintf(self);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::nearbyintf(self);\n     }\n \n@@ -1805,7 +1854,8 @@ namespace xsimd {\n             }\n         };\n     }\n-    template<class A, class T> batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) {\n       using kernel = detail::nextafter_kernel<T, A>;\n       return select(from == to, from,\n                     select(to > from, kernel::next(from), kernel::prev(from)));\n@@ -1822,7 +1872,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n         using batch_type = batch<T, A>;\n         auto negx = self < batch_type(0.);\n         batch_type z = exp(other * log(abs(self)));\n@@ -1831,55 +1882,57 @@ namespace xsimd {\n         return select(invalid, constants::nan<batch_type>(), z);\n     }\n \n-        template <class A, class T>\n-        inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)\n-        {\n-            using cplx_batch = batch<std::complex<T>, A>;\n-            using real_batch = typename cplx_batch::real_batch;\n-            real_batch absa = abs(a);\n-            real_batch arga = arg(a);\n-            real_batch x = z.real();\n-            real_batch y = z.imag();\n-            real_batch r = pow(absa, x);\n-            real_batch theta = x * arga;\n-            real_batch ze(0);\n-            auto cond = (y == ze);\n-            r = select(cond, r, r * exp(-y * arga));\n-            theta = select(cond, theta, theta + y * log(absa));\n-            return select(absa == ze, cplx_batch(ze), cplx_batch(r * cos(theta), r * sin(theta)));\n-        }\n+    template <class A, class T>\n+    inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+    {\n+        using cplx_batch = batch<std::complex<T>, A>;\n+        using real_batch = typename cplx_batch::real_batch;\n+        real_batch absa = abs(a);\n+        real_batch arga = arg(a);\n+        real_batch x = z.real();\n+        real_batch y = z.imag();\n+        real_batch r = pow(absa, x);\n+        real_batch theta = x * arga;\n+        real_batch ze(0);\n+        auto cond = (y == ze);\n+        r = select(cond, r, r * exp(-y * arga));\n+        theta = select(cond, theta, theta + y * log(absa));\n+        return select(absa == ze, cplx_batch(ze), cplx_batch(r * cos(theta), r * sin(theta)));\n+    }\n \n \n     // remainder\n     template<class A>\n-    batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {\n+    inline batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) {\n       return fnma(nearbyint(self / other), other, self);\n     }\n     template<class A>\n-    batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {\n+    inline batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) {\n       return fnma(nearbyint(self / other), other, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    inline batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       auto mod = self % other;\n       return select(mod <= other / 2, mod, mod - other);\n     }\n \n     // select\n     template<class A, class T>\n-    batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {\n+    inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) {\n       return {select(cond, true_br.real(), false_br.real()), select(cond, true_br.imag(), false_br.imag())};\n     }\n \n     // sign\n-    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type> batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n+    inline batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       batch_type res = select(self > batch_type(0), batch_type(1), batch_type(0)) - select(self < batch_type(0), batch_type(1), batch_type(0));\n       return res;\n     }\n \n     namespace detail {\n-    template<class T, class A> batch<T, A> signf(batch<T, A> const& self) {\n+    template<class T, class A>\n+    inline batch<T, A> signf(batch<T, A> const& self) {\n       using batch_type = batch<T, A>;\n       batch_type res = select(self > batch_type(0.f), batch_type(1.f), batch_type(0.f)) - select(self < batch_type(0.f), batch_type(1.f), batch_type(0.f));\n #ifdef XSIMD_NO_NANS\n@@ -1890,10 +1943,12 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A> batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::signf(self);\n     }\n-    template<class A> batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::signf(self);\n     }\n         template <class A, class T>\n@@ -1909,13 +1964,15 @@ namespace xsimd {\n         }\n \n     // signnz\n-    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type> batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n+    inline batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       return (self >> (sizeof(T) * 8 - 1)) | batch_type(1.);\n     }\n \n     namespace detail {\n-    template<class T, class A> batch<T, A> signnzf(batch<T, A> const& self) {\n+    template<class T, class A>\n+    inline batch<T, A> signnzf(batch<T, A> const& self) {\n       using batch_type = batch<T, A>;\n #ifndef XSIMD_NO_NANS\n                 return select(isnan(self), constants::nan<batch_type>(), batch_type(1.) | (constants::signmask<batch_type>() & self));\n@@ -1925,15 +1982,18 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A> batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) {\n       return detail::signnzf(self);\n     }\n-    template<class A> batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) {\n       return detail::signnzf(self);\n     }\n \n     // sqrt\n-    template<class A, class T> batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n \n                 constexpr T csqrt_scale_factor = std::is_same<T, float>::value?6.7108864e7f:1.8014398509481984e16;\n                 constexpr T csqrt_scale = std::is_same<T, float>::value?1.220703125e-4f:7.450580596923828125e-9;\n@@ -2134,7 +2194,7 @@ namespace xsimd {\n      * ====================================================\n      */\n         template <class B>\n-        B tgamma_large_negative(const B& a)\n+        inline B tgamma_large_negative(const B& a)\n         {\n             B st = stirling(a);\n             B p = floor(a);\n@@ -2148,7 +2208,7 @@ namespace xsimd {\n         }\n \n         template <class B, class BB>\n-        B tgamma_other(const B& a, const BB& test)\n+        inline B tgamma_other(const B& a, const BB& test)\n         {\n             B x = select(test, B(2.), a);\n #ifndef XSIMD_NO_INFINITIES\n@@ -2186,7 +2246,8 @@ namespace xsimd {\n         }\n     }\n \n-    template<class A, class T> batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n             auto nan_result = (self < batch_type(0.) && is_flint(self));\n #ifndef XSIMD_NO_INVALIDS"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_memory.hpp",
        "status": "modified",
        "additions": 20,
        "deletions": 17,
        "changes": 37,
        "patch": "@@ -26,7 +26,8 @@ namespace xsimd {\n     using namespace types;\n \n     // extract_pair\n-    template<class A, class T> batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) {\n       constexpr std::size_t size = batch<T, A>::size;\n       assert(0<= i && i< size && \"index in bounds\");\n \n@@ -52,13 +53,13 @@ namespace xsimd {\n     // load_aligned\n     namespace detail {\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n+      inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n         using batch_type_in = batch<T_in, A>;\n         using batch_type_out = batch<T_out, A>;\n         return fast_cast(batch_type_in::load_aligned(mem), batch_type_out(), A{});\n       }\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {\n+      inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) {\n         static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n         using batch_type_out = batch<T_out, A>;\n         alignas(A::alignment()) T_out buffer[batch_type_out::size];\n@@ -67,33 +68,33 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T_in, class T_out>\n-    batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n+    inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n       return detail::load_aligned<A>(mem, cvt, A{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // load_unaligned\n     namespace detail {\n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n+      inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) {\n         using batch_type_in = batch<T_in, A>;\n         using batch_type_out = batch<T_out, A>;\n         return fast_cast(batch_type_in::load_unaligned(mem), batch_type_out(), A{});\n       }\n \n       template<class A, class T_in, class T_out>\n-      batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {\n+      inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) {\n         static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n         return load_aligned<A>(mem, cvt, generic{}, with_slow_conversion{});\n       }\n     }\n     template<class A, class T_in, class T_out>\n-    batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n+    inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) {\n       return detail::load_unaligned<A>(mem, cvt, generic{}, detail::conversion_type<A, T_in, T_out>{});\n     }\n \n     // store\n     template<class T, class A>\n-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {\n+    inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n       constexpr auto size = batch_bool<T, A>::size;\n       alignas(A::alignment()) T buffer[size];\n@@ -104,43 +105,45 @@ namespace xsimd {\n \n \n     // store_aligned\n-    template<class A, class T_in, class T_out> void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n+    template<class A, class T_in, class T_out>\n+    inline void store_aligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n       alignas(A::alignment()) T_in buffer[batch<T_in, A>::size];\n       store_aligned(&buffer[0], self);\n       std::copy(std::begin(buffer), std::end(buffer), mem);\n     }\n \n     // store_unaligned\n-    template<class A, class T_in, class T_out> void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n+    template<class A, class T_in, class T_out>\n+    inline void store_unaligned(T_out *mem, batch<T_in, A> const& self, requires_arch<generic>) {\n       static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n       return store_aligned<A>(mem, self, generic{});\n     }\n \n     namespace detail\n     {\n         template <class A, class T>\n-        batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"load_complex not implemented for the required architecture\");\n         }\n \n         template <class A, class T>\n-        batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n+        inline batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"complex_high not implemented for the required architecture\");\n         }\n \n         template <class A, class T>\n-        batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n+        inline batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>)\n         {\n             static_assert(std::is_same<T, void>::value, \"complex_low not implemented for the required architecture\");\n         }\n     }\n \n     // load_complex_aligned\n     template <class A, class T_out, class T_in>\n-    batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {\n+    inline batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) {\n       using real_batch = batch<T_out, A>;\n       T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n       real_batch hi = real_batch::load_aligned(buffer),\n@@ -150,7 +153,7 @@ namespace xsimd {\n \n     // load_complex_unaligned\n     template <class A, class T_out, class T_in>\n-    batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {\n+    inline batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>> ,requires_arch<generic>) {\n       using real_batch = batch<T_out, A>;\n       T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n       real_batch hi = real_batch::load_unaligned(buffer),\n@@ -160,7 +163,7 @@ namespace xsimd {\n \n     // store_complex_aligned\n     template <class A, class T_out, class T_in>\n-    void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n+    inline void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n         using real_batch = batch<T_in, A>;\n         real_batch hi = detail::complex_high(src, A{});\n         real_batch lo = detail::complex_low(src, A{});\n@@ -171,7 +174,7 @@ namespace xsimd {\n \n     // store_compelx_unaligned\n     template <class A, class T_out, class T_in>\n-    void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n+    inline void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) {\n         using real_batch = batch<T_in, A>;\n         real_batch hi = detail::complex_high(src, A{});\n         real_batch lo = detail::complex_low(src, A{});"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_rounding.hpp",
        "status": "modified",
        "additions": 11,
        "deletions": 6,
        "changes": 17,
        "patch": "@@ -23,20 +23,23 @@ namespace xsimd {\n     using namespace types;\n \n     // ceil\n-    template<class A, class T> batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) {\n       batch<T, A> truncated_self = trunc(self);\n       return select(truncated_self < self, truncated_self + 1, truncated_self);\n     }\n \n \n     // floor\n-    template<class A, class T> batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) {\n       batch<T, A> truncated_self = trunc(self);\n       return select(truncated_self > self, truncated_self - 1, truncated_self);\n     }\n \n     // round\n-    template<class A, class T> batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) {\n       auto v = abs(self);\n       auto c = ceil(v);\n       auto cp = select(c - 0.5 > v, c - 1, c);\n@@ -45,13 +48,15 @@ namespace xsimd {\n \n     // trunc\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) {\n+    inline batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) {\n       return self;\n     }\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) {\n       return select(abs(self) < constants::maxflint<batch<float, A>>(), to_float(to_int(self)), self);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) {\n       return select(abs(self) < constants::maxflint<batch<double, A>>(), to_float(to_int(self)), self);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_trigo.hpp",
        "status": "modified",
        "additions": 55,
        "deletions": 33,
        "changes": 88,
        "patch": "@@ -30,7 +30,8 @@ namespace xsimd {\n     using namespace types;\n \n     // acos\n-    template<class A, class T> batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n         batch_type x = abs(self);\n         auto x_larger_05 = x > batch_type(0.5);\n@@ -41,7 +42,7 @@ namespace xsimd {\n         return select(x_larger_05, x, constants::pio2<batch_type>() - x);\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -59,7 +60,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = self - batch_type(1.);\n                 auto test = x > constants::oneotwoeps<batch_type>();\n@@ -77,7 +79,8 @@ namespace xsimd {\n         }\n \n     // asin\n-    template<class A> batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 batch_type sign = bitofsign(self);\n@@ -94,7 +97,8 @@ namespace xsimd {\n                 z = select(x_larger_05, constants::pio2<batch_type>() - (z1 + z1), z1);\n                 return z ^ sign;\n     }\n-    template<class A> batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 auto small_cond = x < constants::sqrteps<batch_type>();\n@@ -137,7 +141,7 @@ namespace xsimd {\n                                   bitofsign(self));\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -168,29 +172,30 @@ namespace xsimd {\n          */\n     namespace detail {\n         template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-          batch<T, A>\n+          inline batch<T, A>\n             average(const batch<T, A>& x1, const batch<T, A>& x2)\n             {\n                 return (x1 & x2) + ((x1 ^ x2) >> 1);\n             }\n \n         template <class A, class T>\n-          batch<T, A>\n+          inline batch<T, A>\n             averagef(const batch<T, A>& x1, const batch<T, A>& x2)\n             {\n               using batch_type = batch<T, A>;\n                 return fma(x1, batch_type(0.5), x2 * batch_type(0.5));\n             }\n         template<class A>\n-          batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {\n+          inline batch<float, A> average(batch<float, A> const & x1, batch<float, A> const & x2) {\n             return averagef(x1, x2);\n           }\n         template<class A>\n-          batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {\n+          inline batch<double, A> average(batch<double, A> const & x1, batch<double, A> const & x2) {\n             return averagef(x1, x2);\n           }\n     }\n-    template<class A> batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<float, A>;\n                 batch_type x = abs(self);\n                 auto lthalf = x < batch_type(0.5);\n@@ -216,7 +221,8 @@ namespace xsimd {\n                 return select(lthalf, z, log(tmp) + constants::log_2<batch_type>()) ^ bts;\n #endif\n     }\n-    template<class A> batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {\n+    template<class A>\n+    inline batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<double, A>;\n                 batch_type x = abs(self);\n                 auto test = x > constants::oneosqrteps<batch_type>();\n@@ -289,14 +295,15 @@ namespace xsimd {\n                 return yy + z;\n             }\n     }\n-    template<class A, class T> batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type absa = abs(self);\n                 const batch_type x = detail::kernel_atan(absa, batch_type(1.) / absa);\n                 return x ^ bitofsign(self);\n     }\n         template <class A, class T>\n-        batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n+        inline batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>)\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -326,7 +333,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = abs(self);\n                 batch_type t = x + x;\n@@ -345,7 +353,8 @@ namespace xsimd {\n         }\n \n     // atan2\n-    template<class A, class T> batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type q = abs(self / other);\n                 const batch_type z = detail::kernel_atan(q, batch_type(1.) / q);\n@@ -357,17 +366,17 @@ namespace xsimd {\n     namespace detail\n     {\n         template <class T, class A>\n-        batch<T, A> quadrant(const batch<T, A>& x) {\n+        inline batch<T, A> quadrant(const batch<T, A>& x) {\n           return x & batch<T, A>(3);\n         }\n \n         template <class A>\n-        batch<float, A> quadrant(const batch<float, A>& x) {\n+        inline batch<float, A> quadrant(const batch<float, A>& x) {\n           return to_float(quadrant(to_int(x)));\n         }\n \n         template <class A>\n-        batch<double, A> quadrant(const batch<double, A>& x) {\n+        inline batch<double, A> quadrant(const batch<double, A>& x) {\n           using batch_type = batch<double, A>;\n                 batch_type a = x * batch_type(0.25);\n                 return (a - floor(a)) * batch_type(4.);\n@@ -610,7 +619,8 @@ namespace xsimd {\n         };\n \n     }\n-    template<class A, class T> batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -625,7 +635,8 @@ namespace xsimd {\n                 return z1 ^ sign_bit;\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return {cos(z.real()) * cosh(z.imag()), -sin(z.real()) * sinh(z.imag())};\n     }\n \n@@ -641,7 +652,8 @@ namespace xsimd {\n       * ====================================================\n       */\n \n-    template<class A, class T> batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type x = abs(self);\n                 auto test1 = x > (constants::maxlog<batch_type>() - constants::log_2<batch_type>());\n@@ -661,7 +673,8 @@ namespace xsimd {\n \n     // sin\n     namespace detail {\n-    template<class A, class T, class Tag=trigo_radian_tag> batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {\n+    template<class A, class T, class Tag=trigo_radian_tag>\n+    inline batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -677,16 +690,19 @@ namespace xsimd {\n     }\n     }\n \n-    template<class A, class T> batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) {\n       return detail::sin(self);\n     }\n \n-    template<class A, class T> batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n       return {sin(z.real()) * cosh(z.imag()), cos(z.real()) * sinh(z.imag())};\n     }\n \n     // sincos\n-    template<class A, class T> std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -704,7 +720,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T>\n-    std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n+    inline std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n     sincos(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -726,7 +742,8 @@ namespace xsimd {\n          * (See copy at http://boost.org/LICENSE_1_0.txt)\n          * ====================================================\n          */\n-    template<class A> batch<float, A> sinh_kernel(batch<float, A> const& self) {\n+    template<class A>\n+    inline batch<float, A> sinh_kernel(batch<float, A> const& self) {\n       using batch_type = batch<float, A>;\n                 batch_type sqr_self = self * self;\n                 return detail::horner<batch_type,\n@@ -738,7 +755,8 @@ namespace xsimd {\n                     self;\n     }\n \n-    template<class A> batch<double, A> sinh_kernel(batch<double, A> const& self) {\n+    template<class A>\n+    inline batch<double, A> sinh_kernel(batch<double, A> const& self) {\n       using batch_type = batch<double, A>;\n                 batch_type sqrself = self * self;\n                 return fma(self, (detail::horner<batch_type,\n@@ -765,7 +783,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) {\n                 using batch_type = batch<T, A>;\n                 batch_type half(0.5);\n                 batch_type x = abs(a);\n@@ -794,7 +813,8 @@ namespace xsimd {\n         }\n \n     // tan\n-    template<class A, class T> batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n                 batch_type xr = constants::nan<batch_type>();\n@@ -805,7 +825,8 @@ namespace xsimd {\n                 const batch_type y = detail::tan_eval(xr, test);\n                 return y ^ bitofsign(self);\n     }\n-    template<class A, class T> batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n             real_batch d = cos(2 * z.real()) + cosh(2 * z.imag());\n@@ -901,7 +922,8 @@ namespace xsimd {\n      * (See copy at http://boost.org/LICENSE_1_0.txt)\n      * ====================================================\n      */\n-    template<class A, class T> batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {\n+    template<class A, class T>\n+    inline batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) {\n       using batch_type = batch<T, A>;\n                 batch_type one(1.);\n                 batch_type x = abs(self);"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx.hpp",
        "status": "modified",
        "additions": 264,
        "deletions": 166,
        "changes": 430,
        "patch": "@@ -32,15 +32,15 @@ namespace xsimd {\n         return _mm256_insertf128_si256(_mm256_castsi128_si256(low), high, 1);\n       }\n       template <class F>\n-      __m256i fwd_to_sse(F f, __m256i self) {\n+      inline __m256i fwd_to_sse(F f, __m256i self) {\n         __m128i self_low, self_high;\n         split_avx(self, self_low, self_high);\n         __m128i res_low = f(self_low);\n         __m128i res_high = f(self_high);\n         return merge_sse(res_low, res_high);\n       }\n       template<class F>\n-      __m256i fwd_to_sse(F f, __m256i self, __m256i other) {\n+      inline __m256i fwd_to_sse(F f, __m256i self, __m256i other) {\n         __m128i self_low, self_high, other_low, other_high;\n         split_avx(self, self_low, self_high);\n         split_avx(other, other_low, other_high);\n@@ -49,7 +49,7 @@ namespace xsimd {\n         return merge_sse(res_low, res_high);\n       }\n       template<class F>\n-      __m256i fwd_to_sse(F f, __m256i self, int32_t other) {\n+      inline __m256i fwd_to_sse(F f, __m256i self, int32_t other) {\n         __m128i self_low, self_high;\n         split_avx(self, self_low, self_high);\n         __m128i res_low = f(self_low, other);\n@@ -59,230 +59,260 @@ namespace xsimd {\n     }\n \n     // abs\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) {\n       __m256 sign_mask = _mm256_set1_ps(-0.f);  // -0.f = 1 << 31\n       return _mm256_andnot_ps(sign_mask, self);\n     }\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) {\n       __m256d sign_mask = _mm256_set1_pd(-0.f);  // -0.f = 1 << 31\n       return _mm256_andnot_pd(sign_mask, self);\n     }\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, __m128i o) { return add(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_add_ps(self, other);\n     }\n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_add_pd(self, other);\n     }\n \n     // all\n-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool all(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return _mm256_testc_ps(self, batch_bool<float, A>(true)) != 0;\n     }\n-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool all(batch_bool<double, A> const& self, requires_arch<avx>) {\n       return _mm256_testc_pd(self, batch_bool<double, A>(true)) != 0;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool all(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<avx>) {\n                 return _mm256_testc_si256(self, batch_bool<T, A>(true)) != 0;\n     }\n \n     // any\n-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool any(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return !_mm256_testz_ps(self, self);\n     }\n-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline bool any(batch_bool<double, A> const& self, requires_arch<avx>) {\n       return !_mm256_testz_pd(self, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<avx>) {\n                 return !_mm256_testz_si256(self, self);\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_and_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_and_pd(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_and_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_and_pd(self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_pd(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_andnot_pd(self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, int32_t o) { return bitwise_lshift(batch<T, sse4_2>(s), o, sse4_2{}); },self, other);\n     }\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s) { return bitwise_not(batch<T, sse4_2>(s), sse4_2{}); }, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s) { return bitwise_not(batch_bool<T, sse4_2>(s), sse4_2{}); }, self);\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_or_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_or_pd(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_or_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_or_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_or(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_or(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o)); }, self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, int32_t o) { return bitwise_rshift(batch<T, sse4_2>(s), o, sse4_2{}); }, self, other);\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_xor(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2{}); },\n                                 self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return bitwise_xor(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o), sse4_2{}); },\n                                 self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n       return _mm256_castsi256_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n       return _mm256_castsi256_pd(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx>) {\n       return _mm256_castps_pd(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n       return _mm256_castps_si256(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx>) {\n       return _mm256_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx>) {\n       return _mm256_castpd_si256(self);\n     }\n \n     // bitwise_not\n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n     }\n-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {\n+    inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx>) {\n         return _mm256_castps_si256(self);\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx>) {\n         return _mm256_castsi256_ps(self);\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx>) {\n         return _mm256_castpd_si256(self);\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx>) {\n         return _mm256_castsi256_pd(self);\n     }\n \n     // broadcast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<avx>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_set1_epi8(val);\n         case 2: return _mm256_set1_epi16(val);\n@@ -291,18 +321,22 @@ namespace xsimd {\n         default: assert(false && \"unsupported\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> broadcast(float val, requires_arch<avx>) {\n       return _mm256_set1_ps(val);\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> broadcast(double val, requires_arch<avx>) {\n       return _mm256_set1_pd(val);\n     }\n \n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_ceil_ps(self);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_ceil_pd(self);\n     }\n \n@@ -336,98 +370,117 @@ namespace xsimd {\n         }\n \n         // complex_low\n-        template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_f<0>(self.real(), self.imag());\n         }\n-        template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_d<0>(self.real(), self.imag());\n         }\n \n         // complex_high\n-        template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_f<1>(self.real(), self.imag());\n         }\n-        template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n+        template<class A>\n+        inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) {\n                 return get_half_complex_d<1>(self.real(), self.imag());\n         }\n     }\n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) {\n       return _mm256_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) {\n       return _mm256_cvttps_epi32(self);\n     }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_div_pd(self, other);\n     }\n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return  _mm256_castsi256_ps(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); },\n                                   _mm256_castps_si256(self), _mm256_castps_si256(other)));\n     }\n-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return  _mm256_castsi256_pd(detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch_bool<int32_t, sse4_2>(s), batch_bool<int32_t, sse4_2>(o), sse4_2{}); }, _mm256_castpd_si256(self), _mm256_castpd_si256(other)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return eq(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2{}); },self, other);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n       return eq(batch<T, A>(self.data), batch<T, A>(other.data));\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_floor_ps(self);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_floor_pd(self);\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_GE_OQ);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_GE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return ge(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // gt\n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_GT_OQ);\n     }\n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_GT_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return gt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& rhs, requires_arch<avx>) {\n                 // Warning about _mm256_hadd_ps:\n                 // _mm256_hadd_ps(a,b) gives\n                 // (a0+a1,a2+a3,b0+b1,b2+b3,a4+a5,a6+a7,b4+b5,b6+b7). Hence we can't\n@@ -445,7 +498,7 @@ namespace xsimd {\n \n     }\n     template <class A>\n-    double hadd(batch<double, A> const &rhs, requires_arch<avx>) {\n+    inline double hadd(batch<double, A> const &rhs, requires_arch<avx>) {\n                 // rhs = (x0, x1, x2, x3)\n                 // tmp = (x2, x3, x0, x1)\n                 __m256d tmp = _mm256_permute2f128_pd(rhs, rhs, 1);\n@@ -456,15 +509,16 @@ namespace xsimd {\n                 return _mm_cvtsd_f64(_mm256_extractf128_pd(tmp, 0));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx>) {\n       __m128i low, high;\n       detail::split_avx(self, low, high);\n       batch<T, sse4_2> blow(low), bhigh(high);\n       return hadd(blow) + hadd(bhigh);\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) {\n                 // row = (a,b,c,d,e,f,g,h)\n                 // tmp0 = (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)\n                 __m256 tmp0 = _mm256_hadd_ps(row[0], row[1]);\n@@ -489,7 +543,7 @@ namespace xsimd {\n                 return _mm256_add_ps(tmp0, tmp1);\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx>) {\n                 // row = (a,b,c,d)\n                 // tmp0 = (a0+a1, b0+b1, a2+a3, b2+b3)\n                 __m256d tmp0 = _mm256_hadd_pd(row[0], row[1]);\n@@ -503,37 +557,44 @@ namespace xsimd {\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) {\n                 return _mm256_cmp_ps(self, self, _CMP_UNORD_Q);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) {\n                 return _mm256_cmp_pd(self, self, _CMP_UNORD_Q);\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_LE_OQ);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_LE_OQ);\n     }\n \n     // load_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) {\n       return _mm256_load_si256((__m256i const*)mem);\n     }\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) {\n       return _mm256_load_ps(mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) {\n       return _mm256_load_pd(mem);\n     }\n \n     namespace detail\n     {\n     // load_complex\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) {\n             using batch_type = batch<float, A>;\n             __m128 tmp0 = _mm256_extractf128_ps(hi, 0);\n             __m128 tmp1 = _mm256_extractf128_ps(hi, 1);\n@@ -551,7 +612,8 @@ namespace xsimd {\n             imag = _mm256_insertf128_ps(imag, tmp_imag, 1);\n             return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx>) {\n             using batch_type = batch<double, A>;\n             __m128d tmp0 = _mm256_extractf128_pd(hi, 0);\n             __m128d tmp1 = _mm256_extractf128_pd(hi, 1);\n@@ -570,115 +632,133 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) {\n       return _mm256_loadu_si256((__m256i const*)mem);\n     }\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>){\n       return _mm256_loadu_ps(mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>){\n       return _mm256_loadu_pd(mem);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_LT_OQ);\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_LT_OQ);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return detail::fwd_to_sse([](__m128i s, __m128i o) { return lt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_max_ps(self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_max_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return select(self > other, self, other);\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_min_ps(self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_min_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       return select(self <= other, self, other);\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_mul_pd(self, other);\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n \n     // neg\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) {\n       return 0 - self;\n     }\n     template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)));\n     }\n     template <class A>\n-    batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {\n+    inline batch<double, A> neg(batch<double, A> const &self, requires_arch<avx>) {\n       return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)));\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_ps(self, other, _CMP_NEQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_cmp_pd(self, other, _CMP_NEQ_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return ~(self == other);\n     }\n \n \n-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) {\n       return _mm256_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) {\n         return ~(self == other);\n     }\n \n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       if(std::is_signed<T>::value) {\n         auto mask = (other >> (8 * sizeof(T) - 1));\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -693,14 +773,16 @@ namespace xsimd {\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) {\n                 return _mm256_blendv_ps(false_br, true_br, cond);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) {\n                 return _mm256_blendv_pd(false_br, true_br, cond);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n       __m128i cond_low, cond_hi;\n       detail::split_avx(cond, cond_low, cond_hi);\n \n@@ -715,75 +797,79 @@ namespace xsimd {\n       return detail::merge_sse(res_low, res_hi);\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, avx2{});\n     }\n \n \n     // set\n     template<class A, class... Values>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n       return _mm256_setr_ps(values...);\n     }\n \n     template<class A, class... Values>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n       return _mm256_setr_pd(values...);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) {\n       return _mm256_set_epi64x(v3, v2, v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm256_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm256_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n       T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31  ) {\n       return _mm256_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n     }\n \n     template<class A, class T, class... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) {\n       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;\n     }\n \n     template<class A, class... Values>\n-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n       return _mm256_castsi256_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     template<class A, class... Values>\n-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {\n+    inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n       return _mm256_castsi256_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) {\n       return _mm256_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) {\n       return _mm256_sqrt_pd(val);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -795,55 +881,61 @@ namespace xsimd {\n \n     // store_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n       return _mm256_store_si256((__m256i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n       return _mm256_store_si256((__m256i *)mem, self);\n     }\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_store_ps(mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_store_pd(mem, self);\n     }\n \n     // store_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_si256((__m256i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_si256((__m256i *)mem, self);\n     }\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_ps(mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_storeu_pd(mem, self);\n     }\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n         return detail::fwd_to_sse([](__m128i s, __m128i o) { return sub(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); }, self, other);\n     }\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_ps(self, other);\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx>) {\n       return _mm256_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -852,29 +944,31 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n       return {(int64_t)buffer[0], (int64_t)buffer[1], (int64_t)buffer[2], (int64_t)buffer[3]};\n     }\n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) {\n       return _mm256_round_ps(self, _MM_FROUND_TO_ZERO);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) {\n       return _mm256_round_pd(self, _MM_FROUND_TO_ZERO);\n     }\n \n     // zip_hi\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_unpackhi_epi8(self, other);\n         case 2: return _mm256_unpackhi_epi16(self, other);\n@@ -883,16 +977,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_unpackhi_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_unpacklo_epi8(self, other);\n         case 2: return _mm256_unpacklo_epi16(self, other);\n@@ -901,10 +997,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) {\n       return _mm256_unpacklo_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) {\n       return _mm256_unpacklo_pd(self, other);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx2.hpp",
        "status": "modified",
        "additions": 35,
        "deletions": 34,
        "changes": 69,
        "patch": "@@ -25,7 +25,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_abs_epi8(self);\n@@ -39,7 +39,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_add_epi8(self, other);\n         case 2: return _mm256_add_epi16(self, other);\n@@ -51,37 +51,37 @@ namespace xsimd {\n \n     // bitwise_and\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_and_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_and_si256(self, other);\n     }\n \n     // bitwise_andnot\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_andnot_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_andnot_si256(self, other);\n     }\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 2: return _mm256_slli_epi16(self, other);\n         case 4: return _mm256_slli_epi32(self, other);\n@@ -91,7 +91,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 4: return _mm256_sllv_epi32(self, other);\n         case 8: return _mm256_sllv_epi64(self, other);\n@@ -101,17 +101,17 @@ namespace xsimd {\n \n     // bitwise_or\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_or_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_or_si256(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: {\n@@ -139,7 +139,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm256_srav_epi32(self, other);\n@@ -157,31 +157,33 @@ namespace xsimd {\n \n     // bitwise_xor\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) {\n       return _mm256_xor_si256(self, other);\n     }\n \n     // complex_low\n-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n+    template<class A> batch<double, A>\n+    inline complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 1, 1, 0));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(1, 2, 0, 0));\n             return _mm256_blend_pd(tmp0, tmp1, 10);\n     }\n \n     // complex_high\n-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n+    template<class A> batch<double, A>\n+    inline complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 3, 1, 2));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(3, 2, 2, 0));\n             return _mm256_blend_pd(tmp0, tmp1, 10);\n     }\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_cmpeq_epi8(self, other);\n         case 2: return _mm256_cmpeq_epi16(self, other);\n@@ -193,7 +195,7 @@ namespace xsimd {\n \n     // gt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_cmpgt_epi8(self, other);\n@@ -210,7 +212,7 @@ namespace xsimd {\n \n     // hadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx2>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 4:\n           {\n@@ -240,7 +242,8 @@ namespace xsimd {\n       }\n     }\n     // load_complex\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {\n+    template<class A> batch<std::complex<float>, A>\n+    inline load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) {\n             using batch_type = batch<float, A>;\n             batch_type real = _mm256_castpd_ps(\n                          _mm256_permute4x64_pd(\n@@ -252,7 +255,8 @@ namespace xsimd {\n                              _MM_SHUFFLE(3, 1, 2, 0)));\n             return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx2>) {\n             using batch_type = batch<double, A>;\n             batch_type real = _mm256_permute4x64_pd(_mm256_unpacklo_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n             batch_type imag = _mm256_permute4x64_pd(_mm256_unpackhi_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n@@ -261,7 +265,7 @@ namespace xsimd {\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_max_epi8(self, other);\n@@ -282,7 +286,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1:  return _mm256_min_epi8(self, other);\n@@ -303,7 +307,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 2: return _mm256_mullo_epi16(self, other);\n         case 4: return _mm256_mullo_epi32(self, other);\n@@ -313,7 +317,7 @@ namespace xsimd {\n \n     // sadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_adds_epi8(self, other);\n@@ -332,7 +336,7 @@ namespace xsimd {\n \n     // select\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_blendv_epi8(false_br, true_br, cond);\n         case 2: return _mm256_blendv_epi8(false_br, true_br, cond);\n@@ -342,7 +346,7 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) {\n       constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();\n       switch(sizeof(T)) {\n         // FIXME: for some reason mask here is not considered as an immediate,\n@@ -359,7 +363,7 @@ namespace xsimd {\n \n     // ssub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm256_subs_epi8(self, other);\n@@ -378,7 +382,7 @@ namespace xsimd {\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm256_sub_epi8(self, other);\n         case 2: return _mm256_sub_epi16(self, other);\n@@ -387,9 +391,6 @@ namespace xsimd {\n         default: return sub(self, other, avx{});\n       }\n     }\n-\n-\n-\n   }\n \n }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512bw.hpp",
        "status": "modified",
        "additions": 18,
        "deletions": 18,
        "changes": 36,
        "patch": "@@ -23,7 +23,7 @@ namespace xsimd {\n \n     namespace detail {\n     template<class A, class T,  int Cmp>\n-    batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {\n+    inline batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n@@ -46,7 +46,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) {\n       if(std::is_unsigned<T>::value)\n         return self;\n \n@@ -59,7 +59,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_add_epi8(self, other);\n         case 2: return _mm512_add_epi16(self, other);\n@@ -69,7 +69,7 @@ namespace xsimd {\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n         case 2: return _mm512_sllv_epi16(self, _mm512_set1_epi16(other));\n@@ -82,7 +82,7 @@ namespace xsimd {\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1:\n@@ -120,38 +120,38 @@ namespace xsimd {\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_EQ>(self, other);\n     }\n \n     // ge\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GE>(self, other);\n     }\n \n     // gt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GT>(self, other);\n     }\n \n \n     // le\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LE>(self, other);\n     }\n \n     // lt\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LT>(self, other);\n     }\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_max_epi8(self, other);\n@@ -170,7 +170,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_min_epi8(self, other);\n@@ -190,7 +190,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: {\n                 __m512i upper = _mm512_and_si512(_mm512_mullo_epi16(self, other), _mm512_srli_epi16(_mm512_set1_epi16(-1), 8));\n@@ -205,13 +205,13 @@ namespace xsimd {\n \n     // neq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       return detail::compare_int_avx512bw<A, T, _MM_CMPINT_NE>(self, other);\n     }\n \n     // sadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_adds_epi8(self, other);\n@@ -230,7 +230,7 @@ namespace xsimd {\n \n     // select\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_mask_blend_epi8(cond, false_br, true_br);\n         case 2: return _mm512_mask_blend_epi16(cond, false_br, true_br);\n@@ -241,7 +241,7 @@ namespace xsimd {\n \n     // ssub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm512_subs_epi8(self, other);\n@@ -261,7 +261,7 @@ namespace xsimd {\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_sub_epi8(self, other);\n         case 2: return _mm512_sub_epi16(self, other);"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512dq.hpp",
        "status": "modified",
        "additions": 20,
        "deletions": 11,
        "changes": 31,
        "patch": "@@ -20,51 +20,60 @@ namespace xsimd {\n     using namespace types;\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_and_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_and_pd(self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_andnot_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_andnot_pd(self, other);\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_or_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_or_pd(self, other);\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_xor_ps(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) {\n       return _mm512_xor_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512dq>) {\n       return _mm512_cvtepi64_pd(self);\n     }\n \n     // to_int\n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512dq>) {\n       return _mm512_cvttpd_epi64(self);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512f.hpp",
        "status": "modified",
        "additions": 270,
        "deletions": 183,
        "changes": 453,
        "patch": "@@ -75,39 +75,39 @@ namespace xsimd {\n \n     inline uint32_t morton(uint16_t x, uint16_t y) {\n \n-      static const unsigned short MortonTable256[256] = \n+      static const unsigned short MortonTable256[256] =\n       {\n-        0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015, \n-        0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055, \n-        0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115, \n-        0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155, \n-        0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415, \n-        0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455, \n-        0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515, \n-        0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555, \n-        0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015, \n-        0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055, \n-        0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115, \n-        0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155, \n-        0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415, \n-        0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455, \n-        0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515, \n-        0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555, \n-        0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015, \n-        0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055, \n-        0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115, \n-        0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155, \n-        0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415, \n-        0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455, \n-        0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515, \n-        0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555, \n-        0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015, \n-        0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055, \n-        0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115, \n-        0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155, \n-        0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415, \n-        0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455, \n-        0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515, \n+        0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015,\n+        0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055,\n+        0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115,\n+        0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155,\n+        0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415,\n+        0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455,\n+        0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515,\n+        0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555,\n+        0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015,\n+        0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055,\n+        0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115,\n+        0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155,\n+        0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415,\n+        0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455,\n+        0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515,\n+        0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555,\n+        0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015,\n+        0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055,\n+        0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115,\n+        0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155,\n+        0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415,\n+        0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455,\n+        0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515,\n+        0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555,\n+        0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015,\n+        0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055,\n+        0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115,\n+        0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155,\n+        0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415,\n+        0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455,\n+        0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515,\n         0x5540, 0x5541, 0x5544, 0x5545, 0x5550, 0x5551, 0x5554, 0x5555\n       };\n \n@@ -119,7 +119,7 @@ namespace xsimd {\n     }\n \n     template<class A, class T,  int Cmp>\n-    batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {\n+    inline batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n@@ -189,21 +189,23 @@ namespace xsimd {\n     }\n \n     // abs\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) {\n       __m512 self_asf = (__m512)self;\n       __m512i self_asi = *reinterpret_cast<__m512i *>(&self_asf);\n       __m512i res_asi = _mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF), self_asi);\n       return *reinterpret_cast<__m512*>(&res_asi);\n     }\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) {\n                 __m512d self_asd = (__m512d)self;\n                 __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asd);\n                 __m512i res_asi = _mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),\n                                                    self_asi);\n                 return *reinterpret_cast<__m512d*>(&res_asi);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) {\n       if(std::is_unsigned<T>::value)\n         return self;\n \n@@ -218,7 +220,7 @@ namespace xsimd {\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return detail::fwd_to_avx([](__m256i s, __m256i o) { return add(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n         case 2: return detail::fwd_to_avx([](__m256i s, __m256i o) { return add(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n@@ -227,68 +229,74 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_add_ps(self, other);\n     }\n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_add_pd(self, other);\n     }\n \n     // all\n     template<class A, class T>\n-    bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return self.data == register_type(-1);\n     }\n \n     // any\n     template<class A, class T>\n-    bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return self.data != register_type(0);\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_and_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_and_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_and_si512(self, other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data & other.data);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_andnot_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_andnot_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_andnot_si512(self, other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data & ~other.data);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n@@ -312,44 +320,48 @@ namespace xsimd {\n \n     // bitwise_not\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_xor_si512(self, _mm512_set1_epi32(-1));\n     }\n     template<class A, class T>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(~self.data);\n     }\n \n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_xor_ps(self, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<avx512f>) {\n       return _mm512_xor_pd(self, _mm512_castsi512_pd(_mm512_set1_epi32(-1)));\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_or_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_or_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_or_si512(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n@@ -386,71 +398,78 @@ namespace xsimd {\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n     }\n \n-    template<class A, class T> batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    template<class A, class T>\n+    inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data | other.data);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return _mm512_xor_si512(self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n       return _mm512_castsi512_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n       return _mm512_castsi512_pd(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx512f>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<avx512f>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<avx512f>) {\n       return _mm512_castps_pd(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n       return _mm512_castps_si512(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<avx512f>) {\n       return _mm512_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<avx512f>) {\n       return _mm512_castpd_si512(self);\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<avx512f>) {\n         return self.data;\n     }\n \n \n     // broadcast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<avx512f>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_set1_epi8(val);\n         case 2: return _mm512_set1_epi16(val);\n@@ -459,146 +478,168 @@ namespace xsimd {\n         default: assert(false && \"unsupported\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> broadcast(float val, requires_arch<avx512f>) {\n       return _mm512_set1_ps(val);\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<avx512f>) {\n+    template<class A> batch<double, A>\n+    inline broadcast(double val, requires_arch<avx512f>) {\n       return _mm512_set1_pd(val);\n     }\n \n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_ps(self, _MM_FROUND_TO_POS_INF);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_pd(self, _MM_FROUND_TO_POS_INF);\n     }\n \n \n     namespace detail\n     {\n     // complex_low\n-    template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi32(0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);\n         return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n     }\n-    template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi64(0, 8, 1, 9, 2, 10, 3, 11);\n         return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n     }\n \n     // complex_high\n-    template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi32(8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);\n         return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n     }\n-    template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) {\n         __m512i idx = _mm512_setr_epi64(4, 12, 5, 13, 6, 14, 7, 15);\n         return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n     }\n     }\n \n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) {\n       return _mm512_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) {\n       return _mm512_cvttps_epi32(self);\n     }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_div_pd(self, other);\n     }\n \n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_EQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_EQ_OQ);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_EQ>(self, other);\n     }\n     template<class A, class T>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(~self.data ^ other.data);\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_ps(self, _MM_FROUND_TO_NEG_INF);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_pd(self, _MM_FROUND_TO_NEG_INF);\n     }\n \n     // from bool\n     template<class A, class T>\n-    batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return select(self, batch<T, A>(1), batch<T, A>(0));\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_GE_OQ);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_GE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_GE>(self, other);\n     }\n \n     // gt\n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_GT_OQ);\n     }\n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_GT_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_GT>(self, other);\n     }\n \n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& rhs, requires_arch<avx512f>) {\n                 __m256 tmp1 = _mm512_extractf32x8_ps(rhs, 1);\n                 __m256 tmp2 = _mm512_extractf32x8_ps(rhs, 0);\n                 __m256 res1 = _mm256_add_ps(tmp1, tmp2);\n                 return hadd(batch<float, avx2>(res1), avx2{});\n \n     }\n     template <class A>\n-    double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {\n+    inline double hadd(batch<double, A> const &rhs, requires_arch<avx512f>) {\n                 __m256d tmp1 = _mm512_extractf64x4_pd(rhs, 1);\n                 __m256d tmp2 = _mm512_extractf64x4_pd(rhs, 0);\n                 __m256d res1 = _mm256_add_pd(tmp1, tmp2);\n                 return hadd(batch<double, avx2>(res1), avx2{});\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<avx512f>) {\n       __m256i low, high;\n       detail::split_avx512(self, low, high);\n       batch<T, avx2> blow(low), bhigh(high);\n       return hadd(blow, avx2{}) + hadd(bhigh, avx2{});\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) {\n                 // The following folds over the vector once:\n                 // tmp1 = [a0..8, b0..8]\n                 // tmp2 = [a8..f, b8..f]\n@@ -656,8 +697,9 @@ namespace xsimd {\n                 concat = _mm512_insertf32x8(concat, halfx1, 1);\n                 return concat;\n     }\n+\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<avx512f>) {\n #define step1(I, a, b)                                                   \\\n         batch<double, avx512f> res ## I;                                           \\\n         {                                                                    \\\n@@ -690,49 +732,57 @@ namespace xsimd {\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) {\n                 return _mm512_cmp_ps_mask(self, self, _CMP_UNORD_Q);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) {\n                 return _mm512_cmp_pd_mask(self, self, _CMP_UNORD_Q);\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_LE_OQ);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_LE_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_LE>(self, other);\n     }\n \n \n     // load_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n       return _mm512_load_si512((__m512i const*)mem);\n     }\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) {\n       return _mm512_load_ps(mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) {\n       return _mm512_load_pd(mem);\n     }\n \n     // load_complex\n     namespace detail\n     {\n-    template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) {\n         __m512i real_idx = _mm512_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);\n         __m512i imag_idx = _mm512_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);\n         auto real = _mm512_permutex2var_ps(hi, real_idx, lo);\n         auto imag = _mm512_permutex2var_ps(hi, imag_idx, lo);\n         return {real, imag};\n     }\n-    template<class A> batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<std::complex<double>, A> load_complex(batch<double,A> const& hi, batch<double,A> const& lo, requires_arch<avx512f>) {\n         __m512i real_idx = _mm512_setr_epi64(0, 2, 4, 6, 8, 10, 12, 14);\n         __m512i imag_idx = _mm512_setr_epi64(1, 3, 5, 7, 9, 11, 13, 15);\n         auto real = _mm512_permutex2var_pd(hi, real_idx, lo);\n@@ -743,39 +793,44 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) {\n       return _mm512_loadu_si512((__m512i const*)mem);\n     }\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>){\n       return _mm512_loadu_ps(mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>){\n       return _mm512_loadu_pd(mem);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_LT_OQ);\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_LT_OQ);\n     }\n \n-\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       return detail::compare_int_avx512f<A, T, _MM_CMPINT_LT>(self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_max_ps(self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_max_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_max_epi32(self, other);\n@@ -793,14 +848,16 @@ namespace xsimd {\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_min_ps(self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_min_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_min_epi32(self, other);\n@@ -818,14 +875,16 @@ namespace xsimd {\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_mul_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n         switch(sizeof(T)) {\n           case 4: return _mm512_mullo_epi32(self, other);\n           case 8: return _mm512_mullo_epi64(self, other);\n@@ -834,46 +893,52 @@ namespace xsimd {\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n     }\n \n     // neg\n     template<class A, class T>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) {\n       return 0 - self;\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_ps_mask(self, other, _CMP_NEQ_OQ);\n     }\n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_cmp_pd_mask(self, other, _CMP_NEQ_OQ);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n         return ~(self == other);\n     }\n \n     template<class A, class T>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       return register_type(self.data ^ other.data);\n     }\n \n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return add(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n         auto mask = other < 0;\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -888,15 +953,17 @@ namespace xsimd {\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) {\n                 return _mm512_mask_blend_ps(cond, false_br, true_br);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) {\n                 return _mm512_mask_blend_pd(cond, false_br, true_br);\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: {\n           alignas(avx2::alignment()) uint8_t buffer[64];\n@@ -935,8 +1002,9 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/type combination\"); return {};\n       };\n     }\n+\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, avx512f{});\n     }\n \n@@ -954,25 +1022,25 @@ namespace xsimd {\n \n     // set\n     template<class A>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) {\n       return _mm512_setr_ps(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n \n     template<class A>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) {\n       return _mm512_setr_pd(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm512_set_epi64(v7, v6, v5, v4, v3, v2, v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm512_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n     template<class A, class T, detail::enable_signed_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) {\n@@ -987,8 +1055,9 @@ namespace xsimd {\n                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n #endif\n     }\n+\n     template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) {\n@@ -1003,8 +1072,9 @@ namespace xsimd {\n                               v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n #endif\n     }\n+\n     template<class A, class T, detail::enable_signed_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n@@ -1030,7 +1100,7 @@ namespace xsimd {\n #endif\n     }\n     template<class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n                                                            T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n                                                            T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n                                                            T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n@@ -1055,8 +1125,9 @@ namespace xsimd {\n           v48, v49, v50, v51, v52, v53, v54, v55, v56, v57, v58, v59, v60, v61, v62, v63);\n #endif\n     }\n+\n     template<class A, class T, class... Values>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<T, A>::size, \"consistent init\");\n       using register_type = typename batch_bool<T, A>::register_type;\n       register_type r = 0;\n@@ -1066,22 +1137,26 @@ namespace xsimd {\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) {\n       return _mm512_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) {\n       return _mm512_sqrt_pd(val);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -1093,7 +1168,7 @@ namespace xsimd {\n \n     // store\n     template<class T, class A>\n-    void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {\n+    inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) {\n       using register_type = typename batch_bool<T, A>::register_type;\n       constexpr auto size = batch_bool<T, A>::size;\n       for(std::size_t i = 0; i < size; ++i)\n@@ -1102,39 +1177,43 @@ namespace xsimd {\n \n     // store_aligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_si512((__m512i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_si512((__m512i *)mem, self);\n     }\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_ps(mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_store_pd(mem, self);\n     }\n \n     // store_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_si512((__m512i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_si512((__m512i *)mem, self);\n     }\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_ps(mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_storeu_pd(mem, self);\n     }\n \n     // sub\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return detail::fwd_to_avx([](__m256i s, __m256i o) { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n         case 2: return detail::fwd_to_avx([](__m256i s, __m256i o) { return sub(batch<T, avx2>(s), batch<T, avx2>(o)); }, self, other);\n@@ -1143,20 +1222,22 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_ps(self, other);\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<avx512f>) {\n       return _mm512_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<avx512f>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -1166,12 +1247,12 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<avx512f>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -1180,16 +1261,18 @@ namespace xsimd {\n     }\n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx512f>) {\n       return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n     }\n \n     // zip_hi\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_unpackhi_epi8(self, other);\n         case 2: return _mm512_unpackhi_epi16(self, other);\n@@ -1198,16 +1281,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpackhi_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) {\n       switch(sizeof(T)) {\n         case 1: return _mm512_unpacklo_epi8(self, other);\n         case 2: return _mm512_unpacklo_epi16(self, other);\n@@ -1216,10 +1301,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpacklo_ps(self, other);\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) {\n       return _mm512_unpacklo_pd(self, other);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_constants.hpp",
        "status": "modified",
        "additions": 19,
        "deletions": 20,
        "changes": 39,
        "patch": "@@ -160,7 +160,7 @@ namespace constants {\n     }\n \n     template <class T>\n-    constexpr T allbits() noexcept\n+    inline constexpr T allbits() noexcept\n     {\n         return T(detail::allbits_impl<typename T::value_type>::get_value());\n     }\n@@ -170,19 +170,19 @@ namespace constants {\n      *****************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> mask1frexp() noexcept\n+    inline constexpr as_integer_t<T> mask1frexp() noexcept\n     {\n         return as_integer_t<T>(mask1frexp<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t mask1frexp<float>() noexcept\n+    inline constexpr int32_t mask1frexp<float>() noexcept\n     {\n         return 0x7f800000;\n     }\n \n     template <>\n-    constexpr int64_t mask1frexp<double>() noexcept\n+    inline constexpr int64_t mask1frexp<double>() noexcept\n     {\n         return 0x7ff0000000000000;\n     }\n@@ -192,19 +192,19 @@ namespace constants {\n      *****************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> mask2frexp() noexcept\n+    inline constexpr as_integer_t<T> mask2frexp() noexcept\n     {\n         return as_integer_t<T>(mask2frexp<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t mask2frexp<float>() noexcept\n+    inline constexpr int32_t mask2frexp<float>() noexcept\n     {\n         return 0x3f000000;\n     }\n \n     template <>\n-    constexpr int64_t mask2frexp<double>() noexcept\n+    inline constexpr int64_t mask2frexp<double>() noexcept\n     {\n         return 0x3fe0000000000000;\n     }\n@@ -214,19 +214,19 @@ namespace constants {\n      ******************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> maxexponent() noexcept\n+    inline constexpr as_integer_t<T> maxexponent() noexcept\n     {\n         return as_integer_t<T>(maxexponent<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t maxexponent<float>() noexcept\n+    inline constexpr int32_t maxexponent<float>() noexcept\n     {\n         return 127;\n     }\n \n     template <>\n-    constexpr int64_t maxexponent<double>() noexcept\n+    inline constexpr int64_t maxexponent<double>() noexcept\n     {\n         return 1023;\n     }\n@@ -236,19 +236,19 @@ namespace constants {\n      ******************************/\n \n     template <class T>\n-    constexpr as_integer_t<T> maxexponentm1() noexcept\n+    inline constexpr as_integer_t<T> maxexponentm1() noexcept\n     {\n         return as_integer_t<T>(maxexponentm1<typename T::value_type>());\n     }\n \n     template <>\n-    constexpr int32_t maxexponentm1<float>() noexcept\n+    inline constexpr int32_t maxexponentm1<float>() noexcept\n     {\n         return 126;\n     }\n \n     template <>\n-    constexpr int64_t maxexponentm1<double>() noexcept\n+    inline constexpr int64_t maxexponentm1<double>() noexcept\n     {\n         return 1022;\n     }\n@@ -258,19 +258,19 @@ namespace constants {\n      **********************/\n \n     template <class T>\n-    constexpr int32_t nmb() noexcept\n+    inline constexpr int32_t nmb() noexcept\n     {\n         return nmb<typename T::value_type>();\n     }\n \n     template <>\n-    constexpr int32_t nmb<float>() noexcept\n+    inline constexpr int32_t nmb<float>() noexcept\n     {\n         return 23;\n     }\n \n     template <>\n-    constexpr int32_t nmb<double>() noexcept\n+    inline constexpr int32_t nmb<double>() noexcept\n     {\n         return 52;\n     }\n@@ -280,7 +280,7 @@ namespace constants {\n      ***********************/\n \n     template <class T>\n-    constexpr T zero() noexcept\n+    inline constexpr T zero() noexcept\n     {\n         return T(typename T::value_type(0));\n     }\n@@ -346,7 +346,7 @@ namespace constants {\n     }\n \n     template <class T>\n-    constexpr T minvalue() noexcept\n+    inline constexpr T minvalue() noexcept\n     {\n         return T(detail::minvalue_impl<typename T::value_type>::get_value());\n     }\n@@ -356,7 +356,7 @@ namespace constants {\n      ***************************/\n \n     template <class T>\n-    constexpr T maxvalue() noexcept\n+    inline constexpr T maxvalue() noexcept\n     {\n         return T(std::numeric_limits<typename T::value_type>::max());\n     }\n@@ -365,4 +365,3 @@ namespace constants {\n }\n \n #endif\n-"
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma3.hpp",
        "status": "modified",
        "additions": 16,
        "deletions": 8,
        "changes": 24,
        "patch": "@@ -20,38 +20,46 @@ namespace xsimd {\n   namespace kernel {\n     using namespace types;\n     // fnma\n-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmadd_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fnmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fmadd_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3>) {\n       return _mm_fmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n+    template<class A>\n+    inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3>) {\n       return _mm_fmsub_pd(x, y, z);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma4.hpp",
        "status": "modified",
        "additions": 16,
        "deletions": 8,
        "changes": 24,
        "patch": "@@ -21,38 +21,46 @@ namespace xsimd {\n     using namespace types;\n \n     // fnma\n-    template<class A> batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_nmacc_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_nmacc_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_nmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_nmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_macc_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_macc_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) {\n       return _mm_msub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n+    template<class A>\n+    inline batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) {\n       return _mm_msub_pd(x, y, z);\n     }\n   }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma5.hpp",
        "status": "modified",
        "additions": 16,
        "deletions": 8,
        "changes": 24,
        "patch": "@@ -21,38 +21,46 @@ namespace xsimd {\n     using namespace types;\n \n     // fnma\n-    template<class A> batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmadd_pd(x, y, z);\n     }\n \n     // fnms\n-    template<class A> batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fnmsub_pd(x, y, z);\n     }\n \n     // fma\n-    template<class A> batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmadd_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmadd_pd(x, y, z);\n     }\n \n     // fms\n-    template<class A> batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmsub_ps(x, y, z);\n     }\n \n-    template<class A> batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n+    template<class A>\n+    inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma5>) {\n       return _mm256_fmsub_pd(x, y, z);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_generic_fwd.hpp",
        "status": "modified",
        "additions": 6,
        "deletions": 5,
        "changes": 11,
        "patch": "@@ -19,14 +19,15 @@ namespace xsimd {\n   namespace kernel {\n     // forward declaration\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>);\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n-    template<class A, class T> batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    template<class A, class T>\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>);\n \n   }\n }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_neon.hpp",
        "status": "modified",
        "additions": 195,
        "deletions": 195,
        "changes": 390,
        "patch": "@@ -124,7 +124,7 @@ namespace xsimd\n \n             template <class T>\n             using identity_return_type = T;\n-            \n+\n             template <class... T>\n             struct neon_dispatcher_impl : neon_dispatcher_base<identity_return_type, T...>\n             {\n@@ -196,7 +196,7 @@ namespace xsimd\n             {\n                 using type = uint64x2_t;\n             };\n-            \n+\n             template <>\n             struct comp_return_type_impl<float32x4_t>\n             {\n@@ -254,55 +254,55 @@ namespace xsimd\n          *************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u8(uint8_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s8(int8_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u16(uint16_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s16(int16_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u32(uint32_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s32(int32_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_u64(uint64_t(val));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> broadcast(T val, requires_arch<neon>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon>)\n         {\n             return vdupq_n_s64(int64_t(val));\n         }\n \n         template <class A>\n-        batch<float, A> broadcast(float val, requires_arch<neon>)\n+        inline batch<float, A> broadcast(float val, requires_arch<neon>)\n         {\n             return vdupq_n_f32(val);\n         }\n@@ -312,27 +312,27 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>\n-        batch<T, A> set(batch<T, A> const&, requires_arch<neon>, Args... args)\n+        inline batch<T, A> set(batch<T, A> const&, requires_arch<neon>, Args... args)\n         {\n             return xsimd::types::detail::neon_vector_type<T>{args...};\n         }\n \n         template <class A, class T, class... Args, detail::enable_integral_t<T> = 0>\n-        batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<neon>, Args... args)\n+        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<neon>, Args... args)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<T>;\n             return register_type{static_cast<unsigned_type>(args ? -1LL : 0LL)...};\n         }\n \n         template <class A>\n-        batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)\n+        inline batch<float, A> set(batch<float, A> const&, requires_arch<neon>, float f0, float f1, float f2, float f3)\n         {\n             return float32x4_t{f0, f1, f2, f3};\n         }\n \n         template <class A, class... Args>\n-        batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)\n+        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<neon>, Args... args)\n         {\n             using register_type = typename batch_bool<float, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<float>;\n@@ -344,55 +344,55 @@ namespace xsimd\n          *************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u8(arg, vdupq_n_u8(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s8(reinterpret_cast<int8x16_t>(arg.data), vdupq_n_s8(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u16(arg, vdupq_n_u16(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s16(reinterpret_cast<int16x8_t>(arg.data), vdupq_n_s16(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u32(arg, vdupq_n_u32(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s32(reinterpret_cast<int32x4_t>(arg.data), vdupq_n_s32(1));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_u64(arg, vdupq_n_u64(1));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             return vandq_s64(reinterpret_cast<int64x2_t>(arg.data), vdupq_n_s64(1));\n         }\n \n         template <class A>\n-        batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)\n+        inline batch<float, A> from_bool(batch_bool<float, A> const& arg, requires_arch<neon>)\n         {\n             return vreinterpretq_f32_u32(vandq_u32(arg, vreinterpretq_u32_f32(vdupq_n_f32(1.f))));\n         }\n@@ -402,56 +402,56 @@ namespace xsimd\n          ********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u8(src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s8(src);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u16(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s16(src);\n         }\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u32(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s32(src);\n         }\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_u64(src);\n         }\n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return vld1q_s64(src);\n         }\n \n         template <class A>\n-        batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)\n+        inline batch<float, A> load_aligned(float const* src, convert<float>, requires_arch<neon>)\n         {\n             return vld1q_f32(src);\n         }\n \n         template <class A, class T>\n-        batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)\n+        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<neon>)\n         {\n             return load_aligned<A>(src, convert<T>(), A{});\n         }\n@@ -461,61 +461,61 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u8(dst, src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s8(dst, src);\n         }\n-        \n+\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u16(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s16(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u32(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s32(dst, src);\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_u64(dst, src);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             vst1q_s64(dst, src);\n         }\n \n         template <class A>\n-        void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)\n+        inline void store_aligned(float* dst, batch<float, A> const& src, requires_arch<neon>)\n         {\n             vst1q_f32(dst, src);\n         }\n \n         template <class A, class T>\n-        void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n+        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<neon>)\n         {\n             store_aligned<A>(dst, src, A{});\n         }\n@@ -525,7 +525,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)\n+        inline batch<std::complex<float>, A> load_complex_aligned(std::complex<float> const* mem, convert<std::complex<float>>, requires_arch<neon>)\n         {\n             using real_batch = batch<float, A>;\n             const float* buf = reinterpret_cast<const float*>(mem);\n@@ -536,7 +536,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)\n+        inline batch<std::complex<float>, A> load_complex_unaligned(std::complex<float> const* mem, convert<std::complex<float>> cvt, requires_arch<neon>)\n         {\n             return load_complex_aligned<A>(mem, cvt, A{});\n         }\n@@ -546,7 +546,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n+        inline void store_complex_aligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n         {\n             float32x4x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -556,7 +556,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n+        inline void store_complex_unaligned(std::complex<float>* dst, batch<std::complex<float> ,A> const& src, requires_arch<neon>)\n         {\n             store_complex_aligned(dst, src, A{});\n         }\n@@ -566,55 +566,55 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u8_s8(vnegq_s8(vreinterpretq_s8_u8(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s8(rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u16_s16(vnegq_s16(vreinterpretq_s16_u16(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s16(rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vreinterpretq_u32_s32(vnegq_s32(vreinterpretq_s32_u32(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_s32(rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch<T, A>({-rhs.get(0), -rhs.get(1)});\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch<T, A>({-rhs.get(0), -rhs.get(1)});\n         }\n         \n         template <class A>\n-        batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> neg(batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             return vnegq_f32(rhs);\n         }\n@@ -627,7 +627,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vaddq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -646,7 +646,7 @@ namespace xsimd\n         WRAP_BINARY_INT(vqaddq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -666,7 +666,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vsubq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -685,7 +685,7 @@ namespace xsimd\n         WRAP_BINARY_INT(vqsubq, detail::identity_return_type)\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_dispatcher::binary dispatcher =\n@@ -706,7 +706,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vmulq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher =\n@@ -723,20 +723,20 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T,  detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcvtq_s32_f32(vcvtq_f32_s32(lhs) / vcvtq_f32_s32(rhs));\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcvtq_u32_f32(vcvtq_f32_u32(lhs) / vcvtq_f32_u32(rhs));\n         }\n #endif\n \n         template <class A>\n-        batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> div(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             // from stackoverflow & https://projectne10.github.io/Ne10/doc/NE10__divc_8neon_8c_source.html\n             // get an initial estimate of 1/b.\n@@ -760,7 +760,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vceqq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -772,7 +772,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             using dispatcher_type = detail::neon_comp_dispatcher_impl<uint8x16_t, uint16x8_t, uint32x4_t>::binary;\n@@ -784,13 +784,13 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) == rhs.get(0), lhs.get(1) == rhs.get(1)});\n         }\n@@ -803,7 +803,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcltq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -815,7 +815,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) < rhs.get(0), lhs.get(1) < rhs.get(1)});\n         }\n@@ -828,7 +828,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcleq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -840,7 +840,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) <= rhs.get(0), lhs.get(1) <= rhs.get(1)});\n         }\n@@ -853,7 +853,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcgtq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -865,7 +865,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) > rhs.get(0), lhs.get(1) > rhs.get(1)});\n         }\n@@ -878,7 +878,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vcgeq, detail::comp_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_comp_dispatcher::binary dispatcher =\n@@ -890,7 +890,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return batch_bool<T, A>({lhs.get(0) >= rhs.get(0), lhs.get(1) >= rhs.get(1)});\n         }\n@@ -923,14 +923,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_and_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_and_neon(register_type(lhs), register_type(rhs));\n@@ -951,7 +951,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_or_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_or_neon(V const& lhs, V const& rhs)\n             {\n                 const neon_dispatcher::binary dispatcher =\n                 {\n@@ -964,14 +964,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_or_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_or_neon(register_type(lhs), register_type(rhs));\n@@ -992,7 +992,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_xor_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_xor_neon(V const& lhs, V const& rhs)\n             {\n                 const neon_dispatcher::binary dispatcher =\n                 {\n@@ -1005,14 +1005,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_xor_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_xor_neon(register_type(lhs), register_type(rhs));\n@@ -1023,7 +1023,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T>\n-        batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             return bitwise_xor(lhs, rhs, A{});\n         }\n@@ -1052,7 +1052,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_not_neon(V const& arg)\n+            inline V bitwise_not_neon(V const& arg)\n             {\n                 const neon_dispatcher::unary dispatcher =\n                 {\n@@ -1066,14 +1066,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_not_neon(register_type(arg));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_not_neon(register_type(arg));\n@@ -1093,7 +1093,7 @@ namespace xsimd\n             }\n \n             template <class V>\n-            V bitwise_andnot_neon(V const& lhs, V const& rhs)\n+            inline V bitwise_andnot_neon(V const& lhs, V const& rhs)\n             {\n                 const detail::neon_dispatcher::binary dispatcher =\n                 {\n@@ -1106,14 +1106,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             return detail::bitwise_andnot_neon(register_type(lhs), register_type(rhs));\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n+        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return detail::bitwise_andnot_neon(register_type(lhs), register_type(rhs));\n@@ -1127,7 +1127,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vminq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher = \n@@ -1139,7 +1139,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return { std::min(lhs.get(0), rhs.get(0)), std::min(lhs.get(1), rhs.get(1)) };\n         }\n@@ -1152,7 +1152,7 @@ namespace xsimd\n         WRAP_BINARY_FLOAT(vmaxq, detail::identity_return_type)\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::binary dispatcher = \n@@ -1164,7 +1164,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return { std::max(lhs.get(0), rhs.get(0)), std::max(lhs.get(1), rhs.get(1)) };\n         }\n@@ -1199,7 +1199,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::exclude_int64_neon_t<T> = 0>\n-        batch<T, A> abs(batch<T, A> const& arg, requires_arch<neon>)\n+        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::excluding_int64_dispatcher::unary dispatcher = \n@@ -1215,7 +1215,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)\n+        inline batch<float, A> sqrt(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             batch<float, A> sqrt_reciprocal = vrsqrteq_f32(arg);\n             // one iter\n@@ -1231,13 +1231,13 @@ namespace xsimd\n \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n+        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n         {\n             return vfmaq_f32(z, x, y);\n         }\n \n         template <class A>\n-        batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n+        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<neon>)\n         {\n             return vfmaq_f32(-z, x, y);\n         }\n@@ -1250,7 +1250,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A, class V>\n-            T sum_batch(V const& arg)\n+            inline T sum_batch(V const& arg)\n             {\n                 T res = T(0);\n                 for (std::size_t i = 0; i < batch<T, A>::size; ++i)\n@@ -1262,57 +1262,57 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vpadd_u8(vget_low_u8(arg), vget_high_u8(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int8x8_t tmp = vpadd_s8(vget_low_s8(arg), vget_high_s8(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vpadd_u16(vget_low_u16(arg), vget_high_u16(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int16x4_t tmp = vpadd_s16(vget_low_s16(arg), vget_high_s16(arg));\n             return detail::sum_batch<T, A>(tmp);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vpadd_u32(vget_low_u32(arg), vget_high_u32(arg));\n             tmp = vpadd_u32(tmp, tmp);\n             return vget_lane_u32(tmp, 0);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             int32x2_t tmp = vpadd_s32(vget_low_s32(arg), vget_high_s32(arg));\n             tmp = vpadd_s32(tmp, tmp);\n             return vget_lane_s32(tmp, 0);\n         }\n \n         template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon>)\n         {\n             return arg.get(0) + arg.get(1);\n         }\n \n         template <class A>\n-        float hadd(batch<float, A> const& arg, requires_arch<neon>)\n+        inline float hadd(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             float32x2_t tmp = vpadd_f32(vget_low_f32(arg), vget_high_f32(arg));\n             tmp = vpadd_f32(tmp, tmp);\n@@ -1324,7 +1324,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)\n+        inline batch<float, A> haddp(const batch<float, A>* row, requires_arch<neon>)\n         {\n             // row = (a,b,c,d)\n             float32x2_t tmp1, tmp2, tmp3;\n@@ -1385,7 +1385,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<neon>)\n+        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<neon>)\n         {\n             using bool_register_type = typename batch_bool<T, A>::register_type;\n             using register_type = typename batch<T, A>::register_type;\n@@ -1399,7 +1399,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, bool... b, detail::enable_neon_type_t<T> = 0>\n-        batch<T, A> select(batch_bool_constant<batch<T, A>, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<neon>)\n+        inline batch<T, A> select(batch_bool_constant<batch<T, A>, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<neon>)\n         {\n             return select(batch_bool<T, A>{b...}, true_br, false_br, neon{});\n         }\n@@ -1409,61 +1409,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint8x8x2_t tmp = vzip_u8(vget_low_u8(lhs), vget_low_u8(rhs));\n             return vcombine_u8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int8x8x2_t tmp = vzip_s8(vget_low_s8(lhs), vget_low_s8(rhs));\n             return vcombine_s8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint16x4x2_t tmp = vzip_u16(vget_low_u16(lhs), vget_low_u16(rhs));\n             return vcombine_u16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int16x4x2_t tmp = vzip_s16(vget_low_s16(lhs), vget_low_s16(rhs));\n             return vcombine_s16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint32x2x2_t tmp = vzip_u32(vget_low_u32(lhs), vget_low_u32(rhs));\n             return vcombine_u32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int32x2x2_t tmp = vzip_s32(vget_low_s32(lhs), vget_low_s32(rhs));\n             return vcombine_s32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_u64(vget_low_u64(lhs), vget_low_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_s64(vget_low_s64(lhs), vget_low_s64(rhs));\n         }\n \n         template <class A>\n-        batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             float32x2x2_t tmp = vzip_f32(vget_low_f32(lhs), vget_low_f32(rhs));\n             return vcombine_f32(tmp.val[0], tmp.val[1]);\n@@ -1474,61 +1474,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint8x8x2_t tmp = vzip_u8(vget_high_u8(lhs), vget_high_u8(rhs));\n             return vcombine_u8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int8x8x2_t tmp = vzip_s8(vget_high_s8(lhs), vget_high_s8(rhs));\n             return vcombine_s8(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint16x4x2_t tmp = vzip_u16(vget_high_u16(lhs), vget_high_u16(rhs));\n             return vcombine_u16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int16x4x2_t tmp = vzip_s16(vget_high_s16(lhs), vget_high_s16(rhs));\n             return vcombine_s16(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             uint32x2x2_t tmp = vzip_u32(vget_high_u32(lhs), vget_high_u32(rhs));\n             return vcombine_u32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             int32x2x2_t tmp = vzip_s32(vget_high_s32(lhs), vget_high_s32(rhs));\n             return vcombine_s32(tmp.val[0], tmp.val[1]);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_u64(vget_high_u64(lhs), vget_high_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vcombine_s64(vget_high_s64(lhs), vget_high_s64(rhs));\n         }\n \n         template <class A>\n-        batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n+        inline batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon>)\n         {\n             float32x2x2_t tmp = vzip_f32(vget_high_f32(lhs), vget_high_f32(rhs));\n             return vcombine_f32(tmp.val[0], tmp.val[1]);\n@@ -1541,14 +1541,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)\n+            inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>)\n             {\n                 assert(false && \"extract_pair out of bounds\");\n                 return  batch<T, A>{};\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1561,7 +1561,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1574,7 +1574,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1587,7 +1587,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1600,7 +1600,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1613,7 +1613,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1626,7 +1626,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1639,7 +1639,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t I, size_t... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1652,7 +1652,7 @@ namespace xsimd\n             }\n \n             template <class A, size_t I, size_t... Is>\n-            batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n+            inline batch<float, A> extract_pair(batch<float, A> const& lhs, batch<float, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1665,7 +1665,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t... Is>\n-            batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)\n+            inline batch<T, A> extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -1679,7 +1679,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)\n+        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(0<= n && n< size && \"index in bounds\");\n@@ -1693,14 +1693,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n             {\n                 assert(false && \"bitwise_lshift out of bounds\");\n                 return batch<T, A>{};\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1713,7 +1713,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1726,7 +1726,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1739,7 +1739,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1752,7 +1752,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1765,7 +1765,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1778,7 +1778,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1791,7 +1791,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1804,7 +1804,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int... Is>\n-            batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n+            inline batch<T, A> bitwise_lshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -1818,57 +1818,57 @@ namespace xsimd\n         }\n         \n         template <class A, class T>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0<= n && n< size && \"index in bounds\");\n             return detail::bitwise_lshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s64(lhs, rhs);\n         }\n@@ -1880,14 +1880,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& /*lhs*/, int /*n*/, ::xsimd::detail::int_sequence<>)\n             {\n                 assert(false && \"bitwise_rshift out of bounds\");\n                 return batch<T, A>{};\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 1> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1900,7 +1900,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 1> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1913,7 +1913,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 2> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1926,7 +1926,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 2> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1939,7 +1939,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 4> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1952,7 +1952,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 4> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1965,7 +1965,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_unsigned_t<T, 8> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1978,7 +1978,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int I, int... Is, detail::enable_sized_signed_t<T, 8> = 0>\n-            batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n+            inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<I, Is...>)\n             {\n                 if (n == I)\n                 {\n@@ -1991,7 +1991,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int... Is>\n-            batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n+            inline batch<T, A> bitwise_rshift_impl(batch<T, A> const& lhs, int n, ::xsimd::detail::int_sequence<0, Is...>)\n             {\n                 if (n == 0)\n                 {\n@@ -2005,45 +2005,45 @@ namespace xsimd\n         }\n         \n         template <class A, class T>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon>)\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0<= n && n< size && \"index in bounds\");\n             return detail::bitwise_rshift_impl(lhs, n, ::xsimd::detail::make_int_sequence<size>());\n         }\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u8(lhs, vnegq_s8(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s8(lhs, vnegq_s8(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u16(lhs, vnegq_s16(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s16(lhs, vnegq_s16(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_u32(lhs, vnegq_s32(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon>)\n         {\n             return vshlq_s32(lhs, vnegq_s32(rhs));\n         }\n@@ -2055,7 +2055,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vand_u8(vget_low_u8(arg), vget_high_u8(arg));\n             tmp = vpmin_u8(tmp, tmp);\n@@ -2065,7 +2065,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vand_u16(vget_low_u16(arg), vget_high_u16(arg));\n             tmp = vpmin_u16(tmp, tmp);\n@@ -2074,14 +2074,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vand_u32(vget_low_u32(arg), vget_high_u32(arg));\n             return vget_lane_u32(vpmin_u32(tmp, tmp), 0) != 0;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint64x1_t tmp = vand_u64(vget_low_u64(arg), vget_high_u64(arg));\n             return vget_lane_u64(tmp, 0) != 0;\n@@ -2092,7 +2092,7 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint8x8_t tmp = vorr_u8(vget_low_u8(arg), vget_high_u8(arg));\n             tmp = vpmax_u8(tmp, tmp);\n@@ -2102,7 +2102,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint16x4_t tmp = vorr_u16(vget_low_u16(arg), vget_high_u16(arg));\n             tmp = vpmax_u16(tmp, tmp);\n@@ -2111,14 +2111,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint32x2_t tmp = vorr_u32(vget_low_u32(arg), vget_high_u32(arg));\n             return vget_lane_u32(vpmax_u32(tmp, tmp), 0);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon>)\n         {\n             uint64x1_t tmp = vorr_u64(vget_low_u64(arg), vget_high_u64(arg));\n             return bool(vget_lane_u64(tmp, 0));\n@@ -2171,7 +2171,7 @@ namespace xsimd\n             };\n \n             template <class R, class... T>\n-            const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))\n+            inline const bitwise_caster_impl<R, T...> make_bitwise_caster_impl(R (*...arg)(T))\n             {\n                 return {std::make_tuple(arg...)};\n             }\n@@ -2208,7 +2208,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class R>\n-        batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)\n+        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<neon>)\n         {\n             const detail::neon_bitwise_caster caster = {\n                 std::make_tuple(\n@@ -2250,14 +2250,14 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<int32_t, A>::register_type;\n             return register_type(arg);\n         }\n \n         template <class A>\n-        batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& arg, requires_arch<neon>)\n         {\n             using register_type = typename batch_bool<float, A>::register_type;\n             return register_type(arg);\n@@ -2268,7 +2268,7 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)\n+        inline batch<int32_t, A> to_int(const batch<float, A>& x, requires_arch<neon>)\n         {\n             return vcvtq_s32_f32(x);\n         }\n@@ -2278,7 +2278,7 @@ namespace xsimd\n          ************/\n \n         template <class A>\n-        batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)\n+        inline batch<float, A> to_float(const batch<int32_t, A>& x, requires_arch<neon>)\n         {\n             return vcvtq_f32_s32(x);\n         }\n@@ -2290,7 +2290,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class Tin, class Tout, class A>\n-            batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)\n+            inline batch<Tout, A> fast_cast(batch<Tin, A> const& in, batch<Tout, A> const& out, requires_arch<neon>)\n             {\n                 return bitwise_cast(in, out, A{});\n             }\n@@ -2301,7 +2301,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)\n+        inline batch_bool<float, A> isnan(batch<float, A> const& arg, requires_arch<neon>)\n         {\n             return !(arg == arg);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_neon64.hpp",
        "status": "modified",
        "additions": 102,
        "deletions": 102,
        "changes": 204,
        "patch": "@@ -29,25 +29,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vminvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});\n         }\n@@ -57,25 +57,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vmaxvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n+        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>)\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64{});\n         }\n@@ -86,13 +86,13 @@ namespace xsimd\n \n         // Required to avoid ambiguous call\n         template <class A, class T>\n-        batch<T, A> broadcast(T val, requires_arch<neon64>)\n+        inline batch<T, A> broadcast(T val, requires_arch<neon64>)\n         {\n             return broadcast<neon64>(val, neon{});\n         }\n \n         template <class A>\n-        batch<double, A> broadcast(double val, requires_arch<neon64>)\n+        inline batch<double, A> broadcast(double val, requires_arch<neon64>)\n         {\n             return vdupq_n_f64(val);\n         }\n@@ -102,13 +102,13 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)\n+        inline batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1)\n         {\n             return float64x2_t{d0, d1};\n         }\n \n         template <class A>\n-        batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)\n+        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1)\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<double>;\n@@ -121,7 +121,7 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)\n+        inline batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vandq_u64(arg, vreinterpretq_u64_f64(vdupq_n_f64(1.))));\n         }\n@@ -131,13 +131,13 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)\n+        inline batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>)\n         {\n             return vld1q_f64(src);\n         }\n \n         template <class A>\n-        batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)\n+        inline batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>)\n         {\n             return load_aligned<A>(src, convert<double>(), A{});\n         }\n@@ -147,13 +147,13 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n+        inline void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n         {\n             vst1q_f64(dst, src);\n         }\n \n         template <class A>\n-        void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n+        inline void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>)\n         {\n             return store_aligned<A>(dst, src, A{});\n         }\n@@ -163,7 +163,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)\n+        inline batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>)\n         {\n             using real_batch = batch<double, A>;\n             const double* buf = reinterpret_cast<const double*>(mem);\n@@ -174,7 +174,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)\n+        inline batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>)\n         {\n             return load_complex_aligned<A>(mem, cvt, A{});\n         }\n@@ -184,7 +184,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)\n+        inline void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double> ,A> const& src, requires_arch<neon64>)\n         {\n             float64x2x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -194,7 +194,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)\n+        inline void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>)\n         {\n             store_complex_aligned(dst, src, A{});\n         }\n@@ -204,19 +204,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_u64_s64(vnegq_s64(vreinterpretq_s64_u64(rhs)));\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vnegq_s64(rhs);\n         }\n \n         template <class A>\n-        batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vnegq_f64(rhs);\n         }\n@@ -226,7 +226,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vaddq_f64(lhs, rhs);\n         }\n@@ -236,7 +236,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return add(lhs, rhs, neon64{});\n         }\n@@ -246,7 +246,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vsubq_f64(lhs, rhs);\n         }\n@@ -256,7 +256,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return sub(lhs, rhs, neon64{});\n         }\n@@ -266,7 +266,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vmulq_f64(lhs, rhs);\n         }\n@@ -277,19 +277,19 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcvtq_u64_f64(vcvtq_f64_u64(lhs) / vcvtq_f64_u64(rhs));\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcvtq_s64_f64(vcvtq_f64_s64(lhs) / vcvtq_f64_s64(rhs));\n         }\n #endif\n         template <class A>\n-        batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vdivq_f64(lhs, rhs);\n         }\n@@ -299,37 +299,37 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_f64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n@@ -339,41 +339,41 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcltq_f64(lhs, rhs);\n         }\n \n         /******\n          * le *\n          ******/\n-        \n+\n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcleq_f64(lhs, rhs);\n         }\n@@ -383,19 +383,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgtq_f64(lhs, rhs);\n         }\n@@ -405,19 +405,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_u64(lhs, rhs);\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vcgeq_f64(lhs, rhs);\n         }\n@@ -427,14 +427,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vandq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vandq_u64(lhs, rhs);\n         }\n@@ -444,14 +444,14 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vorrq_u64(lhs, rhs);\n         }\n@@ -461,14 +461,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(veorq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return veorq_u64(lhs, rhs);\n         }\n@@ -478,7 +478,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return bitwise_xor(lhs, rhs, A{});\n         }\n@@ -488,13 +488,13 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u32(vmvnq_u32(vreinterpretq_u32_f64(rhs)));\n         }\n \n         template <class A>\n-        batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return detail::bitwise_not_u64(rhs);\n         }\n@@ -504,14 +504,14 @@ namespace xsimd\n          ******************/\n \n         template <class A>\n-        batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vreinterpretq_f64_u64(vbicq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n         \n         template <class A>\n-        batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vbicq_u64(lhs, rhs);\n         }\n@@ -521,7 +521,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vminq_f64(lhs, rhs);\n         }\n@@ -531,7 +531,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vmaxq_f64(lhs, rhs);\n         }\n@@ -541,19 +541,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T,  detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return rhs;\n         }\n \n         template <class A, class T,  detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vabsq_s64(rhs);\n         }\n \n         template <class A>\n-        batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vabsq_f64(rhs);\n         }\n@@ -563,7 +563,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vsqrtq_f64(rhs);\n         }\n@@ -574,13 +574,13 @@ namespace xsimd\n         \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n+        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n         {\n             return vfmaq_f64(z, x, y);\n         }\n \n         template <class A>\n-        batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n+        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>)\n         {\n             return vfmaq_f64(-z, x, y);\n         }\n@@ -591,55 +591,55 @@ namespace xsimd\n          ********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s8(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s16(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s32(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_u64(arg);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n+        inline typename batch<T, A>::value_type hadd(batch<T, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_s64(arg);\n         }\n \n         template <class A>\n-        double hadd(batch<double, A> const& arg, requires_arch<neon64>)\n+        inline double hadd(batch<double, A> const& arg, requires_arch<neon64>)\n         {\n             return vaddvq_f64(arg);\n         }\n@@ -649,7 +649,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)\n+        inline batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>)\n         {\n             return vpaddq_f64(row[0], row[1]);\n         }\n@@ -659,13 +659,13 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)\n+        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>)\n         {\n             return vbslq_f64(cond, a, b);\n         }\n \n         template <class A, bool... b>\n-        batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,\n+        inline batch<double, A> select(batch_bool_constant<batch<double, A>, b...> const&,\n                                 batch<double, A> const& true_br,\n                                 batch<double, A> const& false_br,\n                                 requires_arch<neon64>)\n@@ -677,19 +677,19 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip1q_f64(lhs, rhs);\n         }\n@@ -699,19 +699,19 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n+        inline batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>)\n         {\n             return vzip2q_f64(lhs, rhs);\n         }\n@@ -723,7 +723,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t I, size_t... Is>\n-            batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n+            inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n                                           ::xsimd::detail::index_sequence<I, Is...>)\n             {\n                 if (n == I)\n@@ -738,7 +738,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)\n+        inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>)\n         {\n             constexpr std::size_t size = batch<double, A>::size;\n             assert(0<= n && n< size && \"index in bounds\");\n@@ -750,25 +750,25 @@ namespace xsimd\n          ******************/\n         \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n         {\n             return bitwise_rshift<A>(lhs, n, neon{}); \n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>)\n         {\n             return vshlq_u64(lhs, vnegq_s64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>)\n         {\n             return bitwise_rshift<A>(lhs, n, neon{}); \n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n+        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>)\n         {\n             return vshlq_s64(lhs, vnegq_s64(rhs));\n         }\n@@ -796,7 +796,7 @@ namespace xsimd\n         #undef WRAP_CAST\n \n         template <class A, class T>\n-        batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n         {\n             using caster_type = detail::bitwise_caster_impl<float64x2_t,\n                                                             uint8x16_t, int8x16_t,\n@@ -832,7 +832,7 @@ namespace xsimd\n         }\n \n         template <class A, class R>\n-        batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)\n+        inline batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>)\n         {\n             using caster_type = detail::bitwise_caster_neon64<float64x2_t,\n                                                               uint8x16_t, int8x16_t,\n@@ -851,7 +851,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n+        inline batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>)\n         {\n             return arg;\n         }\n@@ -861,14 +861,14 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& arg, requires_arch<neon64>)\n         {\n             using register_type = typename batch_bool<int64_t, A>::register_type;\n             return register_type(arg);\n         }\n \n         template <class A>\n-        batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& arg, requires_arch<neon64>)\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             return register_type(arg);\n@@ -879,7 +879,7 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)\n+        inline batch<int64_t, A> to_int(const batch<double, A>& x, requires_arch<neon64>)\n         {\n             return vcvtq_s64_f64(x);\n         }\n@@ -889,7 +889,7 @@ namespace xsimd\n          ************/\n \n         template <class A>\n-        batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)\n+        inline batch<double, A> to_float(batch<int64_t, A> const& x, requires_arch<neon64>)\n         {\n             return vcvtq_f64_s64(x);\n         }\n@@ -899,7 +899,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)\n+        inline batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>)\n         {\n             return !(arg == arg);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_scalar.hpp",
        "status": "modified",
        "additions": 11,
        "deletions": 11,
        "changes": 22,
        "patch": "@@ -76,42 +76,42 @@ namespace xsimd\n #else\n     // Windows defines catch all templates\n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isfinite(T var)\n     {\n         return std::isfinite(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isfinite(T var)\n     {\n         return isfinite(double(var));\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isinf(T var)\n     {\n         return std::isinf(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isinf(T var)\n     {\n         return isinf(double(var));\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isnan(T var)\n     {\n         return std::isnan(var);\n     }\n \n     template <class T>\n-    typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isnan(T var)\n     {\n         return isnan(double(var));\n@@ -120,13 +120,13 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_NUMPY_COMPLEX\n     template <class T>\n-    bool isnan(std::complex<T> var)\n+    inline bool isnan(std::complex<T> var)\n     {\n         return std::isnan(std::real(var)) || std::isnan(std::imag(var));\n     }\n \n     template <class T>\n-    bool isinf(std::complex<T> var)\n+    inline bool isinf(std::complex<T> var)\n     {\n         return std::isinf(std::real(var)) || std::isinf(std::imag(var));\n     }\n@@ -228,13 +228,13 @@ namespace xsimd\n     }\n \n     template <class T>\n-    std::complex<T> log2(const std::complex<T>& val)\n+    inline std::complex<T> log2(const std::complex<T>& val)\n     {\n         return log(val) / std::log(T(2));\n     }\n \n     template<typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    T sadd(const T& lhs, const T& rhs)\n+    inline T sadd(const T& lhs, const T& rhs)\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -265,7 +265,7 @@ namespace xsimd\n     }\n \n     template<typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    T ssub(const T& lhs, const T& rhs)\n+    inline T ssub(const T& lhs, const T& rhs)\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse2.hpp",
        "status": "modified",
        "additions": 256,
        "deletions": 165,
        "changes": 421,
        "patch": "@@ -24,18 +24,20 @@ namespace xsimd {\n     using namespace types;\n \n     // abs\n-    template<class A> batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) {\n       __m128d sign_mask = _mm_set1_pd(-0.f);  // -0.f = 1 << 31\n       return _mm_andnot_pd(sign_mask, self);\n     }\n-    template<class A> batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) {\n       __m128 sign_mask = _mm_set1_ps(-0.f);  // -0.f = 1 << 31\n       return _mm_andnot_ps(sign_mask, self);\n     }\n \n     // add\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_add_epi8(self, other);\n         case 2: return _mm_add_epi16(self, other);\n@@ -45,90 +47,104 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A> batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_add_ps(self, other);\n     }\n \n-    template<class A> batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_add_pd(self, other);\n     }\n \n     // all\n-    template<class A> bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool all(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_ps(self) == 0x0F;\n     }\n-    template<class A> bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool all(batch_bool<double, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_pd(self) == 0x03;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool all(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline bool all(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_epi8(self) == 0xFFFF;\n     }\n \n     // any\n-    template<class A> bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool any(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_ps(self) != 0;\n     }\n-    template<class A> bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline bool any(batch_bool<double, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_pd(self) != 0;\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline bool any(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_movemask_epi8(self) != 0;\n     }\n \n     // bitwise_and\n-    template<class A> batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_and_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_and_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_and_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_and_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A> batch<double, A>\n+    inline bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_and_pd(self, other);\n     }\n \n-    template<class A> batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_and_pd(self, other);\n     }\n \n     // bitwise_andnot\n-    template<class A> batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_ps(self, other);\n     }\n \n-    template<class A> batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_pd(self, other);\n     }\n-    \n-    template<class A> batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_andnot_pd(self, other);\n     }\n \n     // bitwise_lshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_and_si128(_mm_set1_epi8(0xFF << other), _mm_slli_epi32(self, other));\n         case 2: return _mm_slli_epi16(self, other);\n@@ -139,56 +155,62 @@ namespace xsimd {\n     }\n \n     // bitwise_not\n-    template<class A> batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n     }\n-    template<class A> batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_si128(self, _mm_set1_epi32(-1));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_si128(self, _mm_set1_epi32(-1));\n     }\n     template <class A>\n-    batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_not(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n     }\n     template <class A>\n-    batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {\n+    inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n     }\n \n     // bitwise_or\n-    template<class A> batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_or_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_or_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_or_si128(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_or_si128(self, other);\n     }\n \n-    template<class A> batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_or_pd(self, other);\n     }\n \n-    template<class A> batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_or_pd(self, other);\n     }\n \n     // bitwise_rshift\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: {\n@@ -222,77 +244,86 @@ namespace xsimd {\n     }\n \n     // bitwise_xor\n-    template<class A> batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_si128(self, other);\n     }\n-    template<class A> batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_pd(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return _mm_xor_si128(self, other);\n     }\n \n     // bitwise_cast\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n+    inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n       return _mm_castsi128_ps(self);\n     }\n     template<class A, class T, class Tp, class=typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-    batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<sse2>) {\n+    inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const &, requires_arch<sse2>) {\n       return batch<Tp, A>(self.data);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n       return _mm_castps_si128(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n       return _mm_castsi128_pd(self);\n     }\n     template<class A>\n-    batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n+    inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const &, requires_arch<sse2>) {\n       return _mm_castps_pd(self);\n     }\n     template<class A>\n-    batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n+    inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const &, requires_arch<sse2>) {\n       return _mm_castpd_ps(self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n+    inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const &, requires_arch<sse2>) {\n       return _mm_castpd_si128(self);\n     }\n \n     // bool_cast\n-    template<class A> batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<int32_t, A>\n+    inline bool_cast(batch_bool<float, A> const& self, requires_arch<sse2>) {\n         return _mm_castps_si128(self);\n     }\n-    template<class A> batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<float, A>\n+    inline bool_cast(batch_bool<int32_t, A> const& self, requires_arch<sse2>) {\n         return _mm_castsi128_ps(self);\n     }\n-    template<class A> batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<int64_t, A>\n+    inline bool_cast(batch_bool<double, A> const& self, requires_arch<sse2>) {\n         return _mm_castpd_si128(self);\n     }\n-    template<class A> batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {\n+    template<class A> batch_bool<double, A>\n+    inline bool_cast(batch_bool<int64_t, A> const& self, requires_arch<sse2>) {\n         return _mm_castsi128_pd(self);\n     }\n \n     // broadcast\n-    template<class A> batch<float, A> broadcast(float val, requires_arch<sse2>) {\n+    template<class A> batch<float, A>\n+    inline broadcast(float val, requires_arch<sse2>) {\n       return _mm_set1_ps(val);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> broadcast(T val, requires_arch<sse2>) {\n+    inline batch<T, A> broadcast(T val, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_set1_epi8(val);\n         case 2: return _mm_set1_epi16(val);\n@@ -301,7 +332,8 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> broadcast(double val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> broadcast(double val, requires_arch<sse2>) {\n       return _mm_set1_pd(val);\n     }\n \n@@ -310,48 +342,58 @@ namespace xsimd {\n     {\n       // Override these methods in SSE-based archs, no need to override store_aligned / store_unaligned\n       // complex_low\n-      template<class A> batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpacklo_ps(self.real(), self.imag());\n       }\n       // complex_high\n-      template<class A> batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpackhi_ps(self.real(), self.imag());\n       }\n-      template<class A> batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpacklo_pd(self.real(), self.imag());\n       }\n-      template<class A> batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) {\n         return _mm_unpackhi_pd(self.real(), self.imag());\n       }\n     }\n \n     // div\n-    template<class A> batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_div_ps(self, other);\n     }\n-    template<class A> batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_div_pd(self, other);\n     }\n \n     // convert\n     namespace detail {\n-    template<class A> batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) {\n       return _mm_cvtepi32_ps(self);\n     }\n-    template<class A> batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) {\n       return _mm_cvttps_epi32(self);\n     }\n     }\n \n     // eq\n-    template<class A> batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpeq_ps(self, other);\n     }\n-    template<class A> batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return  _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_castps_si128(self), _mm_castps_si128(other)));\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_cmpeq_epi8(self, other);\n         case 2: return _mm_cmpeq_epi16(self, other);\n@@ -367,30 +409,35 @@ namespace xsimd {\n       }\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n       return eq(batch<T, A>(self.data), batch<T, A>(other.data));\n     }\n-    template<class A> batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpeq_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return  _mm_castsi128_pd(_mm_cmpeq_epi32(_mm_castpd_si128(self), _mm_castpd_si128(other)));\n     }\n \n     // ge\n-    template<class A> batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpge_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpge_pd(self, other);\n     }\n-    // gt\n \n-    template<class A> batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    // gt\n+    template<class A>\n+    inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpgt_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_cmpgt_epi8(self, other);\n@@ -404,12 +451,14 @@ namespace xsimd {\n       }\n     }\n \n-    template<class A> batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpgt_pd(self, other);\n     }\n-    \n+\n     // hadd\n-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& self, requires_arch<sse2>) {\n       __m128 tmp0 = _mm_add_ps(self, _mm_movehl_ps(self, self));\n       __m128 tmp1 = _mm_add_ss(tmp0, _mm_shuffle_ps(tmp0, tmp0, 1));\n       return _mm_cvtss_f32(tmp1);\n@@ -418,7 +467,7 @@ namespace xsimd {\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd_default(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline T hadd_default(batch<T, A> const& self, requires_arch<sse2>) {\n         alignas(A::alignment()) T buffer[batch<T, A>::size];\n         self.store_aligned(buffer);\n         T res = 0;\n@@ -430,7 +479,7 @@ namespace xsimd {\n     }\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 4: {\n                 __m128i tmp1 = _mm_shuffle_epi32(self, 0x0E);\n@@ -456,12 +505,13 @@ namespace xsimd {\n       }\n     }\n     template <class A>\n-    double hadd(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline double hadd(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_cvtsd_f64(_mm_add_sd(self, _mm_unpackhi_pd(self, self)));\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) {\n       __m128 tmp0 = _mm_unpacklo_ps(row[0], row[1]);\n       __m128 tmp1 = _mm_unpackhi_ps(row[0], row[1]);\n       __m128 tmp2 = _mm_unpackhi_ps(row[2], row[3]);\n@@ -473,69 +523,80 @@ namespace xsimd {\n       return _mm_add_ps(tmp0, tmp2);\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse2>) {\n       return _mm_add_pd(_mm_unpacklo_pd(row[0], row[1]),\n           _mm_unpackhi_pd(row[0], row[1]));\n     }\n \n     // isnan\n-    template<class A> batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_cmpunord_ps(self, self);\n     }\n-    template<class A> batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_cmpunord_pd(self, self);\n     }\n \n     // load_aligned\n-    template<class A> batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) {\n       return _mm_load_ps(mem);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) {\n+    inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) {\n       return _mm_load_si128((__m128i const*)mem);\n     }\n-    template<class A> batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) {\n       return _mm_load_pd(mem);\n     }\n \n     // load_unaligned\n-    template<class A> batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){\n+    template<class A>\n+    inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>){\n       return _mm_loadu_ps(mem);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) {\n       return _mm_loadu_si128((__m128i const*)mem);\n     }\n-    template<class A> batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){\n+    template<class A>\n+    inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>){\n       return _mm_loadu_pd(mem);\n     }\n \n     // load_complex\n     namespace detail\n     {\n       // Redefine these methods in the SSE-based archs if required\n-      template<class A> batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {\n+      template<class A>\n+      inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) {\n         return {_mm_shuffle_ps(hi, lo, _MM_SHUFFLE(2, 0, 2, 0)), _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(3, 1, 3, 1))};\n       }\n-        template<class A> batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {\n-            return {_mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1))};\n-        }\n+      template<class A>\n+      inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) {\n+          return {_mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1))};\n+      }\n     }\n \n     // le\n-    template<class A> batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmple_ps(self, other);\n     }\n-    template<class A> batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmple_pd(self, other);\n     }\n \n     // lt\n-    template<class A> batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmplt_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_cmplt_epi8(self, other);\n@@ -573,116 +634,134 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmplt_pd(self, other);\n     }\n \n     // max\n-    template<class A> batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_max_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return select(self > other, self, other);\n     }\n-    template<class A> batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_max_pd(self, other);\n     }\n \n     // min\n-    template<class A> batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_min_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       return select(self <= other, self, other);\n     }\n-    template<class A> batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_min_pd(self, other);\n     }\n \n     // mul\n-    template<class A> batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_mul_ps(self, other);\n     }\n-    template<class A> batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_mul_pd(self, other);\n     }\n \n     // neg\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) {\n+    inline batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) {\n       return 0 - self;\n     }\n-    template<class A> batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(0x80000000)));\n     }\n \n \n     template <class A>\n-    batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {\n+    inline batch<double, A> neg(batch<double, A> const &self, requires_arch<sse2>) {\n       return _mm_xor_pd(\n           self, _mm_castsi128_pd(_mm_setr_epi32(0, 0x80000000, 0, 0x80000000)));\n     }\n \n     // neq\n-    template<class A> batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n         return ~(self == other);\n     }\n-    template<class A> batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n+    inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) {\n         return ~(self == other);\n     }\n \n \n-    template<class A> batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_pd(self, other);\n     }\n-    template<class A> batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) {\n       return _mm_cmpneq_pd(self, other);\n     }\n \n     // select\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_ps(_mm_and_ps(cond, true_br), _mm_andnot_ps(cond, false_br));\n     }\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_si128(_mm_and_si128(cond, true_br), _mm_andnot_si128(cond, false_br));\n     }\n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) {\n       return select(batch_bool<T, A>{Values...}, true_br, false_br, sse2{});\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) {\n       return _mm_or_pd(_mm_and_pd(cond, true_br), _mm_andnot_pd(cond, false_br));\n     }\n \n     // sqrt\n-    template<class A> batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) {\n       return _mm_sqrt_ps(val);\n     }\n-    template<class A> batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) {\n       return _mm_sqrt_pd(val);\n     }\n     // sadd\n-    template<class A> batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_add_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n     // TODO: move this in xsimd_generic\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sadd_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         auto mask = (other >> (8 * sizeof(T) - 1));\n         auto self_pos_branch = min(std::numeric_limits<T>::max() - other, self);\n@@ -699,7 +778,7 @@ namespace xsimd {\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_adds_epi8(self, other);\n@@ -715,67 +794,69 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_add_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n \n     // set\n     template<class A, class... Values>\n-    batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n       return _mm_setr_ps(values...);\n     }\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) {\n       return _mm_set_epi64x(v1, v0);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) {\n       return _mm_setr_epi32(v0, v1, v2, v3);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) {\n       return _mm_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n+    inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) {\n       return _mm_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n     }\n \n     template<class A, class... Values>\n-    batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n       return _mm_setr_pd(values...);\n     }\n \n     template<class A, class T, class... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) {\n       return set(batch<T, A>(), A{}, static_cast<T>(values ? -1LL : 0LL )...).data;\n     }\n \n     template<class A, class... Values>\n-    batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n       return _mm_castsi128_ps(set(batch<int32_t, A>(), A{}, static_cast<int32_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     template<class A, class... Values>\n-    batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {\n+    inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) {\n       static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n       return _mm_castsi128_pd(set(batch<int64_t, A>(), A{},  static_cast<int64_t>(values ? -1LL : 0LL )...).data);\n     }\n \n     // ssub\n-    template<class A> batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_ps(self, other); // no saturated arithmetic on floating point numbers\n     }\n     // TODO: move this in xsimd_generic\n     namespace detail\n     {\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> ssub_default(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n          return sadd(self, -other);\n       }\n@@ -788,7 +869,7 @@ namespace xsimd {\n \n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_subs_epi8(self, other);\n@@ -804,48 +885,55 @@ namespace xsimd {\n         }\n       }\n     }\n-    template<class A> batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+\n+    template<class A>\n+    inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_pd(self, other); // no saturated arithmetic on floating point numbers\n     }\n \n     // store_aligned\n-    template<class A> void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_aligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_store_ps(mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_aligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_store_si128((__m128i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_aligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_store_si128((__m128i *)mem, self);\n     }\n-    template<class A> void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_aligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_store_pd(mem, self);\n     }\n \n     // store_unaligned\n-    template<class A> void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_unaligned(float *mem, batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_ps(mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_unaligned(T *mem, batch<T, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_si128((__m128i *)mem, self);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n+    inline void store_unaligned(T *mem, batch_bool<T, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_si128((__m128i *)mem, self);\n     }\n-    template<class A> void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n+    template<class A>\n+    inline void store_unaligned(double *mem, batch<double, A> const& self, requires_arch<sse2>) {\n       return _mm_storeu_pd(mem, self);\n     }\n \n     // sub\n-    template<class A> batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_sub_epi8(self, other);\n         case 2: return _mm_sub_epi16(self, other);\n@@ -854,17 +942,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_sub_pd(self, other);\n     }\n \n     // to_float\n     template<class A>\n-    batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {\n+    inline batch<float, A> to_float(batch<int32_t, A> const& self, requires_arch<sse2>) {\n       return _mm_cvtepi32_ps(self);\n     }\n     template<class A>\n-    batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {\n+    inline batch<double, A> to_float(batch<int64_t, A> const& self, requires_arch<sse2>) {\n       // FIXME: call _mm_cvtepi64_pd\n       alignas(A::alignment()) int64_t buffer[batch<int64_t, A>::size];\n       self.store_aligned(&buffer[0]);\n@@ -873,24 +962,25 @@ namespace xsimd {\n \n     // to_int\n     template<class A>\n-    batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {\n+    inline batch<int32_t, A> to_int(batch<float, A> const& self, requires_arch<sse2>) {\n       return _mm_cvttps_epi32(self);\n     }\n \n     template<class A>\n-    batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {\n+    inline batch<int64_t, A> to_int(batch<double, A> const& self, requires_arch<sse2>) {\n       // FIXME: call _mm_cvttpd_epi64\n       alignas(A::alignment()) double buffer[batch<double, A>::size];\n       self.store_aligned(&buffer[0]);\n       return {(int64_t)buffer[0], (int64_t)buffer[1]};\n     }\n \n     // zip_hi\n-    template<class A> batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_unpackhi_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_unpackhi_epi8(self, other);\n         case 2: return _mm_unpackhi_epi16(self, other);\n@@ -899,16 +989,18 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_unpackhi_pd(self, other);\n     }\n \n     // zip_lo\n-    template<class A> batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) {\n       return _mm_unpacklo_ps(self, other);\n     }\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n+    inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_unpacklo_epi8(self, other);\n         case 2: return _mm_unpacklo_epi16(self, other);\n@@ -917,13 +1009,12 @@ namespace xsimd {\n         default: assert(false && \"unsupported arch/op combination\"); return {};\n       }\n     }\n-    template<class A> batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n+    template<class A>\n+    inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) {\n       return _mm_unpacklo_pd(self, other);\n     }\n   }\n \n }\n \n #endif\n-\n-"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse3.hpp",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -22,29 +22,31 @@ namespace xsimd {\n \n     // load_unaligned\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) {\n+    inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) {\n       return _mm_lddqu_si128((__m128i const*)mem);\n     }\n \n     // hadd\n-    template<class A> float hadd(batch<float, A> const& self, requires_arch<sse3>) {\n+    template<class A>\n+    inline float hadd(batch<float, A> const& self, requires_arch<sse3>) {\n       __m128 tmp0 = _mm_hadd_ps(self, self);\n       __m128 tmp1 = _mm_hadd_ps(tmp0, tmp0);\n       return _mm_cvtss_f32(tmp1);\n     }\n     template <class A>\n-    double hadd(batch<double, A> const &self, requires_arch<sse3>) {\n+    inline double hadd(batch<double, A> const &self, requires_arch<sse3>) {\n       __m128d tmp0 = _mm_hadd_pd(self, self);\n       return _mm_cvtsd_f64(tmp0);\n     }\n \n     // haddp\n-    template<class A> batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {\n+    template<class A>\n+    inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) {\n       return _mm_hadd_ps(_mm_hadd_ps(row[0], row[1]),\n                               _mm_hadd_ps(row[2], row[3]));\n     }\n     template <class A>\n-    batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {\n+    inline batch<double, A> haddp(batch<double, A> const *row, requires_arch<sse3>) {\n       return _mm_hadd_pd(row[0], row[1]);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse4_1.hpp",
        "status": "modified",
        "additions": 32,
        "deletions": 20,
        "changes": 52,
        "patch": "@@ -22,37 +22,41 @@ namespace xsimd {\n     using namespace types;\n     // any\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    bool any(batch<T, A> const& self, requires_arch<sse4_1>) {\n+    inline bool any(batch<T, A> const& self, requires_arch<sse4_1>) {\n       return !_mm_testz_si128(self, self);\n     }\n     // ceil\n-    template<class A> batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_ceil_ps(self);\n     }\n-    template<class A> batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_ceil_pd(self);\n     }\n \n     // eq\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       switch(sizeof(T)) {\n         case 8: return _mm_cmpeq_epi64(self, other);\n         default: return eq(self, other, ssse3{});\n       }\n     }\n \n     // floor\n-    template<class A> batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_floor_ps(self);\n     }\n-    template<class A> batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_floor_pd(self);\n     }\n \n     // max\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_max_epi8(self, other);\n@@ -73,7 +77,7 @@ namespace xsimd {\n \n     // min\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       if(std::is_signed<T>::value) {\n         switch(sizeof(T)) {\n           case 1: return _mm_min_epi8(self, other);\n@@ -94,7 +98,7 @@ namespace xsimd {\n \n     // mul\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n+    inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_or_si128(\n                             _mm_and_si128(_mm_mullo_epi16(self, other), _mm_srli_epi16(_mm_cmpeq_epi8(self, self), 8)),\n@@ -115,35 +119,39 @@ namespace xsimd {\n     }\n \n     // nearbyint\n-    template<class A> batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n-    template<class A> batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n     }\n \n     // select\n     namespace detail {\n       template<class T>\n-      constexpr T interleave(T const &cond) {\n+      inline constexpr T interleave(T const &cond) {\n         return (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 49) & 0x5555) |\n                (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 48) & 0xAAAA);\n       }\n     }\n \n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n+    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_epi8(false_br, true_br, cond);\n     }\n-    template<class A> batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_ps(false_br, true_br, cond);\n     }\n-    template<class A> batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n       return _mm_blendv_pd(false_br, true_br, cond);\n     }\n \n     template<class A, class T, bool... Values, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n+    inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<T, A>, Values...>::mask();\n       switch(sizeof(T)) {\n         case 2: return _mm_blend_epi16(false_br, true_br, mask);\n@@ -159,21 +167,25 @@ namespace xsimd {\n         default: return select(batch_bool_constant<batch<T, A>, Values...>(), true_br, false_br, ssse3{});\n       }\n     }\n-    template<class A, bool... Values> batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A, bool... Values>\n+    inline batch<float, A> select(batch_bool_constant<batch<float, A>, Values...> const& , batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<float, A>, Values...>::mask();\n       return _mm_blend_ps(false_br, true_br, mask);\n     }\n-    template<class A, bool... Values> batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n+    template<class A, bool... Values>\n+    inline batch<double, A> select(batch_bool_constant<batch<double, A>, Values...> const& , batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) {\n       constexpr int mask = batch_bool_constant<batch<double, A>, Values...>::mask();\n       return _mm_blend_pd(false_br, true_br, mask);\n     }\n \n \n     // trunc\n-    template<class A> batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_ps(self, _MM_FROUND_TO_ZERO);\n     }\n-    template<class A> batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {\n+    template<class A>\n+    inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) {\n       return _mm_round_pd(self, _MM_FROUND_TO_ZERO);\n     }\n "
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse4_2.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -23,11 +23,11 @@ namespace xsimd {\n \n     // lt\n     template<class A>\n-    batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {\n+    inline batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) {\n       return _mm_cmpgt_epi64(other, self);\n     }\n     template<class A>\n-    batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {\n+    inline batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) {\n       auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n       auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n       return _mm_cmpgt_epi64(xother, xself);"
      },
      {
        "filename": "include/xsimd/arch/xsimd_ssse3.hpp",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -24,7 +24,7 @@ namespace xsimd {\n \n     // abs\n     template<class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-    batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) {\n+    inline batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) {\n       switch(sizeof(T)) {\n         case 1: return _mm_abs_epi8(self);\n         case 2: return _mm_abs_epi16(self);\n@@ -38,12 +38,12 @@ namespace xsimd {\n     namespace detail {\n \n       template<class T, class A>\n-      batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {\n+      inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) {\n         return other;\n       }\n \n       template<class T, class A, std::size_t I, std::size_t... Is>\n-      batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {\n+      inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) {\n         if(i == I) {\n           return _mm_alignr_epi8(self, other, sizeof(T) * I);\n         }\n@@ -53,15 +53,15 @@ namespace xsimd {\n     }\n \n     template<class A, class T, class _ = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) {\n+    inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) {\n       constexpr std::size_t size = batch<T, A>::size;\n       assert(0<= i && i< size && \"index in bounds\");\n       return detail::extract_pair(self, other, i, ::xsimd::detail::make_index_sequence<size>());\n     }\n \n     // hadd\n     template<class A, class T, class=typename std::enable_if<std::is_integral<T>::value, void>::type>\n-    T hadd(batch<T, A> const& self, requires_arch<ssse3>) {\n+    inline T hadd(batch<T, A> const& self, requires_arch<ssse3>) {\n       switch(sizeof(T)) {\n         case 2: {\n                 __m128i tmp1 = _mm_hadd_epi16(self, self);"
      },
      {
        "filename": "include/xsimd/types/xsimd_api.hpp",
        "status": "modified",
        "additions": 141,
        "deletions": 142,
        "changes": 283,
        "patch": "@@ -53,7 +53,7 @@ namespace xsimd {\n  * @return the absolute values of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> abs(batch<T, A> const& x) {\n+inline batch<T, A> abs(batch<T, A> const& x) {\n   return kernel::abs<A>(x, A{});\n }\n \n@@ -65,7 +65,7 @@ batch<T, A> abs(batch<T, A> const& x) {\n  * @return the absolute values of \\c z.\n  */\n template<class T, class A>\n-batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n+inline batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n   return kernel::abs<A>(z, A{});\n }\n \n@@ -78,7 +78,7 @@ batch<T, A> abs(batch<std::complex<T>, A> const& z) {\n  * @return the sum of \\c x and \\c y\n  */\n template<class T, class Tp>\n-auto add(T const& x, Tp const& y) -> decltype(x + y){\n+inline auto add(T const& x, Tp const& y) -> decltype(x + y){\n   return x + y;\n }\n \n@@ -90,7 +90,7 @@ auto add(T const& x, Tp const& y) -> decltype(x + y){\n  * @return the arc cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> acos(batch<T, A> const& x) {\n+inline batch<T, A> acos(batch<T, A> const& x) {\n   return kernel::acos<A>(x, A{});\n }\n \n@@ -102,7 +102,7 @@ batch<T, A> acos(batch<T, A> const& x) {\n  * @return the inverse hyperbolic cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> acosh(batch<T, A> const& x) {\n+inline batch<T, A> acosh(batch<T, A> const& x) {\n   return kernel::acosh<A>(x, A{});\n }\n \n@@ -114,7 +114,7 @@ batch<T, A> acosh(batch<T, A> const& x) {\n  * @return the argument of \\c z.\n  */\n template<class T, class A>\n-real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n+inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n   return kernel::arg<A>(z, A{});\n }\n \n@@ -126,7 +126,7 @@ real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) {\n  * @return the arc sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> asin(batch<T, A> const& x) {\n+inline batch<T, A> asin(batch<T, A> const& x) {\n   return kernel::asin<A>(x, A{});\n }\n \n@@ -138,7 +138,7 @@ batch<T, A> asin(batch<T, A> const& x) {\n  * @return the inverse hyperbolic sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> asinh(batch<T, A> const& x) {\n+inline batch<T, A> asinh(batch<T, A> const& x) {\n   return kernel::asinh<A>(x, A{});\n }\n \n@@ -150,7 +150,7 @@ batch<T, A> asinh(batch<T, A> const& x) {\n  * @return the arc tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> atan(batch<T, A> const& x) {\n+inline batch<T, A> atan(batch<T, A> const& x) {\n   return kernel::atan<A>(x, A{});\n }\n \n@@ -164,7 +164,7 @@ batch<T, A> atan(batch<T, A> const& x) {\n  * @return the arc tangent of \\c x/y.\n  */\n template<class T, class A>\n-batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::atan2<A>(x, y, A{});\n }\n \n@@ -176,7 +176,7 @@ batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the inverse hyperbolic tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> atanh(batch<T, A> const& x) {\n+inline batch<T, A> atanh(batch<T, A> const& x) {\n   return kernel::atanh<A>(x, A{});\n }\n \n@@ -188,7 +188,7 @@ batch<T, A> atanh(batch<T, A> const& x) {\n  * @return \\c x casted to \\c T_out\n  */\n template<class T_out, class T_in, class A>\n-batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n+inline batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n   return kernel::batch_cast<A>(x, batch<T_out, A>{}, A{});\n }\n \n@@ -200,7 +200,7 @@ batch<T_out, A> batch_cast(batch<T_in, A> const & x) {\n  * @return bit of sign of \\c x\n  */\n template<class T, class A>\n-batch<T, A> bitofsign(batch<T, A> const& x) {\n+inline batch<T, A> bitofsign(batch<T, A> const& x) {\n   return kernel::bitofsign<A>(x, A{});\n }\n \n@@ -213,7 +213,7 @@ batch<T, A> bitofsign(batch<T, A> const& x) {\n  * @return the result of the bitwise and.\n  */\n template<class T, class Tp>\n-auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n+inline auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n   return x & y;\n }\n \n@@ -226,7 +226,7 @@ auto bitwise_and(T const& x, Tp const& y) -> decltype(x & y){\n  * @return the result of the bitwise and not.\n  */\n template<class T, class A>\n-batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::bitwise_andnot<A>(x, y, A{});\n }\n \n@@ -240,7 +240,7 @@ batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the bitwise and not.\n  */\n template<class T, class A>\n-batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {\n+inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) {\n   return kernel::bitwise_andnot<A>(x, y, A{});\n }\n \n@@ -252,7 +252,7 @@ batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> cons\n  * @return \\c x reinterpreted as \\c T_out\n  */\n template<class B, class T, class A>\n-B bitwise_cast(batch<T, A> const& x) {\n+inline B bitwise_cast(batch<T, A> const& x) {\n   return kernel::bitwise_cast<A>(x, B{}, A{});\n }\n \n@@ -264,7 +264,7 @@ B bitwise_cast(batch<T, A> const& x) {\n  * @return the result of the bitwise not.\n  */\n template<class T, class A>\n-batch<T, A> bitwise_not(batch<T, A> const& x) {\n+inline batch<T, A> bitwise_not(batch<T, A> const& x) {\n   return kernel::bitwise_not<A>(x, A{});\n }\n \n@@ -277,7 +277,7 @@ batch<T, A> bitwise_not(batch<T, A> const& x) {\n  * @return the result of the bitwise or.\n  */\n template<class T, class Tp>\n-auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n+inline auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n   return x | y;\n }\n \n@@ -290,25 +290,25 @@ auto bitwise_or(T const& x, Tp const& y) -> decltype(x | y){\n  * @return the result of the bitwise xor.\n  */\n template<class T, class Tp>\n-auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){\n+inline auto bitwise_xor(T const& x, Tp const& y) -> decltype(x ^ y){\n   return x ^ y;\n }\n \n // FIXME: check if these need to be exposed, or removed (?)\n template<class A>\n-batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {\n+inline batch_bool<float, A> bool_cast(batch_bool<int32_t, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {\n+inline batch_bool<int32_t, A> bool_cast(batch_bool<float, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {\n+inline batch_bool<double, A> bool_cast(batch_bool<int64_t, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n template<class A>\n-batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n+inline batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n   return kernel::bool_cast<A>(x, A{});\n }\n \n@@ -320,7 +320,7 @@ batch_bool<int64_t, A> bool_cast(batch_bool<double, A> const& x) {\n  * @return a new batch instance\n  */\n template<class T, class A=default_arch>\n-batch<T, A> broadcast(T v) {\n+inline batch<T, A> broadcast(T v) {\n   return kernel::broadcast<A>(v, A{});\n }\n \n@@ -333,7 +333,7 @@ batch<T, A> broadcast(T v) {\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> broadcast_as(From v) {\n+inline simd_return_type<From, To> broadcast_as(From v) {\n     using batch_value_type = typename simd_return_type<From, To>::value_type;\n     using value_type = typename std::conditional<std::is_same<From, bool>::value,\n                                                  bool,\n@@ -349,7 +349,7 @@ simd_return_type<From, To> broadcast_as(From v) {\n  * @return the cubic root of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cbrt(batch<T, A> const& x) {\n+inline batch<T, A> cbrt(batch<T, A> const& x) {\n   return kernel::cbrt<A>(x, A{});\n }\n \n@@ -362,7 +362,7 @@ batch<T, A> cbrt(batch<T, A> const& x) {\n  * @return the batch of smallest integer values not less than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> ceil(batch<T, A> const& x) {\n+inline batch<T, A> ceil(batch<T, A> const& x) {\n   return kernel::ceil<A>(x, A{});\n }\n \n@@ -377,7 +377,7 @@ batch<T, A> ceil(batch<T, A> const& x) {\n  * @return the result of the clipping.\n  */\n template<class A, class T>\n-batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {\n+inline batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) {\n   return kernel::clip(x, lo, hi, A{});\n }\n \n@@ -389,7 +389,7 @@ batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const&\n  * @return the argument of \\c z.\n  */\n template<class A, class T>\n-complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n+inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n   return kernel::conj(z, A{});\n }\n \n@@ -404,7 +404,7 @@ complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) {\n  * matches that of \\c y.\n  */\n template<class A, class T>\n-batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::copysign<A>(x, y, A{});\n }\n \n@@ -416,7 +416,7 @@ batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cos(batch<T, A> const& x) {\n+inline batch<T, A> cos(batch<T, A> const& x) {\n   return kernel::cos<A>(x, A{});\n }\n \n@@ -428,7 +428,7 @@ batch<T, A> cos(batch<T, A> const& x) {\n  * @return the hyperbolic cosine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> cosh(batch<T, A> const& x) {\n+inline batch<T, A> cosh(batch<T, A> const& x) {\n   return kernel::cosh<A>(x, A{});\n }\n \n@@ -441,7 +441,7 @@ batch<T, A> cosh(batch<T, A> const& x) {\n  * @return the result of the division.\n  */\n template<class T, class Tp>\n-auto div(T const& x, Tp const& y) -> decltype(x / y){\n+inline auto div(T const& x, Tp const& y) -> decltype(x / y){\n   return x / y;\n }\n \n@@ -454,7 +454,7 @@ auto div(T const& x, Tp const& y) -> decltype(x / y){\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n   return x == y;\n }\n \n@@ -466,7 +466,7 @@ batch_bool<T, A> eq(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the natural exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp(batch<T, A> const& x) {\n+inline batch<T, A> exp(batch<T, A> const& x) {\n   return kernel::exp<A>(x, A{});\n }\n \n@@ -478,7 +478,7 @@ batch<T, A> exp(batch<T, A> const& x) {\n  * @return the base 10 exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp10(batch<T, A> const& x) {\n+inline batch<T, A> exp10(batch<T, A> const& x) {\n   return kernel::exp10<A>(x, A{});\n }\n \n@@ -490,7 +490,7 @@ batch<T, A> exp10(batch<T, A> const& x) {\n  * @return the base 2 exponential of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> exp2(batch<T, A> const& x) {\n+inline batch<T, A> exp2(batch<T, A> const& x) {\n   return kernel::exp2<A>(x, A{});\n }\n \n@@ -502,7 +502,7 @@ batch<T, A> exp2(batch<T, A> const& x) {\n  * @return the natural exponential of \\c x, minus one.\n  */\n template<class T, class A>\n-batch<T, A> expm1(batch<T, A> const& x) {\n+inline batch<T, A> expm1(batch<T, A> const& x) {\n   return kernel::expm1<A>(x, A{});\n }\n \n@@ -514,7 +514,7 @@ batch<T, A> expm1(batch<T, A> const& x) {\n  * @return the error function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> erf(batch<T, A> const& x) {\n+inline batch<T, A> erf(batch<T, A> const& x) {\n   return kernel::erf<A>(x, A{});\n }\n \n@@ -526,7 +526,7 @@ batch<T, A> erf(batch<T, A> const& x) {\n  * @return the error function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> erfc(batch<T, A> const& x) {\n+inline batch<T, A> erfc(batch<T, A> const& x) {\n   return kernel::erfc<A>(x, A{});\n }\n \n@@ -539,7 +539,7 @@ batch<T, A> erfc(batch<T, A> const& x) {\n  * @return the evaluation ofpolynomial with coefficient \\c Coefs on point \\c x.\n  */\n template <class T, class A, uint64_t... Coefs>\n-batch<T, A> estrin(const batch<T, A>& x) {\n+inline batch<T, A> estrin(const batch<T, A>& x) {\n   return kernel::estrin<T, A, Coefs...>(x);\n }\n \n@@ -554,7 +554,7 @@ batch<T, A> estrin(const batch<T, A>& x) {\n  * @return.\n  */\n template <class T, class A>\n-batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {\n+inline batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_t i) {\n   return kernel::extract_pair<A>(x, y, i, A{});\n }\n \n@@ -566,7 +566,7 @@ batch<T, A> extract_pair(batch<T, A> const & x, batch<T, A> const& y, std::size_\n  * @return the asbolute values of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> fabs(batch<T, A> const& x) {\n+inline batch<T, A> fabs(batch<T, A> const& x) {\n   return kernel::abs<A>(x, A{});\n }\n \n@@ -580,7 +580,7 @@ batch<T, A> fabs(batch<T, A> const& x) {\n  * @return the positive difference.\n  */\n template<class T, class A>\n-batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::fdim<A>(x, y, A{});\n }\n \n@@ -593,7 +593,7 @@ batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the batch of largest integer values not greater than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> floor(batch<T, A> const& x) {\n+inline batch<T, A> floor(batch<T, A> const& x) {\n   return kernel::floor<A>(x, A{});\n }\n \n@@ -607,7 +607,7 @@ batch<T, A> floor(batch<T, A> const& x) {\n  * @return the result of the fused multiply-add operation.\n  */\n template<class T, class A>\n-batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fma<A>(x, y, z, A{});\n }\n \n@@ -621,7 +621,7 @@ batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::max<A>(x, y, A{});\n }\n \n@@ -635,7 +635,7 @@ batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::min<A>(x, y, A{});\n }\n \n@@ -648,7 +648,7 @@ batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the modulo.\n  */\n template<class T, class A>\n-batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::fmod<A>(x, y, A{});\n }\n \n@@ -662,7 +662,7 @@ batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the fused multiply-sub operation.\n  */\n template<class T, class A>\n-batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fms<A>(x, y, z, A{});\n }\n \n@@ -676,7 +676,7 @@ batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z\n  * @return the result of the fused negated multiply-add operation.\n  */\n template<class T, class A>\n-batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fnma<A>(x, y, z, A{});\n }\n \n@@ -690,7 +690,7 @@ batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&\n  * @return the result of the fused negated multiply-sub operation.\n  */\n template<class T, class A>\n-batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n+inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) {\n   return kernel::fnms<A>(x, y, z, A{});\n }\n \n@@ -703,7 +703,7 @@ batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const&\n  * @return the normalized fraction of x\n  */\n template <class T, class A>\n-batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n+inline batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n   return kernel::frexp<A>(x, y, A{});\n }\n \n@@ -717,7 +717,7 @@ batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n   return x >= y;\n }\n \n@@ -731,7 +731,7 @@ batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n   return x > y;\n }\n \n@@ -743,7 +743,7 @@ batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the result of the reduction.\n  */\n template<class T, class A>\n-T hadd(batch<T, A> const& x) {\n+inline T hadd(batch<T, A> const& x) {\n   return kernel::hadd<A>(x, A{});\n }\n \n@@ -757,7 +757,7 @@ T hadd(batch<T, A> const& x) {\n  * @return the result of the reduction.\n  */\n template<class T, class A>\n-batch<T, A> haddp(batch<T, A> const* row) {\n+inline batch<T, A> haddp(batch<T, A> const* row) {\n   return kernel::haddp<A>(row, A{});\n }\n \n@@ -771,7 +771,7 @@ batch<T, A> haddp(batch<T, A> const* row) {\n  * @return the evaluation ofpolynomial with coefficient \\c Coefs on point \\c x.\n  */\n template <class T, class A, uint64_t... Coefs>\n-batch<T, A> horner(const batch<T, A>& x) {\n+inline batch<T, A> horner(const batch<T, A>& x) {\n   return kernel::horner<T, A, Coefs...>(x);\n }\n \n@@ -785,7 +785,7 @@ batch<T, A> horner(const batch<T, A>& x) {\n  * @return the square root of the sum of the squares of \\c x and \\c y.\n  */\n template<class T, class A>\n-batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::hypot<A>(x, y, A{});\n }\n \n@@ -797,7 +797,7 @@ batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the argument of \\c z.\n  */\n template <class T, class A>\n-real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) {\n   return kernel::imag<A>(x, A{});\n }\n \n@@ -821,7 +821,7 @@ B infinity() {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_even(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_even(batch<T, A> const& x) {\n   return kernel::is_even<A>(x, A{});\n }\n \n@@ -833,7 +833,7 @@ batch_bool<T, A> is_even(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_flint(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_flint(batch<T, A> const& x) {\n   return kernel::is_flint<A>(x, A{});\n }\n \n@@ -845,7 +845,7 @@ batch_bool<T, A> is_flint(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> is_odd(batch<T, A> const& x) {\n+inline batch_bool<T, A> is_odd(batch<T, A> const& x) {\n   return kernel::is_odd<A>(x, A{});\n }\n \n@@ -858,7 +858,7 @@ batch_bool<T, A> is_odd(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> isinf(batch<T, A> const& x) {\n+inline batch_bool<T, A> isinf(batch<T, A> const& x) {\n   return kernel::isinf<A>(x, A{});\n }\n \n@@ -871,7 +871,7 @@ batch_bool<T, A> isinf(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-batch_bool<T, A> isfinite(batch<T, A> const& x) {\n+inline batch_bool<T, A> isfinite(batch<T, A> const& x) {\n   return kernel::isfinite<A>(x, A{});\n }\n \n@@ -883,7 +883,7 @@ batch_bool<T, A> isfinite(batch<T, A> const& x) {\n  * @return a batch of booleans.\n  */\n template<class T, class A>\n-typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n+inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n   return kernel::isnan<A>(x, A{});\n }\n \n@@ -896,7 +896,7 @@ typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) {\n  * @return the natural logarithm of the gamma function of \\c x.\n  */\n template <class T, class A>\n-batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n+inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n   return kernel::ldexp<A>(x, y, A{});\n }\n \n@@ -909,7 +909,7 @@ batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n   return x <= y;\n }\n \n@@ -921,7 +921,7 @@ batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the natural logarithm of the gamma function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> lgamma(batch<T, A> const& x) {\n+inline batch<T, A> lgamma(batch<T, A> const& x) {\n   return kernel::lgamma<A>(x, A{});\n }\n \n@@ -934,18 +934,18 @@ batch<T, A> lgamma(batch<T, A> const& x) {\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {\n+inline simd_return_type<From, To> load_as(From const* ptr, aligned_mode) {\n   using batch_value_type = typename simd_return_type<From, To>::value_type;\n   return kernel::load_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n }\n \n template <class To, class A = default_arch>\n-simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {\n+inline simd_return_type<bool, To> load_as(bool const* ptr, aligned_mode) {\n   return simd_return_type<bool, To>::load_aligned(ptr);\n }\n \n template <class To, class A=default_arch, class From>\n-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)\n+inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, aligned_mode)\n {\n   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;\n   return kernel::load_complex_aligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n@@ -960,18 +960,18 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,\n  * @return a new batch instance\n  */\n template <class To, class A=default_arch, class From>\n-simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {\n+inline simd_return_type<From, To> load_as(From const* ptr, unaligned_mode) {\n   using batch_value_type = typename simd_return_type<From, To>::value_type;\n   return kernel::load_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n }\n \n template <class To, class A = default_arch>\n-simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {\n+inline simd_return_type<bool, To> load_as(bool const* ptr, unaligned_mode) {\n   return simd_return_type<bool, To>::load_unaligned(ptr);\n }\n \n template <class To, class A=default_arch, class From>\n-simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)\n+inline simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr, unaligned_mode)\n {\n   using batch_value_type = typename simd_return_type<std::complex<From>, To>::value_type;\n   return kernel::load_complex_unaligned<A>(ptr, kernel::convert<batch_value_type>{}, A{});\n@@ -986,7 +986,7 @@ simd_return_type<std::complex<From>, To> load_as(std::complex<From> const* ptr,\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load(From const* ptr, aligned_mode= {}) {\n+inline batch<From, A> load(From const* ptr, aligned_mode= {}) {\n   return load_as<From, A>(ptr, aligned_mode{});\n }\n \n@@ -999,7 +999,7 @@ batch<From, A> load(From const* ptr, aligned_mode= {}) {\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load(From const* ptr, unaligned_mode) {\n+inline batch<From, A> load(From const* ptr, unaligned_mode) {\n   return load_as<From, A>(ptr, unaligned_mode{});\n }\n \n@@ -1012,7 +1012,7 @@ batch<From, A> load(From const* ptr, unaligned_mode) {\n  * @return a new batch instance\n  */\n template<class A=default_arch, class From>\n-batch<From, A> load_aligned(From const* ptr) {\n+inline batch<From, A> load_aligned(From const* ptr) {\n   return load_as<From, A>(ptr, aligned_mode{});\n }\n \n@@ -1025,7 +1025,7 @@ batch<From, A> load_aligned(From const* ptr) {\n  * @return a new batch instance\n  */\n template <class A=default_arch, class From>\n-batch<From, A> load_unaligned(From const* ptr) {\n+inline batch<From, A> load_unaligned(From const* ptr) {\n   return load_as<From, A>(ptr, unaligned_mode{});\n }\n \n@@ -1037,7 +1037,7 @@ batch<From, A> load_unaligned(From const* ptr) {\n  * @return the natural logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log(batch<T, A> const& x) {\n+inline batch<T, A> log(batch<T, A> const& x) {\n   return kernel::log<A>(x, A{});\n }\n \n@@ -1048,7 +1048,7 @@ batch<T, A> log(batch<T, A> const& x) {\n  * @return the base 2 logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log2(batch<T, A> const& x) {\n+inline batch<T, A> log2(batch<T, A> const& x) {\n   return kernel::log2<A>(x, A{});\n }\n \n@@ -1059,7 +1059,7 @@ batch<T, A> log2(batch<T, A> const& x) {\n  * @return the base 10 logarithm of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log10(batch<T, A> const& x) {\n+inline batch<T, A> log10(batch<T, A> const& x) {\n   return kernel::log10<A>(x, A{});\n }\n \n@@ -1070,7 +1070,7 @@ batch<T, A> log10(batch<T, A> const& x) {\n  * @return the natural logarithm of one plus \\c x.\n  */\n template<class T, class A>\n-batch<T, A> log1p(batch<T, A> const& x) {\n+inline batch<T, A> log1p(batch<T, A> const& x) {\n   return kernel::log1p<A>(x, A{});\n }\n \n@@ -1083,7 +1083,7 @@ batch<T, A> log1p(batch<T, A> const& x) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n   return x < y;\n }\n \n@@ -1096,7 +1096,7 @@ batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the larger values.\n  */\n template<class T, class A>\n-batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::max<A>(x, y, A{});\n }\n \n@@ -1109,7 +1109,7 @@ batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the smaller values.\n  */\n template<class T, class A>\n-batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::min<A>(x, y, A{});\n }\n \n@@ -1120,7 +1120,7 @@ batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of positive infinity\n  */\n template<class B>\n-B minusinfinity() {\n+inline B minusinfinity() {\n   using T = typename B::value_type;\n   return B(-std::numeric_limits<T>::infinity());\n }\n@@ -1134,7 +1134,7 @@ B minusinfinity() {\n  * @return the result of the modulo.\n  */\n template<class T, class Tp>\n-auto mod(T const& x, Tp const& y) -> decltype(x % y){\n+inline auto mod(T const& x, Tp const& y) -> decltype(x % y){\n   return x % y;\n }\n \n@@ -1148,7 +1148,7 @@ auto mod(T const& x, Tp const& y) -> decltype(x % y){\n  * @return the result of the product.\n  */\n template<class T, class Tp>\n-auto mul(T const& x, Tp const& y) -> decltype(x * y){\n+inline auto mul(T const& x, Tp const& y) -> decltype(x * y){\n   return x * y;\n }\n \n@@ -1161,7 +1161,7 @@ auto mul(T const& x, Tp const& y) -> decltype(x * y){\n  * @return the batch of nearest integer values.\n  */\n template<class T, class A>\n-batch<T, A> nearbyint(batch<T, A> const& x) {\n+inline batch<T, A> nearbyint(batch<T, A> const& x) {\n   return kernel::nearbyint<A>(x, A{});\n }\n \n@@ -1174,7 +1174,7 @@ batch<T, A> nearbyint(batch<T, A> const& x) {\n  * @return a boolean batch.\n  */\n template<class T, class A>\n-batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n   return x != y;\n }\n \n@@ -1187,7 +1187,7 @@ batch_bool<T, A> neq(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the opposite of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> neg(batch<T, A> const& x) {\n+inline batch<T, A> neg(batch<T, A> const& x) {\n   return -x;\n }\n \n@@ -1201,7 +1201,7 @@ batch<T, A> neg(batch<T, A> const& x) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class A>\n-batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::nextafter<A>(x, y, A{});\n }\n \n@@ -1213,7 +1213,7 @@ batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the norm of \\c x.\n  */\n template<class A, class T>\n-real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n   return kernel::norm(x, A{});\n }\n \n@@ -1225,7 +1225,7 @@ real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) {\n  * @return \\c x.\n  */\n template<class T, class A>\n-batch<T, A> pos(batch<T, A> const& x) {\n+inline batch<T, A> pos(batch<T, A> const& x) {\n   return +x;\n }\n \n@@ -1239,7 +1239,7 @@ batch<T, A> pos(batch<T, A> const& x) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class A>\n-batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::pow<A>(x, y, A{});\n }\n \n@@ -1253,7 +1253,7 @@ batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return \\c x raised to the power \\c y.\n  */\n template<class T, class ITy, class A, class=typename std::enable_if<std::is_integral<ITy>::value, void>::type>\n-batch<T, A> pow(batch<T, A> const& x, ITy y) {\n+inline batch<T, A> pow(batch<T, A> const& x, ITy y) {\n   return kernel::ipow<A>(x, y, A{});\n }\n \n@@ -1265,7 +1265,7 @@ batch<T, A> pow(batch<T, A> const& x, ITy y) {\n  * @return the projection of \\c x.\n  */\n template<class A, class T>\n-complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n+inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n   return kernel::proj(x, A{});\n }\n \n@@ -1277,7 +1277,7 @@ complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& x) {\n  * @return the argument of \\c z.\n  */\n template <class T, class A>\n-real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n+inline real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n   return kernel::real<A>(x, A{});\n }\n \n@@ -1290,7 +1290,7 @@ real_batch_type_t<batch<T, A>> real(batch<T, A> const& x) {\n  * @return the result of the addition.\n  */\n template<class T, class A>\n-batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::remainder<A>(x, y, A{});\n }\n \n@@ -1303,7 +1303,7 @@ batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return the batch of rounded values.\n  */\n template<class T, class A>\n-batch<T, A> rint(batch<T, A> const& x) {\n+inline batch<T, A> rint(batch<T, A> const& x) {\n   return nearbyint(x);\n }\n \n@@ -1314,10 +1314,10 @@ batch<T, A> rint(batch<T, A> const& x) {\n  * floating point format), rounding halfway cases away from zero, regardless\n  * of the current rounding mode.\n  * @param x batch of flaoting point values.\n- * @return the batch of nearest integer values. \n+ * @return the batch of nearest integer values.\n  */\n template<class T, class A>\n-batch<T, A> round(batch<T, A> const& x) {\n+inline batch<T, A> round(batch<T, A> const& x) {\n   return kernel::round<A>(x, A{});\n }\n \n@@ -1332,7 +1332,7 @@ batch<T, A> round(batch<T, A> const& x) {\n  * @return the result of the saturated addition.\n  */\n template<class T, class Tp>\n-auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n+inline auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n   using B = decltype(x + y);\n   using A = typename B::arch_type;\n   return kernel::sadd<A>(B(x), B(y), A{});\n@@ -1353,7 +1353,7 @@ auto sadd(T const& x, Tp const& y) -> decltype(x + y) {\n  * @return the result of the selection.\n  */\n template<class T, class A>\n-batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n+inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1372,7 +1372,7 @@ batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, bat\n  * @return the result of the selection.\n  */\n template<class T, class A>\n-batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {\n+inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1391,7 +1391,7 @@ batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::comple\n  * @return the result of the selection.\n  */\n template<class T, class A, bool... Values>\n-batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n+inline batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) {\n   return kernel::select<A>(cond, true_br, false_br, A{});\n }\n \n@@ -1403,7 +1403,7 @@ batch<T, A> select(batch_bool_constant<batch<T, A>, Values...> const& cond, batc\n  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n  */\n template<class T, class A>\n-batch<T, A> sign(batch<T, A> const& x) {\n+inline batch<T, A> sign(batch<T, A> const& x) {\n   return kernel::sign<A>(x, A{});\n }\n \n@@ -1415,7 +1415,7 @@ batch<T, A> sign(batch<T, A> const& x) {\n  * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n  */\n template<class T, class A>\n-batch<T, A> signnz(batch<T, A> const& x) {\n+inline batch<T, A> signnz(batch<T, A> const& x) {\n   return kernel::signnz<A>(x, A{});\n }\n \n@@ -1427,7 +1427,7 @@ batch<T, A> signnz(batch<T, A> const& x) {\n  * @return the sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sin(batch<T, A> const& x) {\n+inline batch<T, A> sin(batch<T, A> const& x) {\n   return kernel::sin<A>(x, A{});\n }\n \n@@ -1439,7 +1439,7 @@ batch<T, A> sin(batch<T, A> const& x) {\n  * @return the hyperbolic sine of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sinh(batch<T, A> const& x) {\n+inline batch<T, A> sinh(batch<T, A> const& x) {\n   return kernel::sinh<A>(x, A{});\n }\n \n@@ -1452,7 +1452,7 @@ batch<T, A> sinh(batch<T, A> const& x) {\n  * @return a pair containing the sine then the cosine of  batch \\c x\n  */\n template<class T, class A>\n-std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n+inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n   return kernel::sincos<A>(x, A{});\n }\n \n@@ -1464,7 +1464,7 @@ std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) {\n  * @return the square root of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> sqrt(batch<T, A> const& x) {\n+inline batch<T, A> sqrt(batch<T, A> const& x) {\n   return kernel::sqrt<A>(x, A{});\n }\n \n@@ -1479,7 +1479,7 @@ batch<T, A> sqrt(batch<T, A> const& x) {\n  * @return the result of the saturated difference.\n  */\n template<class T, class Tp>\n-auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n+inline auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n   using B = decltype(x + y);\n   using A = typename B::arch_type;\n   return kernel::ssub<A>(B(x), B(y), A{});\n@@ -1494,17 +1494,17 @@ auto ssub(T const& x, Tp const& y) -> decltype(x - y) {\n  * @param val the batch to copy\n  */\n template <class To, class A=default_arch, class From>\n-void store_as(To* dst, batch<From, A> const& src, aligned_mode) {\n+inline void store_as(To* dst, batch<From, A> const& src, aligned_mode) {\n   kernel::store_aligned(dst, src, A{});\n }\n \n template <class A=default_arch, class From>\n-void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {\n+inline void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) {\n   kernel::store(src, dst, A{});\n }\n \n template <class To, class A=default_arch, class From>\n-void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {\n+inline void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, aligned_mode) {\n   kernel::store_complex_aligned(dst, src, A{});\n }\n \n@@ -1517,17 +1517,17 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>,A> const& src, ali\n  * @param val the batch to copy\n  */\n template <class To, class A=default_arch, class From>\n-void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {\n+inline void store_as(To* dst, batch<From, A> const& src, unaligned_mode) {\n   kernel::store_unaligned(dst, src, A{});\n }\n \n template <class A=default_arch, class From>\n-void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {\n+inline void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) {\n   kernel::store(src, dst, A{});\n }\n \n template <class To, class A=default_arch, class From>\n-void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {\n+inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) {\n   kernel::store_complex_unaligned(dst, src, A{});\n }\n \n@@ -1540,7 +1540,7 @@ void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, un\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n+inline void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n   store_as<T, A>(mem, val, aligned_mode{});\n }\n \n@@ -1553,7 +1553,7 @@ void store(T* mem, batch<T, A> const& val, aligned_mode={}) {\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n+inline void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n   store_as<T, A>(mem, val, unaligned_mode{});\n }\n \n@@ -1566,7 +1566,7 @@ void store(T* mem, batch<T, A> const& val, unaligned_mode) {\n  * @param val the batch to copy from\n  */\n template<class A, class T>\n-void store_aligned(T* mem, batch<T, A> const& val) {\n+inline void store_aligned(T* mem, batch<T, A> const& val) {\n   store_as<T, A>(mem, val, aligned_mode{});\n }\n \n@@ -1579,7 +1579,7 @@ void store_aligned(T* mem, batch<T, A> const& val) {\n  * @param val the batch to copy\n  */\n template<class A, class T>\n-void store_unaligned(T* mem, batch<T, A> const& val) {\n+inline void store_unaligned(T* mem, batch<T, A> const& val) {\n   store_as<T, A>(mem, val, unaligned_mode{});\n }\n \n@@ -1593,7 +1593,7 @@ void store_unaligned(T* mem, batch<T, A> const& val) {\n  * @return the difference between \\c x and \\c y\n  */\n template<class T, class Tp>\n-auto sub(T const& x, Tp const& y) -> decltype(x - y){\n+inline auto sub(T const& x, Tp const& y) -> decltype(x - y){\n   return x - y;\n }\n \n@@ -1605,7 +1605,7 @@ auto sub(T const& x, Tp const& y) -> decltype(x - y){\n  * @return the tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tan(batch<T, A> const& x) {\n+inline batch<T, A> tan(batch<T, A> const& x) {\n   return kernel::tan<A>(x, A{});\n }\n \n@@ -1617,7 +1617,7 @@ batch<T, A> tan(batch<T, A> const& x) {\n  * @return the hyperbolic tangent of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tanh(batch<T, A> const& x) {\n+inline batch<T, A> tanh(batch<T, A> const& x) {\n   return kernel::tanh<A>(x, A{});\n }\n \n@@ -1629,7 +1629,7 @@ batch<T, A> tanh(batch<T, A> const& x) {\n  * @return the gamma function of \\c x.\n  */\n template<class T, class A>\n-batch<T, A> tgamma(batch<T, A> const& x) {\n+inline batch<T, A> tgamma(batch<T, A> const& x) {\n   return kernel::tgamma<A>(x, A{});\n }\n \n@@ -1641,7 +1641,7 @@ batch<T, A> tgamma(batch<T, A> const& x) {\n  * @return \\c i converted to a value of an floating point type of the same size as \\c T\n  */\n template<class T, class A>\n-batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n+inline batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n   return kernel::to_float<A>(i, A{});\n }\n \n@@ -1653,7 +1653,7 @@ batch<as_float_t<T>, A> to_float(batch<T, A> const& i) {\n  * @return \\c x converted to a value of an integer type of the same size as \\c T\n  */\n template<class T, class A>\n-batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n+inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n   return kernel::to_int<A>(x, A{});\n }\n \n@@ -1666,7 +1666,7 @@ batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) {\n  * @return the batch of nearest integer values not greater in magnitude than \\c x.\n  */\n template<class T, class A>\n-batch<T, A> trunc(batch<T, A> const& x) {\n+inline batch<T, A> trunc(batch<T, A> const& x) {\n   return kernel::trunc<A>(x, A{});\n }\n \n@@ -1680,7 +1680,7 @@ batch<T, A> trunc(batch<T, A> const& x) {\n  * @return a batch of the high part of shuffled values.\n  */\n template<class T, class A>\n-batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::zip_hi<A>(x, y, A{});\n }\n \n@@ -1694,26 +1694,26 @@ batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) {\n  * @return a batch of the low part of shuffled values.\n  */\n template<class T, class A>\n-batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {\n+inline batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) {\n   return kernel::zip_lo<A>(x, y, A{});\n }\n \n // bitwise_cast\n template <class A, class T, typename std::enable_if<std::is_integral<T>::value, int>::type = 3>\n-batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n+inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n {\n   T z(0);\n   return select(self, batch<T, A>(T(~z)), batch<T, A>(z));\n }\n-    \n+\n template <class A, class T, typename std::enable_if<std::is_floating_point<T>::value, int>::type = 3>\n-batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n+inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n {\n     T z0(0), z1(0);\n     using int_type = as_unsigned_integer_t<T>;\n     int_type value(~int_type(0));\n     std::memcpy(&z1, &value, sizeof(int_type));\n-    return select(self, batch<T, A>(z1), batch<T, A>(z0)); \n+    return select(self, batch<T, A>(z1), batch<T, A>(z0));\n }\n \n /**\n@@ -1725,7 +1725,7 @@ batch<T, A> bitwise_cast(batch_bool<T, A> const& self)\n  * @return a boolean scalar.\n  */\n template<class T, class A>\n-bool all(batch_bool<T, A> const& x) {\n+inline bool all(batch_bool<T, A> const& x) {\n   return kernel::all<A>(x, A{});\n }\n \n@@ -1738,7 +1738,7 @@ bool all(batch_bool<T, A> const& x) {\n  * @return a boolean scalar.\n  */\n template<class T, class A>\n-bool any(batch_bool<T, A> const& x) {\n+inline bool any(batch_bool<T, A> const& x) {\n   return kernel::any<A>(x, A{});\n }\n \n@@ -1751,7 +1751,7 @@ bool any(batch_bool<T, A> const& x) {\n  * @return a reference to \\c o\n  */\n template<class T, class A>\n-std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n+inline std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n   constexpr auto size = batch<T, A>::size;\n   alignas(A::alignment()) T buffer[size];\n   x.store_aligned(&buffer[0]);\n@@ -1763,4 +1763,3 @@ std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) {\n }\n \n #endif\n-"
      },
      {
        "filename": "include/xsimd/types/xsimd_batch.hpp",
        "status": "modified",
        "additions": 140,
        "deletions": 143,
        "changes": 283,
        "patch": "@@ -181,7 +181,7 @@ namespace xsimd\n             return batch(self).logical_or(other);\n         }\n     private:\n-  \n+\n         template<size_t... Is>\n         batch(T const* data, detail::index_sequence<Is...>);\n \n@@ -358,7 +358,7 @@ namespace xsimd\n         {\n             return batch(self) -= other;\n         }\n-        \n+\n         friend batch operator*(batch const& self, batch const& other)\n         {\n             return batch(self) *= other;\n@@ -430,40 +430,40 @@ namespace xsimd\n      **********************/\n \n     template<class T, class A>\n-    batch<T, A>::batch(T val)\n+    inline batch<T, A>::batch(T val)\n         : types::simd_register<T, A>(kernel::broadcast<A>(val, A{}))\n     {\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(std::initializer_list<T> data)\n+    inline batch<T, A>::batch(std::initializer_list<T> data)\n         : batch(data.begin(), detail::make_index_sequence<size>())\n     {\n         assert(data.size() == size && \"consistent initialization\");\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(batch_bool<T, A> const &b)\n+    inline batch<T, A>::batch(batch_bool<T, A> const &b)\n         : batch(kernel::from_bool(b, A{}))\n     {\n     }\n \n     template<class T, class A>\n-    batch<T, A>::batch(register_type reg)\n+    inline batch<T, A>::batch(register_type reg)\n         : types::simd_register<T, A>({reg})\n     {\n     }\n \n     template<class T, class A>\n     template<size_t... Is>\n-    batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)\n+    inline batch<T, A>::batch(T const*data, detail::index_sequence<Is...>)\n         : batch(kernel::set<A>(batch{}, A{}, data[Is]...))\n     {\n     }\n \n     template <class T, class A>\n     template<class U>\n-    XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)\n+    inline XSIMD_NO_DISCARD batch<T, A> batch<T, A>::broadcast(U val)\n     {\n         return batch(static_cast<T>(val));\n     }\n@@ -479,7 +479,7 @@ namespace xsimd\n     */\n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store_aligned(U* mem) const\n+    inline void batch<T, A>::store_aligned(U* mem) const\n     {\n         kernel::store_aligned<A>(mem, *this, A{});\n     }\n@@ -491,21 +491,21 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store_unaligned(U* mem) const\n+    inline void batch<T, A>::store_unaligned(U* mem) const\n     {\n         kernel::store_unaligned<A>(mem, *this, A{});\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store(U * mem, aligned_mode) const\n+    inline void batch<T, A>::store(U * mem, aligned_mode) const\n     {\n         return store_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<T, A>::store(U * mem, unaligned_mode) const\n+    inline void batch<T, A>::store(U * mem, unaligned_mode) const\n     {\n         return store_unaligned(mem);\n     }\n@@ -519,7 +519,7 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load_aligned(U const* mem)\n+    inline batch<T, A> batch<T, A>::load_aligned(U const* mem)\n     {\n         return kernel::load_aligned<A>(mem, kernel::convert<T>{}, A{});\n     }\n@@ -533,27 +533,27 @@ namespace xsimd\n      */\n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load_unaligned(U const* mem)\n+    inline batch<T, A> batch<T, A>::load_unaligned(U const* mem)\n     {\n         return kernel::load_unaligned<A>(mem, kernel::convert<T>{}, A{});\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)\n+    inline batch<T, A> batch<T, A>::load(U const* mem, aligned_mode)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)\n+    inline batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode)\n     {\n         return load_unaligned(mem);\n     }\n \n     template <class T, class A>\n-    T batch<T, A>::get(std::size_t i) const\n+    inline T batch<T, A>::get(std::size_t i) const\n     {\n         alignas(A::alignment()) T buffer[size];\n         store_aligned(&buffer[0]);\n@@ -565,114 +565,114 @@ namespace xsimd\n      ******************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator==(batch<T, A> const& other) const\n     {\n         return kernel::eq<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator!=(batch<T, A> const& other) const\n     {\n         return kernel::neq<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const\n+    inline batch_bool<T, A> batch<T, A>::operator>=(batch<T, A> const& other) const\n     {\n         return kernel::ge<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator<=(batch<T, A> const& other) const\n+    {\n         return kernel::le<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator>(batch<T, A> const& other) const\n+    {\n         return kernel::gt<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const\n-    { \n+    inline batch_bool<T, A> batch<T, A>::operator<(batch<T, A> const& other) const\n+    {\n         return kernel::lt<A>(*this, other, A{});\n     }\n-    \n+\n     /**************************\n      * batch update operators *\n      **************************/\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other)\n     {\n         return *this = kernel::add<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other)\n     {\n         return *this = kernel::sub<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other)\n     {\n         return *this = kernel::mul<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other)\n     {\n         return *this = kernel::div<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator%=(batch<T, A> const& other)\n     {\n         return *this = kernel::mod<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_and<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_or<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_xor<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)\n+    inline batch<T, A>& batch<T, A>::operator>>=(batch<T, A> const& other)\n     {\n         return *this = kernel::bitwise_rshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator<<=(batch<T, A> const& other)\n+    {\n         return *this = kernel::bitwise_lshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator>>=(int32_t other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator>>=(int32_t other)\n+    {\n         return *this = kernel::bitwise_rshift<A>(*this, other, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator<<=(int32_t other)\n-    { \n+    inline batch<T, A>& batch<T, A>::operator<<=(int32_t other)\n+    {\n         return *this = kernel::bitwise_lshift<A>(*this, other, A{});\n     }\n \n@@ -681,28 +681,28 @@ namespace xsimd\n      *****************************/\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator++()\n-    { \n+    inline batch<T, A>& batch<T, A>::operator++()\n+    {\n         return operator+=(1);\n     }\n \n     template<class T, class A>\n-    batch<T, A>& batch<T, A>::operator--()\n-    { \n+    inline batch<T, A>& batch<T, A>::operator--()\n+    {\n         return operator-=(1);\n     }\n-    \n+\n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator++(int)\n+    inline batch<T, A> batch<T, A>::operator++(int)\n     {\n         batch<T, A> copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator--(int)\n-    { \n+    inline batch<T, A> batch<T, A>::operator--(int)\n+    {\n         batch copy(*this);\n         operator-=(1);\n         return copy;\n@@ -713,25 +713,25 @@ namespace xsimd\n      *************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch<T, A>::operator!() const\n+    inline batch_bool<T, A> batch<T, A>::operator!() const\n     {\n         return kernel::eq<A>(*this, batch(0), A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator~() const\n+    inline batch<T, A> batch<T, A>::operator~() const\n     {\n         return kernel::bitwise_not<A>(*this, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator-() const\n-    { \n+    inline batch<T, A> batch<T, A>::operator-() const\n+    {\n         return kernel::neg<A>(*this, A{});\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::operator+() const\n+    inline batch<T, A> batch<T, A>::operator+() const\n     {\n         return *this;\n     }\n@@ -741,13 +741,13 @@ namespace xsimd\n      ************************/\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const\n+    inline batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const\n     {\n         return kernel::logical_and<A>(*this, other, A());\n     }\n \n     template<class T, class A>\n-    batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const\n+    inline batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const\n     {\n         return kernel::logical_or<A>(*this, other, A());\n     }\n@@ -758,42 +758,41 @@ namespace xsimd\n \n     template<class T, class A>\n     template<size_t... Is>\n-    batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)\n+    inline batch_bool<T, A>::batch_bool(bool const*data, detail::index_sequence<Is...>)\n         : batch_bool(kernel::set<A>(batch_bool{}, A{}, data[Is]...))\n     {\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(register_type reg)\n+    inline batch_bool<T, A>::batch_bool(register_type reg)\n         : types::get_bool_simd_register_t<T, A>({reg})\n     {\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)\n+    inline batch_bool<T, A>::batch_bool(std::initializer_list<bool> data)\n         : batch_bool(data.begin(), detail::make_index_sequence<size>())\n     {\n-        assert(data.size() == size && \"consistent initialization\");\n     }\n \n     /*******************************\n      * batch_bool memory operators *\n      *******************************/\n \n     template<class T, class A>\n-    void batch_bool<T, A>::store_aligned(bool* mem) const\n+    inline void batch_bool<T, A>::store_aligned(bool* mem) const\n     {\n         kernel::store(*this, mem, A{});\n     }\n \n     template<class T, class A>\n-    void batch_bool<T, A>::store_unaligned(bool* mem) const\n+    inline void batch_bool<T, A>::store_unaligned(bool* mem) const\n     {\n         store_aligned(mem);\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)\n+    inline batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem)\n     {\n         batch_type ref(0);\n         alignas(A::alignment()) T buffer[size];\n@@ -803,13 +802,13 @@ namespace xsimd\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)\n+    inline batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n-    bool batch_bool<T, A>::get(std::size_t i) const\n+    inline bool batch_bool<T, A>::get(std::size_t i) const\n     {\n         alignas(A::alignment()) bool buffer[size];\n         store_aligned(&buffer[0]);\n@@ -821,13 +820,13 @@ namespace xsimd\n      ***********************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const\n     {\n         return kernel::eq<A>(*this, other, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const\n     {\n         return kernel::neq<A>(*this, other, A{}).data;\n     }\n@@ -837,37 +836,37 @@ namespace xsimd\n      ********************************/\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator~() const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator~() const\n     {\n         return kernel::bitwise_not<A>(*this, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator!() const\n-    { \n+    inline batch_bool<T, A> batch_bool<T, A>::operator!() const\n+    {\n         return operator==(batch_bool(false));\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const\n     {\n         return kernel::bitwise_and<A>(*this, other, A{}).data;\n     }\n \n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const\n     {\n         return kernel::bitwise_or<A>(*this, other, A{}).data;\n     }\n-        \n+\n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const\n     {\n         return operator&(other);\n     }\n-    \n+\n     template<class T, class A>\n-    batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const\n+    inline batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const\n     {\n         return operator|(other);\n     }\n@@ -877,21 +876,21 @@ namespace xsimd\n      ******************************/\n \n     template<class T, class A>\n-    batch_bool<T, A>::batch_bool(bool val)\n+    inline batch_bool<T, A>::batch_bool(bool val)\n         : base_type{make_register(detail::make_index_sequence<size-1>(), val)}\n     {\n     }\n \n     template <class T, class A>\n     template <class U, class... V, size_t I, size_t... Is>\n-    auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type\n+    inline auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) -> register_type\n     {\n         return make_register(detail::index_sequence<Is...>(), u, u, v...);\n     }\n \n     template <class T, class A>\n     template <class... V>\n-    auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type\n+    inline auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) -> register_type\n     {\n         return kernel::set<A>(batch_bool<T, A>(), A{}, v...).data;\n     }\n@@ -901,38 +900,37 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(value_type const& val)\n+    inline batch<std::complex<T>, A>::batch(value_type const& val)\n         : m_real(val.real()), m_imag(val.imag())\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)\n+    inline batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag)\n         : m_real(real), m_imag(imag)\n     {\n     }\n-        \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(real_batch const& real)\n+    inline batch<std::complex<T>, A>::batch(real_batch const& real)\n         : m_real(real), m_imag(0)\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(T val)\n+    inline batch<std::complex<T>, A>::batch(T val)\n         : m_real(val), m_imag(0)\n     {\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)\n-    { \n-        assert(data.size() == size && \"consistent initialization\");\n+    inline batch<std::complex<T>, A>::batch(std::initializer_list<value_type> data)\n+    {\n         *this = load_unaligned(data.begin());\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>::batch(batch_bool_type const& b)\n+    inline batch<std::complex<T>, A>::batch(batch_bool_type const& b)\n         : m_real(b), m_imag(0)\n     {\n     }\n@@ -942,96 +940,96 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src)\n     {\n         return {batch<T, A>::load_aligned(real_src), imag_src ? batch<T, A>::load_aligned(imag_src) : batch<T, A>(0)};\n     }\n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src)\n     {\n         return {batch<T, A>::load_unaligned(real_src), imag_src?batch<T, A>::load_unaligned(imag_src):batch<T, A>(0)};\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src)\n     {\n         return kernel::load_complex_aligned<A>(src, kernel::convert<value_type>{}, A{});\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src)\n     {\n         return kernel::load_complex_unaligned<A>(src, kernel::convert<value_type>{}, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_aligned(value_type* dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(value_type* dst) const\n     {\n         return kernel::store_complex_aligned(dst, *this, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const\n     {\n         return kernel::store_complex_unaligned(dst, *this, A{});\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const\n     {\n         m_real.store_aligned(real_dst);\n         m_imag.store_aligned(imag_dst);\n     }\n \n     template<class T, class A>\n-    void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const\n     {\n         m_real.store_unaligned(real_dst);\n         m_imag.store_unaligned(imag_dst);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode)\n     {\n         return load_aligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode)\n+    {\n         return load_unaligned(mem);\n     }\n \n     template<class T, class A>\n     template<class U>\n-    void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const\n+    inline void batch<std::complex<T>, A>::store(U * mem, aligned_mode) const\n     {\n         return store_aligned(mem);\n     }\n-    \n+\n     template<class T, class A>\n     template<class U>\n-    void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const\n-    { \n+    inline void batch<std::complex<T>, A>::store(U * mem, unaligned_mode) const\n+    {\n         return store_unaligned(mem);\n     }\n \n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::real() const -> real_batch\n+    inline auto batch<std::complex<T>, A>::real() const -> real_batch\n     {\n-        return m_real; \n+        return m_real;\n     }\n-    \n+\n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::imag() const -> real_batch \n+    inline auto batch<std::complex<T>, A>::imag() const -> real_batch\n     {\n         return m_imag;\n     }\n \n     template<class T, class A>\n-    auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type\n+    inline auto batch<std::complex<T>, A>::get(std::size_t i) const -> value_type\n     {\n         alignas(A::alignment()) value_type buffer[size];\n         store_aligned(&buffer[0]);\n@@ -1046,16 +1044,15 @@ namespace xsimd\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)\n+    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val)\n         : m_real(val.real()), m_imag(val.imag())\n     {\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)\n+    inline batch<std::complex<T>, A>::batch(std::initializer_list<xtl::xcomplex<T, T, i3ec>> data)\n     {\n-        assert(data.size() == size && \"consistent initialization\");\n         *this = load_unaligned(data.begin());\n     }\n \n@@ -1065,28 +1062,28 @@ namespace xsimd\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src)\n     {\n         return load_aligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src)\n     {\n         return load_unaligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const\n+    inline void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const\n     {\n         store_aligned(reinterpret_cast<std::complex<T> *>(dst));\n     }\n \n     template<class T, class A>\n     template<bool i3ec>\n-    void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const\n+    inline void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const\n     {\n         store_unaligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n@@ -1098,14 +1095,14 @@ namespace xsimd\n      ***************************************/\n \n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const\n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const\n     {\n         return m_real == other.m_real && m_imag == other.m_imag;\n     }\n-    \n+\n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const\n-    { \n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const\n+    {\n         return m_real != other.m_real || m_imag != other.m_imag;\n     }\n \n@@ -1114,23 +1111,23 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other)\n     {\n         m_real += other.m_real;\n         m_imag += other.m_imag;\n         return *this;\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other)\n     {\n         m_real -= other.m_real;\n         m_imag -= other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other)\n     {\n         real_batch new_real = real() * other.real() - imag() * other.imag();\n         real_batch new_imag = real() * other.imag() + imag() * other.real();\n@@ -1140,7 +1137,7 @@ namespace xsimd\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)\n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other)\n     {\n         real_batch a = real();\n         real_batch b = imag();\n@@ -1157,27 +1154,27 @@ namespace xsimd\n      **************************************/\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()\n-    { \n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++()\n+    {\n         return operator+=(1);\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()\n-    { \n+    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--()\n+    {\n         return operator-=(1);\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int)\n     {\n         batch copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template<class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)\n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int)\n     {\n         batch copy(*this);\n         operator-=(1);\n@@ -1189,26 +1186,26 @@ namespace xsimd\n      **********************************/\n \n     template <class T, class A>\n-    batch_bool<T, A> batch<std::complex<T>, A>::operator!() const\n-    { \n+    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!() const\n+    {\n         return operator==(batch(0));\n     }\n-    \n+\n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const\n+    {\n         return {~m_real, ~m_imag};\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const\n+    {\n         return {-m_real, -m_imag};\n     }\n \n     template <class T, class A>\n-    batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const\n-    { \n+    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const\n+    {\n         return {+m_real, +m_imag};\n     }\n }"
      },
      {
        "filename": "include/xsimd/types/xsimd_batch_constant.hpp",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -65,13 +65,13 @@ namespace xsimd\n     namespace detail\n     {\n         template <class batch_type, class G, std::size_t... Is>\n-        constexpr auto make_batch_constant(detail::index_sequence<Is...>)\n+        inline constexpr auto make_batch_constant(detail::index_sequence<Is...>)\n             -> batch_constant<batch_type, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n         }\n         template <class batch_type, class G, std::size_t... Is>\n-        constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)\n+        inline constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>)\n             -> batch_bool_constant<batch_type, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n@@ -80,14 +80,14 @@ namespace xsimd\n     } // namespace detail\n \n     template <class batch_type, class G>\n-    constexpr auto make_batch_constant() -> decltype(\n+    inline constexpr auto make_batch_constant() -> decltype(\n         detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>()))\n     {\n         return detail::make_batch_constant<batch_type, G>(detail::make_index_sequence<batch_type::size>());\n     }\n \n     template <class batch_type, class G>\n-    constexpr auto make_batch_bool_constant()\n+    inline constexpr auto make_batch_bool_constant()\n         -> decltype(detail::make_batch_bool_constant<batch_type, G>(\n             detail::make_index_sequence<batch_type::size>()))\n     {"
      },
      {
        "filename": "include/xsimd/types/xsimd_utils.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -178,7 +178,7 @@ namespace xsimd\n      ********************/\n \n     template<class To, class From>\n-    To bit_cast(From val) {\n+    inline To bit_cast(From val) {\n       static_assert(sizeof(From) == sizeof(To), \"casting between compatible layout\");\n       // FIXME: Some old version of GCC don't support that trait\n       //static_assert(std::is_trivially_copyable<From>::value, \"input type is trivially copyable\");\n@@ -269,20 +269,20 @@ namespace xsimd\n     namespace detail\n     {\n         template <class T, class... Types, size_t I, size_t... Is>\n-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)\n+        inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, T>, index_sequence<I, Is...>)\n         {\n             return std::get<I>(t);\n         }\n \n         template <class T, class U, class... Types, size_t I, size_t... Is>\n-        const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)\n+        inline const T& get_impl(const std::tuple<Types...>& t, std::is_same<T, U>, index_sequence<I, Is...>)\n         {\n             using tuple_elem = typename std::tuple_element<I+1, std::tuple<Types...>>::type;\n             return get_impl<T>(t, std::is_same<T, tuple_elem>(), index_sequence<Is...>());\n         }\n \n         template <class T, class... Types>\n-        const T& get(const std::tuple<Types...>& t)\n+        inline const T& get(const std::tuple<Types...>& t)\n         {\n             using tuple_elem = typename std::tuple_element<0, std::tuple<Types...>>::type;\n             return get_impl<T>(t, std::is_same<T, tuple_elem>(), make_index_sequence<sizeof...(Types)>());\n@@ -329,7 +329,7 @@ namespace xsimd\n     {\n         // std::array constructor from scalar value (\"broadcast\")\n         template <typename T, std::size_t... Is>\n-        constexpr std::array<T, sizeof...(Is)>\n+        inline constexpr std::array<T, sizeof...(Is)>\n         array_from_scalar_impl(const T& scalar, index_sequence<Is...>)\n         {\n             // You can safely ignore this silly ternary, the \"scalar\" is all\n@@ -338,22 +338,22 @@ namespace xsimd\n         }\n \n         template <typename T, std::size_t N>\n-        constexpr std::array<T, N>\n+        inline constexpr std::array<T, N>\n         array_from_scalar(const T& scalar)\n         {\n             return array_from_scalar_impl(scalar, make_index_sequence<N>());\n         }\n \n         // std::array constructor from C-style pointer (handled as an array)\n         template <typename T, std::size_t... Is>\n-        constexpr std::array<T, sizeof...(Is)>\n+        inline constexpr std::array<T, sizeof...(Is)>\n         array_from_pointer_impl(const T* c_array, index_sequence<Is...>)\n         {\n             return std::array<T, sizeof...(Is)>{ c_array[Is]... };\n         }\n \n         template <typename T, std::size_t N>\n-        constexpr std::array<T, N>\n+        inline constexpr std::array<T, N>\n         array_from_pointer(const T* c_array)\n         {\n             return array_from_pointer_impl(c_array, make_index_sequence<N>());"
      }
    ],
    "lines_added": 2062,
    "lines_removed": 1636
  },
  "issues": [],
  "pull_requests": [],
  "build_info": {
    "old_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
    "new_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
    "old_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/old/build -- -j 1",
    "new_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/old/build -- -j 1",
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": false,
    "p_value": 1.0,
    "is_pair_significant": false,
    "pair_p_value": 1.0,
    "is_binom_significant": false,
    "binom_p_value": 1.0,
    "is_wilcoxon_significant": false,
    "wilcoxon_p_value": 1.0,
    "is_mannwhitney_significant": false,
    "mannwhitney_p_value": 0.45585448765971415,
    "relative_improvement": 8.752860085663737e-05,
    "absolute_improvement_ms": 0.5701208331929664,
    "old_mean_ms": 6513.537605005938,
    "new_mean_ms": 6512.967484172745,
    "old_std_ms": 42.96771933514864,
    "new_std_ms": 42.08201264459209,
    "effect_size_cohens_d": 0.013406036852133613,
    "old_ci95_ms": [
      6497.4931949236925,
      6529.582015088182
    ],
    "new_ci95_ms": [
      6497.253802404121,
      6528.681165941367
    ],
    "old_ci99_ms": [
      6491.914318861263,
      6535.160891150611
    ],
    "new_ci99_ms": [
      6491.789925413803,
      6534.145042931686
    ],
    "new_times_s": [
      6.462606243003393,
      6.614273203012999,
      6.4779844289587345,
      6.447536680061603,
      6.508723226026632,
      6.533959112886805,
      6.528075924012228,
      6.483446243015351,
      6.520024587123771,
      6.452424200993846,
      6.5582564920187,
      6.503130945988232,
      6.470657062120154,
      6.56687602798047,
      6.462749988044379,
      6.495560393013875,
      6.576167599050677,
      6.50101919194276,
      6.541682983952342,
      6.495898948967806,
      6.476050787969143,
      6.525129888948868,
      6.560297159929178,
      6.465193502954207,
      6.528371147956932,
      6.569802008976694,
      6.54654272699554,
      6.462267019072897,
      6.5054304390068864,
      6.53762832407665,
      6.473864280123962
    ],
    "old_times_s": [
      6.5460315930249635,
      6.496083947029547,
      6.493944745961926,
      6.555872963013826,
      6.531383968918817,
      6.535554225061787,
      6.52009391298634,
      6.487940823004465,
      6.46800477404031,
      6.4923333120386815,
      6.407454012020025,
      6.55929458096216,
      6.5364324639958795,
      6.517994201974943,
      6.5070389909960795,
      6.464973110079882,
      6.562027257983573,
      6.513846911984729,
      6.57806708397402,
      6.6125980059296126,
      6.541345218996867,
      6.494377393973991,
      6.480969968033605,
      6.496473823048291,
      6.576392608054448,
      6.49326694390038,
      6.550939163920702,
      6.473857210075948,
      6.475464396120515,
      6.523660707025556,
      6.458441425071214
    ]
  },
  "tests": {
    "total_tests": 101,
    "significant_improvements": 0,
    "significant_improvements_tests": [],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 0,
    "significant_pair_improvements_tests": [],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 0,
    "significant_binom_improvements_tests": [],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 0,
    "significant_wilcoxon_improvements_tests": [],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 5,
    "significant_mannwhitney_improvements_tests": [
      "arch.boolean_conversions",
      "arch.haddp",
      "arch.huge_exp",
      "arch.sqrt_pp",
      "xsimd.select_dynamic"
    ],
    "significant_mannwhitney_regressions": 6,
    "significant_mannwhitney_regressions_tests": [
      "arch.any_all",
      "arch.conj_norm_proj_real",
      "arch.to_int32",
      "arch.atanh",
      "arch.sqrt_nn",
      "xsimd.select_static"
    ],
    "tests": [
      {
        "test_name": "algorithms.binary_transform",
        "is_significant": false,
        "p_value": 0.9998585877688954,
        "is_pair_significant": false,
        "pair_p_value": 0.9993779065442999,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9990918301045895,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8402006822204959,
        "relative_improvement": -0.023290870495342886,
        "absolute_improvement_ms": -1.5960451740378163,
        "old_mean_ms": 68.52664327668427,
        "new_mean_ms": 70.12268845072208,
        "old_std_ms": 4.6189045115917935,
        "new_std_ms": 5.404945730707473,
        "effect_size_cohens_d": -0.317474906142677,
        "old_ci95_ms": [
          66.76970596321895,
          70.2835805901496
        ],
        "new_ci95_ms": [
          68.06675704800459,
          72.17861985343959
        ],
        "old_ci99_ms": [
          66.15656810176446,
          70.89671845160409
        ],
        "new_ci99_ms": [
          67.34927590972026,
          72.89610099172391
        ],
        "new_times": [
          0.0722680320031941,
          0.07089827999880072,
          0.07403017900651321,
          0.06126486101129558,
          0.06520733200886752,
          0.0679962889989838,
          0.06869144500524271,
          0.06640599798993208,
          0.08080020699708257,
          0.07195792000857182,
          0.07238615601090714,
          0.07884552299219649,
          0.061403647006955,
          0.06767833601043094,
          0.07946228700166102,
          0.07151132299622986,
          0.07149109199235681,
          0.07654155501222704,
          0.06678901199484244,
          0.06968385299842339,
          0.07719096100481693,
          0.06522518199926708,
          0.0667302700021537,
          0.06820936700387392,
          0.07168410999292973,
          0.06223364900506567,
          0.07665401000122074,
          0.06363502101157792,
          0.06668206800532062
        ],
        "old_times": [
          0.07164573800400831,
          0.06234414200298488,
          0.06698753099772148,
          0.06217954700696282,
          0.07097563300339971,
          0.07835429398983251,
          0.06805533000442665,
          0.07273165900551248,
          0.06245864699303638,
          0.06576446299732197,
          0.0724560590024339,
          0.066256161997444,
          0.06683285399049055,
          0.07636775801074691,
          0.06563908900716342,
          0.07192600899725221,
          0.06929895900248084,
          0.07348478800849989,
          0.07531192799797282,
          0.06809099200472701,
          0.06569259100069758,
          0.06608478599810041,
          0.06860246100404765,
          0.07429385899740737,
          0.0645580870041158,
          0.06800422898959368,
          0.06413292000070214,
          0.05944958300096914,
          0.06929255700379144
        ]
      },
      {
        "test_name": "algorithms.unary_transform",
        "is_significant": false,
        "p_value": 0.9981963577482498,
        "is_pair_significant": false,
        "pair_p_value": 0.9995467745043372,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9993601273745298,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7381339882175864,
        "relative_improvement": -0.008166127064203475,
        "absolute_improvement_ms": -0.5385793122294202,
        "old_mean_ms": 65.95284496494249,
        "new_mean_ms": 66.4914242771719,
        "old_std_ms": 4.7946605442407915,
        "new_std_ms": 5.043109373566127,
        "effect_size_cohens_d": -0.10945725453051726,
        "old_ci95_ms": [
          64.12905363207652,
          67.77663629780845
        ],
        "new_ci95_ms": [
          64.573128063845,
          68.4097204904988
        ],
        "old_ci99_ms": [
          63.49258498347852,
          68.41310494640643
        ],
        "new_ci99_ms": [
          63.90367900105765,
          69.07916955328616
        ],
        "new_times": [
          0.061975919001270086,
          0.05860218999441713,
          0.06849066700669937,
          0.06677699199644849,
          0.06211899399932008,
          0.06612846700591035,
          0.057298590007121675,
          0.06564067900762893,
          0.06240711599821225,
          0.06853305900585838,
          0.0576287029980449,
          0.0665038419974735,
          0.06563669801107608,
          0.06582035600149538,
          0.07303219099412672,
          0.06585868699767161,
          0.06550642299407627,
          0.06459874800930265,
          0.07178653399751056,
          0.07003336699563079,
          0.07066711099469103,
          0.060663008989649825,
          0.07222705999447498,
          0.06581185500544962,
          0.06591254800150637,
          0.06289696400926914,
          0.0761622310092207,
          0.07343429600587115,
          0.07609800800855737
        ],
        "old_times": [
          0.06445789299323224,
          0.06241740500263404,
          0.06535200799407903,
          0.06665989800239913,
          0.06458898799610324,
          0.0636811839940492,
          0.0629279150016373,
          0.07168049899337348,
          0.05576443100289907,
          0.06993556299130432,
          0.064789905998623,
          0.0651332889974583,
          0.06684172399400268,
          0.062257059005787596,
          0.06768918699526694,
          0.06368967400339898,
          0.06439851100731175,
          0.06966882299457211,
          0.07909944299899507,
          0.07344584699603729,
          0.06484082801034674,
          0.06287019299634267,
          0.06668612798966933,
          0.07144371001049876,
          0.06951520699658431,
          0.05911357900185976,
          0.07153580400336068,
          0.05884172901278362,
          0.0633060789987212
        ]
      },
      {
        "test_name": "xsimd_reduce.unaligned_begin_unaligned_end",
        "is_significant": false,
        "p_value": 0.9983905945749857,
        "is_pair_significant": false,
        "pair_p_value": 0.9980077631893466,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9959961511194706,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5801068610260458,
        "relative_improvement": -0.007328786581148854,
        "absolute_improvement_ms": -0.4728717251745035,
        "old_mean_ms": 64.5225126886387,
        "new_mean_ms": 64.99538441381321,
        "old_std_ms": 4.253019625732323,
        "new_std_ms": 5.03647730775272,
        "effect_size_cohens_d": -0.10144766775708418,
        "old_ci95_ms": [
          62.90475054252881,
          66.1402748347486
        ],
        "new_ci95_ms": [
          63.07961090340508,
          66.91115792422134
        ],
        "old_ci99_ms": [
          62.34018217972738,
          66.70484319755003
        ],
        "new_ci99_ms": [
          62.411042216178565,
          67.57972661144785
        ],
        "new_times": [
          0.06369256399921142,
          0.05725767700641882,
          0.07037416900857352,
          0.07880947100056801,
          0.06017102999612689,
          0.0670494219957618,
          0.0667746020044433,
          0.06357170999399386,
          0.06608403500285931,
          0.0621619949961314,
          0.0701925419998588,
          0.061988549001398496,
          0.07029879699985031,
          0.06157814300968312,
          0.05645167699549347,
          0.06351923799957149,
          0.06031141499988735,
          0.05854664699290879,
          0.060501422005472705,
          0.06491834000917152,
          0.07246503898932133,
          0.06760844399104826,
          0.06367115399916656,
          0.062475327009451576,
          0.07211881599505432,
          0.0696856430004118,
          0.06205597099324223,
          0.06520030199317262,
          0.06533200701233
        ],
        "old_times": [
          0.0650168739957735,
          0.0698233980074292,
          0.06034694598929491,
          0.06749576899164822,
          0.05852468698867597,
          0.062439725996227935,
          0.05610137300391216,
          0.061914006000733934,
          0.06295333699381445,
          0.0692709770082729,
          0.06932475899520796,
          0.06985534899285994,
          0.06647949999023695,
          0.06256538099842146,
          0.06507232700823806,
          0.0692801480035996,
          0.06878068900550716,
          0.06693571800133213,
          0.0613583449885482,
          0.06377316800353583,
          0.07072532300662715,
          0.06980312798987143,
          0.06488039900432341,
          0.05752929901063908,
          0.06639281699608546,
          0.062231568997958675,
          0.06482951701036654,
          0.05813046199909877,
          0.059317876992281526
        ]
      },
      {
        "test_name": "xsimd_reduce.unaligned_begin_aligned_end",
        "is_significant": false,
        "p_value": 0.9779841111056344,
        "is_pair_significant": false,
        "pair_p_value": 0.9802089608448942,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9772401507943869,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.24202349317663774,
        "relative_improvement": 0.01274495605018202,
        "absolute_improvement_ms": 0.8177483120154344,
        "old_mean_ms": 64.16250544887212,
        "new_mean_ms": 63.34475713685669,
        "old_std_ms": 4.584676178962402,
        "new_std_ms": 4.479588914011771,
        "effect_size_cohens_d": 0.18042132527284088,
        "old_ci95_ms": [
          62.418587896829315,
          65.90642300091494
        ],
        "new_ci95_ms": [
          61.64081264260246,
          65.04870163111093
        ],
        "old_ci99_ms": [
          61.809993685631476,
          66.51501721211278
        ],
        "new_ci99_ms": [
          61.04616827183458,
          65.64334600187881
        ],
        "new_times": [
          0.06272576699848287,
          0.06649282100261189,
          0.06899715699546505,
          0.06947793500148691,
          0.06353902899718378,
          0.06436204000783619,
          0.06624254200141877,
          0.06387475099472795,
          0.06485568798962049,
          0.06306595999922138,
          0.05518568899424281,
          0.06593835999956354,
          0.052980374995968305,
          0.06360045001201797,
          0.0643556900031399,
          0.06413968199922238,
          0.06454295599542093,
          0.06228553999972064,
          0.05514153800322674,
          0.06253838099655695,
          0.06536246799805667,
          0.06972102500731125,
          0.069787386993994,
          0.060147197989863344,
          0.06563531899882946,
          0.059265685005811974,
          0.053911020993837155,
          0.06249723800283391,
          0.06632826499117073
        ],
        "old_times": [
          0.07192153799405787,
          0.06579525400593411,
          0.06346376500732731,
          0.06543614000838716,
          0.07023673399817199,
          0.06700015100068413,
          0.05969557099160738,
          0.058442043999093585,
          0.0677130170079181,
          0.06957750899891835,
          0.055691428991849534,
          0.0657223019952653,
          0.06458896800177172,
          0.05861052998807281,
          0.07009889899927657,
          0.06400613600271754,
          0.05803289799951017,
          0.06323566701030359,
          0.0613118140026927,
          0.06623635100550018,
          0.05564411600062158,
          0.062358442999538966,
          0.06742742699861992,
          0.06913088200963102,
          0.06411186000332236,
          0.0699012010009028,
          0.05859901900112163,
          0.0662208110006759,
          0.06050218199379742
        ]
      },
      {
        "test_name": "xsimd_reduce.aligned_begin_unaligned_end",
        "is_significant": false,
        "p_value": 0.9929622885508416,
        "is_pair_significant": false,
        "pair_p_value": 0.995089766539239,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9921277016401291,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.19618663776787387,
        "relative_improvement": 0.008725313162483612,
        "absolute_improvement_ms": 0.5610622435503199,
        "old_mean_ms": 64.30282020853062,
        "new_mean_ms": 63.7417579649803,
        "old_std_ms": 3.3322528601540156,
        "new_std_ms": 4.637093299409122,
        "effect_size_cohens_d": 0.13895483438380615,
        "old_ci95_ms": [
          63.03529900881469,
          65.57034140824655
        ],
        "new_ci95_ms": [
          61.97790200664524,
          65.50561392331537
        ],
        "old_ci99_ms": [
          62.59295810582072,
          66.01268231124052
        ],
        "new_ci99_ms": [
          61.3623496691095,
          66.1211662608511
        ],
        "new_times": [
          0.06434839901339728,
          0.0659173589956481,
          0.07444754500465933,
          0.06291978500667028,
          0.05840107200492639,
          0.060912586995982565,
          0.06037225799809676,
          0.06570904099498875,
          0.0666675579996081,
          0.0612786919955397,
          0.0619848490023287,
          0.05920007199165411,
          0.057209445993066765,
          0.0668113040010212,
          0.05786962198908441,
          0.06249739798658993,
          0.06562903700978495,
          0.05981418499141,
          0.05593873799080029,
          0.06477942600031383,
          0.06449940499442164,
          0.06444486300460994,
          0.06308195099700242,
          0.06129093200433999,
          0.06498453300446272,
          0.06579480400250759,
          0.06822612699761521,
          0.07727142301155254,
          0.06620857000234537
        ],
        "old_times": [
          0.06445040300604887,
          0.06783487299981061,
          0.06978257700393442,
          0.06336236199422274,
          0.06510516800335608,
          0.0629712469963124,
          0.06108494399813935,
          0.0699229820020264,
          0.06465943100920413,
          0.06805685099971015,
          0.06102105200989172,
          0.06199860900233034,
          0.06413586001144722,
          0.05840726300084498,
          0.0649774229968898,
          0.061102884996216744,
          0.06599686200206634,
          0.0689009429916041,
          0.06222717800119426,
          0.06248112799949013,
          0.06793668700265698,
          0.058188392998999916,
          0.06486390900681727,
          0.0668921659962507,
          0.05935785900510382,
          0.06795895700633992,
          0.06540919000690337,
          0.06557048599643167,
          0.060124098003143445
        ]
      },
      {
        "test_name": "xsimd_reduce.aligned_begin_aligned_end",
        "is_significant": false,
        "p_value": 0.9963963757898918,
        "is_pair_significant": false,
        "pair_p_value": 0.9965767996316538,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9993006382137537,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5679133843285674,
        "relative_improvement": -0.009091805993210028,
        "absolute_improvement_ms": -0.5875793431187049,
        "old_mean_ms": 64.62735165681305,
        "new_mean_ms": 65.21493099993175,
        "old_std_ms": 4.8271205519791085,
        "new_std_ms": 5.755957938346347,
        "effect_size_cohens_d": -0.1106160738004134,
        "old_ci95_ms": [
          62.791213197339964,
          66.46349011628614
        ],
        "new_ci95_ms": [
          63.02548169409787,
          67.40438030576564
        ],
        "old_ci99_ms": [
          62.1504356353021,
          67.10426767832399
        ],
        "new_ci99_ms": [
          62.26140533528105,
          68.16845666458246
        ],
        "new_times": [
          0.06196144799469039,
          0.05938063999929,
          0.060716099993442185,
          0.061610914999619126,
          0.07491386300534941,
          0.07373623800231144,
          0.0682385370018892,
          0.07094350199622568,
          0.07076137499825563,
          0.06231366099382285,
          0.06324156701157335,
          0.0855837609997252,
          0.06590620899805799,
          0.058433722995687276,
          0.06068362999940291,
          0.06103328301105648,
          0.06583805600530468,
          0.06457078800303861,
          0.06432167800085153,
          0.06792902600136586,
          0.05986434800433926,
          0.06448694500431884,
          0.062309850996825844,
          0.06477855499542784,
          0.06106728299346287,
          0.06633263498952147,
          0.06030655500944704,
          0.06282522100082133,
          0.06714360599289648
        ],
        "old_times": [
          0.06633060499734711,
          0.0635012970014941,
          0.06427388598967809,
          0.06241626500559505,
          0.06208689301274717,
          0.06805189099395648,
          0.06693345800158568,
          0.0672872019931674,
          0.05833630899724085,
          0.0670581729937112,
          0.06581448600627482,
          0.06543383000825997,
          0.06242189501062967,
          0.06949256599182263,
          0.0599899130029371,
          0.06570342100167181,
          0.06711804500082508,
          0.06277319900982548,
          0.08230101600929629,
          0.066009293004754,
          0.05882439800188877,
          0.059706582003855146,
          0.060128577999421395,
          0.06366873400111217,
          0.0598226260044612,
          0.06897531599679496,
          0.06111808501009364,
          0.058994234990677796,
          0.0696210010064533
        ]
      },
      {
        "test_name": "xsimd_reduce.using_custom_binary_function",
        "is_significant": false,
        "p_value": 0.999581768656184,
        "is_pair_significant": false,
        "pair_p_value": 0.9997117558097709,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9996656160801649,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8285953637022919,
        "relative_improvement": -0.010690446922265368,
        "absolute_improvement_ms": -0.6837213446048984,
        "old_mean_ms": 63.95629196576293,
        "new_mean_ms": 64.64001331036782,
        "old_std_ms": 3.8555368146033357,
        "new_std_ms": 4.627979776809957,
        "effect_size_cohens_d": -0.1605241508737962,
        "old_ci95_ms": [
          62.48972419498185,
          65.42285973654401
        ],
        "new_ci95_ms": [
          62.87962395063236,
          66.40040267010329
        ],
        "old_ci99_ms": [
          61.977919806878084,
          65.93466412464777
        ],
        "new_ci99_ms": [
          62.26528139038099,
          67.01474523035465
        ],
        "new_times": [
          0.07089847901079338,
          0.06747981799708214,
          0.06556519599689636,
          0.06207835199893452,
          0.06979883699386846,
          0.05699095800810028,
          0.06798471799993422,
          0.06647834100294858,
          0.0618546640034765,
          0.06691578699974343,
          0.07350065901118796,
          0.06361783099418972,
          0.05524878100550268,
          0.06310544098960236,
          0.06321899600152392,
          0.0685142580041429,
          0.06155800299893599,
          0.0638282299914863,
          0.0693545109970728,
          0.06445007299771532,
          0.06800347799435258,
          0.06482321699149907,
          0.06758698298654053,
          0.06748489900201093,
          0.06488628000079188,
          0.06145696800376754,
          0.054505403008079156,
          0.05647055800363887,
          0.06690066700684838
        ],
        "old_times": [
          0.059463051991770044,
          0.06248265800240915,
          0.06257507098780479,
          0.06400596599269193,
          0.0707136819983134,
          0.057947535009589046,
          0.06429790701076854,
          0.06649341101001482,
          0.06872967699018773,
          0.07326936999743339,
          0.06517596999765374,
          0.06348732599872164,
          0.06385510999825783,
          0.06292002499685623,
          0.06656559401017148,
          0.06397716500214301,
          0.06562764800037257,
          0.06523455199203454,
          0.06882441000198014,
          0.06128438199812081,
          0.06029874399246182,
          0.06187832400610205,
          0.0678731640073238,
          0.06204832100775093,
          0.055544072994962335,
          0.06572229199809954,
          0.05934798800444696,
          0.06513908899796661,
          0.059949961010715924
        ]
      },
      {
        "test_name": "xsimd_reduce.load",
        "is_significant": false,
        "p_value": 0.9994789700669323,
        "is_pair_significant": false,
        "pair_p_value": 0.9986377334433658,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9996320437639952,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6280061697584979,
        "relative_improvement": -0.02135619212449193,
        "absolute_improvement_ms": -1.3657938267489789,
        "old_mean_ms": 63.95305955234577,
        "new_mean_ms": 65.31885337909475,
        "old_std_ms": 3.8740937658720798,
        "new_std_ms": 5.983459718718996,
        "effect_size_cohens_d": -0.27097165724863376,
        "old_ci95_ms": [
          62.479433094807604,
          65.42668600988394
        ],
        "new_ci95_ms": [
          63.04286702409405,
          67.59483973409546
        ],
        "old_ci99_ms": [
          61.965165358655085,
          65.94095374603646
        ],
        "new_ci99_ms": [
          62.248590873368826,
          68.38911588482067
        ],
        "new_times": [
          0.06194063799921423,
          0.0636675429996103,
          0.06317408398899715,
          0.06684408399451058,
          0.06394204300886486,
          0.06471669400343671,
          0.0630326079990482,
          0.06657787499716505,
          0.059336577003705315,
          0.062055001995759085,
          0.06585084600374103,
          0.06420276399876457,
          0.06632462398556527,
          0.07413357299810741,
          0.0714882219908759,
          0.07157489500241354,
          0.06296155600284692,
          0.06360949100053404,
          0.05731905999709852,
          0.06393017299706116,
          0.05673995800316334,
          0.06458415801171213,
          0.07261959499737713,
          0.06586619700829033,
          0.06483991800632793,
          0.06174742001167033,
          0.06263132298772689,
          0.08815882899216376,
          0.06037699800799601
        ],
        "old_times": [
          0.06292755500180647,
          0.06543528099427931,
          0.06764885599841364,
          0.05713427299633622,
          0.06566700999974273,
          0.0670802940003341,
          0.06652793299872428,
          0.065010145000997,
          0.060380897994036786,
          0.07187326600251254,
          0.06295830600720365,
          0.06820203599636443,
          0.06409630000416655,
          0.0629693560040323,
          0.0609002269920893,
          0.05764211300993338,
          0.05754657000943553,
          0.06477854499826208,
          0.058804626998608,
          0.06043719899025746,
          0.06615301800775342,
          0.06675018100941088,
          0.06285584199940786,
          0.06707702300627716,
          0.059947610992821865,
          0.06433252801070921,
          0.06203496099624317,
          0.06542318999709096,
          0.07204358300077729
        ]
      },
      {
        "test_name": "xsimd_reduce.store",
        "is_significant": false,
        "p_value": 0.9919061197455697,
        "is_pair_significant": false,
        "pair_p_value": 0.9942596283810916,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9921277016401291,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.767583231030099,
        "relative_improvement": 0.005015763584707659,
        "absolute_improvement_ms": 0.3189898626918958,
        "old_mean_ms": 63.597467724446616,
        "new_mean_ms": 63.27847786175472,
        "old_std_ms": 4.931165350300443,
        "new_std_ms": 4.07584189040395,
        "effect_size_cohens_d": 0.07051426045482963,
        "old_ci95_ms": [
          61.72175274028376,
          65.47318270860949
        ],
        "new_ci95_ms": [
          61.72811052182288,
          64.82884520168656
        ],
        "old_ci99_ms": [
          61.06716372036056,
          66.12777172853266
        ],
        "new_ci99_ms": [
          61.18706167051254,
          65.36989405299691
        ],
        "new_times": [
          0.06963937201362569,
          0.06409852899378166,
          0.07281204299943056,
          0.06574233299761545,
          0.055238101005670615,
          0.06152172099973541,
          0.05750312699819915,
          0.06239423400256783,
          0.06758414200157858,
          0.06670351899811067,
          0.05848486500326544,
          0.06055193398788106,
          0.05573899000592064,
          0.06482477700046729,
          0.06399435600906145,
          0.062403454998275265,
          0.06346472499717493,
          0.06813982399762608,
          0.05659176199696958,
          0.06275888800155371,
          0.06379807800112758,
          0.0628613329899963,
          0.06426762600312941,
          0.06364992199814878,
          0.0608852370060049,
          0.0612837730004685,
          0.06561723799677566,
          0.06562297699565534,
          0.0668989769910695
        ],
        "old_times": [
          0.06358009000541642,
          0.07269977800024208,
          0.06105333298910409,
          0.07618614200328011,
          0.05946536301053129,
          0.06244999599584844,
          0.05927713500568643,
          0.058206834000884555,
          0.07351780000317376,
          0.07146766199730337,
          0.06717012700391933,
          0.07112192800559569,
          0.06073050099075772,
          0.06303207800374366,
          0.06062086699239444,
          0.06230119201063644,
          0.062059500996838324,
          0.05805741899530403,
          0.0600534749974031,
          0.06099817200447433,
          0.06333558099868242,
          0.05763290201139171,
          0.06647491999319755,
          0.06104711300577037,
          0.06282717100111768,
          0.063213966001058,
          0.06303513899911195,
          0.06280698999762535,
          0.05990338898845948
        ]
      },
      {
        "test_name": "arch.supported",
        "is_significant": false,
        "p_value": 0.9997362563978104,
        "is_pair_significant": false,
        "pair_p_value": 0.9997184676118275,
        "is_binom_significant": false,
        "binom_p_value": 0.9999991878867149,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999128542840481,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8285953637022919,
        "relative_improvement": -0.010124756039123052,
        "absolute_improvement_ms": -0.6445252772115523,
        "old_mean_ms": 63.65835134407651,
        "new_mean_ms": 64.30287662128806,
        "old_std_ms": 4.328738788882728,
        "new_std_ms": 3.802145981168513,
        "effect_size_cohens_d": -0.15820610444733416,
        "old_ci95_ms": [
          62.01178716866519,
          65.30491551948782
        ],
        "new_ci95_ms": [
          62.85661763741159,
          65.74913560516453
        ],
        "old_ci99_ms": [
          61.43716744289476,
          65.87953524525825
        ],
        "new_ci99_ms": [
          62.35190063146767,
          66.25385261110844
        ],
        "new_times": [
          0.06707430300593842,
          0.06910891100415029,
          0.06395951399463229,
          0.0639207320054993,
          0.0690855900029419,
          0.06695193800260313,
          0.06338011199841276,
          0.06776681001065299,
          0.06381247800891288,
          0.0670756129984511,
          0.06227276999561582,
          0.06458716800261755,
          0.06517434000852518,
          0.059181641991017386,
          0.06469000199285802,
          0.05762486199091654,
          0.06321189600566868,
          0.06617248900874984,
          0.06066423800075427,
          0.06299627700354904,
          0.06207653299497906,
          0.06995947399991564,
          0.05742428400844801,
          0.06675584199547302,
          0.06408368899428751,
          0.060829894995549694,
          0.06367929300176911,
          0.057847331001539715,
          0.07341539599292446
        ],
        "old_times": [
          0.07041329999628942,
          0.06715271598659456,
          0.05902878599590622,
          0.06691854698874522,
          0.058890879998216406,
          0.05853460599610116,
          0.06157657300354913,
          0.05874643599963747,
          0.06770044799486641,
          0.060990771002252586,
          0.06416963200899772,
          0.06317689399293158,
          0.06284612200397532,
          0.07050450400856789,
          0.06239727500360459,
          0.0605590250052046,
          0.062169376004021615,
          0.06679047299257945,
          0.061973128002136946,
          0.06082002400944475,
          0.07542356199701317,
          0.06558987598691601,
          0.05659667300642468,
          0.06316110400075559,
          0.06662299600429833,
          0.06326222799543757,
          0.06603209400782362,
          0.057576749997679144,
          0.0664673899882473
        ]
      },
      {
        "test_name": "arch.name",
        "is_significant": false,
        "p_value": 0.9999969048122934,
        "is_pair_significant": false,
        "pair_p_value": 0.9999751277362642,
        "is_binom_significant": false,
        "binom_p_value": 0.9999923817813396,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999793320894241,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9418716188543339,
        "relative_improvement": -0.02694711101418287,
        "absolute_improvement_ms": -1.7204339679226244,
        "old_mean_ms": 63.84483913756537,
        "new_mean_ms": 65.565273105488,
        "old_std_ms": 4.156905866417269,
        "new_std_ms": 3.5256983097670154,
        "effect_size_cohens_d": -0.44637375068296176,
        "old_ci95_ms": [
          62.263636710249024,
          65.42604156488173
        ],
        "new_ci95_ms": [
          64.22416919201555,
          66.90637701896044
        ],
        "old_ci99_ms": [
          61.71182699718692,
          65.97785127794383
        ],
        "new_ci99_ms": [
          63.75614931493018,
          67.37439689604582
        ],
        "new_times": [
          0.06398775499837939,
          0.06589982799778227,
          0.06400333599594887,
          0.07383012199716177,
          0.06781667099858169,
          0.06471455299470108,
          0.06393852300243452,
          0.06058743500034325,
          0.06935687000805046,
          0.07512640100321732,
          0.06515587901230901,
          0.06569717101228889,
          0.06529453600523993,
          0.06289318299968727,
          0.06602222299261484,
          0.06445550300122704,
          0.06325788800313603,
          0.05729479000729043,
          0.06515375900198705,
          0.06589790800353512,
          0.06376053699932527,
          0.06428462600160856,
          0.0652090719959233,
          0.06358069000998512,
          0.06736597399867605,
          0.06568122000317089,
          0.06805178099602927,
          0.06250226900738198,
          0.07057241701113526
        ],
        "old_times": [
          0.06865986400225665,
          0.06177326000761241,
          0.06126635200052988,
          0.06412157100567129,
          0.06192084599751979,
          0.06767968599160668,
          0.0666067559941439,
          0.06141844700323418,
          0.07067654099955689,
          0.0625441599986516,
          0.06532897699798923,
          0.06575204200635199,
          0.06140235600469168,
          0.06448746500245761,
          0.05709654200472869,
          0.06158576400775928,
          0.06838947300275322,
          0.0647568139975192,
          0.06597726099425927,
          0.06515730998944491,
          0.057204035998438485,
          0.06418594298884273,
          0.06337755199638195,
          0.0729396669921698,
          0.06867127498844638,
          0.05345058300008532,
          0.06370929500553757,
          0.06036605700501241,
          0.060994440005742945
        ]
      },
      {
        "test_name": "arch.available",
        "is_significant": false,
        "p_value": 0.8751813042560108,
        "is_pair_significant": false,
        "pair_p_value": 0.8676210721323088,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9253172073513269,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.10673068676983688,
        "relative_improvement": 0.02394639994426963,
        "absolute_improvement_ms": 1.5969364823580774,
        "old_mean_ms": 66.68795668971644,
        "new_mean_ms": 65.09102020735837,
        "old_std_ms": 6.045980842543839,
        "new_std_ms": 5.630256838195254,
        "effect_size_cohens_d": 0.2733629268988231,
        "old_ci95_ms": [
          64.38818857108498,
          68.9877248083479
        ],
        "new_ci95_ms": [
          62.949385042874425,
          67.23265537184231
        ],
        "old_ci99_ms": [
          63.58561303507015,
          69.79030034436273
        ],
        "new_ci99_ms": [
          62.20199491420929,
          67.98004550050744
        ],
        "new_times": [
          0.06393037200905383,
          0.064364239005954,
          0.0655662659992231,
          0.06440909099183045,
          0.06555823599046562,
          0.07017736199486535,
          0.05918619099247735,
          0.07218900900625158,
          0.06011287699220702,
          0.07984970099641941,
          0.060379258007742465,
          0.05760356200335082,
          0.057420313998591155,
          0.0632465870003216,
          0.06634393600688782,
          0.0604649610031629,
          0.06773694900039118,
          0.05427234500530176,
          0.06272049699327908,
          0.0663591859920416,
          0.06943823299661744,
          0.06501562400080729,
          0.06714401600765996,
          0.07812086500052828,
          0.06461249900166877,
          0.06765576600446366,
          0.06870366500515956,
          0.06024194200290367,
          0.06481603700376581
        ],
        "old_times": [
          0.06423406400426757,
          0.06482180701277684,
          0.06259262100502383,
          0.06673771099303849,
          0.07022706400312018,
          0.0658918479894055,
          0.06411889000446536,
          0.06856486000469886,
          0.0704392409970751,
          0.0643897309928434,
          0.06703586199728306,
          0.07323173800250515,
          0.05960403798962943,
          0.05890413100132719,
          0.07045151200145483,
          0.0738022709992947,
          0.06804646999808028,
          0.06824976799543947,
          0.060163198999362066,
          0.06887997199373785,
          0.07237285500741564,
          0.06330759001139086,
          0.06911352099268697,
          0.06312870200781617,
          0.08688700999482535,
          0.06542533100582659,
          0.0659528109972598,
          0.052206824999302626,
          0.06516930100042373
        ]
      },
      {
        "test_name": "arch.dispatcher",
        "is_significant": false,
        "p_value": 0.9990609971055417,
        "is_pair_significant": false,
        "pair_p_value": 0.9997190240039585,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9997743275016546,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.604253377250061,
        "relative_improvement": -0.01172057535208575,
        "absolute_improvement_ms": -0.7354431722417509,
        "old_mean_ms": 62.748043517409506,
        "new_mean_ms": 63.48348668965126,
        "old_std_ms": 4.236580960955707,
        "new_std_ms": 4.947024614589996,
        "effect_size_cohens_d": -0.15968724695646858,
        "old_ci95_ms": [
          61.136534304967554,
          64.35955272985146
        ],
        "new_ci95_ms": [
          61.601739163978934,
          65.36523421532357
        ],
        "old_ci99_ms": [
          60.57414809764066,
          64.92193893717835
        ],
        "new_ci99_ms": [
          60.94504490127223,
          66.02192847803028
        ],
        "new_times": [
          0.05903829600720201,
          0.06405759800691158,
          0.06427663599606603,
          0.07049242399807554,
          0.06282612099312246,
          0.06350776700128336,
          0.05830869800411165,
          0.06332226999802515,
          0.05648106800799724,
          0.059686010994482785,
          0.05884405899269041,
          0.06519586101057939,
          0.0584618849970866,
          0.06030289499904029,
          0.06379056700097863,
          0.060679049012833275,
          0.0660768950037891,
          0.06698160000087228,
          0.05828856700100005,
          0.06638610699155834,
          0.07087972899898887,
          0.06571599098970182,
          0.057944253989262506,
          0.0696847630024422,
          0.056415066006593406,
          0.07442992400319781,
          0.07337150399689563,
          0.0611704579932848,
          0.06440505100181326
        ],
        "old_times": [
          0.06969588399806526,
          0.0605936359934276,
          0.07027093600481749,
          0.0636609729990596,
          0.06418313299946021,
          0.05660080300003756,
          0.061732108995784074,
          0.06344676500884816,
          0.05562016500334721,
          0.06016508900211193,
          0.05832953999924939,
          0.061685826993198134,
          0.06058029599080328,
          0.06082860399328638,
          0.06167661699873861,
          0.06520474200078752,
          0.06504651599971112,
          0.0659751920029521,
          0.054341816998203285,
          0.06649955100147054,
          0.06921502499608323,
          0.06373268499737605,
          0.06762989499839023,
          0.06651116200373508,
          0.061490849999245256,
          0.06671179900877178,
          0.05831058800686151,
          0.06341083299776074,
          0.05654223001329228
        ]
      },
      {
        "test_name": "arch.default_load",
        "is_significant": false,
        "p_value": 0.6556562723348782,
        "is_pair_significant": false,
        "pair_p_value": 0.6590259318611762,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.731924507766962,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.05812838114566615,
        "relative_improvement": 0.04167502938191132,
        "absolute_improvement_ms": 2.78258234501319,
        "old_mean_ms": 66.76857548229927,
        "new_mean_ms": 63.98599313728607,
        "old_std_ms": 6.102858909667024,
        "new_std_ms": 4.648160984301563,
        "effect_size_cohens_d": 0.512966175437169,
        "old_ci95_ms": [
          64.44717210400923,
          69.0899788605893
        ],
        "new_ci95_ms": [
          62.217927256768725,
          65.75405901780343
        ],
        "old_ci99_ms": [
          63.63704627195513,
          69.90010469264341
        ],
        "new_ci99_ms": [
          61.60090573609028,
          66.37108053848186
        ],
        "new_times": [
          0.06406399799743667,
          0.06473541399464011,
          0.06979641701036599,
          0.06524556400836445,
          0.06109708499570843,
          0.05897556399577297,
          0.05862634000368416,
          0.07104444500873797,
          0.0651118179957848,
          0.05709930199373048,
          0.05734481201216113,
          0.06812809300026856,
          0.06468417200085241,
          0.06433915799425449,
          0.05980135498975869,
          0.061332514000241645,
          0.0708424170006765,
          0.06799126799160149,
          0.06442230200627819,
          0.061810391998733394,
          0.060693339008139446,
          0.06581961599295028,
          0.06798215799790341,
          0.0652268519916106,
          0.056762759006232955,
          0.06477507499221247,
          0.06214054499287158,
          0.07610144899808802,
          0.05959957800223492
        ],
        "old_times": [
          0.06024959299247712,
          0.06574900299892761,
          0.06858462101081386,
          0.07437863199447747,
          0.06180425199272577,
          0.07755864399950951,
          0.06379094799922314,
          0.062355203001061454,
          0.061790421008481644,
          0.06692455799202435,
          0.063041499000974,
          0.06236106299911626,
          0.061725459003355354,
          0.06516480099526234,
          0.06032698499620892,
          0.06298670699470676,
          0.06603406400245149,
          0.08267057900957298,
          0.06161852499644738,
          0.06427501600410324,
          0.06544908099749591,
          0.06147274898830801,
          0.07733845600159839,
          0.07088280899915844,
          0.06285022200609092,
          0.07532872899901122,
          0.07619773200713098,
          0.0651679709990276,
          0.0682103669969365
        ]
      },
      {
        "test_name": "arch.basic_functions",
        "is_significant": false,
        "p_value": 0.9819627881230548,
        "is_pair_significant": false,
        "pair_p_value": 0.9707190430275997,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9759867954999208,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5186054601812679,
        "relative_improvement": 0.007340288953733812,
        "absolute_improvement_ms": 0.472773584549252,
        "old_mean_ms": 64.4080345513869,
        "new_mean_ms": 63.93526096683766,
        "old_std_ms": 5.545813850925327,
        "new_std_ms": 4.431201379282401,
        "effect_size_cohens_d": 0.09418660440870909,
        "old_ci95_ms": [
          62.298519781429995,
          66.51754932134382
        ],
        "new_ci95_ms": [
          62.24972210644299,
          65.62079982723233
        ],
        "old_ci99_ms": [
          61.562339062376935,
          67.25373004039687
        ],
        "new_ci99_ms": [
          61.66150095345122,
          66.20902098022411
        ],
        "new_times": [
          0.07528655701025855,
          0.06019494100473821,
          0.062271859002066776,
          0.07046595300198533,
          0.06664072599960491,
          0.058737595012644306,
          0.0600425940065179,
          0.0719514189986512,
          0.06041600900061894,
          0.06384380999952555,
          0.07276428100885823,
          0.06319267500657588,
          0.061934836994623765,
          0.06694587800302543,
          0.06125178099318873,
          0.06703200199990533,
          0.06471794300887268,
          0.06407584900443908,
          0.06062860699603334,
          0.05959660799999256,
          0.06373060599435121,
          0.05948751400865149,
          0.06092417899344582,
          0.057379432997549884,
          0.06277754998882301,
          0.06209320299967658,
          0.0638743110030191,
          0.06838815299852286,
          0.06347569500212558
        ],
        "old_times": [
          0.060318744988762774,
          0.06639547800295986,
          0.06753539099008776,
          0.0653526870009955,
          0.05969989199365955,
          0.061375866003800184,
          0.07734363600320648,
          0.05986920800933149,
          0.06887436199758667,
          0.0665152820001822,
          0.06330563899246044,
          0.06357460899744183,
          0.059429080996778794,
          0.06296892699901946,
          0.06114144700404722,
          0.07002377600292675,
          0.060179470005095936,
          0.05807521899987478,
          0.05683265200059395,
          0.07027324600494467,
          0.07881067199923564,
          0.05757033999543637,
          0.06296342599671334,
          0.06148456899973098,
          0.06309227099700365,
          0.06055496398766991,
          0.07244603900471702,
          0.06318785500479862,
          0.06863825301115867
        ]
      },
      {
        "test_name": "arch.stream_dump",
        "is_significant": false,
        "p_value": 0.9899856653665869,
        "is_pair_significant": false,
        "pair_p_value": 0.9933199177838413,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9885458294302225,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4937961801185684,
        "relative_improvement": -0.00034239452277883567,
        "absolute_improvement_ms": -0.02168486464833308,
        "old_mean_ms": 63.33297761990127,
        "new_mean_ms": 63.3546624845496,
        "old_std_ms": 5.1263813555771405,
        "new_std_ms": 5.262333837898117,
        "effect_size_cohens_d": -0.004174338718665828,
        "old_ci95_ms": [
          61.38300643862597,
          65.28294880117656
        ],
        "new_ci95_ms": [
          61.352977744691636,
          65.35634722440756
        ],
        "old_ci99_ms": [
          60.70250341167002,
          65.96345182813252
        ],
        "new_ci99_ms": [
          60.65442766479044,
          66.05489730430877
        ],
        "new_times": [
          0.06623257100000046,
          0.05797893600538373,
          0.057723726000403985,
          0.05422660299518611,
          0.06395347400393803,
          0.06392339200829156,
          0.06312495200836565,
          0.061544711992610246,
          0.07103438500780612,
          0.060861536010634154,
          0.0690702389983926,
          0.0542917049897369,
          0.06444942300731782,
          0.06180448200029787,
          0.06016525000450201,
          0.06105035300424788,
          0.06388982200587634,
          0.0639405340043595,
          0.06349925599351991,
          0.05554248199041467,
          0.06126057100482285,
          0.061376625002594665,
          0.07344279700191692,
          0.07672368201019708,
          0.062258968988317065,
          0.06215267500374466,
          0.06362058100057766,
          0.06994669300911482,
          0.06819478599936701
        ],
        "old_times": [
          0.0641480720078107,
          0.06314168300013989,
          0.05607653399056289,
          0.05944390200602356,
          0.05483278600149788,
          0.06826305900176521,
          0.055721319004078396,
          0.06178891099989414,
          0.06391580200579483,
          0.06867008398694452,
          0.07806714299658779,
          0.060875795999891125,
          0.06254897099279333,
          0.06908131099771708,
          0.06433512800140306,
          0.06116233799548354,
          0.05488219700055197,
          0.06261159300629515,
          0.062363723991438746,
          0.06169996799144428,
          0.06807334099721629,
          0.07100342398916837,
          0.06157869299931917,
          0.06394507399818394,
          0.06279825999808963,
          0.05970525200245902,
          0.0680045280023478,
          0.06733407301362604,
          0.06058338499860838
        ]
      },
      {
        "test_name": "arch.load_store",
        "is_significant": false,
        "p_value": 0.9824412211446025,
        "is_pair_significant": false,
        "pair_p_value": 0.9828630838516993,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9817322865128517,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6851290682966794,
        "relative_improvement": -0.0035293759160070232,
        "absolute_improvement_ms": -0.23005392998537433,
        "old_mean_ms": 65.18260889750874,
        "new_mean_ms": 65.41266282749412,
        "old_std_ms": 6.292365600839024,
        "new_std_ms": 6.322160445556246,
        "effect_size_cohens_d": -0.03647434512087445,
        "old_ci95_ms": [
          62.789121029046655,
          67.57609676597082
        ],
        "new_ci95_ms": [
          63.007841606220516,
          67.81748404876771
        ],
        "old_ci99_ms": [
          61.95383907456896,
          68.41137872044851
        ],
        "new_ci99_ms": [
          62.168604526165645,
          68.65672112882258
        ],
        "new_times": [
          0.0674771779886214,
          0.05910874900291674,
          0.06330339000851382,
          0.07669743099540938,
          0.061426018000929616,
          0.0607701730041299,
          0.07067956100217998,
          0.07616352099284995,
          0.06643140899541322,
          0.06880117900436744,
          0.061910415010061115,
          0.05494324999745004,
          0.06773908899049275,
          0.07268202700652182,
          0.07010249899758492,
          0.07518283299577888,
          0.07000024399894755,
          0.07127722400764469,
          0.06069372899946757,
          0.07103870499122422,
          0.05943836098595057,
          0.054469961003633216,
          0.0644115420000162,
          0.06446716400387231,
          0.06427165599598084,
          0.06331889001012314,
          0.053480413000215776,
          0.05952306500694249,
          0.0671575460000895
        ],
        "old_times": [
          0.057865782000590116,
          0.0692655970051419,
          0.06466332099807914,
          0.06192961600027047,
          0.061909846001071855,
          0.055066013999748975,
          0.058527996006887406,
          0.06888573300966527,
          0.06853080900327768,
          0.06758542200259399,
          0.058974254003260285,
          0.05929113599995617,
          0.0718181449919939,
          0.06684804400720168,
          0.06286791298771277,
          0.06824894799501635,
          0.08670564400381409,
          0.06293811499199364,
          0.06044312000449281,
          0.06861819200275932,
          0.06738562500686385,
          0.0693306690081954,
          0.05747694699675776,
          0.058950902996002696,
          0.06654909299686551,
          0.0636084209982073,
          0.06945970500237308,
          0.07275474999914877,
          0.06379589800781105
        ]
      },
      {
        "test_name": "arch.constructors",
        "is_significant": false,
        "p_value": 0.9956784116856086,
        "is_pair_significant": false,
        "pair_p_value": 0.9903027532092493,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9878328964114189,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5982502817760561,
        "relative_improvement": -0.006140566367701335,
        "absolute_improvement_ms": -0.38927617173738216,
        "old_mean_ms": 63.39418034546944,
        "new_mean_ms": 63.78345651720682,
        "old_std_ms": 5.269311543711124,
        "new_std_ms": 4.95317502576249,
        "effect_size_cohens_d": -0.07612436603854268,
        "old_ci95_ms": [
          61.38984142826824,
          65.39851926267063
        ],
        "new_ci95_ms": [
          61.899369500242784,
          65.66754353417086
        ],
        "old_ci99_ms": [
          60.69036509072103,
          66.09799560021784
        ],
        "new_ci99_ms": [
          61.24185879936468,
          66.32505423504897
        ],
        "new_times": [
          0.06241159500495996,
          0.06873683699814137,
          0.07392516499385238,
          0.056561741992481984,
          0.06433841900434345,
          0.05679092099308036,
          0.0547697430010885,
          0.06742802700318862,
          0.06206194199330639,
          0.06702647100610193,
          0.05680266101262532,
          0.06083082398981787,
          0.06723729999794159,
          0.06000835300073959,
          0.06684268399840221,
          0.07224805100122467,
          0.06527844398806337,
          0.06638645700877532,
          0.06516209000255913,
          0.06409677900956012,
          0.07099336398823652,
          0.06334939099906478,
          0.05755521000537556,
          0.06422472400299739,
          0.06463315000291914,
          0.0637074650003342,
          0.06899966699711513,
          0.059362008003517985,
          0.05795075499918312
        ],
        "old_times": [
          0.059296626001014374,
          0.05896722299803514,
          0.06557809599325992,
          0.07411860200227238,
          0.06612678700184915,
          0.07342684600735083,
          0.06512270899838768,
          0.06221785799425561,
          0.0661700790078612,
          0.05954484599351417,
          0.06882067999686114,
          0.054896668007131666,
          0.061168547996203415,
          0.05699380899022799,
          0.06497734300501179,
          0.06844377600646112,
          0.06334372100536712,
          0.0648366669920506,
          0.05793494499812368,
          0.06661389500368387,
          0.05801155700464733,
          0.0638170090096537,
          0.05322631400485989,
          0.059965441992972046,
          0.06288979300006758,
          0.07005862699588761,
          0.0654046600102447,
          0.05709058199136052,
          0.06936752200999763
        ]
      },
      {
        "test_name": "arch.static_builders",
        "is_significant": false,
        "p_value": 0.9847008591492922,
        "is_pair_significant": false,
        "pair_p_value": 0.9699205904676943,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9943391662091017,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5433428684028868,
        "relative_improvement": 0.0038263801276742604,
        "absolute_improvement_ms": 0.24598269374109805,
        "old_mean_ms": 64.28600544991083,
        "new_mean_ms": 64.04002275616973,
        "old_std_ms": 4.582102652474791,
        "new_std_ms": 5.725296768215684,
        "effect_size_cohens_d": 0.04743846219334849,
        "old_ci95_ms": [
          62.54306681499046,
          66.02894408483122
        ],
        "new_ci95_ms": [
          61.862236335710925,
          66.21780917662856
        ],
        "old_ci99_ms": [
          61.93481422733627,
          66.63719667248542
        ],
        "new_ci99_ms": [
          61.10223010309749,
          66.97781540924198
        ],
        "new_times": [
          0.06340217300748918,
          0.06737000499560963,
          0.0505146909999894,
          0.06570948199077975,
          0.06677352199039888,
          0.07122406200505793,
          0.07226917200023308,
          0.057204946991987526,
          0.06639857699337881,
          0.06554704500013031,
          0.06523293300415389,
          0.05915377999190241,
          0.06651114199485164,
          0.06869186599215027,
          0.06872176600154489,
          0.06334233099187259,
          0.07621140299306717,
          0.0620389199903002,
          0.05583176399522927,
          0.06060951700783335,
          0.061567862998344935,
          0.0552010999963386,
          0.056521639999118634,
          0.0651008080021711,
          0.0625242399983108,
          0.06050138300633989,
          0.07146214098611381,
          0.06304280000040308,
          0.06847958700382151
        ],
        "old_times": [
          0.05964494000363629,
          0.0654138500103727,
          0.06970101399929263,
          0.06335641100304201,
          0.07093013101257384,
          0.06180829199729487,
          0.06765728601021692,
          0.061953738011652604,
          0.059097478006151505,
          0.06360799100366421,
          0.07036277900624555,
          0.06179691200668458,
          0.06605858499824535,
          0.06590475799748674,
          0.06750042000203393,
          0.061042942994390614,
          0.05872289399849251,
          0.06141540699172765,
          0.0762196729920106,
          0.062365002988372,
          0.06265771499602124,
          0.05741787400620524,
          0.07038946999819018,
          0.06198366900207475,
          0.06883713998831809,
          0.06137309501355048,
          0.05646071799856145,
          0.06580294499872252,
          0.06481102701218333
        ]
      },
      {
        "test_name": "arch.access_operator",
        "is_significant": false,
        "p_value": 0.9495059964356437,
        "is_pair_significant": false,
        "pair_p_value": 0.9489914642826995,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9313350468873978,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3093683709254245,
        "relative_improvement": 0.013030322755295335,
        "absolute_improvement_ms": 0.8617685851841778,
        "old_mean_ms": 66.135628515722,
        "new_mean_ms": 65.27385993053782,
        "old_std_ms": 6.167634846657209,
        "new_std_ms": 5.290794259382928,
        "effect_size_cohens_d": 0.14997804750183744,
        "old_ci95_ms": [
          63.789585688742974,
          68.48167134270105
        ],
        "new_ci95_ms": [
          63.26134942531968,
          67.28637043575598
        ],
        "old_ci99_ms": [
          62.970861155550935,
          69.30039587589309
        ],
        "new_ci99_ms": [
          62.559021358252586,
          67.98869850282307
        ],
        "new_times": [
          0.06039636899367906,
          0.06202507000125479,
          0.07073612300155219,
          0.0746663830068428,
          0.051790259007248096,
          0.0659169390128227,
          0.06304190900118556,
          0.05852894700365141,
          0.05986512800154742,
          0.06721124799514655,
          0.0726027049968252,
          0.06523616300546564,
          0.06911185100034345,
          0.07013552999706008,
          0.06527062499662861,
          0.07212854600220453,
          0.06817635599873029,
          0.0691971240012208,
          0.07105585499084555,
          0.06170922800083645,
          0.06395475400495343,
          0.0551838689862052,
          0.06371574499644339,
          0.06311133199778851,
          0.07060357798764016,
          0.0673321429931093,
          0.06216285499976948,
          0.06460125900048297,
          0.06347404500411358
        ],
        "old_times": [
          0.06199933899915777,
          0.06336382200242952,
          0.06697731900203507,
          0.07399520800390746,
          0.06343987399304751,
          0.0782677910028724,
          0.07410985200840514,
          0.07108700700337067,
          0.05558693499187939,
          0.06725264000124298,
          0.06815552499028854,
          0.06764409599418286,
          0.05543596899951808,
          0.07326227000157814,
          0.056019130992353894,
          0.06270008599676657,
          0.06593165900267195,
          0.06982036799308844,
          0.06234198299353011,
          0.07155929500004277,
          0.07285419400432147,
          0.06699511098850053,
          0.06125983099627774,
          0.0735395410010824,
          0.06444016299792565,
          0.06176563000190072,
          0.06996942299883813,
          0.056010319996858016,
          0.06214884499786422
        ]
      },
      {
        "test_name": "arch.arithmetic",
        "is_significant": false,
        "p_value": 0.8686224824746993,
        "is_pair_significant": false,
        "pair_p_value": 0.9295163562781366,
        "is_binom_significant": false,
        "binom_p_value": 0.644464448094368,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8851463589817286,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.09054416765406514,
        "relative_improvement": 0.02529063865628423,
        "absolute_improvement_ms": 1.625982827731756,
        "old_mean_ms": 64.29188482860778,
        "new_mean_ms": 62.66590200087602,
        "old_std_ms": 5.567967171896643,
        "new_std_ms": 5.404209634959129,
        "effect_size_cohens_d": 0.29634990267814665,
        "old_ci95_ms": [
          62.17394338602015,
          66.40982627119539
        ],
        "new_ci95_ms": [
          60.61025059400674,
          64.72155340774529
        ],
        "old_ci99_ms": [
          61.43482191774607,
          67.14894773946948
        ],
        "new_ci99_ms": [
          59.89286716897277,
          65.43893683277926
        ],
        "new_times": [
          0.05560200499894563,
          0.060528352987603284,
          0.0622378080006456,
          0.06505524599924684,
          0.06136843499552924,
          0.06272222699772101,
          0.06248058800701983,
          0.06201368999609258,
          0.06194564700126648,
          0.0639776250027353,
          0.06404690800991375,
          0.06233248200442176,
          0.05796725500840694,
          0.06709070400393102,
          0.06252980900171679,
          0.06828636900172569,
          0.05605586199089885,
          0.056404835006105714,
          0.05847467501007486,
          0.06371340500481892,
          0.06303196899534669,
          0.06269584600522649,
          0.05773960700025782,
          0.07830710300186183,
          0.07756457399227656,
          0.06543865001003724,
          0.05501757300226018,
          0.06412042998999823,
          0.05856147799931932
        ],
        "old_times": [
          0.05893670300429221,
          0.05723264699918218,
          0.07360863299982157,
          0.05827731700264849,
          0.06141103699337691,
          0.06443397300608922,
          0.06251403900387231,
          0.06705451199377421,
          0.05619954700523522,
          0.06935810099821538,
          0.06840979399567004,
          0.07623627300199587,
          0.055976759002078325,
          0.07189684800687246,
          0.06474986400280613,
          0.06330652900214773,
          0.05928032600786537,
          0.06285905200638808,
          0.06195507700613234,
          0.059736063005402684,
          0.05909527798939962,
          0.06684356401092373,
          0.06558809599664528,
          0.06761353400361259,
          0.0726109249953879,
          0.07151044299826026,
          0.05762061200221069,
          0.06410798999422695,
          0.06604112399509177
        ]
      },
      {
        "test_name": "arch.saturated_arithmetic",
        "is_significant": false,
        "p_value": 0.9997673618981392,
        "is_pair_significant": false,
        "pair_p_value": 0.9982148322920469,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.99824458360672,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7279088040315412,
        "relative_improvement": -0.011646368831617623,
        "absolute_improvement_ms": -0.7134123779592507,
        "old_mean_ms": 61.25620682924536,
        "new_mean_ms": 61.96961920720461,
        "old_std_ms": 4.02365058459879,
        "new_std_ms": 3.910744642545639,
        "effect_size_cohens_d": -0.17980958565262456,
        "old_ci95_ms": [
          59.7256920003099,
          62.78672165818083
        ],
        "new_ci95_ms": [
          60.482051501771544,
          63.457186912637674
        ],
        "old_ci99_ms": [
          59.19157129950527,
          63.32084235898544
        ],
        "new_ci99_ms": [
          59.96291853402836,
          63.97631988038087
        ],
        "new_times": [
          0.06273559700639453,
          0.06189984601223841,
          0.06360358100209851,
          0.06484061799710616,
          0.06281207100255415,
          0.06651065200276207,
          0.05483195499982685,
          0.06179953100217972,
          0.0649750629963819,
          0.06046336000144947,
          0.059264424999128096,
          0.06352117699861992,
          0.06240685499506071,
          0.06237351399613544,
          0.05579797200334724,
          0.057516738001140766,
          0.06104236299870536,
          0.062963635995402,
          0.07189317799929995,
          0.05743561498820782,
          0.061850423997384496,
          0.054369267003494315,
          0.06266272399807349,
          0.06232164100219961,
          0.062064732002909295,
          0.06736059499962721,
          0.06632815499324352,
          0.05656866201024968,
          0.06490501000371296
        ],
        "old_times": [
          0.06879486900288612,
          0.05582171300193295,
          0.06197274899750482,
          0.06339353299699724,
          0.06222888801130466,
          0.056269330001669005,
          0.062178356005460955,
          0.05956184701062739,
          0.05747518700081855,
          0.06250068800000008,
          0.06312011199770495,
          0.055995640010223724,
          0.0640561079926556,
          0.06010308700206224,
          0.07046569300291594,
          0.058855698996922,
          0.06329520900908392,
          0.06567075000202749,
          0.062780328997178,
          0.06359335999877658,
          0.06035839600372128,
          0.06389000099443365,
          0.05520969899953343,
          0.05198381700029131,
          0.06513519000145607,
          0.060323904996039346,
          0.059078698002849706,
          0.06242426601238549,
          0.05989287899865303
        ]
      },
      {
        "test_name": "arch.computed_assignment",
        "is_significant": false,
        "p_value": 0.9979410472215448,
        "is_pair_significant": false,
        "pair_p_value": 0.9971751369608464,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9988299291580915,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6397085369195689,
        "relative_improvement": -0.015427268134716262,
        "absolute_improvement_ms": -0.9924278299500849,
        "old_mean_ms": 64.32946010167582,
        "new_mean_ms": 65.32188793162591,
        "old_std_ms": 4.941145999717436,
        "new_std_ms": 5.924675314128695,
        "effect_size_cohens_d": -0.1819258889483694,
        "old_ci95_ms": [
          62.44994868151189,
          66.20897152183976
        ],
        "new_ci95_ms": [
          63.0682619682755,
          67.57551389497632
        ],
        "old_ci99_ms": [
          61.794034777295444,
          66.8648854260562
        ],
        "new_ci99_ms": [
          62.281789170927034,
          68.36198669232479
        ],
        "new_times": [
          0.059030215998063795,
          0.06461104900517967,
          0.06590945900825318,
          0.06368337299500126,
          0.06599696200282779,
          0.06455075700068846,
          0.0656870609964244,
          0.06350141699658707,
          0.06653302299673669,
          0.0631051620002836,
          0.06627911300165579,
          0.07369203599228058,
          0.06471203299588524,
          0.07088593899970874,
          0.06700547999935225,
          0.06998490401019808,
          0.061116034994483925,
          0.06932642000901978,
          0.06885234099172521,
          0.06521378199977335,
          0.05890349100809544,
          0.059179592004511505,
          0.05793513399839867,
          0.06204663999960758,
          0.05470058000355493,
          0.06366229298873805,
          0.06894495400774758,
          0.08707456701085903,
          0.06221093700150959
        ],
        "old_times": [
          0.060808994006947614,
          0.06713764599408023,
          0.06895948501187377,
          0.06004670500988141,
          0.06774258900259156,
          0.06702200100698974,
          0.0620486109983176,
          0.054026094992877916,
          0.06294106499990448,
          0.07067144100437872,
          0.06025090199545957,
          0.06722621999506373,
          0.06605091399978846,
          0.05908580799587071,
          0.06850448799377773,
          0.05677761000697501,
          0.06566878999001347,
          0.07346448699536268,
          0.06448097398970276,
          0.06331190999480896,
          0.06378439799300395,
          0.0751414419937646,
          0.05405720599810593,
          0.06329268899571616,
          0.0660592639906099,
          0.0652411830087658,
          0.06182236199674662,
          0.06527826399542391,
          0.0646507999917958
        ]
      },
      {
        "test_name": "arch.comparison",
        "is_significant": false,
        "p_value": 0.9918074167928822,
        "is_pair_significant": false,
        "pair_p_value": 0.9904215176437284,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9910575468093157,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4077760871187119,
        "relative_improvement": 0.007109084262658503,
        "absolute_improvement_ms": 0.4582729307003319,
        "old_mean_ms": 64.46300448392164,
        "new_mean_ms": 64.0047315532213,
        "old_std_ms": 3.1946535146175417,
        "new_std_ms": 5.144236288107407,
        "effect_size_cohens_d": 0.1070261244938958,
        "old_ci95_ms": [
          63.24782327605329,
          65.67818569179
        ],
        "new_ci95_ms": [
          62.047968718835826,
          65.96149438760679
        ],
        "old_ci99_ms": [
          62.82374803935488,
          66.1022609284884
        ],
        "new_ci99_ms": [
          61.36509553351861,
          66.64436757292401
        ],
        "new_times": [
          0.06869827599439304,
          0.07238568599859718,
          0.06646298999839928,
          0.06563357799313962,
          0.06015112900058739,
          0.07577999599743634,
          0.0689402239950141,
          0.061970918002771214,
          0.06440987100359052,
          0.0637238550116308,
          0.06503325499943458,
          0.06331975001376122,
          0.067751100010355,
          0.060721250003552996,
          0.0670141010050429,
          0.05481006500485819,
          0.07049716499750502,
          0.06326471800275613,
          0.06497239299642388,
          0.057573450001655146,
          0.0633325109956786,
          0.05378043500240892,
          0.06109151500277221,
          0.06205657200189307,
          0.056743537992588244,
          0.06553970499953721,
          0.07004815700929612,
          0.06297480600187555,
          0.05745620600646362
        ],
        "old_times": [
          0.06496297199919354,
          0.06937159100198187,
          0.06550725299166515,
          0.0630609200015897,
          0.061915896003483795,
          0.06494824199762661,
          0.0633920219988795,
          0.06347162500605918,
          0.0689064629987115,
          0.06296453600225504,
          0.0610261029942194,
          0.06293331600318197,
          0.0694648640055675,
          0.06067666900344193,
          0.06499060300120618,
          0.06631651399948169,
          0.06219119700836018,
          0.06133674399461597,
          0.06550702400272712,
          0.059590748001937754,
          0.06537232799746562,
          0.061736400006338954,
          0.06757303200720344,
          0.06887138199817855,
          0.06545100099174306,
          0.06556880500284024,
          0.05701431901252363,
          0.06490066098922398,
          0.07040390001202468
        ]
      },
      {
        "test_name": "arch.logical",
        "is_significant": false,
        "p_value": 0.9472323615226523,
        "is_pair_significant": false,
        "pair_p_value": 0.927046434622562,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8808706644922495,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3778904498491375,
        "relative_improvement": 0.001992211497798449,
        "absolute_improvement_ms": 0.13149658806123343,
        "old_mean_ms": 66.00533538058416,
        "new_mean_ms": 65.87383879252293,
        "old_std_ms": 6.296230390221711,
        "new_std_ms": 8.450898258291831,
        "effect_size_cohens_d": 0.017646161900983396,
        "old_ci95_ms": [
          63.61037742486263,
          68.4002933363057
        ],
        "new_ci95_ms": [
          62.65928901236842,
          69.08838857267743
        ],
        "old_ci99_ms": [
          62.77458243776191,
          69.23608832340642
        ],
        "new_ci99_ms": [
          61.537471994403525,
          70.21020559064232
        ],
        "new_times": [
          0.06939970199891832,
          0.062174085003789514,
          0.06501929499791004,
          0.06240072500077076,
          0.06613730799290352,
          0.056823810999048874,
          0.05546025899820961,
          0.08803749400249217,
          0.07269130799977574,
          0.08993353700498119,
          0.06818091600143816,
          0.0666297459974885,
          0.05928700600634329,
          0.06173793900234159,
          0.06017255900951568,
          0.07049752399325371,
          0.05663003399968147,
          0.07124076300533488,
          0.07103721500607207,
          0.0733069819980301,
          0.06299359699187335,
          0.05578701199556235,
          0.05879969699890353,
          0.0709179309924366,
          0.06565032900834922,
          0.06865866399311926,
          0.05459150599199347,
          0.063792917993851,
          0.0623514629987767
        ],
        "old_times": [
          0.059008014999562874,
          0.05935700799454935,
          0.07413030399766285,
          0.06439456100633834,
          0.06488457998784725,
          0.06842787501227576,
          0.07506991899572313,
          0.05894688300031703,
          0.05892741199932061,
          0.07020421299966983,
          0.05825347700738348,
          0.07019795299856924,
          0.07735674700234085,
          0.06815434400050435,
          0.05969891099084634,
          0.08101751600042917,
          0.06546145200263709,
          0.07408614200539887,
          0.06229822100431193,
          0.05987820899463259,
          0.06142977801209781,
          0.06316056400828529,
          0.06381658899772447,
          0.06186289399920497,
          0.06455053600075189,
          0.060414919003960676,
          0.06967794299998786,
          0.07309271300619002,
          0.06639504800841678
        ]
      },
      {
        "test_name": "arch.min_max",
        "is_significant": false,
        "p_value": 0.9526006309076682,
        "is_pair_significant": false,
        "pair_p_value": 0.9750426859554844,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9620055668056011,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5,
        "relative_improvement": 0.012940893341427449,
        "absolute_improvement_ms": 0.8628735497417245,
        "old_mean_ms": 66.67805127327827,
        "new_mean_ms": 65.81517772353655,
        "old_std_ms": 6.677114160290901,
        "new_std_ms": 4.568120441672439,
        "effect_size_cohens_d": 0.15083498828404238,
        "old_ci95_ms": [
          64.13821287977144,
          69.2178896667851
        ],
        "new_ci95_ms": [
          64.07755763716415,
          67.55279780990895
        ],
        "old_ci99_ms": [
          63.251857362645296,
          70.10424518391126
        ],
        "new_ci99_ms": [
          63.47116112226665,
          68.15919432480645
        ],
        "new_times": [
          0.06257627200102434,
          0.06575421299203299,
          0.05798100600077305,
          0.06913214200176299,
          0.06909238098887727,
          0.06151210000098217,
          0.06161209499987308,
          0.06547980199684389,
          0.061213690001750365,
          0.06118617799074855,
          0.06287076299486216,
          0.06613915800699033,
          0.06444599300448317,
          0.0631296729989117,
          0.0634517050057184,
          0.06478589499602094,
          0.062077612994471565,
          0.06341108299966436,
          0.0629944670072291,
          0.06424178500310518,
          0.06639517800067551,
          0.070814035992953,
          0.07342828600667417,
          0.0799363149999408,
          0.0715222239959985,
          0.06571465100569185,
          0.07052036499953829,
          0.06718516799446661,
          0.07003591700049583
        ],
        "old_times": [
          0.0681402139889542,
          0.07037749000301119,
          0.06385632000456098,
          0.0620580719987629,
          0.07263928599422798,
          0.05893856199691072,
          0.06513840999105014,
          0.058076418994460255,
          0.06907511998724658,
          0.06116363700130023,
          0.06120029899466317,
          0.06801538899890147,
          0.06256883099558763,
          0.07174248198862188,
          0.059765433994471096,
          0.06516334999469109,
          0.06887408198963385,
          0.06394842400914058,
          0.07623301299463492,
          0.060577165000722744,
          0.0714867719943868,
          0.06450123499962501,
          0.06075404198782053,
          0.08864673800417222,
          0.07129286399867851,
          0.07532990899926517,
          0.05968810101330746,
          0.0716174170083832,
          0.06279440999787766
        ]
      },
      {
        "test_name": "arch.fused_operations",
        "is_significant": false,
        "p_value": 0.8698803093164152,
        "is_pair_significant": false,
        "pair_p_value": 0.8632390640337656,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8719960395246744,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.09839386151494528,
        "relative_improvement": 0.023848403291773304,
        "absolute_improvement_ms": 1.5348569672667123,
        "old_mean_ms": 64.35889851779612,
        "new_mean_ms": 62.82404155052941,
        "old_std_ms": 5.164982893462012,
        "new_std_ms": 6.27351578699174,
        "effect_size_cohens_d": 0.2671154111323937,
        "old_ci95_ms": [
          62.3942440967836,
          66.32355293880865
        ],
        "new_ci95_ms": [
          60.43772376779157,
          65.21035933326726
        ],
        "old_ci99_ms": [
          61.70861689713081,
          67.00918013846143
        ],
        "new_ci99_ms": [
          59.60494403749325,
          66.04313906356558
        ],
        "new_times": [
          0.06438744100159965,
          0.06347020600514952,
          0.05237134099297691,
          0.07098780298838392,
          0.05646270800207276,
          0.07071365200681612,
          0.060107777011580765,
          0.06703337200451642,
          0.05985638800484594,
          0.06493194198992569,
          0.05409448700083885,
          0.07744780900247861,
          0.07629069598624483,
          0.07036423899990041,
          0.06918378399859648,
          0.06273924699053168,
          0.05307384800107684,
          0.059562326001469046,
          0.05569040800037328,
          0.059722001999034546,
          0.0635412580013508,
          0.061158266995335,
          0.058900210991851054,
          0.0630846309941262,
          0.06394909399386961,
          0.062992517996463,
          0.059225973003776744,
          0.05948182300198823,
          0.061071953998180106
        ],
        "old_times": [
          0.06495482199534308,
          0.06332073899102397,
          0.06383353999990504,
          0.06869881600141525,
          0.05879371700575575,
          0.06720697799755726,
          0.06264201400335878,
          0.07300948099873494,
          0.0632386060024146,
          0.07534550900163595,
          0.07003431700286455,
          0.06417483299446758,
          0.06762080400949344,
          0.05915060099505354,
          0.05437263799831271,
          0.060823284002253786,
          0.06621377999545075,
          0.05520168899965938,
          0.07114490900130477,
          0.07361115301318932,
          0.06376834699767642,
          0.0577372960106004,
          0.06620951100194361,
          0.06398916599573568,
          0.06217074599408079,
          0.06000116300128866,
          0.0648422880040016,
          0.06058880500495434,
          0.06370850499661174
        ]
      },
      {
        "test_name": "arch.abs",
        "is_significant": false,
        "p_value": 0.9918256141796499,
        "is_pair_significant": false,
        "pair_p_value": 0.9896162861737068,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9862954132258892,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5,
        "relative_improvement": 0.002811655814014209,
        "absolute_improvement_ms": 0.1789214498973679,
        "old_mean_ms": 63.63561606849544,
        "new_mean_ms": 63.45669461859807,
        "old_std_ms": 5.140059916300942,
        "new_std_ms": 4.334071635609384,
        "effect_size_cohens_d": 0.03763458108735083,
        "old_ci95_ms": [
          61.68044184098487,
          65.590790296006
        ],
        "new_ci95_ms": [
          61.808101936778456,
          65.10528730041767
        ],
        "old_ci99_ms": [
          60.998123049392746,
          66.27310908759813
        ],
        "new_ci99_ms": [
          61.23277430067361,
          65.68061493652253
        ],
        "new_times": [
          0.06185142400499899,
          0.06256947100337129,
          0.05718506599077955,
          0.0647236829972826,
          0.06846446599229239,
          0.06387394099147059,
          0.06551171299361158,
          0.07627079400117509,
          0.06379318800463807,
          0.059748872998170555,
          0.06594687099277508,
          0.06505940700299107,
          0.054183410000405274,
          0.05635823399643414,
          0.061891085002571344,
          0.06752263999078423,
          0.06321654599742033,
          0.06426382600329816,
          0.06386994999775197,
          0.0659165589895565,
          0.05791683300049044,
          0.06240478499967139,
          0.062298600998474285,
          0.06932604899338912,
          0.06346340499294456,
          0.06278217000362929,
          0.05845881400455255,
          0.0658976880076807,
          0.06547465198673308
        ],
        "old_times": [
          0.06466682199970819,
          0.05486366699915379,
          0.06565719799255021,
          0.0631745150021743,
          0.0747247860126663,
          0.06831846099521499,
          0.0546730289934203,
          0.06648544099880382,
          0.06081951399391983,
          0.0643201390048489,
          0.06404863699572161,
          0.06988666999677662,
          0.05979428499995265,
          0.06769990800239611,
          0.07407350999710616,
          0.06693087800522335,
          0.06284534200676717,
          0.06370448500092607,
          0.06322186600300483,
          0.06022541198763065,
          0.06089277799765114,
          0.061194288995466195,
          0.062294670002302155,
          0.06138626500614919,
          0.06130276300245896,
          0.07002721600292716,
          0.06677763200423215,
          0.05804092799371574,
          0.053381759993499145
        ]
      },
      {
        "test_name": "arch.horizontal_operations",
        "is_significant": false,
        "p_value": 0.9594275284247742,
        "is_pair_significant": false,
        "pair_p_value": 0.9737892968708592,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9836883824318647,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.17943339848682827,
        "relative_improvement": 0.009818952962620734,
        "absolute_improvement_ms": 0.6399230358134639,
        "old_mean_ms": 65.17222745129277,
        "new_mean_ms": 64.5323044154793,
        "old_std_ms": 4.561064965261263,
        "new_std_ms": 6.625235045918547,
        "effect_size_cohens_d": 0.11251242688337296,
        "old_ci95_ms": [
          63.43729112461706,
          66.90716377796848
        ],
        "new_ci95_ms": [
          62.01219978169874,
          67.05240904925986
        ],
        "old_ci99_ms": [
          62.83183119104689,
          67.51262371153864
        ],
        "new_ci99_ms": [
          61.13273097313316,
          67.93187785782546
        ],
        "new_times": [
          0.06521962200349662,
          0.06613441799709108,
          0.061916926002595574,
          0.05766580399358645,
          0.05743026499112602,
          0.059000314999138936,
          0.06568393000634387,
          0.06054940400645137,
          0.06368964300781954,
          0.06547942200268153,
          0.05754410900408402,
          0.060808384005213156,
          0.063985335000325,
          0.06588427799579222,
          0.06513185900985263,
          0.06321113600279205,
          0.06505202599510085,
          0.0642983279976761,
          0.06341972399968654,
          0.07033319800393656,
          0.0948692050005775,
          0.07046604300558101,
          0.0635961399966618,
          0.06298457700177096,
          0.062113263993524015,
          0.06354272800672334,
          0.062453177000861615,
          0.06463463000545744,
          0.06433893801295199
        ],
        "old_times": [
          0.06085358600830659,
          0.06463850100408308,
          0.060906377009814605,
          0.060893517002114095,
          0.06213037400448229,
          0.054983282010653056,
          0.06335721100913361,
          0.06341550400247797,
          0.06383990999893285,
          0.06812144299328793,
          0.07037895001121797,
          0.06500006400165148,
          0.062172716003260575,
          0.06796721700811759,
          0.06864756300637964,
          0.0607487210072577,
          0.06455455700051971,
          0.07324383899685927,
          0.07032974700268824,
          0.06537021799886134,
          0.0686444430029951,
          0.0678734139946755,
          0.07403853899450041,
          0.06362760200863704,
          0.058873570000287145,
          0.0638257690006867,
          0.06379363800806459,
          0.06349225599842612,
          0.07427206799911801
        ]
      },
      {
        "test_name": "arch.boolean_conversions",
        "is_significant": false,
        "p_value": 0.3796466593731416,
        "is_pair_significant": false,
        "pair_p_value": 0.3917961146157739,
        "is_binom_significant": false,
        "binom_p_value": 0.5,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.6008620504289865,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 0.017215978975605385,
        "relative_improvement": 0.056717723813081065,
        "absolute_improvement_ms": 3.7609483466628095,
        "old_mean_ms": 66.30993089668758,
        "new_mean_ms": 62.54898255002477,
        "old_std_ms": 6.308452081352902,
        "new_std_ms": 4.975787017915513,
        "effect_size_cohens_d": 0.6619837714183326,
        "old_ci95_ms": [
          63.91032405828658,
          68.70953773508857
        ],
        "new_ci95_ms": [
          60.656294391243286,
          64.44167070880624
        ],
        "old_ci99_ms": [
          63.072906699139686,
          69.54695509423546
        ],
        "new_ci99_ms": [
          59.99578205470089,
          65.10218304534864
        ],
        "new_times": [
          0.0534895540040452,
          0.055842183995991945,
          0.06554306400357746,
          0.0637685769906966,
          0.06690294700092636,
          0.06555994600057602,
          0.05950219399528578,
          0.05835895000200253,
          0.0586529319989495,
          0.059533214996918105,
          0.06605642499926034,
          0.06275094799639191,
          0.06278446900250856,
          0.06160629399528261,
          0.06688572700659279,
          0.06469345299410634,
          0.06151691099512391,
          0.06969452399061993,
          0.060560723999515176,
          0.05370859299728181,
          0.058857119001913816,
          0.05845916399266571,
          0.06895581600838341,
          0.05449364199012052,
          0.0721253660012735,
          0.06720397899334785,
          0.06705085199791938,
          0.06103173299925402,
          0.068331192000187
        ],
        "old_times": [
          0.057244987998274155,
          0.06736364400421735,
          0.062421205002465285,
          0.06758558300498407,
          0.06546622198948171,
          0.0615509320050478,
          0.05961940799898002,
          0.061831662998883985,
          0.06403568798850756,
          0.06360224999662023,
          0.0699385030020494,
          0.06292622500041034,
          0.0647770849900553,
          0.07403978001093492,
          0.06362237200664822,
          0.06548248299804982,
          0.0653783189918613,
          0.06392072199378163,
          0.07518509301007725,
          0.0899516570061678,
          0.06522656200104393,
          0.06341595300182234,
          0.060183940004208125,
          0.06979010699433275,
          0.07541613200737629,
          0.06242835499870125,
          0.06782821199158207,
          0.06913404200167861,
          0.06362087100569624
        ]
      },
      {
        "test_name": "arch.iterator",
        "is_significant": false,
        "p_value": 0.9651105544538467,
        "is_pair_significant": false,
        "pair_p_value": 0.9667883263415875,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9772401507943869,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3719938302415021,
        "relative_improvement": 0.008812017402624672,
        "absolute_improvement_ms": 0.5735103432329969,
        "old_mean_ms": 65.08275199980586,
        "new_mean_ms": 64.50924165657287,
        "old_std_ms": 6.365799355976135,
        "new_std_ms": 4.9297312342770985,
        "effect_size_cohens_d": 0.1007355605895348,
        "old_ci95_ms": [
          62.661331424758394,
          67.50417257485334
        ],
        "new_ci95_ms": [
          62.63407218097096,
          66.38441113217479
        ],
        "old_ci99_ms": [
          61.81630148448226,
          68.34920251512948
        ],
        "new_ci99_ms": [
          61.9796735332084,
          67.03880977993735
        ],
        "new_times": [
          0.06782540299172979,
          0.06337612200877629,
          0.06379255800857209,
          0.057037279999349266,
          0.05654254100227263,
          0.06625006199465133,
          0.058785536006325856,
          0.06958120901254006,
          0.06490682098956313,
          0.06562380801187828,
          0.060942380005144514,
          0.06636083600460552,
          0.06504644600499887,
          0.0655104140023468,
          0.07179830399400089,
          0.06547647199477069,
          0.06160508499306161,
          0.06543748099647928,
          0.059430891007650644,
          0.06230169199989177,
          0.05743440399237443,
          0.07338907501252834,
          0.06037261699384544,
          0.06400073600525502,
          0.061767759994836524,
          0.07173240100382827,
          0.06716079701436684,
          0.07660418799787294,
          0.06067468899709638
        ],
        "old_times": [
          0.06099642100161873,
          0.0675832130073104,
          0.06122909000259824,
          0.06412368999735918,
          0.05913617000624072,
          0.05877523600065615,
          0.07480747799854726,
          0.07192409900017083,
          0.06608863599831238,
          0.08474688899877947,
          0.054800423997221515,
          0.06440855099936016,
          0.07112113699258771,
          0.06864815299923066,
          0.05923791399982292,
          0.06794863700633869,
          0.05676470999605954,
          0.06525192399567459,
          0.06113737600389868,
          0.06425221600511577,
          0.05924141399736982,
          0.06863364300807007,
          0.06516469999041874,
          0.06665538699598983,
          0.07046817299851682,
          0.054767182999057695,
          0.06359185000474099,
          0.06956882899976335,
          0.06632666499353945
        ]
      },
      {
        "test_name": "arch.any_all",
        "is_significant": false,
        "p_value": 0.999999995112053,
        "is_pair_significant": false,
        "pair_p_value": 0.9999999957805724,
        "is_binom_significant": false,
        "binom_p_value": 0.9999999441206455,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999999385327101,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9996883705122507,
        "relative_improvement": -0.0711162146727581,
        "absolute_improvement_ms": -4.402698207835273,
        "old_mean_ms": 61.9085004466891,
        "new_mean_ms": 66.31119865452436,
        "old_std_ms": 2.831500258099302,
        "new_std_ms": 5.021512379447242,
        "effect_size_cohens_d": -1.0800634504146989,
        "old_ci95_ms": [
          60.83145534994452,
          62.98554554343368
        ],
        "new_ci95_ms": [
          64.40111749841091,
          68.22127981063781
        ],
        "old_ci99_ms": [
          60.45558700094491,
          63.36141389243328
        ],
        "new_ci99_ms": [
          63.73453533507009,
          68.88786197397864
        ],
        "new_times": [
          0.07467811400420032,
          0.06535958799941,
          0.06731931299145799,
          0.07063076000486035,
          0.058467783994274214,
          0.07035347899363842,
          0.0604184389958391,
          0.060945208999328315,
          0.0649322410026798,
          0.05938941999920644,
          0.06993715299176984,
          0.06533704599132761,
          0.06492977199377492,
          0.07682854699669406,
          0.07352077899849974,
          0.06291520500963088,
          0.06758200300100725,
          0.06075742200482637,
          0.06182830200123135,
          0.06954640800540801,
          0.06250249900040217,
          0.0638085980026517,
          0.06510126800276339,
          0.07396549699478783,
          0.07253558099910151,
          0.06558448600117117,
          0.06931492900184821,
          0.060112306993687525,
          0.0644226120057283
        ],
        "old_times": [
          0.0672865609958535,
          0.05746109598840121,
          0.05857034800283145,
          0.06236409400298726,
          0.06059126499167178,
          0.06443321199913044,
          0.06514789999346249,
          0.06257159099914134,
          0.06152480198943522,
          0.06479265600501094,
          0.06416375200205948,
          0.06082450499525294,
          0.06622113099729177,
          0.06328410901187453,
          0.06372554499830585,
          0.059715652008890174,
          0.06425573499291204,
          0.0573896429996239,
          0.05948197298857849,
          0.06295451600453816,
          0.060297403993899934,
          0.060436849991674535,
          0.06327428900112864,
          0.05908729799557477,
          0.06551714400120545,
          0.05584807400009595,
          0.061203529010526836,
          0.06286021300184075,
          0.060061625990783796
        ]
      },
      {
        "test_name": "arch.logical_operations",
        "is_significant": false,
        "p_value": 0.9975187918391876,
        "is_pair_significant": false,
        "pair_p_value": 0.9926776535279561,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9970026835799217,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6683771238396271,
        "relative_improvement": -0.006120238199529716,
        "absolute_improvement_ms": -0.3952003442401347,
        "old_mean_ms": 64.57270638101959,
        "new_mean_ms": 64.96790672525972,
        "old_std_ms": 4.857110058007891,
        "new_std_ms": 4.817403611171057,
        "effect_size_cohens_d": -0.08169858026494166,
        "old_ci95_ms": [
          62.72516052353208,
          66.42025223850709
        ],
        "new_ci95_ms": [
          63.135464392397516,
          66.80034905812192
        ],
        "old_ci99_ms": [
          62.08040199554299,
          67.06501076649619
        ],
        "new_ci99_ms": [
          62.49597670857471,
          67.43983674194473
        ],
        "new_times": [
          0.06052856300084386,
          0.06158425399917178,
          0.055971039007999934,
          0.06587438800488599,
          0.057526509001036175,
          0.07163038800354116,
          0.06641080799454357,
          0.07124921200738754,
          0.07119177099957597,
          0.06511590800073463,
          0.05854771799931768,
          0.07303939099074341,
          0.06831633100227918,
          0.06788765500823501,
          0.06408718900638632,
          0.06347848600125872,
          0.06738412599952426,
          0.07457948000228498,
          0.062873713002773,
          0.058911831001751125,
          0.06155418200069107,
          0.06418939300056081,
          0.061402057006489486,
          0.05883808899670839,
          0.0658353460021317,
          0.06857755099190399,
          0.06326359800004866,
          0.0669862600043416,
          0.06723405899538193
        ],
        "old_times": [
          0.061631445001694374,
          0.06664580700453371,
          0.06649684099829756,
          0.06443929299712181,
          0.0684300249995431,
          0.06034042600367684,
          0.06824314799450804,
          0.07062221900559962,
          0.05804945899581071,
          0.06763305500498973,
          0.06892414401227143,
          0.0648249279911397,
          0.06961938099993858,
          0.06575124200026039,
          0.061062174005201086,
          0.06444320200535003,
          0.0672371600085171,
          0.06185589400411118,
          0.06491034000646323,
          0.061558362998766825,
          0.06671844000811689,
          0.05738302299869247,
          0.08095137399504893,
          0.05913486999634188,
          0.060542614010046236,
          0.061102185005438514,
          0.06433773800381459,
          0.06027672399068251,
          0.05944297100359108
        ]
      },
      {
        "test_name": "arch.bitwise_operations",
        "is_significant": false,
        "p_value": 0.964356292311684,
        "is_pair_significant": false,
        "pair_p_value": 0.9487588960648061,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9827331658452749,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.13142222065826842,
        "relative_improvement": 0.013553105049076631,
        "absolute_improvement_ms": 0.8693376570088895,
        "old_mean_ms": 64.14306196705277,
        "new_mean_ms": 63.27372431004388,
        "old_std_ms": 4.8678391258178575,
        "new_std_ms": 5.048712773756873,
        "effect_size_cohens_d": 0.17530147483033026,
        "old_ci95_ms": [
          62.29143499043264,
          65.99468894367291
        ],
        "new_ci95_ms": [
          61.35329667727484,
          65.19415194281292
        ],
        "old_ci99_ms": [
          61.64525222912678,
          66.64087170497876
        ],
        "new_ci99_ms": [
          60.68310378945252,
          65.86434483063525
        ],
        "new_times": [
          0.05984398699365556,
          0.059637248996295966,
          0.06134837499121204,
          0.06472001399379224,
          0.07088727899827063,
          0.07605752600647975,
          0.07870960800210014,
          0.062141643997165374,
          0.06643036899913568,
          0.05837465099466499,
          0.0618905849987641,
          0.062410365004325286,
          0.05994828099210281,
          0.06233515200437978,
          0.06487877000472508,
          0.06282873099553399,
          0.06383334900601767,
          0.06253880000440404,
          0.06574842300324235,
          0.05789688299410045,
          0.05875205599295441,
          0.063445555002545,
          0.06269239600806031,
          0.059130800000275485,
          0.061253430001670495,
          0.055505740994703956,
          0.06440367200411856,
          0.05972291200305335,
          0.06757140200352296
        ],
        "old_times": [
          0.0808904510049615,
          0.06174123899836559,
          0.0646582710032817,
          0.06390425200515892,
          0.059370859002228826,
          0.06283646199153736,
          0.0628885540063493,
          0.06457045799470507,
          0.07299226900795475,
          0.06371539398969617,
          0.06599595199804753,
          0.06330082900240086,
          0.0634751750039868,
          0.06242861600185279,
          0.06335949100321159,
          0.06341874400095548,
          0.056363584008067846,
          0.06751513000926934,
          0.06026767400908284,
          0.06187162399874069,
          0.058682023009168915,
          0.0629629559989553,
          0.060818024998297915,
          0.07489317300496623,
          0.06469645199831575,
          0.06282457998895552,
          0.06374750600662082,
          0.06089290700037964,
          0.06506614699901547
        ]
      },
      {
        "test_name": "arch.cast",
        "is_significant": false,
        "p_value": 0.989158579651252,
        "is_pair_significant": false,
        "pair_p_value": 0.9877395802696697,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9836883824318647,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.2984914893517004,
        "relative_improvement": 0.014029903110965143,
        "absolute_improvement_ms": 0.890919619544947,
        "old_mean_ms": 63.50148055182593,
        "new_mean_ms": 62.61056093228098,
        "old_std_ms": 4.192190745395073,
        "new_std_ms": 3.348899404291511,
        "effect_size_cohens_d": 0.23482037498462444,
        "old_ci95_ms": [
          61.90685647434953,
          65.09610462930232
        ],
        "new_ci95_ms": [
          61.336707725817135,
          63.88441413874484
        ],
        "old_ci99_ms": [
          61.35036285946963,
          65.65259824418223
        ],
        "new_ci99_ms": [
          60.89215707234067,
          64.32896479222129
        ],
        "new_times": [
          0.060840534992166795,
          0.06232536199968308,
          0.05521001000306569,
          0.06924629599961918,
          0.06460127900936641,
          0.06682391300273594,
          0.061968408001121134,
          0.05701198799943086,
          0.06843574499362148,
          0.06389163200219627,
          0.06442931300261989,
          0.061850213998695835,
          0.05725281800550874,
          0.05994314000417944,
          0.05997355100407731,
          0.06064600800164044,
          0.0643189580005128,
          0.060555625008419156,
          0.06170535799174104,
          0.06532152600993868,
          0.05835988999751862,
          0.06647183001041412,
          0.06425648499862291,
          0.06408073900092859,
          0.06288661400321871,
          0.06307027000002563,
          0.06268298599752598,
          0.06091896799625829,
          0.06662680600129534
        ],
        "old_times": [
          0.05787071101076435,
          0.06169727799715474,
          0.0632408670062432,
          0.06266999499348458,
          0.07102323400613386,
          0.0631291220051935,
          0.061557922992506064,
          0.0658094049867941,
          0.06254682000144385,
          0.06043339001189452,
          0.0678093809983693,
          0.07116737999604084,
          0.0613160929933656,
          0.06726815100410022,
          0.06016427899885457,
          0.059417659998871386,
          0.06454311599372886,
          0.06673738099925686,
          0.05749318799644243,
          0.061727457999950275,
          0.06168913700093981,
          0.05972477199975401,
          0.05626160101382993,
          0.06469658299465664,
          0.07136236700171139,
          0.06477548601105809,
          0.05970591100049205,
          0.06483180799114052,
          0.07087243899877649
        ]
      },
      {
        "test_name": "arch.conj_norm_proj",
        "is_significant": false,
        "p_value": 0.9007486073857807,
        "is_pair_significant": false,
        "pair_p_value": 0.9084345087655885,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8893151227384806,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.156047868507946,
        "relative_improvement": 0.01957793743589803,
        "absolute_improvement_ms": 1.2776801705858127,
        "old_mean_ms": 65.26122451709695,
        "new_mean_ms": 63.983544346511145,
        "old_std_ms": 6.223798628413439,
        "new_std_ms": 5.705159775148665,
        "effect_size_cohens_d": 0.21401269970202727,
        "old_ci95_ms": [
          62.89381813008216,
          67.62863090411176
        ],
        "new_ci95_ms": [
          61.81341762856656,
          66.15367106445574
        ],
        "old_ci99_ms": [
          62.06763811887563,
          68.45481091531829
        ],
        "new_ci99_ms": [
          61.05608448712318,
          66.91100420589912
        ],
        "new_times": [
          0.06288366300577763,
          0.07109842800127808,
          0.07001352499355562,
          0.06213116399885621,
          0.06391115199949127,
          0.0787915599939879,
          0.06677810200199019,
          0.06337136100046337,
          0.06065195800329093,
          0.06342663400573656,
          0.056323633005376905,
          0.059458392002852634,
          0.0737824690004345,
          0.06241650501033291,
          0.06284485200012568,
          0.07360029200208373,
          0.06819603699841537,
          0.06473887400352396,
          0.05859813001006842,
          0.06989185100246686,
          0.06176590999530163,
          0.05954092500905972,
          0.05323803400096949,
          0.06289544299943373,
          0.0633378500060644,
          0.06175778999750037,
          0.058476245001656935,
          0.06467232199793216,
          0.05692968500079587
        ],
        "old_times": [
          0.0709767020016443,
          0.07064737999462523,
          0.06051820301217958,
          0.07206659398798365,
          0.06429267700877972,
          0.06578679400263354,
          0.06300355700659566,
          0.07487040098931175,
          0.06182290200376883,
          0.07352944099693559,
          0.06477544599329121,
          0.06345822500588838,
          0.07142880000174046,
          0.05591835601080675,
          0.07726032199570909,
          0.05930171599902678,
          0.07055892600328662,
          0.07287026499398053,
          0.06393099299748428,
          0.06905186899530236,
          0.0661331470037112,
          0.0642353340081172,
          0.0642280350002693,
          0.05858419799187686,
          0.05595856800209731,
          0.05666075499902945,
          0.05648903899418656,
          0.05662817299889866,
          0.06758869299665093
        ]
      },
      {
        "test_name": "arch.conj_norm_proj_real",
        "is_significant": false,
        "p_value": 0.9999913231809433,
        "is_pair_significant": false,
        "pair_p_value": 0.9999369300407374,
        "is_binom_significant": false,
        "binom_p_value": 0.9999923817813396,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.99993796646595,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9549709570874504,
        "relative_improvement": -0.031548662457479865,
        "absolute_improvement_ms": -1.9808996188783456,
        "old_mean_ms": 62.78870369062824,
        "new_mean_ms": 64.76960330950658,
        "old_std_ms": 4.369751233070317,
        "new_std_ms": 4.145602692566059,
        "effect_size_cohens_d": -0.4650925179311353,
        "old_ci95_ms": [
          61.126539215767394,
          64.45086816548907
        ],
        "new_ci95_ms": [
          63.19270037958186,
          66.3465062394313
        ],
        "old_ci99_ms": [
          60.54647528082159,
          65.03093210043488
        ],
        "new_ci99_ms": [
          62.64239110971476,
          66.8968155092984
        ],
        "new_times": [
          0.06706885299354326,
          0.06103644201357383,
          0.05992835000506602,
          0.060699619993101805,
          0.06717699699220248,
          0.06629237299785018,
          0.061473428999306634,
          0.0653480469918577,
          0.06745602800219785,
          0.061884185008239,
          0.06496357198921032,
          0.06543408999277744,
          0.06963300101051573,
          0.06107832399720792,
          0.07179882399213966,
          0.06804276999901049,
          0.07139483799983282,
          0.05931912700179964,
          0.06701200098905247,
          0.06305070999951568,
          0.05806486899382435,
          0.06566389001091011,
          0.06417345198860858,
          0.05766558399773203,
          0.061404645995935425,
          0.0669797590089729,
          0.06606314401142299,
          0.07437835200107656,
          0.06383321899920702
        ],
        "old_times": [
          0.05657981299737003,
          0.06417869300639722,
          0.06965842199861072,
          0.06384500999411102,
          0.06649384200864006,
          0.06874404700647574,
          0.05580251300125383,
          0.05753999899025075,
          0.05848290500580333,
          0.06893603400385473,
          0.06668443900707643,
          0.0597462730074767,
          0.07073367400153074,
          0.06542332000390161,
          0.059848956996575,
          0.0643912410014309,
          0.06432897900231183,
          0.06230016100744251,
          0.05840931201237254,
          0.059378568999818526,
          0.06682078300218564,
          0.056953175997477956,
          0.0669806689984398,
          0.05997054099861998,
          0.05641277600079775,
          0.06489135998708662,
          0.0637591159902513,
          0.0610764840093907,
          0.06250129899126478
        ]
      },
      {
        "test_name": "arch.boolean_conversion",
        "is_significant": false,
        "p_value": 0.9822677962535549,
        "is_pair_significant": false,
        "pair_p_value": 0.9868746526706708,
        "is_binom_significant": false,
        "binom_p_value": 0.644464448094368,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9655649084597826,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.28779200453139825,
        "relative_improvement": 0.01451619704042331,
        "absolute_improvement_ms": 0.9024780673167615,
        "old_mean_ms": 62.170420035194,
        "new_mean_ms": 61.267941967877235,
        "old_std_ms": 3.931226103750688,
        "new_std_ms": 4.053166803512077,
        "effect_size_cohens_d": 0.22603417608854812,
        "old_ci95_ms": [
          60.67506159857453,
          63.665778471813454
        ],
        "new_ci95_ms": [
          59.72619976971796,
          62.80968416603651
        ],
        "old_ci99_ms": [
          60.15320981313525,
          64.18763025725275
        ],
        "new_ci99_ms": [
          59.188160929601885,
          63.347723006152584
        ],
        "new_times": [
          0.06296248600119725,
          0.0635648890020093,
          0.06392471300205216,
          0.06163899500097614,
          0.05449317200691439,
          0.05445928100380115,
          0.06769392700516619,
          0.05998184299096465,
          0.05968623100488912,
          0.06241078500170261,
          0.06392118299845606,
          0.06143554799200501,
          0.06425622499955352,
          0.05289938100031577,
          0.06102322200604249,
          0.06242104500415735,
          0.05955806600104552,
          0.062240908009698614,
          0.06693332800932694,
          0.06174963900411967,
          0.0631865550094517,
          0.06013375800102949,
          0.06223142800445203,
          0.06412215100135654,
          0.06462298000406008,
          0.06164069499936886,
          0.052147712995065376,
          0.05468233900319319,
          0.06674783100606874
        ],
        "old_times": [
          0.06686754499969538,
          0.06271444600133691,
          0.06849722799961455,
          0.06076231200131588,
          0.05926218500826508,
          0.06213732500327751,
          0.07131856499472633,
          0.06408001900126692,
          0.0566789059957955,
          0.05678390999673866,
          0.06481017700571101,
          0.06583228599629365,
          0.05611221400613431,
          0.062429326004348695,
          0.06534870799805503,
          0.0605763250059681,
          0.0628865729959216,
          0.05561928499082569,
          0.05790405299921986,
          0.06718779700167943,
          0.06025605299510062,
          0.056690947007155046,
          0.06460548900940921,
          0.06201519999012817,
          0.060765171991079114,
          0.06538240901136305,
          0.06060323600831907,
          0.06133641400083434,
          0.06347807600104716
        ]
      },
      {
        "test_name": "arch.isnan",
        "is_significant": false,
        "p_value": 0.9996317054915873,
        "is_pair_significant": false,
        "pair_p_value": 0.9997471294883653,
        "is_binom_significant": false,
        "binom_p_value": 0.9999481420964003,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999796012416482,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7330461243094436,
        "relative_improvement": -0.017221332938770047,
        "absolute_improvement_ms": -1.057872862820032,
        "old_mean_ms": 61.42804779288969,
        "new_mean_ms": 62.48592065570972,
        "old_std_ms": 3.7600444171648486,
        "new_std_ms": 5.049570773237116,
        "effect_size_cohens_d": -0.23763095972799259,
        "old_ci95_ms": [
          59.99780338748473,
          62.858292198294656
        ],
        "new_ci95_ms": [
          60.56516665739275,
          64.40667465402669
        ],
        "old_ci99_ms": [
          59.49867516625206,
          63.357420419527315
        ],
        "new_ci99_ms": [
          59.894859874172695,
          65.07698143724674
        ],
        "new_times": [
          0.05839182098861784,
          0.06199793999257963,
          0.06385736000083853,
          0.06438837099994998,
          0.06314343299891334,
          0.07665825998992659,
          0.06078239300404675,
          0.05749247700441629,
          0.06288629400660284,
          0.05634815299708862,
          0.06049655200331472,
          0.0644864040077664,
          0.05542694899486378,
          0.06320164501084946,
          0.06637525701080449,
          0.06263982399832457,
          0.06172301899641752,
          0.0632148050062824,
          0.06346972500614356,
          0.066647996995016,
          0.06197751799481921,
          0.07674407299782615,
          0.0616502449993277,
          0.056288581006811,
          0.06413728100596927,
          0.05549080000491813,
          0.06396938499528915,
          0.056630284001585096,
          0.061574852996272966
        ],
        "old_times": [
          0.06061712600057945,
          0.0600981169991428,
          0.05853162599669304,
          0.06234174300334416,
          0.06498079298762605,
          0.06173292899620719,
          0.06035813699418213,
          0.06435052900633309,
          0.06256840100104455,
          0.06300158800149802,
          0.057761497009778395,
          0.06092340900795534,
          0.06505597601062618,
          0.056921244991826825,
          0.06519557099090889,
          0.060569175009732135,
          0.056379663990810513,
          0.06348919599258807,
          0.06961489100649487,
          0.06630718399537727,
          0.06703683199884836,
          0.06289516399556305,
          0.061599234002642334,
          0.056328213002416305,
          0.06075516100099776,
          0.054786694003269076,
          0.053845906993956305,
          0.05908194700896274,
          0.06428543699439615
        ]
      },
      {
        "test_name": "arch.sqrt",
        "is_significant": false,
        "p_value": 0.9999182713303484,
        "is_pair_significant": false,
        "pair_p_value": 0.9999245237559522,
        "is_binom_significant": false,
        "binom_p_value": 0.9999923817813396,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999563954770565,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7068808939129148,
        "relative_improvement": -0.011353239856792678,
        "absolute_improvement_ms": -0.6894301748553502,
        "old_mean_ms": 60.725412618043705,
        "new_mean_ms": 61.41484279289906,
        "old_std_ms": 2.811381988729114,
        "new_std_ms": 4.11421976148735,
        "effect_size_cohens_d": -0.19566405093601036,
        "old_ci95_ms": [
          59.65602010169998,
          61.79480513438743
        ],
        "new_ci95_ms": [
          59.8498772915312,
          62.97980829426692
        ],
        "old_ci99_ms": [
          59.28282235838768,
          62.16800287769973
        ],
        "new_ci99_ms": [
          59.30373395824051,
          63.52595162755762
        ],
        "new_times": [
          0.06123673099500593,
          0.06155094200221356,
          0.06277193898858968,
          0.0666804479988059,
          0.063249947008444,
          0.06048268100130372,
          0.05853149700851645,
          0.06025303299247753,
          0.06448638399888296,
          0.062111293998896144,
          0.05900647499947809,
          0.06301478800014593,
          0.06137710600160062,
          0.05957526700512972,
          0.061105315005988814,
          0.06022891099564731,
          0.06323484600579832,
          0.06020905099285301,
          0.05948485300177708,
          0.05676424900593702,
          0.05572474899236113,
          0.061680526996497065,
          0.06253196900070179,
          0.07453754800371826,
          0.0678555630001938,
          0.051528808995499276,
          0.06406130899267737,
          0.056412974998238496,
          0.06134123500669375
        ],
        "old_times": [
          0.0587700549949659,
          0.06175346000236459,
          0.06211516399343964,
          0.060943059012060985,
          0.06408969798940234,
          0.05851202599296812,
          0.0548237859911751,
          0.05746634599927347,
          0.06534784799441695,
          0.060258322992012836,
          0.05786144098965451,
          0.06549188199278433,
          0.058153693011263385,
          0.060370356994098984,
          0.06117241700121667,
          0.062478406995069236,
          0.05328516601002775,
          0.05872715498844627,
          0.05944604099204298,
          0.06125907099340111,
          0.06215913500636816,
          0.06455950700910762,
          0.06249121799191926,
          0.06176590999530163,
          0.06099843099946156,
          0.06106886400084477,
          0.06304831999295857,
          0.06224850899889134,
          0.06037167699832935
        ]
      },
      {
        "test_name": "arch.haddp",
        "is_significant": false,
        "p_value": 0.9499234118143649,
        "is_pair_significant": false,
        "pair_p_value": 0.922529504830187,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9283730499446392,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.031009795578744098,
        "relative_improvement": 0.020540205482657545,
        "absolute_improvement_ms": 1.3348728248745168,
        "old_mean_ms": 64.98828972288376,
        "new_mean_ms": 63.65341689800924,
        "old_std_ms": 4.457808493014345,
        "new_std_ms": 4.48291317746261,
        "effect_size_cohens_d": 0.2986039856334022,
        "old_ci95_ms": [
          63.2926300576961,
          66.68394938807141
        ],
        "new_ci95_ms": [
          61.94820792156407,
          65.35862587445442
        ],
        "old_ci99_ms": [
          62.700876935430635,
          67.27570251033687
        ],
        "new_ci99_ms": [
          61.353122270449724,
          65.95371152556876
        ],
        "new_times": [
          0.05573375000676606,
          0.06309377100842539,
          0.07587053999304771,
          0.0619071859982796,
          0.06280100000731181,
          0.06470648299728055,
          0.06054705299902707,
          0.06371367399697192,
          0.06427111600351054,
          0.06357055901025888,
          0.0643434389930917,
          0.06033981600194238,
          0.07162558799609542,
          0.07253427201067097,
          0.061864834002335556,
          0.06710551500145812,
          0.06388470099773258,
          0.06146987900137901,
          0.06428175600012764,
          0.060452638994320296,
          0.06309921099455096,
          0.06080593400110956,
          0.061894704995211214,
          0.07263221600442193,
          0.06160706400987692,
          0.06042811900260858,
          0.05811050000193063,
          0.06326551700476557,
          0.05998825300775934
        ],
        "old_times": [
          0.05734047199075576,
          0.06296320600085892,
          0.06384669999533799,
          0.07331423100549728,
          0.07367997600522358,
          0.062336322996998206,
          0.0770540639932733,
          0.06575824299943633,
          0.06503157499537338,
          0.06018616000073962,
          0.06509536699741147,
          0.06213150400435552,
          0.06344029400497675,
          0.05592384700139519,
          0.06789608500548638,
          0.06324844699702226,
          0.06328900899097789,
          0.06381753899040632,
          0.06594308000057936,
          0.06566280899278354,
          0.06269008599338122,
          0.06270575699454639,
          0.06671662000007927,
          0.06437654999899678,
          0.061013711005216464,
          0.06559826699958649,
          0.06996502299443819,
          0.06699433000176214,
          0.06664112700673286
        ]
      },
      {
        "test_name": "arch.modulo",
        "is_significant": false,
        "p_value": 0.9902131015880588,
        "is_pair_significant": false,
        "pair_p_value": 0.9867890530062815,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.99824458360672,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7906467723161779,
        "relative_improvement": -0.004669560572026741,
        "absolute_improvement_ms": -0.2966318655847483,
        "old_mean_ms": 63.52457817160155,
        "new_mean_ms": 63.8212100371863,
        "old_std_ms": 7.2424481531162135,
        "new_std_ms": 3.45497772730831,
        "effect_size_cohens_d": -0.05227858651634352,
        "old_ci95_ms": [
          60.76969822785864,
          66.27945811534447
        ],
        "new_ci95_ms": [
          62.507006794606674,
          65.13541327976593
        ],
        "old_ci99_ms": [
          59.80829728069631,
          67.24085906250679
        ],
        "new_ci99_ms": [
          62.04837474240071,
          65.59404533197188
        ],
        "new_times": [
          0.06299932699766941,
          0.06057078400044702,
          0.05799431601190008,
          0.06659022500389256,
          0.06970286401337944,
          0.06451817600463983,
          0.06239042400557082,
          0.06240323500242084,
          0.06380692801030818,
          0.0645415870094439,
          0.06607425601396244,
          0.06541144999209791,
          0.06736546399770305,
          0.0678144019911997,
          0.06449044399778359,
          0.061966039007529616,
          0.06660639599431306,
          0.056677386004594155,
          0.07007891799730714,
          0.0627677680022316,
          0.06563751900102943,
          0.06131284299772233,
          0.06290077399171423,
          0.05637348399613984,
          0.06661398500727955,
          0.058764036002685316,
          0.06454482700792141,
          0.06455756700597703,
          0.06533966700953897
        ],
        "old_times": [
          0.06554566499835346,
          0.06209434299671557,
          0.06561285800125916,
          0.05708633099857252,
          0.0636739429901354,
          0.06603527300467249,
          0.05775367699970957,
          0.061950247996719554,
          0.05366554099600762,
          0.06545557100616861,
          0.05828355699486565,
          0.062476777005940676,
          0.0652577840082813,
          0.0663660760037601,
          0.0904722870036494,
          0.06563968799309805,
          0.06282390998967458,
          0.0756913929944858,
          0.06499831400287803,
          0.05560699499619659,
          0.06278986899997108,
          0.054078846995253116,
          0.06314568199741188,
          0.06560985700343736,
          0.07043648199760355,
          0.052533258000039496,
          0.06426360600744374,
          0.0644232619961258,
          0.05844167299801484
        ]
      },
      {
        "test_name": "arch.shift",
        "is_significant": false,
        "p_value": 0.9934301953380925,
        "is_pair_significant": false,
        "pair_p_value": 0.996436482670495,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9970026835799217,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6570163357750011,
        "relative_improvement": -0.006318790042541191,
        "absolute_improvement_ms": -0.41152103573229015,
        "old_mean_ms": 65.12655634412958,
        "new_mean_ms": 65.53807737986187,
        "old_std_ms": 5.978108195284144,
        "new_std_ms": 5.213031286985047,
        "effect_size_cohens_d": -0.07337281956731279,
        "old_ci95_ms": [
          62.85260559977703,
          67.40050708848214
        ],
        "new_ci95_ms": [
          63.55514632740551,
          67.52100843231824
        ],
        "old_ci99_ms": [
          62.05903983863145,
          68.19407284962773
        ],
        "new_ci99_ms": [
          62.863140929377515,
          68.21301383034624
        ],
        "new_times": [
          0.06693556900427211,
          0.0597786639991682,
          0.059804106000228785,
          0.06371208500058856,
          0.06421270399005152,
          0.0567073770071147,
          0.0686863850132795,
          0.06210952400579117,
          0.06854308900074102,
          0.06644639899604954,
          0.0602164509909926,
          0.06537937799294014,
          0.0672307689965237,
          0.06473706400720403,
          0.07247000900679268,
          0.06359745000372641,
          0.06711982499109581,
          0.06331780999607872,
          0.07580916699953377,
          0.06790942599764094,
          0.07183096501103137,
          0.06866157399781514,
          0.07091546000447124,
          0.07325488999777008,
          0.05885141900216695,
          0.054635007996694185,
          0.07140115900256205,
          0.06697687999985646,
          0.0593536380038131
        ],
        "old_times": [
          0.06389674199454021,
          0.06302704900735989,
          0.05740183299349155,
          0.06047887100430671,
          0.07842332699510735,
          0.06941761299094651,
          0.05690307400072925,
          0.05618400700041093,
          0.06399408500874415,
          0.06131926299713086,
          0.061912635996122845,
          0.06908541999291629,
          0.06746477801061701,
          0.0595298350090161,
          0.07661240799643565,
          0.06616905900591519,
          0.0681221929989988,
          0.07166210899595171,
          0.07289286601007916,
          0.06435119899106212,
          0.06408871899475344,
          0.07274273000075482,
          0.07257363399548922,
          0.06261040200479329,
          0.06777853998937644,
          0.06266763499297667,
          0.0598952080035815,
          0.0600320939993253,
          0.057432804998825304
        ]
      },
      {
        "test_name": "arch.more_shift",
        "is_significant": false,
        "p_value": 0.9392070349553331,
        "is_pair_significant": false,
        "pair_p_value": 0.9608125603309117,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9688541498035192,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.10389691049687055,
        "relative_improvement": 0.022425919870047113,
        "absolute_improvement_ms": 1.4842644480362044,
        "old_mean_ms": 66.18522034490336,
        "new_mean_ms": 64.70095589686716,
        "old_std_ms": 4.571909771589034,
        "new_std_ms": 4.496687381002641,
        "effect_size_cohens_d": 0.32733031669391033,
        "old_ci95_ms": [
          64.44615887451732,
          67.92428181528939
        ],
        "new_ci95_ms": [
          62.99050749360507,
          66.41140430012923
        ],
        "old_ci99_ms": [
          63.83925934388622,
          68.5311813459205
        ],
        "new_ci99_ms": [
          62.39359338172031,
          67.008318412014
        ],
        "new_times": [
          0.06521393200091552,
          0.06352661800337955,
          0.06730534198868554,
          0.059457372000906616,
          0.06068898900412023,
          0.060582964011700824,
          0.06744160699599888,
          0.06404403799388092,
          0.06608442499418743,
          0.06170091799867805,
          0.08067358301195782,
          0.06330799900752027,
          0.06439560100261588,
          0.0682777689944487,
          0.064329828004702,
          0.05836205100058578,
          0.06227525000576861,
          0.060518442987813614,
          0.06722557899774984,
          0.06733586300106253,
          0.06390196099528112,
          0.06633349499315955,
          0.07039767000242136,
          0.0647476540034404,
          0.06649243099673185,
          0.05784807101008482,
          0.06191699599730782,
          0.07012183900224045,
          0.06181943300180137
        ],
        "old_times": [
          0.0573251710011391,
          0.06717703700996935,
          0.06368750400724821,
          0.06066630900022574,
          0.07302356100990437,
          0.059274585990351625,
          0.06584405599278398,
          0.07081117600318976,
          0.06801421899581328,
          0.06432345800567418,
          0.07363308400090318,
          0.06999459500366356,
          0.06438494099711534,
          0.06806339100876357,
          0.06488159899890888,
          0.06500709400279447,
          0.06955810799263418,
          0.06456229700415861,
          0.06314272299641743,
          0.06311403200379573,
          0.061096624995116144,
          0.06706417299574241,
          0.06945812499907333,
          0.06307943999127019,
          0.06355922800139524,
          0.06684757499897387,
          0.07663208899612073,
          0.07333260199811775,
          0.061812592000933364
        ]
      },
      {
        "test_name": "arch.less_than_underflow",
        "is_significant": false,
        "p_value": 0.9410450510680386,
        "is_pair_significant": false,
        "pair_p_value": 0.9634944972128558,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9581621773540974,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.38381565907171994,
        "relative_improvement": 0.015355196416054102,
        "absolute_improvement_ms": 1.0412384475910474,
        "old_mean_ms": 67.81016793132133,
        "new_mean_ms": 66.76892948373029,
        "old_std_ms": 5.626813166154197,
        "new_std_ms": 5.905338120097901,
        "effect_size_cohens_d": 0.18052744305185253,
        "old_ci95_ms": [
          65.6698426696269,
          69.95049319301576
        ],
        "new_ci95_ms": [
          64.52265899561111,
          69.01519997184946
        ],
        "old_ci99_ms": [
          64.92290967223727,
          70.6974261904054
        ],
        "new_ci99_ms": [
          63.7387531198703,
          69.79910584759027
        ],
        "new_times": [
          0.06540336000034586,
          0.06084701500367373,
          0.0663348640082404,
          0.06394653300230857,
          0.05767476400069427,
          0.0651300190074835,
          0.0692391370102996,
          0.05776957700436469,
          0.06379874800040852,
          0.07461890099511947,
          0.06781384198984597,
          0.0644268220057711,
          0.06793699700210709,
          0.07147612099652179,
          0.07472545599739533,
          0.0758447980042547,
          0.06926119700074196,
          0.06890796299558133,
          0.06301221800094936,
          0.06261340300261509,
          0.05339808099961374,
          0.07932920199527871,
          0.07584904800751247,
          0.06337380099284928,
          0.06958329000917729,
          0.06487171900516842,
          0.06976291499449871,
          0.062359963994822465,
          0.06698920000053477
        ],
        "old_times": [
          0.06234101200243458,
          0.06348971699480899,
          0.06209776298783254,
          0.06633847499324474,
          0.06274240800121333,
          0.063143642997602,
          0.07532592800271232,
          0.07157042500330135,
          0.06708849300048314,
          0.06008096599543933,
          0.07499142599408515,
          0.06630084400239866,
          0.06306518999917898,
          0.06948444599402137,
          0.062331221997737885,
          0.07692078000400215,
          0.0672602900012862,
          0.07256212300853804,
          0.06496432200947311,
          0.06324582699744496,
          0.06281656000646763,
          0.06710508400283288,
          0.08212986899889074,
          0.061220180010423064,
          0.06997573398984969,
          0.07453444800921716,
          0.06896979600423947,
          0.0751225410058396,
          0.06927535799331963
        ]
      },
      {
        "test_name": "arch.to_int32",
        "is_significant": false,
        "p_value": 0.9999229454196035,
        "is_pair_significant": false,
        "pair_p_value": 0.9999731113798371,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999986095353961,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9631441849382252,
        "relative_improvement": -0.03461984906330212,
        "absolute_improvement_ms": -2.2293919339125696,
        "old_mean_ms": 64.39635048194884,
        "new_mean_ms": 66.62574241586141,
        "old_std_ms": 5.437382645602927,
        "new_std_ms": 5.055829071492155,
        "effect_size_cohens_d": -0.4246401603193495,
        "old_ci95_ms": [
          62.328080736671886,
          66.46462022722581
        ],
        "new_ci95_ms": [
          64.70260788820129,
          68.54887694352155
        ],
        "old_ci99_ms": [
          61.60629375041401,
          67.18640721348368
        ],
        "new_ci99_ms": [
          64.03147034530676,
          69.22001448641608
        ],
        "new_times": [
          0.06828408000001218,
          0.05756674900476355,
          0.0688002789975144,
          0.06912793200172018,
          0.06225791999895591,
          0.06442071200581267,
          0.07256992399925366,
          0.06383399901096709,
          0.06387939100386575,
          0.07050146399706136,
          0.06213568500243127,
          0.0734518360113725,
          0.06218745600199327,
          0.06727233099809382,
          0.06630778399994597,
          0.06497659299930092,
          0.06939361199329142,
          0.06168460799381137,
          0.0631712340109516,
          0.0679801980004413,
          0.06717847801337484,
          0.06680146300641354,
          0.0580728100030683,
          0.07452488799754065,
          0.06243797599745449,
          0.07677930399950128,
          0.06028334400616586,
          0.07571422400360461,
          0.07055025600129738
        ],
        "old_times": [
          0.059697390999644995,
          0.06098417998873629,
          0.05805127799976617,
          0.06676482199691236,
          0.0740759910113411,
          0.061026852010400034,
          0.06387503100268077,
          0.06659571400086861,
          0.0625977919989964,
          0.0596142790018348,
          0.06308120000176132,
          0.07214658700104337,
          0.06592696999723557,
          0.0557068889902439,
          0.06862028299656231,
          0.06382351899810601,
          0.06476261500210967,
          0.0649346009886358,
          0.06572012099786662,
          0.06298232699919026,
          0.061558632005471736,
          0.061750309003400616,
          0.05917335199774243,
          0.06753784099419136,
          0.06727947099716403,
          0.07916215399745852,
          0.06349538698850665,
          0.07244230899959803,
          0.05410626700904686
        ]
      },
      {
        "test_name": "arch.to_int64",
        "is_significant": false,
        "p_value": 0.9971827980041202,
        "is_pair_significant": false,
        "pair_p_value": 0.9985769866940067,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9990918301045895,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.717488919694676,
        "relative_improvement": -0.014440267483057012,
        "absolute_improvement_ms": -0.9231238288740645,
        "old_mean_ms": 63.92705882748927,
        "new_mean_ms": 64.85018265636333,
        "old_std_ms": 4.901427973024948,
        "new_std_ms": 6.110970818345368,
        "effect_size_cohens_d": -0.16664953813283415,
        "old_ci95_ms": [
          62.06265533669216,
          65.79146231828638
        ],
        "new_ci95_ms": [
          62.52569367303326,
          67.1746716396934
        ],
        "old_ci99_ms": [
          61.41201381381339,
          66.44210384116515
        ],
        "new_ci99_ms": [
          61.714491023234416,
          67.98587428949224
        ],
        "new_times": [
          0.06336736099910922,
          0.06457757799944375,
          0.06197992799570784,
          0.07440775400027633,
          0.069215975003317,
          0.05787389200122561,
          0.0702985559910303,
          0.06142372699105181,
          0.06931010801054072,
          0.05365056000300683,
          0.07398961699800566,
          0.05851965700276196,
          0.06036028701055329,
          0.05988977800006978,
          0.07299621899437625,
          0.056384484996669926,
          0.06813366401183885,
          0.06888126199191902,
          0.07502291699347552,
          0.07128569400811102,
          0.06775698901037686,
          0.05772220599465072,
          0.056721168002695777,
          0.059690121008316055,
          0.060909688007086515,
          0.06350150700018276,
          0.06716465600766242,
          0.07087431900436059,
          0.06474562399671413
        ],
        "old_times": [
          0.06447897398902569,
          0.06413093100127298,
          0.06293248500151094,
          0.07524455600650981,
          0.06999274498957675,
          0.06669145899650175,
          0.07061666900699493,
          0.07362823399307672,
          0.06919830400147475,
          0.0681152920005843,
          0.06178307101072278,
          0.05896653298987076,
          0.06299876699631568,
          0.06077550200279802,
          0.06420565300504677,
          0.05573962000198662,
          0.0638995920016896,
          0.05991275000269525,
          0.05941307099419646,
          0.06703867200121749,
          0.05976287399244029,
          0.0645490669994615,
          0.06019989000924397,
          0.06338897200475913,
          0.05377795500680804,
          0.06474985499517061,
          0.05950042400218081,
          0.06584281599498354,
          0.062349972999072634
        ]
      },
      {
        "test_name": "arch.to_float",
        "is_significant": false,
        "p_value": 0.9750047605960899,
        "is_pair_significant": false,
        "pair_p_value": 0.9633010247271265,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9561296049505472,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.23719341373614222,
        "relative_improvement": 0.012902028015810021,
        "absolute_improvement_ms": 0.8541486910463997,
        "old_mean_ms": 66.20266906874944,
        "new_mean_ms": 65.34852037770304,
        "old_std_ms": 4.689025316900235,
        "new_std_ms": 4.871072852247765,
        "effect_size_cohens_d": 0.1786579708250056,
        "old_ci95_ms": [
          64.41905922741795,
          67.98627891008094
        ],
        "new_ci95_ms": [
          63.49566335733292,
          67.20137739807316
        ],
        "old_ci99_ms": [
          63.79661315868132,
          68.60872497881756
        ],
        "new_ci99_ms": [
          62.849051334044546,
          67.84798942136155
        ],
        "new_times": [
          0.0651055880007334,
          0.06365386200195644,
          0.06435586999577936,
          0.06413873100245837,
          0.06631806401128415,
          0.05690090399002656,
          0.07331038199481554,
          0.06889609299832955,
          0.06881683999381494,
          0.06516193099378143,
          0.06106625299435109,
          0.0639631949889008,
          0.0637450960057322,
          0.07206181398942135,
          0.07379291999677662,
          0.06265820399858057,
          0.06666600800235756,
          0.05900270500569604,
          0.06621412000095006,
          0.06520220199308824,
          0.06868812500033528,
          0.05433102599636186,
          0.07509504900372121,
          0.06478630600031465,
          0.06807878099789377,
          0.06518589099869132,
          0.064502374996664,
          0.06654277299821842,
          0.05686598300235346
        ],
        "old_times": [
          0.06874197701108642,
          0.0641448410024168,
          0.058235474993125536,
          0.06445783299568575,
          0.0717414109967649,
          0.06964793200313579,
          0.05769076501019299,
          0.07128704400383867,
          0.06945639500918332,
          0.05829886799619999,
          0.06685855401156005,
          0.07182374400144909,
          0.07439513299323153,
          0.06476355499762576,
          0.06579122399853077,
          0.07651435400475748,
          0.06832553099957295,
          0.06623207099619322,
          0.06562716700136662,
          0.06286807199649047,
          0.06286183299380355,
          0.06363009099732153,
          0.06485998799325898,
          0.059089617992867716,
          0.06670470899553038,
          0.06559221699717455,
          0.0667370699957246,
          0.07056862600438762,
          0.06293130500125699
        ]
      },
      {
        "test_name": "arch.to_double",
        "is_significant": false,
        "p_value": 0.9969041708325983,
        "is_pair_significant": false,
        "pair_p_value": 0.9945009694236289,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9981008060276508,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6740007630295373,
        "relative_improvement": -0.009732453715487303,
        "absolute_improvement_ms": -0.634116792533515,
        "old_mean_ms": 65.15487369073651,
        "new_mean_ms": 65.78899048327003,
        "old_std_ms": 5.090567204129922,
        "new_std_ms": 5.549839813220686,
        "effect_size_cohens_d": -0.11907945247118469,
        "old_ci95_ms": [
          63.21852548410159,
          67.09122189737144
        ],
        "new_ci95_ms": [
          63.677944319156424,
          67.90003664738363
        ],
        "old_ci99_ms": [
          62.542776617395766,
          67.76697076407726
        ],
        "new_ci99_ms": [
          62.94122917253384,
          68.63675179400622
        ],
        "new_times": [
          0.059647670001140796,
          0.07080220599891618,
          0.07486910100851674,
          0.07498282499727793,
          0.06225347898725886,
          0.07416177399863955,
          0.07142973000009079,
          0.05491230900224764,
          0.06818367500090972,
          0.06679227299173363,
          0.06401710699719843,
          0.06828332001168747,
          0.06289883400313556,
          0.0637105950008845,
          0.06173633900471032,
          0.06285026200930588,
          0.06091528799152002,
          0.06256573100108653,
          0.06952766698668711,
          0.07752173299377318,
          0.06544029100041371,
          0.06799394800327718,
          0.06846273600240238,
          0.06813925399910659,
          0.062037201001658104,
          0.06210748400189914,
          0.05589996700291522,
          0.06312358200375456,
          0.0626143430126831
        ],
        "old_times": [
          0.06418128299992532,
          0.06414871099696029,
          0.0687296870019054,
          0.06710507400566712,
          0.0694475339987548,
          0.07068086101207882,
          0.060916047004866414,
          0.06538657900819089,
          0.06346153499907814,
          0.06509841700608376,
          0.06565857899840921,
          0.06402450600580778,
          0.0662087300006533,
          0.06253760000981856,
          0.06374014599714428,
          0.06537034799112007,
          0.06216698599746451,
          0.07252145200618543,
          0.05636491399491206,
          0.06136141599563416,
          0.06829660999937914,
          0.06500708400562871,
          0.06557355599943548,
          0.061771410997607745,
          0.08471357799135149,
          0.060330256004817784,
          0.060999631008598953,
          0.06038105700281449,
          0.06330774899106473
        ]
      },
      {
        "test_name": "arch.init_from_generator",
        "is_significant": false,
        "p_value": 0.9781602683658676,
        "is_pair_significant": false,
        "pair_p_value": 0.9751214871791545,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.951834112405777,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.06375096846994267,
        "relative_improvement": 0.007024542686771479,
        "absolute_improvement_ms": 0.43814779050519587,
        "old_mean_ms": 62.373852653826326,
        "new_mean_ms": 61.93570486332113,
        "old_std_ms": 3.4295832982313295,
        "new_std_ms": 6.131889311298158,
        "effect_size_cohens_d": 0.08819394598674284,
        "old_ci95_ms": [
          61.06930893546668,
          63.67839637218597
        ],
        "new_ci95_ms": [
          59.60325891081839,
          64.26815081582387
        ],
        "old_ci99_ms": [
          60.61404787434704,
          64.13365743330561
        ],
        "new_ci99_ms": [
          58.78927942941333,
          65.08213029722894
        ],
        "new_times": [
          0.0675302309973631,
          0.06094030899112113,
          0.05728632899990771,
          0.06061957600468304,
          0.08433867299754638,
          0.05953798600239679,
          0.06413078100013081,
          0.05797173600876704,
          0.05731372999434825,
          0.057803358999080956,
          0.05746419601200614,
          0.05676288899849169,
          0.07631306600524113,
          0.06256074100383557,
          0.06050814200716559,
          0.06670474998827558,
          0.05571811999834608,
          0.06196678799460642,
          0.057488097008899786,
          0.05927265599893872,
          0.0659959920012625,
          0.06326309700671118,
          0.055040993000147864,
          0.0616730070032645,
          0.06454753699654248,
          0.05901414500840474,
          0.06318508500407916,
          0.06060970599355642,
          0.0605737240111921
        ],
        "old_times": [
          0.06106187299883459,
          0.0644096510077361,
          0.06111203499312978,
          0.06591291900258511,
          0.06185462400026154,
          0.05532528400362935,
          0.06121560899191536,
          0.06578771400381811,
          0.06376390700461343,
          0.06633537499874365,
          0.06442209199303761,
          0.0643573700072011,
          0.06430335699405987,
          0.0568032309965929,
          0.06059251599072013,
          0.06097440099983942,
          0.057209776001400314,
          0.061232551001012325,
          0.06281290999322664,
          0.05431544600287452,
          0.0647652659972664,
          0.062098572991089895,
          0.06611107601202093,
          0.06481176699162461,
          0.06057915498968214,
          0.06279546998848673,
          0.06969666299119126,
          0.06054528401000425,
          0.06363583200436551
        ]
      },
      {
        "test_name": "arch.init_from_generator_arange",
        "is_significant": false,
        "p_value": 0.8239866998379193,
        "is_pair_significant": false,
        "pair_p_value": 0.8547021979913543,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9862954132258892,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5309891194919264,
        "relative_improvement": 0.023563758712133002,
        "absolute_improvement_ms": 1.4936407959763032,
        "old_mean_ms": 63.38720465709198,
        "new_mean_ms": 61.89356386111568,
        "old_std_ms": 8.552071037082266,
        "new_std_ms": 5.114858455748652,
        "effect_size_cohens_d": 0.21197625018200078,
        "old_ci95_ms": [
          60.134170810051536,
          66.64023850413243
        ],
        "new_ci95_ms": [
          59.947975756523874,
          63.83915196570748
        ],
        "old_ci99_ms": [
          58.9989235812933,
          67.77548573289066
        ],
        "new_ci99_ms": [
          59.26900234035351,
          64.51812538187784
        ],
        "new_times": [
          0.06003422400681302,
          0.0643867299950216,
          0.05611688399221748,
          0.05841969299945049,
          0.0688825929973973,
          0.05476471300062258,
          0.056674526000279,
          0.06086118600796908,
          0.0675816729926737,
          0.05921314298757352,
          0.06352756799606141,
          0.06675876201188657,
          0.0694866859994363,
          0.0531373209960293,
          0.05285193999588955,
          0.06154661200707778,
          0.07018221200269181,
          0.06435849900299218,
          0.065005463999114,
          0.056904294004198164,
          0.057615252007963136,
          0.0682779689959716,
          0.05507293400296476,
          0.06300169699534308,
          0.06504504500480834,
          0.06083395499445032,
          0.06777278998924885,
          0.06420190299104434,
          0.0623970839951653
        ],
        "old_times": [
          0.06267674500122666,
          0.05963345800410025,
          0.06346232499345206,
          0.06503929500468075,
          0.06024974200408906,
          0.05467239899735432,
          0.059544265997828916,
          0.05831562800449319,
          0.05530116400041152,
          0.060387356992578134,
          0.0617366700025741,
          0.065018894005334,
          0.06951266700343695,
          0.05602554100914858,
          0.05917621099797543,
          0.06130185200890992,
          0.08446579799056053,
          0.09570404700934887,
          0.06432052800664678,
          0.056498539008316584,
          0.058541728009004146,
          0.06359573999361601,
          0.07216137800423894,
          0.05515579799248371,
          0.061682316998485476,
          0.06389022100483999,
          0.06388413099921308,
          0.06216814600338694,
          0.06410635000793263
        ]
      },
      {
        "test_name": "arch.init_from_constant",
        "is_significant": false,
        "p_value": 0.9900416535004083,
        "is_pair_significant": false,
        "pair_p_value": 0.998376861108157,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9985038619488478,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.38381565907171994,
        "relative_improvement": 0.004047014665433466,
        "absolute_improvement_ms": 0.25379451509597445,
        "old_mean_ms": 62.71153827627382,
        "new_mean_ms": 62.457743761177845,
        "old_std_ms": 4.481784893886939,
        "new_std_ms": 4.874587165601874,
        "effect_size_cohens_d": 0.0542028791249317,
        "old_ci95_ms": [
          61.00675847594807,
          64.41631807659958
        ],
        "new_ci95_ms": [
          60.60354996749978,
          64.31193755485592
        ],
        "old_ci99_ms": [
          60.41182259917458,
          65.01125395337307
        ],
        "new_ci99_ms": [
          59.95647143563366,
          64.95901608672203
        ],
        "new_times": [
          0.06272132700541988,
          0.06155772200145293,
          0.0591738719958812,
          0.06379420700250193,
          0.06185414400533773,
          0.06264299400208984,
          0.08194787200773135,
          0.06004865499562584,
          0.06319996500678826,
          0.0656036170112202,
          0.06548381299944595,
          0.05476199199620169,
          0.05642553600773681,
          0.06156185199506581,
          0.06521886198606808,
          0.057928884009015746,
          0.05727283901069313,
          0.06147842900827527,
          0.06119610898895189,
          0.0664866409933893,
          0.060024545004125684,
          0.06421768500877079,
          0.05788888200186193,
          0.05974247300764546,
          0.06010885701107327,
          0.06413903100474272,
          0.06463357000029646,
          0.06264170400390867,
          0.06751849000283983
        ],
        "old_times": [
          0.061403456988045946,
          0.0643113880068995,
          0.05596459899970796,
          0.061193898000055924,
          0.06987802000367083,
          0.05904406598710921,
          0.07211570600338746,
          0.06401995700434782,
          0.06075013100053184,
          0.05647695799416397,
          0.06106874399119988,
          0.05712923299870454,
          0.06035000699921511,
          0.0646852620120626,
          0.06261185300536454,
          0.060083246004069224,
          0.06326854799408466,
          0.06230156100355089,
          0.0631072120013414,
          0.06690236600115895,
          0.05633992300136015,
          0.06753987100091763,
          0.0644709540065378,
          0.06357170999399386,
          0.06075376099033747,
          0.06281935100560077,
          0.05554274300811812,
          0.07116617899737321,
          0.0697639060090296
        ]
      },
      {
        "test_name": "arch.init_from_generator_split",
        "is_significant": false,
        "p_value": 0.999868888090024,
        "is_pair_significant": false,
        "pair_p_value": 0.9995870068733915,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999555790796876,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7769740442733022,
        "relative_improvement": -0.012175349587920464,
        "absolute_improvement_ms": -0.7605818285935928,
        "old_mean_ms": 62.46899303394081,
        "new_mean_ms": 63.2295748625344,
        "old_std_ms": 4.583610915435608,
        "new_std_ms": 3.0664623334238423,
        "effect_size_cohens_d": -0.19504444913476773,
        "old_ci95_ms": [
          60.72548068647285,
          64.21250538140877
        ],
        "new_ci95_ms": [
          62.06315497218867,
          64.39599475288013
        ],
        "old_ci99_ms": [
          60.11702788400062,
          64.820958183881
        ],
        "new_ci99_ms": [
          61.65609651219503,
          64.80305321287378
        ],
        "new_times": [
          0.06771888800722081,
          0.06289333300082944,
          0.06580902500718366,
          0.060924567995243706,
          0.06159609400492627,
          0.06514284000149928,
          0.06474321401037741,
          0.061867234006058425,
          0.06047225098882336,
          0.057522588002029806,
          0.06145083799492568,
          0.06464446999598294,
          0.06873775599524379,
          0.06396977500116918,
          0.06998595499317162,
          0.055894926001201384,
          0.06535959801112767,
          0.0647975859901635,
          0.06247432799136732,
          0.06429940700763837,
          0.06327565800165758,
          0.05942528099694755,
          0.06327650899766013,
          0.06443915200361516,
          0.06377965700812638,
          0.06348280599922873,
          0.06253988899698015,
          0.06420203299785499,
          0.05893201200524345
        ],
        "old_times": [
          0.06685862501035444,
          0.07061428899760358,
          0.06406456799595617,
          0.06529538499307819,
          0.06343634499353357,
          0.06287602300290018,
          0.062183425994589925,
          0.05634699399524834,
          0.05649253800220322,
          0.07189669698709622,
          0.05456997500732541,
          0.06330746899766382,
          0.0659620610094862,
          0.06297095600166358,
          0.062335412003449164,
          0.06127014099911321,
          0.05596546799642965,
          0.0595541460061213,
          0.06353515799855813,
          0.06369698399794288,
          0.06331683999451343,
          0.06146955900476314,
          0.06215526499727275,
          0.05276189600408543,
          0.06289372399623971,
          0.06924950699612964,
          0.05642568599432707,
          0.06326034700032324,
          0.06683531400631182
        ]
      },
      {
        "test_name": "arch.exp",
        "is_significant": false,
        "p_value": 0.9626900462430821,
        "is_pair_significant": false,
        "pair_p_value": 0.9537630814438118,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.979587173089385,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4751997153246261,
        "relative_improvement": 0.010945385771995017,
        "absolute_improvement_ms": 0.6953013126521634,
        "old_mean_ms": 63.52460544891575,
        "new_mean_ms": 62.82930413626359,
        "old_std_ms": 6.291386035109935,
        "new_std_ms": 4.259506499198637,
        "effect_size_cohens_d": 0.1294214690444584,
        "old_ci95_ms": [
          61.131490187329625,
          65.91772071050187
        ],
        "new_ci95_ms": [
          61.20907451545009,
          64.44953375707709
        ],
        "old_ci99_ms": [
          60.29633826559775,
          66.75287263223376
        ],
        "new_ci99_ms": [
          60.64364505068938,
          65.01496322183779
        ],
        "new_times": [
          0.07267721700191032,
          0.05811143000028096,
          0.06456923799123615,
          0.060557274002349004,
          0.06542754999827594,
          0.05505107299541123,
          0.06455138701130636,
          0.06303887900139671,
          0.05974504299229011,
          0.06825886899605393,
          0.06149665001430549,
          0.0618733150040498,
          0.06092926899145823,
          0.061992468996322714,
          0.07060124799318146,
          0.05140428499726113,
          0.06428687699371949,
          0.06687698599125724,
          0.06354972800181713,
          0.06532994601002429,
          0.06473187399387825,
          0.06063473699032329,
          0.062193637000746094,
          0.05955409599118866,
          0.06445644400082529,
          0.06242194499645848,
          0.06054553399735596,
          0.0604802110028686,
          0.06670260899409186
        ],
        "old_times": [
          0.05646004700975027,
          0.061230100007378496,
          0.06320860600681044,
          0.06435965899436269,
          0.0629367950023152,
          0.06325065699638799,
          0.06331743000191636,
          0.06202153999765869,
          0.052931532001821324,
          0.059111969007062726,
          0.06772621799609624,
          0.05937274899042677,
          0.05566356699273456,
          0.06354082800680771,
          0.07993643499503378,
          0.06765601498773322,
          0.06047135099652223,
          0.06136525599868037,
          0.0649754330079304,
          0.05720324600406457,
          0.06556995600112714,
          0.08244712099258322,
          0.06487472901062574,
          0.05695665600069333,
          0.06136820500250906,
          0.06060378599795513,
          0.06825712800491601,
          0.06844962600735016,
          0.06694691799930297
        ]
      },
      {
        "test_name": "arch.expm1",
        "is_significant": false,
        "p_value": 0.9973133548548762,
        "is_pair_significant": false,
        "pair_p_value": 0.9964756358141262,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9943391662091017,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6627152751632892,
        "relative_improvement": -0.013461616005183074,
        "absolute_improvement_ms": -0.8579605511903482,
        "old_mean_ms": 63.73384524265239,
        "new_mean_ms": 64.59180579384274,
        "old_std_ms": 3.459359620925017,
        "new_std_ms": 6.639309269743002,
        "effect_size_cohens_d": -0.1620704434558059,
        "old_ci95_ms": [
          62.41797521688218,
          65.0497152684226
        ],
        "new_ci95_ms": [
          62.066347611632416,
          67.11726397605307
        ],
        "old_ci99_ms": [
          61.95876148889693,
          65.50892899640785
        ],
        "new_ci99_ms": [
          61.18501051601398,
          67.9986010716715
        ],
        "new_times": [
          0.05985385699023027,
          0.05513555598736275,
          0.06471680301183369,
          0.06771531800040975,
          0.07116571000369731,
          0.056563320991699584,
          0.07728639399283566,
          0.056748718008748256,
          0.05507445400871802,
          0.07310774399957154,
          0.06146721899858676,
          0.07104917499236763,
          0.06359691001125611,
          0.05619903800834436,
          0.07093516099848785,
          0.062050010994425975,
          0.05876989600074012,
          0.0641267109895125,
          0.06450938500347547,
          0.06006204499863088,
          0.06511108801350929,
          0.0552473510033451,
          0.06198701799439732,
          0.06439244000648614,
          0.06640913800220005,
          0.07470116400509141,
          0.07315744500374421,
          0.07544066300033592,
          0.06658263500139583
        ],
        "old_times": [
          0.06611083600728307,
          0.06813369400333613,
          0.0622659500077134,
          0.06089183600852266,
          0.05822516500484198,
          0.060243343003094196,
          0.06338886200683191,
          0.061035292004817165,
          0.06305624000378884,
          0.06395337400317658,
          0.06572173201129772,
          0.06323225599771831,
          0.06349770700035151,
          0.06434151899884455,
          0.06667264799762052,
          0.0691448020079406,
          0.0616947880043881,
          0.06278867900255136,
          0.05950950398982968,
          0.058183953995467164,
          0.060755501996027306,
          0.06633866499760188,
          0.06451466599537525,
          0.06474561399954837,
          0.07430261999252252,
          0.06249055800435599,
          0.06296996599121485,
          0.06808988199918531,
          0.06198185800167266
        ]
      },
      {
        "test_name": "arch.huge_exp",
        "is_significant": false,
        "p_value": 0.6698623321903707,
        "is_pair_significant": false,
        "pair_p_value": 0.6480099809423578,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.7660309970378876,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.04652341622426988,
        "relative_improvement": 0.039875912211583764,
        "absolute_improvement_ms": 2.619150206085358,
        "old_mean_ms": 65.68251510305281,
        "new_mean_ms": 63.06336489696746,
        "old_std_ms": 6.447085161044382,
        "new_std_ms": 5.305419417059115,
        "effect_size_cohens_d": 0.4436295242045908,
        "old_ci95_ms": [
          63.23017506136101,
          68.13485514474462
        ],
        "new_ci95_ms": [
          61.04529127929004,
          65.08143851464487
        ],
        "old_ci99_ms": [
          62.37435481256885,
          68.99067539353678
        ],
        "new_ci99_ms": [
          60.3410217912911,
          65.78570800264382
        ],
        "new_times": [
          0.05312125998898409,
          0.05868733298848383,
          0.07075436400191393,
          0.05894973300746642,
          0.06419585300318431,
          0.060641407006187364,
          0.061595174003741704,
          0.06527254400134552,
          0.06360910100920592,
          0.05969490099232644,
          0.06537713900615927,
          0.062304110993864015,
          0.06400892601232044,
          0.06581594599992968,
          0.05388280899205711,
          0.06405520800035447,
          0.06797896699572448,
          0.06389947199204471,
          0.0652768050058512,
          0.0655137139983708,
          0.0638606809952762,
          0.08049319600104354,
          0.06610854600148741,
          0.05926334499963559,
          0.06134171500161756,
          0.06327854801202193,
          0.05820238500018604,
          0.06665860700013582,
          0.05499579200113658
        ],
        "old_times": [
          0.06798303799587302,
          0.06647759099723771,
          0.055785571996239014,
          0.06342059400049038,
          0.0642531460034661,
          0.0690525690006325,
          0.059940441002254374,
          0.06747205900319386,
          0.06720386799133848,
          0.06249816900526639,
          0.07067152998934034,
          0.06264340398774948,
          0.09193830400181469,
          0.06372690600983333,
          0.07499176599958446,
          0.06588650798948947,
          0.0633452309994027,
          0.06995941398781724,
          0.06397683500836138,
          0.05790400299883913,
          0.06510301800153684,
          0.06395917500776704,
          0.05884721799520776,
          0.06677483199746348,
          0.06606913500581868,
          0.06310301100893412,
          0.06377968800370581,
          0.06052091300080065,
          0.06750499999907333
        ]
      },
      {
        "test_name": "arch.log",
        "is_significant": false,
        "p_value": 0.9699384741321161,
        "is_pair_significant": false,
        "pair_p_value": 0.9835431279484579,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9854679815471172,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4077760871187119,
        "relative_improvement": 0.014283306471982098,
        "absolute_improvement_ms": 0.927362411387328,
        "old_mean_ms": 64.92631192969415,
        "new_mean_ms": 63.99894951830683,
        "old_std_ms": 5.203111252885364,
        "new_std_ms": 4.229167663145855,
        "effect_size_cohens_d": 0.19559597082658214,
        "old_ci95_ms": [
          62.94715425640526,
          66.90546960298305
        ],
        "new_ci95_ms": [
          62.39026017354243,
          65.60763886307122
        ],
        "old_ci99_ms": [
          62.256465696272095,
          67.59615816311621
        ],
        "new_ci99_ms": [
          61.82885804665151,
          66.16904098996216
        ],
        "new_times": [
          0.06457589800993446,
          0.06835659198986832,
          0.06849172699730843,
          0.06315585300035309,
          0.05590568599291146,
          0.06496029200206976,
          0.0664876410010038,
          0.057243778006522916,
          0.06337055101175793,
          0.06242384600045625,
          0.06091638799989596,
          0.06024167200666852,
          0.06573207200563047,
          0.05969766099588014,
          0.06651505100307986,
          0.05646373800118454,
          0.05985235801199451,
          0.06641762799699791,
          0.0633816319896141,
          0.07045000299694948,
          0.07068668099236675,
          0.063305969000794,
          0.0692103649926139,
          0.06320327600406017,
          0.0682323070068378,
          0.057508757992764004,
          0.06635452600312419,
          0.06923604600888211,
          0.06359154100937303
        ],
        "old_times": [
          0.06476181499601807,
          0.06452737600193359,
          0.07186999599798582,
          0.06447451499116141,
          0.06171251799969468,
          0.058977433989639394,
          0.06142807700962294,
          0.0649942540039774,
          0.07048870400467422,
          0.06036004600173328,
          0.06333552100113593,
          0.05851662599889096,
          0.06423576499219052,
          0.06355574900226202,
          0.06060467600764241,
          0.062186526993173175,
          0.06686623499263078,
          0.060085495992098004,
          0.06599541200557724,
          0.06975058499665465,
          0.07540579199849162,
          0.07582676800666377,
          0.06066005799220875,
          0.06147734899423085,
          0.06397162500070408,
          0.05925363500136882,
          0.06009524699766189,
          0.07199227099772543,
          0.07545297399337869
        ]
      },
      {
        "test_name": "arch.log2",
        "is_significant": false,
        "p_value": 0.9948077352728941,
        "is_pair_significant": false,
        "pair_p_value": 0.9937598767500838,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999010942876339,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4566571315971132,
        "relative_improvement": -0.004248664220758118,
        "absolute_improvement_ms": -0.2720823445244991,
        "old_mean_ms": 64.03950286190154,
        "new_mean_ms": 64.31158520642603,
        "old_std_ms": 4.344714288744583,
        "new_std_ms": 5.691173227724155,
        "effect_size_cohens_d": -0.053740371438930616,
        "old_ci95_ms": [
          62.386861931333634,
          65.69214379246944
        ],
        "new_ci95_ms": [
          62.146778686592285,
          66.47639172625978
        ],
        "old_ci99_ms": [
          61.81012153305388,
          66.26888419074918
        ],
        "new_ci99_ms": [
          61.391302193571796,
          67.23186821928026
        ],
        "new_times": [
          0.07694387999072205,
          0.06219891599903349,
          0.07018754300952423,
          0.07007373799569905,
          0.0610438630101271,
          0.06127639100304805,
          0.06402092600183096,
          0.05362912898999639,
          0.06237259300542064,
          0.06263431400293484,
          0.06342923399643041,
          0.059980432997690514,
          0.0607453910051845,
          0.059766284000943415,
          0.08304336400760803,
          0.06418222299544141,
          0.06455072700919118,
          0.06282079099037219,
          0.06457244699413422,
          0.06071020998933818,
          0.06051983199722599,
          0.06370190400048159,
          0.0669038170017302,
          0.0648239769943757,
          0.06269856699509546,
          0.06992909299151506,
          0.05718879500636831,
          0.06722980900667608,
          0.06385777999821585
        ],
        "old_times": [
          0.061659257000428624,
          0.06780610101122875,
          0.06103300199902151,
          0.07921243700548075,
          0.06511261899140663,
          0.059453051988384686,
          0.06284666199644562,
          0.07309847300348338,
          0.06285938200016972,
          0.06857467099325731,
          0.06309809099184349,
          0.05889938100881409,
          0.0631555540021509,
          0.06133434400544502,
          0.06542585999704897,
          0.06501299400406424,
          0.05850976600777358,
          0.06012087799899746,
          0.06575980300840456,
          0.06336114199075382,
          0.06404102699889336,
          0.06361489099799655,
          0.06082502400386147,
          0.06667602800007444,
          0.0655711859872099,
          0.06358270000782795,
          0.058854508999502286,
          0.06192969699623063,
          0.06571705199894495
        ]
      },
      {
        "test_name": "arch.log10",
        "is_significant": false,
        "p_value": 0.929084074446507,
        "is_pair_significant": false,
        "pair_p_value": 0.9041657370350633,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9772401507943869,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.2138550760748389,
        "relative_improvement": 0.020245817139866906,
        "absolute_improvement_ms": 1.3074170690680202,
        "old_mean_ms": 64.57714499917748,
        "new_mean_ms": 63.26972793010946,
        "old_std_ms": 5.240783284821137,
        "new_std_ms": 4.841066845642591,
        "effect_size_cohens_d": 0.25915694059813504,
        "old_ci95_ms": [
          62.58365765130537,
          66.57063234704958
        ],
        "new_ci95_ms": [
          61.42828458423736,
          65.11117127598156
        ],
        "old_ci99_ms": [
          61.88796830602071,
          67.26632169233424
        ],
        "new_ci99_ms": [
          60.78565571727414,
          65.75380014294478
        ],
        "new_times": [
          0.05176930798916146,
          0.062115404012729414,
          0.06440022100287024,
          0.060232832009205595,
          0.06379406798805576,
          0.061438538003130816,
          0.06325725799251813,
          0.06327104699448682,
          0.056347274003201164,
          0.060487452006782405,
          0.06340678299602587,
          0.060776522994274274,
          0.068514498998411,
          0.056447137001669034,
          0.06203290000848938,
          0.06440862199815456,
          0.06776919998810627,
          0.06511259799299296,
          0.06862027200986631,
          0.06087896699318662,
          0.06328789799590595,
          0.06450825500360224,
          0.07190400800027419,
          0.06675639099557884,
          0.07329380098963156,
          0.06419496299349703,
          0.059931330994004384,
          0.07037432900688145,
          0.055490231010480784
        ],
        "old_times": [
          0.08217071999388281,
          0.062545439999667,
          0.06811332299548667,
          0.06440524099161848,
          0.06383269900106825,
          0.07072415300353896,
          0.06228997099970002,
          0.06273988798784558,
          0.06371657398995012,
          0.0596393190062372,
          0.05463506799424067,
          0.06407660800323356,
          0.06363490100193303,
          0.05760302100679837,
          0.06270633600070141,
          0.058836137992329895,
          0.0652357619983377,
          0.060545714004547335,
          0.058723983995150775,
          0.06333896999421995,
          0.06878056899586227,
          0.06752795100328512,
          0.06470171301043592,
          0.07062260899692774,
          0.06404208800813649,
          0.06347202599863522,
          0.06969545299944002,
          0.0709927930001868,
          0.06338817300274968
        ]
      },
      {
        "test_name": "arch.log1p",
        "is_significant": false,
        "p_value": 0.9632528310516849,
        "is_pair_significant": false,
        "pair_p_value": 0.9711464012161906,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9733120389282703,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.28779200453139825,
        "relative_improvement": 0.011151849584466245,
        "absolute_improvement_ms": 0.7160521390374153,
        "old_mean_ms": 64.2092716202726,
        "new_mean_ms": 63.49321948123518,
        "old_std_ms": 5.355577317999,
        "new_std_ms": 5.324239216939749,
        "effect_size_cohens_d": 0.1340938871012568,
        "old_ci95_ms": [
          62.172118957443836,
          66.24642428310136
        ],
        "new_ci95_ms": [
          61.46798719453586,
          65.5184517679345
        ],
        "old_ci99_ms": [
          61.46119124387644,
          66.95735199666875
        ],
        "new_ci99_ms": [
          60.76121946657053,
          66.22521949589984
        ],
        "new_times": [
          0.06210945399652701,
          0.06402344700472895,
          0.06584684600238688,
          0.07462515198858455,
          0.05743715500284452,
          0.06531554600223899,
          0.06197000799875241,
          0.05610447400249541,
          0.05426572299620602,
          0.06084426499728579,
          0.06064064700331073,
          0.0666874479938997,
          0.05981433599663433,
          0.06205301200679969,
          0.07083101700118277,
          0.07292494700232055,
          0.07161845600057859,
          0.06519046099856496,
          0.0645448269933695,
          0.058250995993148535,
          0.06826207799895201,
          0.057546379990526475,
          0.06282297099824063,
          0.05675003898795694,
          0.07259646400052588,
          0.060537144003319554,
          0.06212416499329265,
          0.06313468200096395,
          0.06243122500018217
        ],
        "old_times": [
          0.06381613899429794,
          0.07888173400715459,
          0.07211017499503214,
          0.06380090799939353,
          0.055472310006734915,
          0.06844044600438792,
          0.06451370600552764,
          0.05931313599285204,
          0.06466410100983921,
          0.06496974200126715,
          0.0596378089976497,
          0.06406266799604055,
          0.0693161679955665,
          0.06320484599564224,
          0.06169141799909994,
          0.07199196200235747,
          0.06419345299946144,
          0.057999426004244015,
          0.05656145200191531,
          0.06098329099768307,
          0.059048665993032046,
          0.05913598000188358,
          0.057800538997980766,
          0.06781780099845491,
          0.0653497570019681,
          0.06272570700093638,
          0.06644112900539767,
          0.07234565498947632,
          0.06577875299262814
        ]
      },
      {
        "test_name": "arch.sign",
        "is_significant": false,
        "p_value": 0.9863548771157088,
        "is_pair_significant": false,
        "pair_p_value": 0.9722053890314571,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9870832320302725,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3316228761603729,
        "relative_improvement": 0.009027883762312294,
        "absolute_improvement_ms": 0.5879481383278123,
        "old_mean_ms": 65.1257984492735,
        "new_mean_ms": 64.5378503109457,
        "old_std_ms": 4.9657216620099645,
        "new_std_ms": 4.231803651216334,
        "effect_size_cohens_d": 0.127444110966163,
        "old_ci95_ms": [
          63.23693894711817,
          67.01465795142884
        ],
        "new_ci95_ms": [
          62.9281582899434,
          66.14754233194799
        ],
        "old_ci99_ms": [
          62.577762739257196,
          67.67383415928983
        ],
        "new_ci99_ms": [
          62.366406248027324,
          66.70929437386407
        ],
        "new_times": [
          0.06685021398880053,
          0.06719782800064422,
          0.05982067598961294,
          0.06227085000136867,
          0.06326257700857241,
          0.06040783799835481,
          0.07119907101150602,
          0.06728180099162273,
          0.06743442699371371,
          0.06419445300707594,
          0.06494529200426769,
          0.06989203099510632,
          0.06112452599336393,
          0.053672591006034054,
          0.06021224999858532,
          0.06508717700489797,
          0.06377404599334113,
          0.063340380002046,
          0.06861900200601667,
          0.06386132999614347,
          0.06416239200916607,
          0.060896338007296436,
          0.05942888100980781,
          0.06498120300238952,
          0.06025789299746975,
          0.06667908899544273,
          0.07330971099145245,
          0.07110060700506438,
          0.06633318500826135
        ],
        "old_times": [
          0.06036816700361669,
          0.06747721899591852,
          0.06695177899382543,
          0.06483321799896657,
          0.06677493199822493,
          0.06226477900054306,
          0.06415924199973233,
          0.06538090799585916,
          0.060547284010681324,
          0.06808565200481098,
          0.06472381300409324,
          0.06697202999203,
          0.062284380008350126,
          0.06604790399433114,
          0.06769373700080905,
          0.05989977900753729,
          0.07101333301397972,
          0.06374398600019049,
          0.06469309200474527,
          0.06777670100564137,
          0.05813058100466151,
          0.05451097399054561,
          0.08102531600161456,
          0.061771859996952116,
          0.06955981798819266,
          0.07140730900573544,
          0.058740045002195984,
          0.06627573200967163,
          0.0655345849954756
        ]
      },
      {
        "test_name": "arch.sinh",
        "is_significant": false,
        "p_value": 0.9993635787720965,
        "is_pair_significant": false,
        "pair_p_value": 0.9997412939516593,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9998790118843317,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7906467723161779,
        "relative_improvement": -0.021236288282274512,
        "absolute_improvement_ms": -1.353790413524056,
        "old_mean_ms": 63.74891862124686,
        "new_mean_ms": 65.10270903477091,
        "old_std_ms": 4.401176486960857,
        "new_std_ms": 5.821510393177535,
        "effect_size_cohens_d": -0.26233997587677405,
        "old_ci95_ms": [
          62.074800619092514,
          65.4230366234012
        ],
        "new_ci95_ms": [
          62.88832490865809,
          67.31709316088374
        ],
        "old_ci99_ms": [
          61.490565129415984,
          66.00727211307773
        ],
        "new_ci99_ms": [
          62.11554676961017,
          68.08987129993167
        ],
        "new_times": [
          0.0566577249992406,
          0.061843302988563664,
          0.07024928501050454,
          0.06798266799887642,
          0.06806571100605652,
          0.06145351901068352,
          0.06045369998901151,
          0.06551534400205128,
          0.06606618499790784,
          0.06417180200514849,
          0.06753189099254087,
          0.06288196300738491,
          0.07256128299923148,
          0.06527954399643932,
          0.06760816399764735,
          0.05942734099517111,
          0.07315037600346841,
          0.06892638400313444,
          0.06019541999557987,
          0.05957441600912716,
          0.06348008599888999,
          0.06211918500775937,
          0.08385223499499261,
          0.06819898600224406,
          0.059806304998346604,
          0.06496206299925689,
          0.06828688000678085,
          0.053446172998519614,
          0.06423062499379739
        ],
        "old_times": [
          0.055505109994555824,
          0.05917029100237414,
          0.06613475699850824,
          0.0677442290034378,
          0.06274277799820993,
          0.06001753400778398,
          0.055814242994529195,
          0.06110435500158928,
          0.0636365719983587,
          0.06551608401059639,
          0.06386781101173256,
          0.07112957799108699,
          0.06532361599965952,
          0.061511701002018526,
          0.07078294501116034,
          0.06519765200209804,
          0.06375188600213733,
          0.06986659999529365,
          0.06616843899246305,
          0.05665280499670189,
          0.05788062200008426,
          0.06290580400673207,
          0.060863795995828696,
          0.07178429399209563,
          0.06408822900266387,
          0.06897642600233667,
          0.06409053900279105,
          0.06275648799783085,
          0.0637334560015006
        ]
      },
      {
        "test_name": "arch.cosh",
        "is_significant": false,
        "p_value": 0.91357174728753,
        "is_pair_significant": false,
        "pair_p_value": 0.903149251675879,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.979587173089385,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.16360928226507232,
        "relative_improvement": 0.022702579716826838,
        "absolute_improvement_ms": 1.474833895717273,
        "old_mean_ms": 64.96327351838961,
        "new_mean_ms": 63.48843962267234,
        "old_std_ms": 5.360941782713927,
        "new_std_ms": 4.678888871114854,
        "effect_size_cohens_d": 0.2931209520711937,
        "old_ci95_ms": [
          62.92408032231303,
          67.0024667144662
        ],
        "new_ci95_ms": [
          61.70868547911107,
          65.26819376623361
        ],
        "old_ci99_ms": [
          62.2124405012719,
          67.71410653550731
        ],
        "new_ci99_ms": [
          61.08758497590324,
          65.88929426944142
        ],
        "new_times": [
          0.06475999500253238,
          0.05380887600767892,
          0.05735149201063905,
          0.0594529919908382,
          0.07414825400337577,
          0.055851604003692046,
          0.059137920005014166,
          0.05935337800474372,
          0.059422231002827175,
          0.06388693100598175,
          0.06549247300426941,
          0.06276519800303504,
          0.06402523700671736,
          0.06695722800213844,
          0.06298062700079754,
          0.0639202629972715,
          0.06488018999516498,
          0.06643456799793057,
          0.06442299199989066,
          0.06827226000314113,
          0.07364686399523634,
          0.0630110269994475,
          0.061499301009462215,
          0.06510996801080182,
          0.06568817999504972,
          0.05960589701135177,
          0.059342777996789664,
          0.06822038699465338,
          0.06771563799702562
        ],
        "old_times": [
          0.06429081699752714,
          0.06871611600217875,
          0.06680689300992526,
          0.06300818800809793,
          0.06493199100077618,
          0.08155741699738428,
          0.06409065899788402,
          0.06309566099662334,
          0.05935275799129158,
          0.05949544299801346,
          0.06602054300310556,
          0.05522913999448065,
          0.06845951599825639,
          0.06925926700932905,
          0.06130472299992107,
          0.06256414100062102,
          0.05984839600569103,
          0.06058491600560956,
          0.06518750199757051,
          0.0703111769980751,
          0.06635718600591645,
          0.06360798100649845,
          0.06724057899555191,
          0.06540789900464006,
          0.06439403000695165,
          0.06458055699476972,
          0.057773658001678996,
          0.07749220100231469,
          0.06296557700261474
        ]
      },
      {
        "test_name": "arch.tanh",
        "is_significant": false,
        "p_value": 0.9835482757519987,
        "is_pair_significant": false,
        "pair_p_value": 0.9871615675093535,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9772401507943869,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5371704753549303,
        "relative_improvement": 0.004536399880951982,
        "absolute_improvement_ms": 0.2938022396471035,
        "old_mean_ms": 64.76550730916767,
        "new_mean_ms": 64.47170506952057,
        "old_std_ms": 5.759112417653836,
        "new_std_ms": 4.749857134766587,
        "effect_size_cohens_d": 0.05565848110501544,
        "old_ci95_ms": [
          62.57485810357705,
          66.9561565147583
        ],
        "new_ci95_ms": [
          62.664956042179256,
          66.27845409686189
        ],
        "old_ci99_ms": [
          61.81036300246067,
          67.72065161587467
        ],
        "new_ci99_ms": [
          62.03443483557793,
          66.90897530346321
        ],
        "new_times": [
          0.06461649900302291,
          0.07888863400148693,
          0.06363044200406875,
          0.06312295299721882,
          0.059597357001621276,
          0.07186637599079404,
          0.06203622100292705,
          0.06507206699461676,
          0.067396735001239,
          0.06012454800656997,
          0.060543963001691736,
          0.058281047007767484,
          0.060945289995288476,
          0.07014218099357095,
          0.06496330299705733,
          0.0648928589944262,
          0.05626657999528106,
          0.06311043100140523,
          0.06779642100445926,
          0.0648619889980182,
          0.06501452501106542,
          0.0638443100033328,
          0.07061852799961343,
          0.06687540501297917,
          0.06799141800729558,
          0.05781972999102436,
          0.06623141099407803,
          0.05943158100126311,
          0.06369664300291333
        ],
        "old_times": [
          0.06393583299359307,
          0.06381864899594802,
          0.07047057300223969,
          0.06498996399750467,
          0.06304265899234451,
          0.08280984401062597,
          0.06746923898754176,
          0.06094695899810176,
          0.05628205099492334,
          0.061741009005345404,
          0.07201163099671248,
          0.05881894798949361,
          0.06441613199422136,
          0.06310434099577833,
          0.07155960499949288,
          0.07060998798988294,
          0.058508144997176714,
          0.07363353300024755,
          0.0638221289991634,
          0.0676421240059426,
          0.06140761700225994,
          0.064712042993051,
          0.06746139901224524,
          0.06395205399894621,
          0.05533704500703607,
          0.061784760997397825,
          0.06074879100196995,
          0.0639792340080021,
          0.059183411998674273
        ]
      },
      {
        "test_name": "arch.asinh",
        "is_significant": false,
        "p_value": 0.9989866258678415,
        "is_pair_significant": false,
        "pair_p_value": 0.9993703035801932,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9987289663404226,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6280061697584979,
        "relative_improvement": -0.0171158557432497,
        "absolute_improvement_ms": -1.10916362107924,
        "old_mean_ms": 64.80328168906672,
        "new_mean_ms": 65.91244531014597,
        "old_std_ms": 4.100887636414606,
        "new_std_ms": 6.02467495080269,
        "effect_size_cohens_d": -0.21523166262677138,
        "old_ci95_ms": [
          63.24338745686005,
          66.3631759212734
        ],
        "new_ci95_ms": [
          63.620781519305375,
          68.20410910098656
        ],
        "old_ci99_ms": [
          62.69901390050136,
          66.90754947763209
        ],
        "new_ci99_ms": [
          62.82103424026485,
          69.00385638002709
        ],
        "new_times": [
          0.07399032700050157,
          0.07022517400037032,
          0.0701377099903766,
          0.060780802989029326,
          0.05956787700415589,
          0.05919696199998725,
          0.07365685499098618,
          0.06026437300897669,
          0.059836186992470175,
          0.060028224004781805,
          0.07000460400013253,
          0.06177911099803168,
          0.06300842700875364,
          0.06752820099063683,
          0.055159298004582524,
          0.0729598280013306,
          0.0638981420052005,
          0.08026183700712863,
          0.06523850299709011,
          0.0671358359977603,
          0.06468376200064085,
          0.0630625100020552,
          0.07454984801006503,
          0.06702708099328447,
          0.06388961199263576,
          0.0753994710103143,
          0.0609517289994983,
          0.06245458600460552,
          0.06478403598885052
        ],
        "old_times": [
          0.06559724699764047,
          0.06187265500193462,
          0.06335510099597741,
          0.06288659299025312,
          0.05503893298737239,
          0.06647089098987635,
          0.06837103300495073,
          0.05907978799950797,
          0.06721258800826035,
          0.06442333200538997,
          0.06571835100476164,
          0.06650081100815441,
          0.06439792099990882,
          0.058467303999350406,
          0.06660071598889772,
          0.0655444849980995,
          0.07274740999855567,
          0.07164099700457882,
          0.06919515499612316,
          0.05701906900503673,
          0.06229274100041948,
          0.06958670899621211,
          0.06447856400336605,
          0.06991764200211037,
          0.06417974299984053,
          0.06533933700120542,
          0.06251726900518406,
          0.0663108739972813,
          0.06253190999268554
        ]
      },
      {
        "test_name": "arch.acosh",
        "is_significant": false,
        "p_value": 0.9923314877736322,
        "is_pair_significant": false,
        "pair_p_value": 0.9889552863933441,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9967742282897234,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5679133843285674,
        "relative_improvement": -0.005690455424893693,
        "absolute_improvement_ms": -0.3703504134419111,
        "old_mean_ms": 65.08273693205034,
        "new_mean_ms": 65.45308734549225,
        "old_std_ms": 5.935401368968006,
        "new_std_ms": 5.395003375094752,
        "effect_size_cohens_d": -0.0652986206478129,
        "old_ci95_ms": [
          62.82503099564006,
          67.34044286846063
        ],
        "new_ci95_ms": [
          63.40093781259171,
          67.5052368783928
        ],
        "old_ci99_ms": [
          62.0371343649325,
          68.12833949916819
        ],
        "new_ci99_ms": [
          62.68477647527812,
          68.22139821570639
        ],
        "new_times": [
          0.0592981560039334,
          0.06350419700902421,
          0.07676402399374638,
          0.06930856799590401,
          0.06382062900229357,
          0.07100078300572932,
          0.061009451004792936,
          0.0642877959908219,
          0.06532830600917805,
          0.06275970800197683,
          0.06529599500936456,
          0.07280576199991629,
          0.05741225399833638,
          0.0675739220023388,
          0.061441527999704704,
          0.06697364899446256,
          0.06444216299860273,
          0.07857286199578084,
          0.061472608998883516,
          0.06201998000324238,
          0.07223447000433225,
          0.06964917200093623,
          0.05991101000108756,
          0.06684794400644023,
          0.05512213699694257,
          0.0656551089923596,
          0.06543113000225276,
          0.06097165000392124,
          0.06722456899296958
        ],
        "old_times": [
          0.06304628899670206,
          0.0702063030039426,
          0.06369604400242679,
          0.05752337801095564,
          0.05861700000241399,
          0.07073434299672954,
          0.06187780499749351,
          0.06320002498978283,
          0.0686360940017039,
          0.06217851700785104,
          0.054829814995173365,
          0.07282299399957992,
          0.08335533599893097,
          0.06349239600240253,
          0.06526480399770662,
          0.06537632900290191,
          0.07307057199068367,
          0.06618010900274385,
          0.06438049000280444,
          0.06439158100693021,
          0.06802030900144018,
          0.061541842005681247,
          0.06731497200962622,
          0.06277060900174547,
          0.05762879300164059,
          0.06703823199495673,
          0.07033545800368302,
          0.06489887100178748,
          0.05497006099903956
        ]
      },
      {
        "test_name": "arch.atanh",
        "is_significant": false,
        "p_value": 0.9999913112882364,
        "is_pair_significant": false,
        "pair_p_value": 0.9999161916070975,
        "is_binom_significant": false,
        "binom_p_value": 0.9999481420964003,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999891409650445,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9879313922900197,
        "relative_improvement": -0.03689477866453662,
        "absolute_improvement_ms": -2.303856380829777,
        "old_mean_ms": 62.4439680686919,
        "new_mean_ms": 64.74782444952167,
        "old_std_ms": 4.94684342114403,
        "new_std_ms": 4.051905109004324,
        "effect_size_cohens_d": -0.5095257309391907,
        "old_ci95_ms": [
          60.56228946532039,
          64.3256466720634
        ],
        "new_ci95_ms": [
          63.2065621742864,
          66.28908672475696
        ],
        "old_ci99_ms": [
          59.905619255191894,
          64.98231688219191
        ],
        "new_ci99_ms": [
          62.66869081818528,
          66.82695808085808
        ],
        "new_times": [
          0.06241723499260843,
          0.06613097700756043,
          0.06007796600169968,
          0.06975582599989139,
          0.06789850500354078,
          0.06325393699808046,
          0.07132319601078052,
          0.06977656700473744,
          0.06371491499885451,
          0.06060622600489296,
          0.06194197699369397,
          0.06502289399213623,
          0.05564359700656496,
          0.06200985000759829,
          0.06050419301027432,
          0.06265113400877453,
          0.06813146399508696,
          0.06876646800083108,
          0.06272323700250126,
          0.06990033100009896,
          0.06593437999254093,
          0.06891967399860732,
          0.06355357900611125,
          0.06292250500700902,
          0.06552880399976857,
          0.06373625599371735,
          0.05669802600459661,
          0.06938213200191967,
          0.06876105799165089
        ],
        "old_times": [
          0.06328738899901509,
          0.062258068996015936,
          0.061167367006419227,
          0.05780633899848908,
          0.06077410299621988,
          0.06524165300652385,
          0.059176672002649866,
          0.061382315994706005,
          0.06056295499729458,
          0.05896221401053481,
          0.06444673299847636,
          0.0640205460076686,
          0.06366744299884886,
          0.05614942499960307,
          0.06410156900528818,
          0.06591418900643475,
          0.07758060499327257,
          0.06503242600592785,
          0.05961321899667382,
          0.057234666994190775,
          0.059750732994871214,
          0.06191228599345777,
          0.06767971700173803,
          0.052405801994609646,
          0.057730895990971476,
          0.06164474500110373,
          0.07145783099986147,
          0.060631157000898384,
          0.06928200800030027
        ]
      },
      {
        "test_name": "arch.arg",
        "is_significant": false,
        "p_value": 0.9819795825004556,
        "is_pair_significant": false,
        "pair_p_value": 0.9793602962802371,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9718872215598822,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.4077760871187119,
        "relative_improvement": 0.01047580576701552,
        "absolute_improvement_ms": 0.6888218290284909,
        "old_mean_ms": 65.75358920812944,
        "new_mean_ms": 65.06476737910096,
        "old_std_ms": 4.546304400097739,
        "new_std_ms": 4.874535595155343,
        "effect_size_cohens_d": 0.14614495768430247,
        "old_ci95_ms": [
          64.02426750016659,
          67.48291091609231
        ],
        "new_ci95_ms": [
          63.21059320177165,
          66.91894155643025
        ],
        "old_ci99_ms": [
          63.42076696223568,
          68.0864114540232
        ],
        "new_ci99_ms": [
          62.56352151563989,
          67.56601324256202
        ],
        "new_times": [
          0.057548109005438164,
          0.07261037398711778,
          0.057640062004793435,
          0.06645689900324214,
          0.07114049899973907,
          0.06903353800589684,
          0.06697196899040136,
          0.06353723800566513,
          0.06141795699659269,
          0.06330322999565396,
          0.06817031498940196,
          0.07277673200587742,
          0.07050715399964247,
          0.0650628369912738,
          0.05797212499601301,
          0.06671159999677911,
          0.06887876200198662,
          0.06835481300367974,
          0.06127987201034557,
          0.06486067800142337,
          0.06480216600175481,
          0.06807925099565182,
          0.06628339299641084,
          0.05924696399597451,
          0.06239598500542343,
          0.05483627499779686,
          0.058897581009659916,
          0.06799460800539237,
          0.0701072679948993
        ],
        "old_times": [
          0.06671865899988916,
          0.07114467900828458,
          0.07158496599004138,
          0.07027799599745777,
          0.06394042300235014,
          0.06798529799561948,
          0.06100969199906103,
          0.06235016300342977,
          0.0639967860042816,
          0.06803442000818904,
          0.0638457599998219,
          0.07204660300340038,
          0.05784688000858296,
          0.07613462999870535,
          0.06158929300727323,
          0.07238019599753898,
          0.059685030006221496,
          0.06097608999698423,
          0.06515609999769367,
          0.06709662399953231,
          0.06338253201101907,
          0.066521572007332,
          0.05883487800019793,
          0.07003060600254685,
          0.06118947800132446,
          0.06304409899166785,
          0.06528368500585202,
          0.06669625799986534,
          0.0680706909915898
        ]
      },
      {
        "test_name": "arch.pow",
        "is_significant": false,
        "p_value": 0.9768228779230753,
        "is_pair_significant": false,
        "pair_p_value": 0.9745019227104941,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9827331658452749,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5982502817760561,
        "relative_improvement": 0.0006526104770035415,
        "absolute_improvement_ms": 0.04258538206942941,
        "old_mean_ms": 65.25390500158618,
        "new_mean_ms": 65.21131961951674,
        "old_std_ms": 6.765095207243396,
        "new_std_ms": 5.5747020975430885,
        "effect_size_cohens_d": 0.0068702269281754255,
        "old_ci95_ms": [
          62.68060040762409,
          67.82720959554827
        ],
        "new_ci95_ms": [
          63.09081634822235,
          67.33182289081115
        ],
        "old_ci99_ms": [
          61.782565820094646,
          68.7252441830777
        ],
        "new_ci99_ms": [
          62.35080085022811,
          68.07183838880539
        ],
        "new_times": [
          0.05934750799497124,
          0.07840197600307874,
          0.06245329699595459,
          0.06975693500135094,
          0.064296447992092,
          0.06645769999886397,
          0.06654956401325762,
          0.06007635599235073,
          0.07394909500726499,
          0.06522515299730003,
          0.06390637200092897,
          0.05516039799840655,
          0.058343248994788155,
          0.0746443630050635,
          0.06555128500622232,
          0.06168426699878182,
          0.060175580001669005,
          0.06080523399577942,
          0.06575259300007019,
          0.07349433799390681,
          0.06968275299004745,
          0.06271669699344784,
          0.0689402840071125,
          0.06447329299408011,
          0.06363280200457666,
          0.06084088499483187,
          0.062707845994737,
          0.059284285991452634,
          0.07281771200359799
        ],
        "old_times": [
          0.061760051001328975,
          0.06325186698813923,
          0.06583039498946164,
          0.07198183999571484,
          0.06416604200785514,
          0.06850687800033484,
          0.06182370299939066,
          0.06396666500950232,
          0.05952023499412462,
          0.08964150500833057,
          0.06916009400447365,
          0.0606495179963531,
          0.06406280800001696,
          0.06508924700028729,
          0.057392743008676916,
          0.06420606300525833,
          0.06570885100518353,
          0.058115610998356715,
          0.07431880899821408,
          0.06913498201174662,
          0.059933690004982054,
          0.06158752400369849,
          0.06087236599705648,
          0.06427416601218283,
          0.062223376997280866,
          0.055769462007447146,
          0.07758210500469431,
          0.06728155098971911,
          0.06455109700618777
        ]
      },
      {
        "test_name": "arch.sqrt_nn",
        "is_significant": false,
        "p_value": 0.9999812571220467,
        "is_pair_significant": false,
        "pair_p_value": 0.9999958676105264,
        "is_binom_significant": false,
        "binom_p_value": 0.9999923817813396,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999940358102322,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.972113780750794,
        "relative_improvement": -0.04919589763188209,
        "absolute_improvement_ms": -3.1700303455902965,
        "old_mean_ms": 64.4368839310683,
        "new_mean_ms": 67.60691427665859,
        "old_std_ms": 6.177955833481324,
        "new_std_ms": 4.93253695388935,
        "effect_size_cohens_d": -0.5670856096536526,
        "old_ci95_ms": [
          62.08691521066202,
          66.78685265147456
        ],
        "new_ci95_ms": [
          65.73067756239314,
          69.48315099092403
        ],
        "old_ci99_ms": [
          61.26682061498578,
          67.6069472471508
        ],
        "new_ci99_ms": [
          65.07590646854113,
          70.13792208477604
        ],
        "new_times": [
          0.07109079700603615,
          0.06725356000242755,
          0.06752717999916058,
          0.07221003899758216,
          0.06904350899276324,
          0.06896705499093514,
          0.06707173300674185,
          0.056624774006195366,
          0.07110277700121514,
          0.07037440899875946,
          0.0758669490023749,
          0.07053177600027993,
          0.06484596800873987,
          0.06804289000865538,
          0.0705831679952098,
          0.072775210996042,
          0.0675115710037062,
          0.0676606159977382,
          0.0627016849903157,
          0.06030987499980256,
          0.07530168700031936,
          0.06012384800123982,
          0.06081163400085643,
          0.0641822829929879,
          0.07667632101220079,
          0.062386143996263854,
          0.06957425900327507,
          0.06584216600458603,
          0.06360663000668865
        ],
        "old_times": [
          0.07030608700006269,
          0.07041390099038836,
          0.05498216100386344,
          0.072176739005954,
          0.07100526300200727,
          0.05675987900758628,
          0.06141702699824236,
          0.055306763999396935,
          0.05696074700972531,
          0.06311149199609645,
          0.06888516199251171,
          0.06327745800081175,
          0.05915979100973345,
          0.07315121599822305,
          0.06202952099556569,
          0.06778476999897975,
          0.07413166199694388,
          0.06711868499405682,
          0.07294909701158758,
          0.0671516669972334,
          0.06339684300473891,
          0.0553250140073942,
          0.05859917899942957,
          0.05572988999483641,
          0.06712394599162508,
          0.06303331900562625,
          0.07231276298989542,
          0.06343407499662135,
          0.061635516001842916
        ]
      },
      {
        "test_name": "arch.sqrt_pn",
        "is_significant": false,
        "p_value": 0.9910597034408226,
        "is_pair_significant": false,
        "pair_p_value": 0.9906530093416105,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9939453694969416,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7015085106482997,
        "relative_improvement": -0.003314227055852491,
        "absolute_improvement_ms": -0.21140489743316782,
        "old_mean_ms": 63.787089378757656,
        "new_mean_ms": 63.998494276190826,
        "old_std_ms": 5.730919281150509,
        "new_std_ms": 5.165828466313711,
        "effect_size_cohens_d": -0.038749396894624574,
        "old_ci95_ms": [
          61.607164268757664,
          65.96701448875766
        ],
        "new_ci95_ms": [
          62.03351821646707,
          65.96347033591456
        ],
        "old_ci99_ms": [
          60.84641167398234,
          66.72776708353298
        ],
        "new_ci99_ms": [
          61.34777877099307,
          66.64920978138858
        ],
        "new_times": [
          0.06431401800364256,
          0.06438321000314318,
          0.06372928600467276,
          0.06568585999775678,
          0.06523944200307596,
          0.0611705180053832,
          0.06407534900063183,
          0.07514216098934412,
          0.06308673099556472,
          0.06079505299567245,
          0.06365831299626734,
          0.05302264599595219,
          0.062445296003716066,
          0.06024090200662613,
          0.05881683800544124,
          0.05574828998942394,
          0.06934152900066692,
          0.07553776699933223,
          0.06249180799932219,
          0.06996739300666377,
          0.06158274400513619,
          0.05957284598844126,
          0.06763963600678835,
          0.06864600301196333,
          0.06517817098938394,
          0.07094670100195799,
          0.0657037110067904,
          0.05773991699970793,
          0.06005419499706477
        ],
        "old_times": [
          0.054776032993686385,
          0.06391554299625568,
          0.06031421400257386,
          0.06713495499570854,
          0.06059985600586515,
          0.06118242799129803,
          0.07041074099834077,
          0.060545714004547335,
          0.06123638000281062,
          0.06427369700395502,
          0.06362492199696135,
          0.060866405998240225,
          0.06546655199781526,
          0.058504606000497006,
          0.06376608699792996,
          0.06302849898929708,
          0.06328801899508107,
          0.06619759999739472,
          0.08149976399727166,
          0.06512187799671665,
          0.05503062299976591,
          0.05865834199357778,
          0.05979268500232138,
          0.05960174699430354,
          0.07621929301240016,
          0.07051096500072163,
          0.06360846100142226,
          0.069486845008214,
          0.0611627370089991
        ]
      },
      {
        "test_name": "arch.sqrt_np",
        "is_significant": false,
        "p_value": 0.9996630470856517,
        "is_pair_significant": false,
        "pair_p_value": 0.9991079188753552,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9983787517994642,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5922239128812882,
        "relative_improvement": -0.01420314777659962,
        "absolute_improvement_ms": -0.915312000494134,
        "old_mean_ms": 64.44430593070012,
        "new_mean_ms": 65.35961793119425,
        "old_std_ms": 4.593213278096727,
        "new_std_ms": 4.387676499078681,
        "effect_size_cohens_d": -0.20378212203469293,
        "old_ci95_ms": [
          62.697141039817495,
          66.19147082158275
        ],
        "new_ci95_ms": [
          63.69063504979895,
          67.02860081258956
        ],
        "old_ci99_ms": [
          62.08741356884062,
          66.80119829255962
        ],
        "new_ci99_ms": [
          63.1081916200533,
          67.61104424233521
        ],
        "new_times": [
          0.06733977398835123,
          0.06323504701140337,
          0.06706131200189702,
          0.06305129900283646,
          0.05929998599458486,
          0.06362131099740509,
          0.06696762899809983,
          0.0609197380108526,
          0.06062738700711634,
          0.06420563299616333,
          0.05876296501082834,
          0.06808537199685816,
          0.06487415899755433,
          0.06585058600467164,
          0.06820587599941064,
          0.06401500600622967,
          0.07131011500314344,
          0.06111577499541454,
          0.0677208779961802,
          0.06255164000322111,
          0.07380841100530233,
          0.06030674499925226,
          0.06677207098982763,
          0.07062328899337444,
          0.07523602599394508,
          0.06407765799667686,
          0.07261679500516038,
          0.062426706004771404,
          0.06073973099410068
        ],
        "old_times": [
          0.061695898009929806,
          0.061246280994964764,
          0.0648980799887795,
          0.05503733400837518,
          0.06315742399601731,
          0.064218063998851,
          0.06873265700414777,
          0.06488101001014002,
          0.06503615499241278,
          0.06515912000031676,
          0.060964060001424514,
          0.06171153800096363,
          0.06769585599249694,
          0.0716693389986176,
          0.06760559399845079,
          0.054706910988898017,
          0.056385304997093044,
          0.0678073520102771,
          0.06308080999588128,
          0.07273379000253044,
          0.06965698199928738,
          0.06581859500147402,
          0.06295056600356475,
          0.059464742997079156,
          0.06531678600003943,
          0.0695525580085814,
          0.06409392898785882,
          0.07098174199927598,
          0.06262639300257433
        ]
      },
      {
        "test_name": "arch.sqrt_pp",
        "is_significant": false,
        "p_value": 0.6299921052044211,
        "is_pair_significant": false,
        "pair_p_value": 0.6186036605801254,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.5759943872690201,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.02242186565540446,
        "relative_improvement": 0.04163014830040317,
        "absolute_improvement_ms": 2.81779596668362,
        "old_mean_ms": 67.68642634540747,
        "new_mean_ms": 64.86863037872385,
        "old_std_ms": 6.574374891382292,
        "new_std_ms": 6.6844141483752155,
        "effect_size_cohens_d": 0.425031112509145,
        "old_ci95_ms": [
          65.1856678797603,
          70.18718481105464
        ],
        "new_ci95_ms": [
          62.32601521825399,
          67.41124553919373
        ],
        "old_ci99_ms": [
          64.31295051762694,
          71.059902173188
        ],
        "new_ci99_ms": [
          61.438690662025294,
          68.29857009542242
        ],
        "new_times": [
          0.06631737398856785,
          0.05916831101058051,
          0.059599108004476875,
          0.06105044300784357,
          0.06561294799030293,
          0.07008619900443591,
          0.0615690130071016,
          0.07004664700070862,
          0.09068924598977901,
          0.05869386300037149,
          0.05825709700002335,
          0.060586455001612194,
          0.06812641400028951,
          0.0656900809990475,
          0.06393872300395742,
          0.06018595999921672,
          0.07488018099684268,
          0.06890654400922358,
          0.06092168799659703,
          0.062391653991653584,
          0.06728330098849256,
          0.06424209500255529,
          0.06079600300290622,
          0.0690060769993579,
          0.06206281100458,
          0.06451948599715251,
          0.05616622600064147,
          0.06997211399720982,
          0.06042421898746397
        ],
        "old_times": [
          0.0659383699967293,
          0.05897771399759222,
          0.07104269500996452,
          0.0669580490066437,
          0.06188626500079408,
          0.06363198200415354,
          0.057752827007789165,
          0.07328188100655098,
          0.061586954005178995,
          0.06992768199415877,
          0.07385211299697403,
          0.06140581700310577,
          0.06629048399918247,
          0.06482726700778585,
          0.07070366200059652,
          0.06237276400497649,
          0.057823719005682506,
          0.0779201679979451,
          0.08743572200182825,
          0.07182057399768382,
          0.06761811400065199,
          0.0733056009921711,
          0.0744961769960355,
          0.07188143699022476,
          0.06533175699587446,
          0.06585640599951148,
          0.06301742799405474,
          0.07226928099407814,
          0.0636934540088987
        ]
      },
      {
        "test_name": "arch.sin",
        "is_significant": false,
        "p_value": 0.9815424116904933,
        "is_pair_significant": false,
        "pair_p_value": 0.9806139486751648,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9862954132258892,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.18353501204933736,
        "relative_improvement": 0.013330813884944686,
        "absolute_improvement_ms": 0.8485455207470488,
        "old_mean_ms": 63.652941828657845,
        "new_mean_ms": 62.80439630791079,
        "old_std_ms": 4.366111230849412,
        "new_std_ms": 4.1674012186894185,
        "effect_size_cohens_d": 0.19881978627143504,
        "old_ci95_ms": [
          61.99216193659297,
          65.31372172072271
        ],
        "new_ci95_ms": [
          61.2192016620979,
          64.38959095372368
        ],
        "old_ci99_ms": [
          61.41258119483115,
          65.89330246248454
        ],
        "new_ci99_ms": [
          60.66599874035803,
          64.94279387546356
        ],
        "new_times": [
          0.05745653600024525,
          0.05441960999451112,
          0.058445513001061045,
          0.06147898899507709,
          0.061821652998332866,
          0.06364435200521257,
          0.06047189098899253,
          0.05902697499550413,
          0.06447006399685051,
          0.06461741999373771,
          0.06466298199666198,
          0.06667176699556876,
          0.06169289699755609,
          0.0643106179923052,
          0.05812870200315956,
          0.05856405799568165,
          0.06504729599691927,
          0.06847458599077072,
          0.07226529199397191,
          0.058123142007389106,
          0.0631353630014928,
          0.0614262479939498,
          0.0626538740034448,
          0.06342579399643,
          0.0660162520071026,
          0.07266503699065652,
          0.05977012400398962,
          0.06589885799621698,
          0.06254159999662079
        ],
        "old_times": [
          0.07456985898897983,
          0.05428754398599267,
          0.06488717999309301,
          0.05632835300639272,
          0.06201193999731913,
          0.06462653999915347,
          0.06169285700889304,
          0.06387772099697031,
          0.06219564600905869,
          0.06450579600641504,
          0.061415378004312515,
          0.06599720200756565,
          0.062277339995489456,
          0.06458466799813323,
          0.06752152100671083,
          0.06285670200304594,
          0.07369720599672291,
          0.059532725004828535,
          0.06681448400195222,
          0.06504683500679675,
          0.060474541009170935,
          0.06068573000084143,
          0.06325718700827565,
          0.06384311999136116,
          0.056201166997198015,
          0.06635199500306044,
          0.06815034399915021,
          0.06431038799928501,
          0.06393334400490858
        ]
      },
      {
        "test_name": "arch.cos",
        "is_significant": false,
        "p_value": 0.999821220615597,
        "is_pair_significant": false,
        "pair_p_value": 0.9996037158595118,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999555790796876,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8038133622321262,
        "relative_improvement": -0.01481385351643183,
        "absolute_improvement_ms": -0.9580476205506977,
        "old_mean_ms": 64.67241082733882,
        "new_mean_ms": 65.63045844788952,
        "old_std_ms": 3.997684966712917,
        "new_std_ms": 4.54936267549039,
        "effect_size_cohens_d": -0.22371657973812362,
        "old_ci95_ms": [
          63.1517727912265,
          66.19304886345115
        ],
        "new_ci95_ms": [
          63.89997343418204,
          67.360943461597
        ],
        "old_ci99_ms": [
          62.62109890413754,
          66.7237227505401
        ],
        "new_ci99_ms": [
          63.29606692456905,
          67.96484997121
        ],
        "new_times": [
          0.06554587499704212,
          0.07980541000142694,
          0.074421904006158,
          0.06122681999113411,
          0.06390507199103013,
          0.06348607699328568,
          0.05950729399046395,
          0.06561522699485067,
          0.06427580599847715,
          0.06497948300966527,
          0.06175352999707684,
          0.06672318000346422,
          0.06744230700132903,
          0.06758172299305443,
          0.06036182701063808,
          0.060343375997035764,
          0.06999038399953861,
          0.06550435398821719,
          0.06609266600571573,
          0.06326852699567098,
          0.06508617699728347,
          0.06095805000222754,
          0.06774989901168738,
          0.06425432600372005,
          0.06626998199499212,
          0.060085896009695716,
          0.07249058000161313,
          0.06423257499409374,
          0.07032496800820809
        ],
        "old_times": [
          0.07593143200210761,
          0.06415641200146638,
          0.06319544500729535,
          0.06568945999606512,
          0.06343827400996815,
          0.06412508100038394,
          0.060864305996801704,
          0.06222780799726024,
          0.06291808400419541,
          0.061240579991135746,
          0.06651209200208541,
          0.06446283399418462,
          0.062485357993864454,
          0.06087482599832583,
          0.06540830999438185,
          0.06678283200017177,
          0.06778407099773176,
          0.06537496799137443,
          0.07038758999260608,
          0.060362607007846236,
          0.058856809002463706,
          0.059378559002652764,
          0.0696187810099218,
          0.07342195599630941,
          0.060552253999048844,
          0.06501820500125177,
          0.06524972400802653,
          0.06690654599515256,
          0.0622747099987464
        ]
      },
      {
        "test_name": "arch.sincos",
        "is_significant": false,
        "p_value": 0.9998899930160617,
        "is_pair_significant": false,
        "pair_p_value": 0.9993752496304245,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9986204374581575,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7906467723161779,
        "relative_improvement": -0.01513948247523501,
        "absolute_improvement_ms": -0.9795244163953998,
        "old_mean_ms": 64.69999341111519,
        "new_mean_ms": 65.67951782751058,
        "old_std_ms": 4.320098563749944,
        "new_std_ms": 4.018245801628664,
        "effect_size_cohens_d": -0.23479078811778376,
        "old_ci95_ms": [
          63.056715801575585,
          66.34327102065478
        ],
        "new_ci95_ms": [
          64.15105886806701,
          67.20797678695415
        ],
        "old_ci99_ms": [
          62.48324302507573,
          66.91674379715464
        ],
        "new_ci99_ms": [
          63.617655626794985,
          67.74138002822617
        ],
        "new_times": [
          0.0628946639917558,
          0.06721362900862005,
          0.06829757000377867,
          0.0652952949894825,
          0.07316542600165121,
          0.0640659280034015,
          0.06758121200255118,
          0.07120649199350737,
          0.059492243992281146,
          0.06188962599844672,
          0.07089555900893174,
          0.06875237700296566,
          0.06248251799843274,
          0.0635305879986845,
          0.06386112100153696,
          0.06575164200330619,
          0.06630044399935286,
          0.06404287701298017,
          0.06192845599434804,
          0.0687905189988669,
          0.06323539599543437,
          0.06495910299418028,
          0.0767714329995215,
          0.06263641400437336,
          0.05942801100900397,
          0.06473453300714027,
          0.06433211799594574,
          0.06928198698733468,
          0.06188883499999065
        ],
        "old_times": [
          0.06633468499057926,
          0.06383375999575946,
          0.06215361499926075,
          0.06419216199719813,
          0.06945021399587858,
          0.06416590200387873,
          0.07153815399215091,
          0.0710091530054342,
          0.06980830700194929,
          0.06984722899505869,
          0.062421586000709794,
          0.06610164599260315,
          0.057523797993781045,
          0.059978551988024265,
          0.06268419499974698,
          0.07005071699677501,
          0.06094976900203619,
          0.07202342199161649,
          0.060526953006046824,
          0.060528072994202375,
          0.062132973995176144,
          0.06410855900321621,
          0.059316856000805274,
          0.06491542099684011,
          0.07176046298991423,
          0.0596645010082284,
          0.06328567898890469,
          0.06023324099078309,
          0.06576022300578188
        ]
      },
      {
        "test_name": "arch.tan",
        "is_significant": false,
        "p_value": 0.9995130005084525,
        "is_pair_significant": false,
        "pair_p_value": 0.9952309084917033,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9972169864922762,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7769740442733022,
        "relative_improvement": -0.016012091371541897,
        "absolute_improvement_ms": -1.0221236202753448,
        "old_mean_ms": 63.834485861850126,
        "new_mean_ms": 64.85660948212548,
        "old_std_ms": 4.4685777908321755,
        "new_std_ms": 4.935279026755988,
        "effect_size_cohens_d": -0.21711670244180462,
        "old_ci95_ms": [
          62.13472977485325,
          65.53424194884701
        ],
        "new_ci95_ms": [
          62.97932973912378,
          66.73388922512717
        ],
        "old_ci99_ms": [
          61.541547078926584,
          66.12742464477367
        ],
        "new_ci99_ms": [
          62.3241946479886,
          67.38902431626234
        ],
        "new_times": [
          0.0677330879989313,
          0.06518153099750634,
          0.06757960299728438,
          0.062459206994390115,
          0.0611722970061237,
          0.06456225700094365,
          0.06598738100728951,
          0.056975358005729504,
          0.06432685800245963,
          0.06264690299576614,
          0.06108088399923872,
          0.0832837119960459,
          0.06873020700004417,
          0.06764078499691095,
          0.06669776899798308,
          0.06079690299520735,
          0.06614489700587001,
          0.06169554799271282,
          0.06720724799379241,
          0.05642946700390894,
          0.07137513799534645,
          0.06657452399667818,
          0.06032437499379739,
          0.06643194898788352,
          0.06058694500825368,
          0.06550193400471471,
          0.062368122991756536,
          0.06433494901284575,
          0.06501183500222396
        ],
        "old_times": [
          0.06159524399845395,
          0.06588788700173609,
          0.0636344119993737,
          0.06453674600925297,
          0.06989493199216668,
          0.06567891901067924,
          0.05816966301063076,
          0.06432523799594492,
          0.05826838599750772,
          0.06771714698697906,
          0.0621231740078656,
          0.05187510199903045,
          0.07006999799341429,
          0.06072116100403946,
          0.0718954079929972,
          0.06282445999386255,
          0.060175709004397504,
          0.07246812000812497,
          0.06773225800134242,
          0.0657059310033219,
          0.059512184001505375,
          0.0584013519983273,
          0.0657838740007719,
          0.06570794199069496,
          0.06402713600255083,
          0.06114973599324003,
          0.06343087399727665,
          0.06454947699967306,
          0.0633376199984923
        ]
      },
      {
        "test_name": "arch.asin",
        "is_significant": false,
        "p_value": 0.9760610485675904,
        "is_pair_significant": false,
        "pair_p_value": 0.9820816344456953,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9772401507943869,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.09054416765406514,
        "relative_improvement": 0.011077474358485377,
        "absolute_improvement_ms": 0.7316521037739332,
        "old_mean_ms": 66.048638895154,
        "new_mean_ms": 65.31698679138006,
        "old_std_ms": 4.7599006904256225,
        "new_std_ms": 5.134119696312192,
        "effect_size_cohens_d": 0.14779216130661052,
        "old_ci95_ms": [
          64.23806950355684,
          67.85920828675116
        ],
        "new_ci95_ms": [
          63.36407210271085,
          67.26990148004928
        ],
        "old_ci99_ms": [
          63.60621506215435,
          68.49106272815365
        ],
        "new_ci99_ms": [
          62.68254184739856,
          67.95143173536157
        ],
        "new_times": [
          0.06111022499680985,
          0.06569566100370139,
          0.06647537100070622,
          0.05779803900804836,
          0.07475465699099004,
          0.06375848599418532,
          0.06162817499716766,
          0.06420785399677698,
          0.07650266400014516,
          0.06559102599567268,
          0.07126325300487224,
          0.06167810701299459,
          0.06597456098825205,
          0.0582741270045517,
          0.07856980300857686,
          0.0657273619872285,
          0.061008592005237006,
          0.0675803519989131,
          0.06224988900066819,
          0.06247040699236095,
          0.0583268189948285,
          0.06949111599533353,
          0.06337137200171128,
          0.06888071198773105,
          0.06239866498799529,
          0.06319587500183843,
          0.06868482400022913,
          0.06502959498902783,
          0.06249502800346818
        ],
        "old_times": [
          0.06502339500002563,
          0.072860104002757,
          0.06883787100377958,
          0.06950049599981867,
          0.07070780199137516,
          0.056093694001901895,
          0.06834575200628024,
          0.07556644799478818,
          0.0698159979947377,
          0.06392234200029634,
          0.06593060000159312,
          0.06251188900205307,
          0.06778599999961443,
          0.05883173800248187,
          0.06378113699611276,
          0.06313915298960637,
          0.06362385099055246,
          0.06944217400450725,
          0.05373466400487814,
          0.06270622598822229,
          0.06778903999656904,
          0.0624070740013849,
          0.06589805899420753,
          0.06740019599965308,
          0.06823627800622489,
          0.06449275500199292,
          0.06668621899734717,
          0.07248310999420937,
          0.06785646299249493
        ]
      },
      {
        "test_name": "arch.acos",
        "is_significant": false,
        "p_value": 0.9003282348065358,
        "is_pair_significant": false,
        "pair_p_value": 0.9057364245283437,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9221661407500505,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.16747773903378738,
        "relative_improvement": 0.0232963194483224,
        "absolute_improvement_ms": 1.567135793799218,
        "old_mean_ms": 67.26967310332212,
        "new_mean_ms": 65.7025373095229,
        "old_std_ms": 6.269562190313314,
        "new_std_ms": 4.460575760721199,
        "effect_size_cohens_d": 0.28803520125448584,
        "old_ci95_ms": [
          64.88485918833241,
          69.65448701831183
        ],
        "new_ci95_ms": [
          64.00582503199429,
          67.3992495870515
        ],
        "old_ci99_ms": [
          64.05260427940816,
          70.48674192723607
        ],
        "new_ci99_ms": [
          63.41370456794906,
          67.99137005109674
        ],
        "new_times": [
          0.06775043800007552,
          0.06091513799037784,
          0.0648615090030944,
          0.06258961198909674,
          0.06535538799653295,
          0.06694957798754331,
          0.06961695000063628,
          0.0676990170031786,
          0.06815202499274164,
          0.06482529699860606,
          0.07087055800366215,
          0.06175379001069814,
          0.07206928401137702,
          0.06628501300292555,
          0.0635410579998279,
          0.06926874700002372,
          0.06838631299615372,
          0.059809636004501954,
          0.06221017800271511,
          0.07477403699886054,
          0.06219180699554272,
          0.060742390996892937,
          0.06554515499738045,
          0.068192894992535,
          0.06444855299196206,
          0.07508243901247624,
          0.05941547099791933,
          0.06545516199548729,
          0.05661614300333895
        ],
        "old_times": [
          0.06882847100496292,
          0.0708653079927899,
          0.07275662099709734,
          0.05486698599997908,
          0.06374651599617209,
          0.061030893004499376,
          0.06316323399369139,
          0.07105650499579497,
          0.06718862800335046,
          0.06066441800794564,
          0.06338069199409802,
          0.06779587100027129,
          0.06828755000606179,
          0.06979245599359274,
          0.07665439900301863,
          0.07175943200127222,
          0.06931062899820972,
          0.06916570400062483,
          0.0661644190113293,
          0.06434351899952162,
          0.06649514099990483,
          0.06464943999890238,
          0.08479074999922886,
          0.06135617499239743,
          0.07786929499707185,
          0.07184327499999199,
          0.05715226399479434,
          0.06427880600676872,
          0.0615631230029976
        ]
      },
      {
        "test_name": "arch.atan",
        "is_significant": false,
        "p_value": 0.9998677631835491,
        "is_pair_significant": false,
        "pair_p_value": 0.9990443756913839,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9995126537978649,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8285953637022919,
        "relative_improvement": -0.023402741112934743,
        "absolute_improvement_ms": -1.4989845839064042,
        "old_mean_ms": 64.05166713902246,
        "new_mean_ms": 65.55065172292886,
        "old_std_ms": 4.815531508612647,
        "new_std_ms": 4.618527273090813,
        "effect_size_cohens_d": -0.31771221613437317,
        "old_ci95_ms": [
          62.219936915889136,
          65.88339736215578
        ],
        "new_ci95_ms": [
          63.793857903315114,
          67.3074455425426
        ],
        "old_ci99_ms": [
          61.58069774488061,
          66.5226365331643
        ],
        "new_ci99_ms": [
          63.180770118498316,
          67.9205333273594
        ],
        "new_times": [
          0.06482532699010335,
          0.07401217799633741,
          0.06338135300029535,
          0.06463712999538984,
          0.07351505898986943,
          0.07517589299823157,
          0.06692696700338274,
          0.06585236599494237,
          0.06644491000042763,
          0.06182556299609132,
          0.06282546099100728,
          0.05903672600106802,
          0.05755234899697825,
          0.06522737300838344,
          0.07489067199639976,
          0.06561897799838334,
          0.06441264200839214,
          0.06633744500868488,
          0.06480974699661601,
          0.062338723000721075,
          0.06775717900018208,
          0.06960173000697978,
          0.061789511993993074,
          0.0622908009972889,
          0.06461621999915224,
          0.058473525001318194,
          0.06723272999806795,
          0.06877242799964733,
          0.060787912996602245
        ],
        "old_times": [
          0.06720191899512429,
          0.055041822997736745,
          0.06757628299237695,
          0.07482634999905713,
          0.059884479007450864,
          0.05886139000358526,
          0.06232953199651092,
          0.06707313298829831,
          0.06719732801138889,
          0.06446588400285691,
          0.06321457600279246,
          0.06902046700997744,
          0.05834006999793928,
          0.06290001400338951,
          0.06366733300092164,
          0.06982206799148116,
          0.07310095400316641,
          0.06237407399748918,
          0.05443277100857813,
          0.06467580099706538,
          0.0682063659915002,
          0.06447255400416907,
          0.06608583600609563,
          0.06573890200525057,
          0.06550359399989247,
          0.05831617800868116,
          0.0626858949981397,
          0.059126009000465274,
          0.061356764010270126
        ]
      },
      {
        "test_name": "arch.u8_casting",
        "is_significant": false,
        "p_value": 0.9921183878695258,
        "is_pair_significant": false,
        "pair_p_value": 0.999085499136804,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9987289663404226,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.25682834657950204,
        "relative_improvement": 0.006266199475656108,
        "absolute_improvement_ms": 0.4028265840165246,
        "old_mean_ms": 64.28563048167894,
        "new_mean_ms": 63.882803897662406,
        "old_std_ms": 4.556173045622174,
        "new_std_ms": 4.269856401023498,
        "effect_size_cohens_d": 0.09123350317905561,
        "old_ci95_ms": [
          62.55255494171721,
          66.01870602164064
        ],
        "new_ci95_ms": [
          62.25863738474361,
          65.5069704105812
        ],
        "old_ci99_ms": [
          61.947744387483304,
          66.62351657587456
        ],
        "new_ci99_ms": [
          61.691834019168276,
          66.07377377615653
        ],
        "new_times": [
          0.05888661999779288,
          0.060027443993021734,
          0.07591189100639895,
          0.06278342900623102,
          0.061692357005085796,
          0.06933542000479065,
          0.06473412300692871,
          0.06072641001082957,
          0.06015435900189914,
          0.0641497309989063,
          0.05782714999804739,
          0.06147737900028005,
          0.059544105009990744,
          0.0673451039911015,
          0.06233495200285688,
          0.06382664799457416,
          0.060840145000838675,
          0.06011075701098889,
          0.06341169399092905,
          0.06393321299401578,
          0.059126218999153934,
          0.06623054099327419,
          0.06174251899938099,
          0.06977372600522358,
          0.0640844090085011,
          0.0694453739997698,
          0.06981758799520321,
          0.06958724001015071,
          0.0637407659960445
        ],
        "old_times": [
          0.06620497000403702,
          0.06065559800481424,
          0.07398664799984545,
          0.06550010299542919,
          0.054254164002486505,
          0.0702085319935577,
          0.06555166500038467,
          0.06242326599021908,
          0.0618123519961955,
          0.06888961199729238,
          0.062055931994109415,
          0.0630622799944831,
          0.06273831801081542,
          0.06414339199545793,
          0.065400088991737,
          0.06343085398839321,
          0.06570326100336388,
          0.05926495499443263,
          0.06514720899576787,
          0.07064915999944787,
          0.06127823099086527,
          0.06419726301101036,
          0.057997356008854695,
          0.06171988799178507,
          0.061423298000590876,
          0.0611068250000244,
          0.0756895730010001,
          0.06196788800298236,
          0.06782060200930573
        ]
      },
      {
        "test_name": "arch.error",
        "is_significant": false,
        "p_value": 0.8753861914638777,
        "is_pair_significant": false,
        "pair_p_value": 0.8815007294773604,
        "is_binom_significant": false,
        "binom_p_value": 0.5,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8266577161848545,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.054601734928806184,
        "relative_improvement": 0.029526508164499934,
        "absolute_improvement_ms": 1.9606662430431132,
        "old_mean_ms": 66.40359341239171,
        "new_mean_ms": 64.44292716934861,
        "old_std_ms": 4.9409076567628984,
        "new_std_ms": 4.1826654983859575,
        "effect_size_cohens_d": 0.4283256085877456,
        "old_ci95_ms": [
          64.52417265303906,
          68.28301417174437
        ],
        "new_ci95_ms": [
          62.851926302061614,
          66.03392803663559
        ],
        "old_ci99_ms": [
          63.868290387729424,
          68.938896437054
        ],
        "new_ci99_ms": [
          62.29669711894606,
          66.58915721975114
        ],
        "new_times": [
          0.06727562099695206,
          0.06249919800029602,
          0.057927203000872396,
          0.061019381988444366,
          0.06382805900648236,
          0.06790862498746719,
          0.06969775300240144,
          0.06418732299061958,
          0.05934307799907401,
          0.06778581099933945,
          0.06910524099657778,
          0.06694833800429478,
          0.0668401239963714,
          0.07047205300477799,
          0.06913855198945384,
          0.05945567200251389,
          0.0630592399975285,
          0.06342376398970373,
          0.066582785002538,
          0.05809246998978779,
          0.06297909699787851,
          0.06010864699783269,
          0.06719280799734406,
          0.07018300199706573,
          0.06507526698987931,
          0.059683940999093466,
          0.06939545199566055,
          0.05594916800328065,
          0.0636872139875777
        ],
        "old_times": [
          0.07034557899169158,
          0.07599956498597749,
          0.06375158599985298,
          0.06795824700384401,
          0.06585847699898295,
          0.06602595400181599,
          0.06777603000227828,
          0.06024741199507844,
          0.056849663000321016,
          0.06418969199876301,
          0.06497844299883582,
          0.07143858999188524,
          0.0716178860020591,
          0.07301377000112552,
          0.06771592699806206,
          0.0649037699913606,
          0.07160469600057695,
          0.06881364999571815,
          0.0746907239954453,
          0.06498435398680158,
          0.06855620900751092,
          0.06814549400587566,
          0.0617895210016286,
          0.05959371700009797,
          0.05740570400666911,
          0.0638943410012871,
          0.059825656993780285,
          0.06386426099925302,
          0.06986529000278097
        ]
      },
      {
        "test_name": "arch.gamma",
        "is_significant": false,
        "p_value": 0.9999109930167394,
        "is_pair_significant": false,
        "pair_p_value": 0.999582842782214,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999595507979393,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9362490315300573,
        "relative_improvement": -0.0327628035479873,
        "absolute_improvement_ms": -2.058059758414388,
        "old_mean_ms": 62.81696117366664,
        "new_mean_ms": 64.87502093208103,
        "old_std_ms": 4.871295635867894,
        "new_std_ms": 5.204780260657167,
        "effect_size_cohens_d": -0.40828066713457556,
        "old_ci95_ms": [
          60.964019410939535,
          64.66990293639375
        ],
        "new_ci95_ms": [
          62.8952284021885,
          66.85481346197355
        ],
        "old_ci99_ms": [
          60.31737781417283,
          65.31654453316045
        ],
        "new_ci99_ms": [
          62.204318289119264,
          67.5457235750428
        ],
        "new_times": [
          0.06304334000742529,
          0.06715692600118928,
          0.06514145999972243,
          0.07016659100190736,
          0.06714021500374656,
          0.06416192199685611,
          0.07401443801063579,
          0.0675453409930924,
          0.07275032000325155,
          0.06755563100159634,
          0.06015848800598178,
          0.07313842499570455,
          0.07133240599068813,
          0.05336171900853515,
          0.06400147599924821,
          0.07099456300784368,
          0.06263599399244413,
          0.06432527799915988,
          0.060148158998345025,
          0.0602382619981654,
          0.05675015899760183,
          0.06114774699381087,
          0.06974676500249188,
          0.06185476400423795,
          0.057323100001667626,
          0.06849110800249036,
          0.06230994200450368,
          0.06342555400624406,
          0.061315514001762494
        ],
        "old_times": [
          0.058626869998988695,
          0.06897943599324208,
          0.07332659300300293,
          0.05984331600484438,
          0.06336978200124577,
          0.06525799399241805,
          0.06216450600186363,
          0.06433947899495251,
          0.0652592340047704,
          0.059887078998144716,
          0.06301648799853865,
          0.05626397000742145,
          0.05550498100637924,
          0.05858028800867032,
          0.06576773300184868,
          0.05820173399115447,
          0.06305953899573069,
          0.06917277300090063,
          0.06982440799765754,
          0.062370233004912734,
          0.0657708030048525,
          0.0726813770015724,
          0.06539279900607653,
          0.0585949999949662,
          0.05748707700695377,
          0.06260202301200479,
          0.05590017599752173,
          0.057811089005554095,
          0.062635094000143
        ]
      },
      {
        "test_name": "arch.extract_pair_128",
        "is_significant": false,
        "p_value": 0.99272022547415,
        "is_pair_significant": false,
        "pair_p_value": 0.9949463432275134,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9935284163802862,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3487185502252087,
        "relative_improvement": 0.007834802820681316,
        "absolute_improvement_ms": 0.49121155411582795,
        "old_mean_ms": 62.69609655257603,
        "new_mean_ms": 62.20488499846021,
        "old_std_ms": 4.234746241538322,
        "new_std_ms": 3.961352957597544,
        "effect_size_cohens_d": 0.11979808810421667,
        "old_ci95_ms": [
          61.085285230076614,
          64.30690787507545
        ],
        "new_ci95_ms": [
          60.6980669195104,
          63.711703077410014
        ],
        "old_ci99_ms": [
          60.523142573127764,
          64.86905053202432
        ],
        "new_ci99_ms": [
          60.17221593584269,
          64.23755406107773
        ],
        "new_times": [
          0.06326242799696047,
          0.0676657550066011,
          0.06371438498899806,
          0.057241996997618116,
          0.0632767880015308,
          0.06623906100867316,
          0.057913202996132895,
          0.06393762299558148,
          0.05646586700459011,
          0.06489375900127925,
          0.07103035498585086,
          0.06383443898812402,
          0.053238234992022626,
          0.06303971899615135,
          0.062169875993276946,
          0.06106370300403796,
          0.06039390800287947,
          0.06390869199822191,
          0.06312857299053576,
          0.06980500799545553,
          0.0644570830045268,
          0.061178667994681746,
          0.06298407699796371,
          0.06254368000372779,
          0.06349029600096401,
          0.05661145300837234,
          0.05941950099077076,
          0.05838757200399414,
          0.058645961005822755
        ],
        "old_times": [
          0.06735326400666963,
          0.06200629999511875,
          0.07203536298766267,
          0.05689681401418056,
          0.05982249599765055,
          0.06617161800386384,
          0.06318055400333833,
          0.06103638300555758,
          0.05987841800379101,
          0.05925256399495993,
          0.06496504299866501,
          0.058846899002674036,
          0.06442500199773349,
          0.06786770299368072,
          0.06257186199945863,
          0.05565204699814785,
          0.06475400400813669,
          0.05973788299888838,
          0.05676062899874523,
          0.06383223900047597,
          0.061579872999573126,
          0.058669713005656376,
          0.06856614000571426,
          0.0646796520013595,
          0.07099719299003482,
          0.059066307003377005,
          0.058306017992435955,
          0.06354173801082652,
          0.06573308100632858
        ]
      },
      {
        "test_name": "arch.fp_manipulations",
        "is_significant": false,
        "p_value": 0.999991139944232,
        "is_pair_significant": false,
        "pair_p_value": 0.9998321401527868,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9996964391320944,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.8874353145954007,
        "relative_improvement": -0.023864685456525887,
        "absolute_improvement_ms": -1.4337327216896387,
        "old_mean_ms": 60.07758720731764,
        "new_mean_ms": 61.511319929007286,
        "old_std_ms": 3.5011719050699104,
        "new_std_ms": 3.841489029493176,
        "effect_size_cohens_d": -0.3901025223703342,
        "old_ci95_ms": [
          58.745812639249365,
          61.409361775385925
        ],
        "new_ci95_ms": [
          60.05009564990689,
          62.97254420810768
        ],
        "old_ci99_ms": [
          58.28104852709427,
          61.87412588754101
        ],
        "new_ci99_ms": [
          59.54015603924094,
          63.48248381877362
        ],
        "new_times": [
          0.07268785800260957,
          0.06603136399644427,
          0.06493870199483354,
          0.060219491002499126,
          0.06542697999975644,
          0.06226977999904193,
          0.05599386998801492,
          0.06274257799668703,
          0.05977594500291161,
          0.06038509699283168,
          0.062487488001352176,
          0.059727201994974166,
          0.06276234799588565,
          0.061225769997690804,
          0.057239866990130395,
          0.05890116099908482,
          0.06415344199922401,
          0.05633948299509939,
          0.05824676499469206,
          0.06211511399305891,
          0.06180593199678697,
          0.05469631000596564,
          0.06231450199265964,
          0.06793083499360364,
          0.06259810199844651,
          0.06436491900240071,
          0.060744611007976346,
          0.05874847499944735,
          0.05695428700710181
        ],
        "old_times": [
          0.0576392719958676,
          0.05820707400562242,
          0.05600722999952268,
          0.06052182300481945,
          0.05591173699940555,
          0.05544898900552653,
          0.0635848699894268,
          0.06731845298781991,
          0.05518528900574893,
          0.06308515100681689,
          0.05531484400853515,
          0.0636586929904297,
          0.05622360898996703,
          0.05630186099733692,
          0.05730527899868321,
          0.06025227300415281,
          0.05923301400616765,
          0.06210862300940789,
          0.06116807699436322,
          0.058947243000147864,
          0.05769072500697803,
          0.06271294600446709,
          0.06368953399942257,
          0.06293514498975128,
          0.05736888200044632,
          0.06519108099746518,
          0.06503955500375014,
          0.0614842100039823,
          0.06271454700618051
        ]
      },
      {
        "test_name": "arch.hyperbolic",
        "is_significant": false,
        "p_value": 0.9999409597286248,
        "is_pair_significant": false,
        "pair_p_value": 0.9998934309356047,
        "is_binom_significant": false,
        "binom_p_value": 0.9999923817813396,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999891409650445,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9322715276581539,
        "relative_improvement": -0.026255187713972575,
        "absolute_improvement_ms": -1.6215508628098645,
        "old_mean_ms": 61.761160517123464,
        "new_mean_ms": 63.382711379933326,
        "old_std_ms": 2.86196343807236,
        "new_std_ms": 5.315186551732378,
        "effect_size_cohens_d": -0.3798780644618835,
        "old_ci95_ms": [
          60.67252784643204,
          62.84979318781489
        ],
        "new_ci95_ms": [
          61.3609225429198,
          65.40450021694686
        ],
        "old_ci99_ms": [
          60.29261565349175,
          63.22970538075518
        ],
        "new_ci99_ms": [
          60.655356513706145,
          66.1100662461605
        ],
        "new_times": [
          0.06299922699690796,
          0.06485646800138056,
          0.06242393499996979,
          0.0661835889914073,
          0.05838095099898055,
          0.06456088699633256,
          0.06849579801200889,
          0.06272272700152826,
          0.06246271700365469,
          0.055573673991602845,
          0.06201906000205781,
          0.08039704299881123,
          0.06722240900853649,
          0.06816563499160111,
          0.06645708900759928,
          0.06506648600043263,
          0.06378934800159186,
          0.0545975060085766,
          0.06504679600766394,
          0.06641298800241202,
          0.07100591300695669,
          0.05980109499068931,
          0.06142824799462687,
          0.06322197600093205,
          0.05766480299644172,
          0.06489556000451557,
          0.05450929299695417,
          0.0572365970001556,
          0.06050081200373825
        ],
        "old_times": [
          0.059602866997011006,
          0.06475235499965493,
          0.06591817899607122,
          0.0632887789979577,
          0.05840232101036236,
          0.06292689401016105,
          0.062204197005485184,
          0.06310295200091787,
          0.06151807100104634,
          0.05629162199329585,
          0.060687958990456536,
          0.06340486300177872,
          0.06174994001048617,
          0.056781188992317766,
          0.06069318999652751,
          0.0646184889919823,
          0.06315702400752343,
          0.06591846999072004,
          0.05983775599452201,
          0.06403353701171,
          0.05714679299853742,
          0.05816335299459752,
          0.06442066200543195,
          0.06411429999570828,
          0.06450328500068281,
          0.06215441499080043,
          0.06413849101227242,
          0.056847143001505174,
          0.06069455899705645
        ]
      },
      {
        "test_name": "arch.reciprocal",
        "is_significant": false,
        "p_value": 0.9867833722924539,
        "is_pair_significant": false,
        "pair_p_value": 0.9762676665010358,
        "is_binom_significant": false,
        "binom_p_value": 0.9319770261645317,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9655649084597826,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.1125646854045993,
        "relative_improvement": 0.011530088690286966,
        "absolute_improvement_ms": 0.7406852422844196,
        "old_mean_ms": 64.23933606932005,
        "new_mean_ms": 63.498650827035625,
        "old_std_ms": 3.5138392093631095,
        "new_std_ms": 4.765488723126187,
        "effect_size_cohens_d": 0.1769137680655105,
        "old_ci95_ms": [
          62.90274311639062,
          65.57592902224947
        ],
        "new_ci95_ms": [
          61.68595586147697,
          65.3113457925943
        ],
        "old_ci99_ms": [
          62.436297479136826,
          66.04237465950325
        ],
        "new_ci99_ms": [
          61.053359635001584,
          65.94394201906967
        ],
        "new_times": [
          0.06463132999488153,
          0.06131204300618265,
          0.06003162400156725,
          0.060101875991676934,
          0.06314456299878657,
          0.06310559199482668,
          0.06630125400261022,
          0.0665457130089635,
          0.06129081200924702,
          0.06379024799389299,
          0.06039134798629675,
          0.06505436699080747,
          0.06486770900664851,
          0.05816783300542738,
          0.06440924100752454,
          0.07118329999502748,
          0.06168784799228888,
          0.05676761901122518,
          0.06471984399831854,
          0.07662238900957163,
          0.06299010699149221,
          0.06072978099109605,
          0.060769671996240504,
          0.06347028599702753,
          0.07100262399762869,
          0.052957034000428393,
          0.07184781500836834,
          0.06110723500023596,
          0.06245976699574385
        ],
        "old_times": [
          0.06293152499711141,
          0.06386358001327608,
          0.06494078099785838,
          0.06262030199286528,
          0.06883139999990817,
          0.0676036840013694,
          0.06294314499245957,
          0.06077165200258605,
          0.06852807800169103,
          0.05980409499898087,
          0.06332888000179082,
          0.06368065401329659,
          0.06301088799955323,
          0.07358132100489456,
          0.0651061880053021,
          0.06200720999913756,
          0.06736524400184862,
          0.06278059899341315,
          0.0690757999982452,
          0.06331130900071003,
          0.06635397499485407,
          0.06350887699227314,
          0.06272068699763622,
          0.05503898300230503,
          0.06385536000016145,
          0.06515064000268467,
          0.06570745099452324,
          0.06556345500575844,
          0.058954983003786765
        ]
      },
      {
        "test_name": "arch.load",
        "is_significant": false,
        "p_value": 0.9958145197382321,
        "is_pair_significant": false,
        "pair_p_value": 0.995093783146516,
        "is_binom_significant": false,
        "binom_p_value": 0.9997269436717033,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9976060185581446,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6851290682966794,
        "relative_improvement": -0.018621410476983055,
        "absolute_improvement_ms": -1.2444831698867898,
        "old_mean_ms": 66.83076834727625,
        "new_mean_ms": 68.07525151716304,
        "old_std_ms": 4.942326257628968,
        "new_std_ms": 7.63972399107937,
        "effect_size_cohens_d": -0.19342380094690728,
        "old_ci95_ms": [
          64.95080798101286,
          68.71072871353964
        ],
        "new_ci95_ms": [
          65.16925592647311,
          70.98124710785297
        ],
        "old_ci99_ms": [
          64.29473740310678,
          69.36679929144572
        ],
        "new_ci99_ms": [
          64.15511847932795,
          71.99538455499813
        ],
        "new_times": [
          0.06334713000978809,
          0.057109601999400184,
          0.06465655099600554,
          0.06954193800629582,
          0.06479572498938069,
          0.06219846699968912,
          0.06558186699112412,
          0.0632393569976557,
          0.08089260099222884,
          0.06253839000419248,
          0.06282438100606669,
          0.06654852299834602,
          0.07111149799311534,
          0.07326940000348259,
          0.06498474298859946,
          0.06813903400325216,
          0.06819993600947782,
          0.06648076999408659,
          0.06257289100904018,
          0.06976786599261686,
          0.09651755800587125,
          0.07352518000698183,
          0.07303597099962644,
          0.06643011899723206,
          0.06498306400317233,
          0.07347014801052865,
          0.07286487499368377,
          0.05526481199194677,
          0.0702898970048409
        ],
        "old_times": [
          0.06395287399936933,
          0.06546642100147437,
          0.06787429400719702,
          0.06218471699685324,
          0.06245125700661447,
          0.0652526640042197,
          0.06520783100859262,
          0.06084339501103386,
          0.06236830299894791,
          0.06438092999451328,
          0.06493968100403436,
          0.06010520699783228,
          0.0628025400073966,
          0.07539750199066475,
          0.06895562499994412,
          0.06821962700632866,
          0.07573528400098439,
          0.05558336401009001,
          0.07259903399972245,
          0.0625604899978498,
          0.06783991200791206,
          0.06830754100519698,
          0.07321987800241914,
          0.0682849000004353,
          0.0725609240034828,
          0.07222380000166595,
          0.06569201999809593,
          0.0736812649993226,
          0.06940100200881716
        ]
      },
      {
        "test_name": "arch.store",
        "is_significant": false,
        "p_value": 0.9982000629740377,
        "is_pair_significant": false,
        "pair_p_value": 0.9996802057887441,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9996964391320944,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7068808939129148,
        "relative_improvement": -0.014151499132951884,
        "absolute_improvement_ms": -0.915605001801878,
        "old_mean_ms": 64.70021254991197,
        "new_mean_ms": 65.61581755171385,
        "old_std_ms": 4.852074694467986,
        "new_std_ms": 5.7117804777307635,
        "effect_size_cohens_d": -0.17277555186124277,
        "old_ci95_ms": [
          62.85458204228024,
          66.5458430575437
        ],
        "new_ci95_ms": [
          63.44317245319252,
          67.78846265023518
        ],
        "old_ci99_ms": [
          62.21049193513094,
          67.189933164693
        ],
        "new_ci99_ms": [
          62.68496044460384,
          68.54667465882386
        ],
        "new_times": [
          0.07828893199621234,
          0.06979543699708302,
          0.06262610301200766,
          0.05744632499408908,
          0.06911620100436267,
          0.07202567299827933,
          0.06449227400298696,
          0.06762494500435423,
          0.06205871199199464,
          0.07534732000203803,
          0.06792951600800734,
          0.06837580200226512,
          0.05765334400348365,
          0.06032666499959305,
          0.06031558499671519,
          0.0687967489939183,
          0.06208384199999273,
          0.05640723599935882,
          0.06189547499525361,
          0.06409577900194563,
          0.05841929299640469,
          0.07235857499472331,
          0.07217803799721878,
          0.06800411800213624,
          0.0672106790007092,
          0.06615322899597231,
          0.06543724099174142,
          0.0694096530060051,
          0.05698596801084932
        ],
        "old_times": [
          0.07131097500678152,
          0.07743908000702504,
          0.06508596699859481,
          0.05944634199840948,
          0.059775955000077374,
          0.06895006400009152,
          0.06895627499034163,
          0.06165050600247923,
          0.058991015001083724,
          0.061936935991980135,
          0.06984154900419526,
          0.06941724299394991,
          0.06196402799105272,
          0.064871579001192,
          0.06381016899831593,
          0.055595864992938004,
          0.06817447498906404,
          0.06533654699160252,
          0.060546833992702886,
          0.06259403200238012,
          0.05785583100805525,
          0.06991738200304098,
          0.0681367439974565,
          0.06885999099176843,
          0.06458895800460596,
          0.058617581002181396,
          0.06375835600192659,
          0.06479100599244703,
          0.06408487899170723
        ]
      },
      {
        "test_name": "xsimd.alignment",
        "is_significant": false,
        "p_value": 0.9993516773958752,
        "is_pair_significant": false,
        "pair_p_value": 0.9986539860157564,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9981008060276508,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6795850999251122,
        "relative_improvement": -0.009452682929806592,
        "absolute_improvement_ms": -0.6276887243388818,
        "old_mean_ms": 66.40323482760918,
        "new_mean_ms": 67.03092355194806,
        "old_std_ms": 4.782983141375457,
        "new_std_ms": 4.328083927760905,
        "effect_size_cohens_d": -0.1376145779683538,
        "old_ci95_ms": [
          64.58388534123885,
          68.22258431397951
        ],
        "new_ci95_ms": [
          65.3846084723856,
          68.67723863151052
        ],
        "old_ci99_ms": [
          63.948966812978306,
          68.85750284224005
        ],
        "new_ci99_ms": [
          64.81007567635073,
          69.25177142754539
        ],
        "new_times": [
          0.07048156300152186,
          0.07194302999414504,
          0.06490759999724105,
          0.06397393399674911,
          0.06716928699461278,
          0.05844563299615402,
          0.0733021310006734,
          0.06683163400157355,
          0.06514156000048388,
          0.07690908899530768,
          0.06779424101114273,
          0.06658353500824887,
          0.06586243699712213,
          0.07065472000977024,
          0.06621886100037955,
          0.06687483499990776,
          0.06230119000247214,
          0.06704809199436568,
          0.07212530600372702,
          0.062346571998205036,
          0.06741731600777712,
          0.06691824700101279,
          0.06009197600360494,
          0.06097543999203481,
          0.07408225099788979,
          0.06457844699616544,
          0.06893430500349496,
          0.06313164299353957,
          0.070851908007171
        ],
        "old_times": [
          0.06833074199676048,
          0.07424848701339215,
          0.0689815660007298,
          0.06694410799536854,
          0.05630326099344529,
          0.06837438300135545,
          0.05802060700079892,
          0.07052938600827474,
          0.06360559099994134,
          0.07004939699254464,
          0.06877852798788808,
          0.0693605709966505,
          0.06336282199481502,
          0.0750417480012402,
          0.0633702710038051,
          0.05819045398675371,
          0.06572603198583238,
          0.06684060400584713,
          0.05879848700715229,
          0.0720465240010526,
          0.0667237089946866,
          0.07510564000403974,
          0.0657337720040232,
          0.06384740000066813,
          0.06593627900292631,
          0.06527015400934033,
          0.0661197169974912,
          0.06558388601115439,
          0.06446968400268815
        ]
      },
      {
        "test_name": "xsimd.poly_evaluation",
        "is_significant": false,
        "p_value": 0.7249430698252686,
        "is_pair_significant": false,
        "pair_p_value": 0.7216660746517046,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.7789897117763758,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.14872046560509522,
        "relative_improvement": 0.035334044646418344,
        "absolute_improvement_ms": 2.376604070064042,
        "old_mean_ms": 67.26102527594274,
        "new_mean_ms": 64.8844212058787,
        "old_std_ms": 7.376439422184103,
        "new_std_ms": 5.3755348851870455,
        "effect_size_cohens_d": 0.3682373789372972,
        "old_ci95_ms": [
          64.45517777924152,
          70.06687277264396
        ],
        "new_ci95_ms": [
          62.83967709049,
          66.9291653212674
        ],
        "old_ci99_ms": [
          63.47599012097406,
          71.04606043091142
        ],
        "new_ci99_ms": [
          62.12610010369454,
          67.64274230806286
        ],
        "new_times": [
          0.06453557599161286,
          0.06295506599417422,
          0.07325765899440739,
          0.06390329200075939,
          0.07262127600552049,
          0.06265405399608426,
          0.05872217299474869,
          0.05400059401290491,
          0.06526363400917035,
          0.06996325400541537,
          0.05824014499376062,
          0.06720427799155004,
          0.0683585319929989,
          0.06058344499615487,
          0.0718659359990852,
          0.06448832400201354,
          0.05915586999617517,
          0.06649173099140171,
          0.0645816879987251,
          0.05999029299709946,
          0.06569017100264318,
          0.059184402009123005,
          0.07642159999522846,
          0.06649004999781027,
          0.06824636801320594,
          0.07324298899038695,
          0.06329474899393972,
          0.06068219000007957,
          0.059558876004302874
        ],
        "old_times": [
          0.07056028700026218,
          0.07464198199159,
          0.07537120099004824,
          0.05816593200142961,
          0.06305532999977004,
          0.06322208599885926,
          0.07022538399905898,
          0.07058991699886974,
          0.06198456900892779,
          0.06414186100300867,
          0.06604870400042273,
          0.06377159699331969,
          0.058957603003364056,
          0.0552488009998342,
          0.07127300299180206,
          0.07399215799523517,
          0.06665038700157311,
          0.05732259100477677,
          0.06509550700138789,
          0.07252118199539836,
          0.06592825001280289,
          0.06239777500741184,
          0.06884460100263823,
          0.08421756900497712,
          0.06984514898795169,
          0.0649575220013503,
          0.08710159899783321,
          0.058578929005307145,
          0.06585825700312853
        ]
      },
      {
        "test_name": "xsimd.power",
        "is_significant": false,
        "p_value": 0.99588943988961,
        "is_pair_significant": false,
        "pair_p_value": 0.9954431011503111,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9939453694969416,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3778904498491375,
        "relative_improvement": 0.003122827306965523,
        "absolute_improvement_ms": 0.20683624235720566,
        "old_mean_ms": 66.23364727721341,
        "new_mean_ms": 66.02681103485621,
        "old_std_ms": 3.7622342638710933,
        "new_std_ms": 4.918771598109458,
        "effect_size_cohens_d": 0.04723524766697603,
        "old_ci95_ms": [
          64.80256989866962,
          67.66472465575721
        ],
        "new_ci95_ms": [
          64.1558103818997,
          67.8978116878127
        ],
        "old_ci99_ms": [
          64.30315098558069,
          68.16414356884613
        ],
        "new_ci99_ms": [
          63.50286657431954,
          68.55075549539288
        ],
        "new_times": [
          0.07124822300102096,
          0.05825648599420674,
          0.05878720599866938,
          0.06621274999633897,
          0.08001998800318688,
          0.06097666001005564,
          0.06474059401080012,
          0.06570946199644823,
          0.06781376099388581,
          0.06906290899496526,
          0.06359556000097655,
          0.07240573700983077,
          0.06571297100163065,
          0.060599265998462215,
          0.06971941400843207,
          0.07125105199520476,
          0.06117602800077293,
          0.07010652899043635,
          0.06125331101065967,
          0.06366437299584504,
          0.060688258992740884,
          0.06458690800354816,
          0.06401124699914362,
          0.06505923499935307,
          0.06909898099547718,
          0.06692982699314598,
          0.0718865680100862,
          0.06924778599932324,
          0.06095642900618259
        ],
        "old_times": [
          0.06430683800135739,
          0.06929222799954005,
          0.06866900400200393,
          0.06958376900001895,
          0.06896411599882413,
          0.0613040030002594,
          0.07006654700671788,
          0.06390404200647026,
          0.06608825500006787,
          0.065877767992788,
          0.06614147800428327,
          0.0671299050009111,
          0.060268483008258045,
          0.06467899099516217,
          0.06294552600593306,
          0.06743813700450119,
          0.057573729995056055,
          0.0640255369944498,
          0.06301221900503151,
          0.06346788599330466,
          0.06795846699969843,
          0.07508166899788193,
          0.07204003300284967,
          0.0652142220060341,
          0.06371137400856242,
          0.07176514199818484,
          0.06473215301230084,
          0.06937564100371674,
          0.06615860799502116
        ]
      },
      {
        "test_name": "xsimd.rounding",
        "is_significant": false,
        "p_value": 0.9612588820232546,
        "is_pair_significant": false,
        "pair_p_value": 0.951691840979549,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9121282212436199,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3093683709254245,
        "relative_improvement": 0.010816295711453152,
        "absolute_improvement_ms": 0.7111084806042389,
        "old_mean_ms": 65.74417892913735,
        "new_mean_ms": 65.03307044853311,
        "old_std_ms": 6.4420950461477835,
        "new_std_ms": 4.678482393347086,
        "effect_size_cohens_d": 0.1263120077433729,
        "old_ci95_ms": [
          63.29373702563791,
          68.19462083263679
        ],
        "new_ci95_ms": [
          63.2534709208457,
          66.81266997622052
        ],
        "old_ci99_ms": [
          62.43857919114083,
          69.04977866713386
        ],
        "new_ci99_ms": [
          62.632424375650764,
          67.43371652141546
        ],
        "new_times": [
          0.0647578549978789,
          0.06438139099918772,
          0.06841855500533711,
          0.058902960998238996,
          0.06274923800083343,
          0.06482562799646985,
          0.06129869300639257,
          0.05900704499799758,
          0.06612921701162122,
          0.06152353099605534,
          0.0684259850095259,
          0.06814119400223717,
          0.06273760698968545,
          0.06012729799840599,
          0.06535879800503608,
          0.07702203300141264,
          0.06010581699956674,
          0.06649359100265428,
          0.06301575800171122,
          0.07214944699080661,
          0.057345911001903005,
          0.06634028600819875,
          0.06856408999010455,
          0.06849305699870456,
          0.06353555800160393,
          0.06149485999776516,
          0.0657077210053103,
          0.06315643299603835,
          0.07574948499677703
        ],
        "old_times": [
          0.06785952299833298,
          0.06520608199934941,
          0.05935600900556892,
          0.06797781700151972,
          0.06867923399840947,
          0.05142850500124041,
          0.06625515199266374,
          0.06424009500187822,
          0.06294092600001022,
          0.0592516039905604,
          0.06402068700117525,
          0.07861991500249133,
          0.07313705499109346,
          0.06726822099881247,
          0.06057368499750737,
          0.07648094299656805,
          0.06322479598748032,
          0.0630821999948239,
          0.07153760400251485,
          0.05664040400006343,
          0.06411372999718878,
          0.07283775399264414,
          0.0640513580001425,
          0.07501494599273428,
          0.07190372800687328,
          0.06342759401013609,
          0.06126791099086404,
          0.07110879699757788,
          0.05507491399475839
        ]
      },
      {
        "test_name": "xsimd.select_dynamic",
        "is_significant": false,
        "p_value": 0.4654203116093893,
        "is_pair_significant": false,
        "pair_p_value": 0.46914083933170225,
        "is_binom_significant": false,
        "binom_p_value": 0.644464448094368,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.5339360814541578,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 0.04357354712374595,
        "relative_improvement": 0.051996367840090825,
        "absolute_improvement_ms": 3.501909380529786,
        "old_mean_ms": 67.34911544782366,
        "new_mean_ms": 63.84720606729388,
        "old_std_ms": 6.355287430353003,
        "new_std_ms": 5.703198309496258,
        "effect_size_cohens_d": 0.579973350936377,
        "old_ci95_ms": [
          64.93169339543827,
          69.76653750020905
        ],
        "new_ci95_ms": [
          61.677825450981224,
          66.01658668360655
        ],
        "old_ci99_ms": [
          64.08805886387437,
          70.61017203177298
        ],
        "new_ci99_ms": [
          60.92075268488278,
          66.773659449705
        ],
        "new_times": [
          0.05371095299778972,
          0.06304490900947712,
          0.05557655400480144,
          0.0644351819937583,
          0.06675374000042211,
          0.06129981299454812,
          0.06580217399459798,
          0.06674145099532325,
          0.06656436399498489,
          0.07118321099551395,
          0.056043901000521146,
          0.06836062199727166,
          0.06005363501026295,
          0.05962632899172604,
          0.06963156099664047,
          0.06369545399502385,
          0.06301295799494255,
          0.059037375991465524,
          0.06412324099801481,
          0.07115642999997362,
          0.06172051800240297,
          0.06353642800240777,
          0.05694190500071272,
          0.0646980729943607,
          0.07286321399442386,
          0.05174752700258978,
          0.07417650500428863,
          0.06663241599744651,
          0.06939853199583013
        ],
        "old_times": [
          0.06536149700696114,
          0.07044856199354399,
          0.06219610699918121,
          0.06744369700027164,
          0.0700406169926282,
          0.0709284309996292,
          0.06707077300234232,
          0.06578256300417706,
          0.07348899799399078,
          0.06435786999645643,
          0.07387011199898552,
          0.07799724099459127,
          0.06405939800606575,
          0.06494099101109896,
          0.0673467839951627,
          0.05980746599379927,
          0.07943316599994432,
          0.06299618699995335,
          0.06657779398665298,
          0.06302354800573085,
          0.0859192339994479,
          0.0626208530011354,
          0.06759526299720164,
          0.05998205300420523,
          0.0685578200063901,
          0.07076584499736782,
          0.05804189800983295,
          0.06366457299736794,
          0.05880500699277036
        ]
      },
      {
        "test_name": "xsimd.select_static",
        "is_significant": false,
        "p_value": 0.9999109357830341,
        "is_pair_significant": false,
        "pair_p_value": 0.9999586303916134,
        "is_binom_significant": false,
        "binom_p_value": 0.9999481420964003,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999991673976183,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9689902044212559,
        "relative_improvement": -0.043399795757806135,
        "absolute_improvement_ms": -2.767798550535355,
        "old_mean_ms": 63.77446027583009,
        "new_mean_ms": 66.54225882636544,
        "old_std_ms": 2.9498615704838276,
        "new_std_ms": 7.155568916201978,
        "effect_size_cohens_d": -0.5057339495632602,
        "old_ci95_ms": [
          62.652392943688085,
          64.89652760797209
        ],
        "new_ci95_ms": [
          63.82042597695344,
          69.26409167577745
        ],
        "old_ci99_ms": [
          62.26081268686006,
          65.28810786480012
        ],
        "new_ci99_ms": [
          62.87055784009119,
          70.2139598126397
        ],
        "new_times": [
          0.07065751000482123,
          0.06535499800520483,
          0.06297252599324565,
          0.06342539400793612,
          0.06583825600682758,
          0.06423407400143333,
          0.06518740099272691,
          0.07092392000777181,
          0.06802997000340838,
          0.06685499499144498,
          0.060850545996800065,
          0.07147600199095905,
          0.07058174799021799,
          0.06333598999481183,
          0.06535507799708284,
          0.06405383699166123,
          0.06264709400420543,
          0.06264665399794467,
          0.06494991200452205,
          0.061504299999796785,
          0.0794018739979947,
          0.09629096899880096,
          0.06612470699474216,
          0.05721876700408757,
          0.06098889099666849,
          0.06811464400379919,
          0.06472465398837812,
          0.06574565300252289,
          0.06023514199478086
        ],
        "old_times": [
          0.0627506379969418,
          0.06136212499404792,
          0.06318860501050949,
          0.06373702600831166,
          0.06662696700368542,
          0.0635831099934876,
          0.06945136400463525,
          0.06256598100299016,
          0.06208880299527664,
          0.060607306004385464,
          0.05771168500359636,
          0.061546752011054195,
          0.06793576601194218,
          0.06329562899190933,
          0.06341556301049422,
          0.0617074090114329,
          0.0646322889951989,
          0.062201048000133596,
          0.06232363099115901,
          0.061389845999656245,
          0.06503724498907104,
          0.06708668298961129,
          0.06134528398979455,
          0.06346874599694274,
          0.06698863000201527,
          0.07186648598872125,
          0.060997441003564745,
          0.0657496030034963,
          0.0647976869950071
        ]
      },
      {
        "test_name": "xsimd.shuffle_128_low_high",
        "is_significant": false,
        "p_value": 0.994693416788277,
        "is_pair_significant": false,
        "pair_p_value": 0.9893270250502314,
        "is_binom_significant": false,
        "binom_p_value": 0.995934970676899,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9892234113067389,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.22302595572669776,
        "relative_improvement": 0.009734722086780206,
        "absolute_improvement_ms": 0.6236256881022861,
        "old_mean_ms": 64.06199196473985,
        "new_mean_ms": 63.43836627663756,
        "old_std_ms": 3.32889724623117,
        "new_std_ms": 4.178854881765165,
        "effect_size_cohens_d": 0.16507399001881065,
        "old_ci95_ms": [
          62.79574717229665,
          65.32823675718303
        ],
        "new_ci95_ms": [
          61.84881489039344,
          65.02791766288168
        ],
        "old_ci99_ms": [
          62.353851711276924,
          65.77013221820276
        ],
        "new_ci99_ms": [
          61.294091548721426,
          65.5826410045537
        ],
        "new_times": [
          0.06515414100431371,
          0.05860364901309367,
          0.058838539000134915,
          0.06048030100646429,
          0.06481880799401551,
          0.0672015579912113,
          0.06799116800539196,
          0.06378796799981501,
          0.0656811299995752,
          0.06956875800096896,
          0.06379358800768387,
          0.06264756400196347,
          0.057374272000743076,
          0.066437898989534,
          0.05664594500558451,
          0.0636519919935381,
          0.05880312698718626,
          0.06536599699757062,
          0.0621402740071062,
          0.06621065000945237,
          0.059546044998569414,
          0.06619887000124436,
          0.06529719500395004,
          0.056247658998472616,
          0.06411484000273049,
          0.06542144999548327,
          0.06008057600411121,
          0.07491753299837,
          0.06269112600421067
        ],
        "old_times": [
          0.06439527100883424,
          0.06964960099139716,
          0.05981702600547578,
          0.06853757900535129,
          0.06435484999383334,
          0.05786747200181708,
          0.06680122199759353,
          0.06404830799147021,
          0.0701768919971073,
          0.06437018999713473,
          0.06558151599892881,
          0.06399396501365118,
          0.06340989399177488,
          0.0670277119934326,
          0.06598498100356665,
          0.06067365901253652,
          0.06747504899976775,
          0.06330439899466,
          0.06055783400370274,
          0.06840014399494976,
          0.06481923699902836,
          0.06604458299989346,
          0.06381710899586324,
          0.06017363000137266,
          0.06292138498974964,
          0.06032810600299854,
          0.06050781199883204,
          0.05781097899307497,
          0.064947361999657
        ]
      },
      {
        "test_name": "xsimd.simd_traits",
        "is_significant": false,
        "p_value": 0.9034598491056957,
        "is_pair_significant": false,
        "pair_p_value": 0.9529529262112266,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9369833711534739,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.0957237558300531,
        "relative_improvement": 0.022040398057134515,
        "absolute_improvement_ms": 1.4415389681556035,
        "old_mean_ms": 65.40439807025089,
        "new_mean_ms": 63.96285910209529,
        "old_std_ms": 4.776828074971534,
        "new_std_ms": 5.932450920242142,
        "effect_size_cohens_d": 0.26765923942794523,
        "old_ci95_ms": [
          63.5873898459276,
          67.22140629457418
        ],
        "new_ci95_ms": [
          61.7062754563591,
          66.21944274783147
        ],
        "old_ci99_ms": [
          62.95328837379857,
          67.85550776670321
        ],
        "new_ci99_ms": [
          60.918770483850494,
          67.00694772034008
        ],
        "new_times": [
          0.058594759000698105,
          0.05617478700878564,
          0.06284873200638685,
          0.06079998399945907,
          0.06694687801063992,
          0.059104329004185274,
          0.056615762994624674,
          0.05725980899296701,
          0.07353332999628037,
          0.059368027999880724,
          0.06617656898743007,
          0.06603607299621217,
          0.06209417300124187,
          0.06133743499231059,
          0.06390718200418632,
          0.0607452319964068,
          0.07276919099967927,
          0.06352971799788065,
          0.07404449900786858,
          0.06259282199607696,
          0.07251156099664513,
          0.05623995899804868,
          0.07355364999966696,
          0.06057344499276951,
          0.07577855499403086,
          0.06469167199975345,
          0.06104523300018627,
          0.05839961099263746,
          0.06764993499382399
        ],
        "old_times": [
          0.061241030009114183,
          0.06934290999197401,
          0.06485431799956132,
          0.057865060996846296,
          0.07165268799872138,
          0.06468442300683819,
          0.06429326700163074,
          0.060756902006687596,
          0.06733218299632426,
          0.06314063300669659,
          0.06321641500107944,
          0.060009373002685606,
          0.06042850999801885,
          0.06470978300785646,
          0.06288296300044749,
          0.058873058995231986,
          0.06955816799018066,
          0.06479079599375837,
          0.06459484799415804,
          0.06729273201199248,
          0.0665844650065992,
          0.06674317100259941,
          0.07927573900087737,
          0.057249767996836454,
          0.0702446850045817,
          0.07096559200726915,
          0.07050486400839873,
          0.06604280399915297,
          0.06759639400115702
        ]
      },
      {
        "test_name": "xsimd.revert_simd_traits",
        "is_significant": false,
        "p_value": 0.9969570329411702,
        "is_pair_significant": false,
        "pair_p_value": 0.9941825573396499,
        "is_binom_significant": false,
        "binom_p_value": 0.9988421499729156,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9985038619488478,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.61618434092828,
        "relative_improvement": -0.0021750572158564566,
        "absolute_improvement_ms": -0.14196955127207533,
        "old_mean_ms": 65.27163986174855,
        "new_mean_ms": 65.41360941302062,
        "old_std_ms": 4.3248669181125265,
        "new_std_ms": 4.937763311161971,
        "effect_size_cohens_d": -0.030587375723408243,
        "old_ci95_ms": [
          63.62654846721244,
          66.91673125628466
        ],
        "new_ci95_ms": [
          63.53538469876889,
          67.29183412727237
        ],
        "old_ci99_ms": [
          63.05244271408594,
          67.49083700941117
        ],
        "new_ci99_ms": [
          62.879919830556894,
          67.94729899548436
        ],
        "new_times": [
          0.0656592089944752,
          0.061123327002860606,
          0.07126903400057927,
          0.08009201099048369,
          0.06468459199822973,
          0.0595063740038313,
          0.05898868400254287,
          0.05938096898898948,
          0.06950122700072825,
          0.06405447798897512,
          0.05925648400443606,
          0.06596008001361042,
          0.06391075199644547,
          0.06823627800622489,
          0.06442638199951034,
          0.07332382199820131,
          0.0703435979958158,
          0.058772985998075455,
          0.06961325999873225,
          0.05863938099355437,
          0.06890964299964253,
          0.06365139198896941,
          0.06471737299580127,
          0.06925453600706533,
          0.06479952699737623,
          0.06579072399472352,
          0.06611842699931003,
          0.06364918200415559,
          0.0633609410142526
        ],
        "old_times": [
          0.05859093899198342,
          0.06774933799169958,
          0.06422438401205,
          0.0697076240030583,
          0.0672044079983607,
          0.0621879859972978,
          0.06462237999949139,
          0.06055911400471814,
          0.06991040200227872,
          0.06296337599633262,
          0.08149721399240661,
          0.06432693899841979,
          0.06504284600669052,
          0.06253059000300709,
          0.0652780550008174,
          0.06445135299873073,
          0.062195046004489996,
          0.06838963300106116,
          0.06036780698923394,
          0.06345649500144646,
          0.0690427489898866,
          0.0689948860090226,
          0.06180971200228669,
          0.06425162599771284,
          0.06713216500065755,
          0.060944488999666646,
          0.06322880700463429,
          0.06519740099611226,
          0.06701979199715424
        ]
      },
      {
        "test_name": "xsimd.simd_return_type",
        "is_significant": false,
        "p_value": 0.9997974054077239,
        "is_pair_significant": false,
        "pair_p_value": 0.9997332956684292,
        "is_binom_significant": false,
        "binom_p_value": 0.9999991878867149,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999733194708824,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9042762441699469,
        "relative_improvement": -0.03785907223912257,
        "absolute_improvement_ms": -2.392947174014587,
        "old_mean_ms": 63.206704033855836,
        "new_mean_ms": 65.59965120787042,
        "old_std_ms": 4.989865654818865,
        "new_std_ms": 6.3315107735467615,
        "effect_size_cohens_d": -0.41979336807096607,
        "old_ci95_ms": [
          61.30866064799916,
          65.10474741971251
        ],
        "new_ci95_ms": [
          63.191273312043705,
          68.00802910369714
        ],
        "old_ci99_ms": [
          60.64627943858844,
          65.76712862912323
        ],
        "new_ci99_ms": [
          62.350795019901945,
          68.84850739583891
        ],
        "new_times": [
          0.06364961199869867,
          0.06528509499912616,
          0.062360814001294784,
          0.06322539600660093,
          0.06715952600643504,
          0.06116516799374949,
          0.06689569700392894,
          0.06223414799023885,
          0.06272896699374542,
          0.05966465000528842,
          0.06330190801236313,
          0.06414876200142317,
          0.0653484770009527,
          0.06721218899474479,
          0.06309011099801864,
          0.064587268003379,
          0.06377933699695859,
          0.06198285799473524,
          0.06709906399191823,
          0.09513650501321536,
          0.06374648600467481,
          0.06825205800123513,
          0.06459608799195848,
          0.07143736000580247,
          0.06366503299796022,
          0.061463779013138264,
          0.06516050899517722,
          0.06226982900989242,
          0.07174319100158755
        ],
        "old_times": [
          0.07355986100446898,
          0.06466691200330388,
          0.05788449199462775,
          0.06407662799756508,
          0.06916699299472384,
          0.06765407598868478,
          0.06648472099914216,
          0.06400293599290308,
          0.06309761200100183,
          0.06060918599541765,
          0.06020509099471383,
          0.061198167997645214,
          0.06106183301017154,
          0.06083845499961171,
          0.06447404400387313,
          0.07326075999299064,
          0.061757019997457974,
          0.06301674799760804,
          0.06601889300509356,
          0.06441250200441573,
          0.060063565004384145,
          0.0653451770049287,
          0.05995383999834303,
          0.06956729899684433,
          0.05645334800647106,
          0.05098496799473651,
          0.06429164699511603,
          0.0648248869983945,
          0.054062755007180385
        ]
      },
      {
        "test_name": "xsimd.trigonometric",
        "is_significant": false,
        "p_value": 0.9984543204756892,
        "is_pair_significant": false,
        "pair_p_value": 0.9983557211848401,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9985038619488478,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5433428684028868,
        "relative_improvement": -0.007449017161489001,
        "absolute_improvement_ms": -0.47327948238394535,
        "old_mean_ms": 63.53582924077336,
        "new_mean_ms": 64.00910872315731,
        "old_std_ms": 4.0521523452571175,
        "new_std_ms": 5.032661914833555,
        "effect_size_cohens_d": -0.1035897513200542,
        "old_ci95_ms": [
          61.994472921897064,
          65.07718555964965
        ],
        "new_ci95_ms": [
          62.094786510598794,
          65.92343093571583
        ],
        "old_ci99_ms": [
          61.456568746345596,
          65.61508973520112
        ],
        "new_ci99_ms": [
          61.42672429884696,
          66.59149314746765
        ],
        "new_times": [
          0.057360961989616044,
          0.060299325006781146,
          0.060485841997433454,
          0.057678604003740475,
          0.06525583399343304,
          0.06045556999742985,
          0.059400629994343035,
          0.0749570850020973,
          0.06393983299494721,
          0.06643520900979638,
          0.07708002600702457,
          0.06502371399255935,
          0.06983236799715087,
          0.06822246700176038,
          0.06444799200107809,
          0.06042633899778593,
          0.06683837399759796,
          0.0626048520061886,
          0.06364954200398643,
          0.06655714299995452,
          0.06712276500184089,
          0.07091894999030046,
          0.06335588099318556,
          0.06067799899028614,
          0.061896384999272414,
          0.06548060200293548,
          0.06241916499857325,
          0.05681028099206742,
          0.056630414008395746
        ],
        "old_times": [
          0.06061146600404754,
          0.06469343201024458,
          0.06456595700001344,
          0.06743733699840959,
          0.06145066799945198,
          0.062034810995101,
          0.059223882999503985,
          0.06382979000045452,
          0.06900240699178539,
          0.060028944004443474,
          0.06957504899764899,
          0.060387156991055235,
          0.05520465999143198,
          0.06318098399788141,
          0.06287393199454527,
          0.06802422901091632,
          0.06227085899445228,
          0.06378085698815994,
          0.07149839200428687,
          0.06868081400170922,
          0.06568338099168614,
          0.06788570400385652,
          0.06319660499866586,
          0.05683692199818324,
          0.06208433200663421,
          0.061084553992259316,
          0.05642888600414153,
          0.06420971400802955,
          0.06677332200342789
        ]
      },
      {
        "test_name": "xsimd.reciprocal",
        "is_significant": false,
        "p_value": 0.8620024653619092,
        "is_pair_significant": false,
        "pair_p_value": 0.8782150239586,
        "is_binom_significant": false,
        "binom_p_value": 0.867534551769495,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8933776952326298,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.0957237558300531,
        "relative_improvement": 0.027883475486051686,
        "absolute_improvement_ms": 1.797765138627283,
        "old_mean_ms": 64.47421303440431,
        "new_mean_ms": 62.67644789577703,
        "old_std_ms": 6.05543642014923,
        "new_std_ms": 3.9375981650874765,
        "effect_size_cohens_d": 0.35198575760142664,
        "old_ci95_ms": [
          62.17084820640734,
          66.77757786240127
        ],
        "new_ci95_ms": [
          61.178665656652406,
          64.17423013490166
        ],
        "old_ci99_ms": [
          61.367017486913994,
          67.58140858189464
        ],
        "new_ci99_ms": [
          60.655968010024324,
          64.69692778152974
        ],
        "new_times": [
          0.06213946499337908,
          0.06266562499513384,
          0.0604044689971488,
          0.05972573200415354,
          0.062351033004233614,
          0.05547303998901043,
          0.06265881400031503,
          0.05695394599752035,
          0.06469314199057408,
          0.05942638100532349,
          0.06899221699859481,
          0.0631507829966722,
          0.06190331600373611,
          0.06924659600190353,
          0.06131084301159717,
          0.056139495005481876,
          0.06854798899439629,
          0.06456160699599423,
          0.06592437899962533,
          0.057161664008162916,
          0.06171767799241934,
          0.060624076999374665,
          0.06010725699889008,
          0.06479680699703749,
          0.06383098900550976,
          0.06393160299921874,
          0.07230146300571505,
          0.06312401199829765,
          0.06375256698811427
        ],
        "old_times": [
          0.06600749200151768,
          0.054091397003503516,
          0.06135838499176316,
          0.06612083700019866,
          0.07340271498833317,
          0.05757077899761498,
          0.067153676005546,
          0.06010847601282876,
          0.06717364699579775,
          0.07002220599679276,
          0.06554596500063781,
          0.06492219099891372,
          0.06454678600130137,
          0.059106939006596804,
          0.05536875600228086,
          0.06539432899444364,
          0.08648663500207476,
          0.07097556200460531,
          0.060439099004724994,
          0.06302364799194038,
          0.06106546400405932,
          0.06407525899703614,
          0.06863753400102723,
          0.06436907900206279,
          0.06112699599179905,
          0.06263573399337474,
          0.06341465300647542,
          0.06401423599163536,
          0.06159370300883893
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}