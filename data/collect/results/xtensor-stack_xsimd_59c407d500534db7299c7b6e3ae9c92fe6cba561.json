{
  "metadata": {
    "collection_date": "2026-01-14T16:14:05.488206",
    "repository": "https://github.com/xtensor-stack/xsimd",
    "repository_name": "xtensor-stack/xsimd"
  },
  "commit_info": {
    "old_sha": "be9dcb5df413a893fb6646fa950eeb4aeac70ffc",
    "new_sha": "59c407d500534db7299c7b6e3ae9c92fe6cba561",
    "commit_message": [
      "Force all functions to be always inline (when possible)\n\nFix #644"
    ],
    "commit_date": "2024-04-15T07:32:56+00:00",
    "patch": [
      "--- include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp\n@@ -28,7 +28,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x << y; },\n@@ -37,7 +37,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x >> y; },\n@@ -46,21 +46,21 @@ namespace xsimd\n \n         // decr\n         template <class A, class T>\n-        inline batch<T, A> decr(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> decr(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self - T(1);\n         }\n \n         // decr_if\n         template <class A, class T, class Mask>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n         {\n             return select(mask, decr(self), self);\n         }\n \n         // div\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x / y; },\n@@ -69,13 +69,13 @@ namespace xsimd\n \n         // fma\n         template <class A, class T>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return x * y + z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n             auto res_i = fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n@@ -84,13 +84,13 @@ namespace xsimd\n \n         // fms\n         template <class A, class T>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return x * y - z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n             auto res_i = fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n@@ -99,13 +99,13 @@ namespace xsimd\n \n         // fnma\n         template <class A, class T>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return -x * y + z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = -fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n             auto res_i = -fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n@@ -114,13 +114,13 @@ namespace xsimd\n \n         // fnms\n         template <class A, class T>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return -x * y - z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = -fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n             auto res_i = -fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n@@ -129,7 +129,7 @@ namespace xsimd\n \n         // hadd\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(buffer);\n@@ -143,21 +143,21 @@ namespace xsimd\n \n         // incr\n         template <class A, class T>\n-        inline batch<T, A> incr(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> incr(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self + T(1);\n         }\n \n         // incr_if\n         template <class A, class T, class Mask>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n         {\n             return select(mask, incr(self), self);\n         }\n \n         // mul\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x * y; },\n@@ -166,28 +166,28 @@ namespace xsimd\n \n         // rotl\n         template <class A, class T, class STy>\n-        inline batch<T, A> rotl(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n         {\n             constexpr auto N = std::numeric_limits<T>::digits;\n             return (self << other) | (self >> (N - other));\n         }\n \n         // rotr\n         template <class A, class T, class STy>\n-        inline batch<T, A> rotr(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n         {\n             constexpr auto N = std::numeric_limits<T>::digits;\n             return (self >> other) | (self << (N - other));\n         }\n \n         // sadd\n         template <class A>\n-        inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return add(self, other); // no saturated arithmetic on floating point numbers\n         }\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -204,19 +204,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return add(self, other); // no saturated arithmetic on floating point numbers\n         }\n \n         // ssub\n         template <class A>\n-        inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sub(self, other); // no saturated arithmetic on floating point numbers\n         }\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -229,7 +229,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sub(self, other); // no saturated arithmetic on floating point numbers\n         }\n--- include/xsimd/arch/generic/xsimd_generic_complex.hpp\n@@ -26,54 +26,54 @@ namespace xsimd\n \n         // real\n         template <class A, class T>\n-        inline batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self.real();\n         }\n \n         // imag\n         template <class A, class T>\n-        inline batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) noexcept\n         {\n             return batch<T, A>(T(0));\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self.imag();\n         }\n \n         // arg\n         template <class A, class T>\n-        inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return atan2(imag(self), real(self));\n         }\n \n         // conj\n         template <class A, class T>\n-        inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { real(self), -imag(self) };\n         }\n \n         // norm\n         template <class A, class T>\n-        inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { fma(real(self), real(self), imag(self) * imag(self)) };\n         }\n \n         // proj\n         template <class A, class T>\n-        inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = complex_batch_type_t<batch<T, A>>;\n             using real_batch = typename batch_type::real_batch;\n@@ -86,19 +86,19 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isnan(self.real()) || isnan(self.imag()));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isinf(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isinf(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isinf(self.real()) || isinf(self.imag()));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isfinite(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isfinite(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isfinite(self.real()) && isfinite(self.imag()));\n         }\n--- include/xsimd/arch/generic/xsimd_generic_details.hpp\n@@ -23,89 +23,89 @@ namespace xsimd\n {\n     // Forward declaration. Should we put them in a separate file?\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<std::complex<T>, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& self) noexcept;\n     template <class T, class A>\n-    inline bool any(batch_bool<T, A> const& self) noexcept;\n+    XSIMD_INLINE bool any(batch_bool<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+    XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n     template <class A, class T_out, class T_in>\n-    inline batch<T_out, A> batch_cast(batch<T_in, A> const&, batch<T_out, A> const& out) noexcept;\n+    XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const&, batch<T_out, A> const& out) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> bitofsign(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& self) noexcept;\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> cos(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> cos(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> cosh(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> exp(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> exp(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n+    XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n+    XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n+    XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n     template <class T, class A, uint64_t... Coefs>\n-    inline batch<T, A> horner(const batch<T, A>& self) noexcept;\n+    XSIMD_INLINE batch<T, A> horner(const batch<T, A>& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> hypot(const batch<T, A>& self) noexcept;\n+    XSIMD_INLINE batch<T, A> hypot(const batch<T, A>& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_even(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_flint(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_odd(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n+    XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> log(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> log(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> nearbyint(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> nearbyint_as_int(const batch<T, A>& x) noexcept;\n+    XSIMD_INLINE batch<as_integer_t<T>, A> nearbyint_as_int(const batch<T, A>& x) noexcept;\n     template <class T, class A>\n-    inline T reduce_add(batch<T, A> const&) noexcept;\n+    XSIMD_INLINE T reduce_add(batch<T, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const&, batch<T, A> const&) noexcept;\n+    XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const&, batch<T, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const&, batch<std::complex<T>, A> const&) noexcept;\n+    XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const&, batch<std::complex<T>, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sign(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sign(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> signnz(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sin(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sinh(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sqrt(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> tan(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> tan(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_float_t<T>, A> to_float(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<as_float_t<T>, A> to_float(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<as_integer_t<T>, A> to_int(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> trunc(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& self) noexcept;\n \n     namespace kernel\n     {\n \n         namespace detail\n         {\n             template <class F, class A, class T, class... Batches>\n-            inline batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 constexpr std::size_t size = batch<T, A>::size;\n                 alignas(A::alignment()) T self_buffer[size];\n@@ -120,7 +120,7 @@ namespace xsimd\n             }\n \n             template <class U, class F, class A, class T>\n-            inline batch<U, A> apply_transform(F&& func, batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<U, A> apply_transform(F&& func, batch<T, A> const& self) noexcept\n             {\n                 static_assert(batch<T, A>::size == batch<U, A>::size,\n                               \"Source and destination sizes must match\");\n@@ -141,50 +141,50 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<uint8_t, A> fast_cast(batch<int8_t, A> const& self, batch<uint8_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint8_t, A> fast_cast(batch<int8_t, A> const& self, batch<uint8_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint8_t>(self);\n             }\n             template <class A>\n-            inline batch<uint16_t, A> fast_cast(batch<int16_t, A> const& self, batch<uint16_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint16_t, A> fast_cast(batch<int16_t, A> const& self, batch<uint16_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint16_t>(self);\n             }\n             template <class A>\n-            inline batch<uint32_t, A> fast_cast(batch<int32_t, A> const& self, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<int32_t, A> const& self, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint32_t>(self);\n             }\n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<int64_t, A> const& self, batch<uint64_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<int64_t, A> const& self, batch<uint64_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint64_t>(self);\n             }\n             template <class A>\n-            inline batch<int8_t, A> fast_cast(batch<uint8_t, A> const& self, batch<int8_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int8_t, A> fast_cast(batch<uint8_t, A> const& self, batch<int8_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int8_t>(self);\n             }\n             template <class A>\n-            inline batch<int16_t, A> fast_cast(batch<uint16_t, A> const& self, batch<int16_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int16_t, A> fast_cast(batch<uint16_t, A> const& self, batch<int16_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int16_t>(self);\n             }\n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<uint32_t, A> const& self, batch<int32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<uint32_t, A> const& self, batch<int32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int32_t>(self);\n             }\n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<uint64_t, A> const& self, batch<int64_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<uint64_t, A> const& self, batch<int64_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int64_t>(self);\n             }\n \n             // Provide a generic uint32_t -> float cast only if we have a\n             // non-generic int32_t -> float fast_cast\n             template <class A, class _ = decltype(fast_cast(std::declval<batch<int32_t, A> const&>(), std::declval<batch<float, A> const&>(), A {}))>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& v, batch<float, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& v, batch<float, A> const&, requires_arch<generic>) noexcept\n             {\n                 // see https://stackoverflow.com/questions/34066228/how-to-perform-uint32-float-conversion-with-sse\n                 batch<uint32_t, A> msk_lo(0xFFFF);\n@@ -201,7 +201,7 @@ namespace xsimd\n             // Provide a generic float -> uint32_t cast only if we have a\n             // non-generic float -> int32_t fast_cast\n             template <class A, class _ = decltype(fast_cast(std::declval<batch<float, A> const&>(), std::declval<batch<int32_t, A> const&>(), A {}))>\n-            inline batch<uint32_t, A> fast_cast(batch<float, A> const& v, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<float, A> const& v, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 auto is_large = v >= batch<float, A>(1u << 31);\n                 auto small = bitwise_cast<float>(batch_cast<int32_t>(v));\n@@ -258,25 +258,25 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B, uint64_t c>\n-            inline B coef() noexcept\n+            XSIMD_INLINE B coef() noexcept\n             {\n                 using value_type = typename B::value_type;\n                 return B(bit_cast<value_type>(as_unsigned_integer_t<value_type>(c)));\n             }\n             template <class B>\n-            inline B horner(const B&) noexcept\n+            XSIMD_INLINE B horner(const B&) noexcept\n             {\n                 return B(typename B::value_type(0.));\n             }\n \n             template <class B, uint64_t c0>\n-            inline B horner(const B&) noexcept\n+            XSIMD_INLINE B horner(const B&) noexcept\n             {\n                 return coef<B, c0>();\n             }\n \n             template <class B, uint64_t c0, uint64_t c1, uint64_t... args>\n-            inline B horner(const B& self) noexcept\n+            XSIMD_INLINE B horner(const B& self) noexcept\n             {\n                 return fma(self, horner<B, c1, args...>(self), coef<B, c0>());\n             }\n@@ -291,19 +291,19 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B>\n-            inline B horner1(const B&) noexcept\n+            XSIMD_INLINE B horner1(const B&) noexcept\n             {\n                 return B(1.);\n             }\n \n             template <class B, uint64_t c0>\n-            inline B horner1(const B& x) noexcept\n+            XSIMD_INLINE B horner1(const B& x) noexcept\n             {\n                 return x + detail::coef<B, c0>();\n             }\n \n             template <class B, uint64_t c0, uint64_t c1, uint64_t... args>\n-            inline B horner1(const B& x) noexcept\n+            XSIMD_INLINE B horner1(const B& x) noexcept\n             {\n                 return fma(x, horner1<B, c1, args...>(x), detail::coef<B, c0>());\n             }\n--- include/xsimd/arch/generic/xsimd_generic_logical.hpp\n@@ -24,7 +24,7 @@ namespace xsimd\n \n         // from  mask\n         template <class A, class T>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             // This is inefficient but should never be called. It's just a\n@@ -36,98 +36,98 @@ namespace xsimd\n \n         // ge\n         template <class A, class T>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return other <= self;\n         }\n \n         // gt\n         template <class A, class T>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return other < self;\n         }\n \n         // is_even\n         template <class A, class T>\n-        inline batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return is_flint(self * T(0.5));\n         }\n \n         // is_flint\n         template <class A, class T>\n-        inline batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             auto frac = select(isnan(self - self), constants::nan<batch<T, A>>(), self - trunc(self));\n             return frac == T(0.);\n         }\n \n         // is_odd\n         template <class A, class T>\n-        inline batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return is_even(self - T(1.));\n         }\n \n         // isinf\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isinf(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isinf(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(false);\n         }\n         template <class A>\n-        inline batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return abs(self) == std::numeric_limits<float>::infinity();\n         }\n         template <class A>\n-        inline batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return abs(self) == std::numeric_limits<double>::infinity();\n         }\n \n         // isfinite\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isfinite(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isfinite(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(true);\n         }\n         template <class A>\n-        inline batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return (self - self) == 0.f;\n         }\n         template <class A>\n-        inline batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return (self - self) == 0.;\n         }\n \n         // isnan\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isnan(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(false);\n         }\n \n         // le\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return (self < other) || (self == other);\n         }\n \n         // neq\n         template <class A, class T>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return !(other == self);\n         }\n \n         // logical_and\n         template <class A, class T>\n-        inline batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x && y; },\n@@ -136,7 +136,7 @@ namespace xsimd\n \n         // logical_or\n         template <class A, class T>\n-        inline batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x || y; },\n@@ -145,7 +145,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             self.store_aligned(buffer);\n--- include/xsimd/arch/generic/xsimd_generic_math.hpp\n@@ -27,7 +27,7 @@ namespace xsimd\n         using namespace types;\n         // abs\n         template <class A, class T, class>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n                 return self;\n@@ -40,7 +40,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return hypot(z.real(), z.imag());\n         }\n@@ -49,13 +49,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::false_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::false_type) noexcept\n             {\n                 return (x & y) + ((x ^ y) >> 1);\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::true_type) noexcept\n             {\n                 // Inspired by\n                 // https://stackoverflow.com/questions/5697500/take-the-average-of-two-signed-numbers-in-c\n@@ -66,14 +66,14 @@ namespace xsimd\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::false_type, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::false_type, std::true_type) noexcept\n             {\n                 return (x + y) / 2;\n             }\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n         {\n             return detail::avg(x, y, typename std::is_integral<T>::type {}, typename std::is_signed<T>::type {});\n         }\n@@ -82,42 +82,42 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::true_type) noexcept\n             {\n                 constexpr unsigned shift = 8 * sizeof(T) - 1;\n                 auto adj = std::is_signed<T>::value ? ((x ^ y) & 0x1) : (((x ^ y) << shift) >> shift);\n                 return ::xsimd::kernel::avg(x, y, A {}) + adj;\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::false_type) noexcept\n+            XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::false_type) noexcept\n             {\n                 return ::xsimd::kernel::avg(x, y, A {});\n             }\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n         {\n             return detail::avgr(x, y, typename std::is_integral<T>::type {});\n         }\n \n         // batch_cast\n         template <class A, class T>\n-        inline batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         namespace detail\n         {\n             template <class A, class T_out, class T_in>\n-            inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 return fast_cast(self, out, A {});\n             }\n             template <class A, class T_out, class T_in>\n-            inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be no conversion for this type combination\");\n                 using batch_type_in = batch<T_in, A>;\n@@ -133,14 +133,14 @@ namespace xsimd\n         }\n \n         template <class A, class T_out, class T_in>\n-        inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) noexcept\n         {\n             return detail::batch_cast(self, out, A {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n \n         // bitofsign\n         template <class A, class T>\n-        inline batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(std::is_integral<T>::value, \"int type implementation\");\n             if (std::is_unsigned<T>::value)\n@@ -150,19 +150,19 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self & constants::signmask<batch<float, A>>();\n         }\n         template <class A>\n-        inline batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self & constants::signmask<batch<double, A>>();\n         }\n \n         // bitwise_cast\n         template <class A, class T>\n-        inline batch<T, A> bitwise_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n@@ -178,7 +178,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type z = abs(self);\n@@ -225,7 +225,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type z = abs(self);\n@@ -274,14 +274,14 @@ namespace xsimd\n \n         // clip\n         template <class A, class T>\n-        inline batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) noexcept\n         {\n             return min(hi, max(self, lo));\n         }\n \n         // copysign\n         template <class A, class T, class _ = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return abs(self) | bitofsign(other);\n         }\n@@ -308,7 +308,7 @@ namespace xsimd\n                 using batch_type = batch<float, A>;\n                 // computes erf(a0)/a0\n                 // x is sqr(a0) and 0 <= abs(a0) <= 2/3\n-                static inline batch_type erf1(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erf1(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3f906eba, //   1.128379154774254e+00\n@@ -322,7 +322,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(sqr(x))\n                 // x >=  2/3\n-                static inline batch_type erfc2(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc2(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3f0a0e8b, //   5.392844046572836e-01\n@@ -339,7 +339,7 @@ namespace xsimd\n                                           >(x);\n                 }\n \n-                static inline batch_type erfc3(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc3(const batch_type& x) noexcept\n                 {\n                     return (batch_type(1.) - x) * detail::horner<batch_type,\n                                                                  0x3f7ffffe, //   9.9999988e-01\n@@ -361,7 +361,7 @@ namespace xsimd\n                 using batch_type = batch<double, A>;\n                 // computes erf(a0)/a0\n                 // x is sqr(a0) and 0 <= abs(a0) <= 0.65\n-                static inline batch_type erf1(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erf1(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3ff20dd750429b61ull, // 1.12837916709551\n@@ -381,7 +381,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(x*x)\n                 // 0.65 <= abs(x) <= 2.2\n-                static inline batch_type erfc2(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc2(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3feffffffbbb552bull, // 0.999999992049799\n@@ -405,7 +405,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(x*x)\n                 // 2.2 <= abs(x) <= 6\n-                static inline batch_type erfc3(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc3(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3fefff5a9e697ae2ull, // 0.99992114009714\n@@ -429,7 +429,7 @@ namespace xsimd\n \n                 // computes erfc(rx)*exp(rx*rx)\n                 // x >=  6 rx = 1/x\n-                static inline batch_type erfc4(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc4(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0xbc7e4ad1ec7d0000ll, // -2.627435221016534e-17\n@@ -461,7 +461,7 @@ namespace xsimd\n          */\n \n         template <class A>\n-        inline batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -485,7 +485,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -521,7 +521,7 @@ namespace xsimd\n \n         // erfc\n         template <class A>\n-        inline batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -546,7 +546,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -590,62 +590,62 @@ namespace xsimd\n                 B x;\n \n                 template <typename... Ts>\n-                inline B operator()(const Ts&... coefs) noexcept\n+                XSIMD_INLINE B operator()(const Ts&... coefs) noexcept\n                 {\n                     return eval(coefs...);\n                 }\n \n             private:\n-                inline B eval(const B& c0) noexcept\n+                XSIMD_INLINE B eval(const B& c0) noexcept\n                 {\n                     return c0;\n                 }\n \n-                inline B eval(const B& c0, const B& c1) noexcept\n+                XSIMD_INLINE B eval(const B& c0, const B& c1) noexcept\n                 {\n                     return fma(x, c1, c0);\n                 }\n \n                 template <size_t... Is, class Tuple>\n-                inline B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)\n+                XSIMD_INLINE B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)\n                 {\n                     return estrin { x * x }(std::get<Is>(tuple)...);\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple) noexcept\n                 {\n                     return eval(::xsimd::detail::make_index_sequence<sizeof...(Args)>(), tuple);\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0))));\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))));\n                 }\n \n                 template <class... Args, class... Ts>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))), coefs...);\n                 }\n \n                 template <class... Ts>\n-                inline B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept\n+                XSIMD_INLINE B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept\n                 {\n                     return eval(std::make_tuple(eval(c0, c1)), coefs...);\n                 }\n             };\n         }\n \n         template <class T, class A, uint64_t... Coefs>\n-        inline batch<T, A> estrin(const batch<T, A>& self) noexcept\n+        XSIMD_INLINE batch<T, A> estrin(const batch<T, A>& self) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return detail::estrin<batch_type> { self }(detail::coef<batch_type, Coefs>()...);\n@@ -722,7 +722,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp_tag> : exp_reduction_base<batch<float, A>, exp_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type y = detail::horner<batch_type,\n                                                   0x3f000000, //  5.0000000e-01\n@@ -734,7 +734,7 @@ namespace xsimd\n                     return ++fma(y, x * x, x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n                     x = fnma(k, constants::log_2hi<batch_type>(), a);\n@@ -747,7 +747,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp10_tag> : exp_reduction_base<batch<float, A>, exp10_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     return ++(detail::horner<batch_type,\n                                              0x40135d8e, //    2.3025851e+00\n@@ -760,7 +760,7 @@ namespace xsimd\n                               * x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog10_2<batch_type>() * a);\n                     x = fnma(k, constants::log10_2hi<batch_type>(), a);\n@@ -773,7 +773,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp2_tag> : exp_reduction_base<batch<float, A>, exp2_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type y = detail::horner<batch_type,\n                                                   0x3e75fdf1, //    2.4022652e-01\n@@ -785,7 +785,7 @@ namespace xsimd\n                     return ++fma(y, x * x, x * constants::log_2<batch_type>());\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(a);\n                     x = (a - k);\n@@ -797,7 +797,7 @@ namespace xsimd\n             struct exp_reduction<double, A, exp_tag> : exp_reduction_base<batch<double, A>, exp_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type t = x * x;\n                     return fnma(t,\n@@ -810,7 +810,7 @@ namespace xsimd\n                                 x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& hi, batch_type& lo, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& hi, batch_type& lo, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n                     hi = fnma(k, constants::log_2hi<batch_type>(), a);\n@@ -819,7 +819,7 @@ namespace xsimd\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type& x, const batch_type& c, const batch_type& hi, const batch_type& lo) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type& x, const batch_type& c, const batch_type& hi, const batch_type& lo) noexcept\n                 {\n                     return batch_type(1.) - (((lo - (x * c) / (batch_type(2.) - c)) - hi));\n                 }\n@@ -829,23 +829,23 @@ namespace xsimd\n             struct exp_reduction<double, A, exp10_tag> : exp_reduction_base<batch<double, A>, exp10_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type xx = x * x;\n                     batch_type px = x * detail::horner<batch_type, 0x40a2b4798e134a01ull, 0x40796b7a050349e4ull, 0x40277d9474c55934ull, 0x3fa4fd75f3062dd4ull>(xx);\n                     batch_type x2 = px / (detail::horner1<batch_type, 0x40a03f37650df6e2ull, 0x4093e05eefd67782ull, 0x405545fdce51ca08ull>(xx) - px);\n                     return ++(x2 + x2);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog10_2<batch_type>() * a);\n                     x = fnma(k, constants::log10_2hi<batch_type>(), a);\n                     x = fnma(k, constants::log10_2lo<batch_type>(), x);\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type&, const batch_type& c, const batch_type&, const batch_type&) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type&, const batch_type& c, const batch_type&, const batch_type&) noexcept\n                 {\n                     return c;\n                 }\n@@ -855,7 +855,7 @@ namespace xsimd\n             struct exp_reduction<double, A, exp2_tag> : exp_reduction_base<batch<double, A>, exp2_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type t = x * x;\n                     return fnma(t,\n@@ -868,21 +868,21 @@ namespace xsimd\n                                 x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(a);\n                     x = (a - k) * constants::log_2<batch_type>();\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type& x, const batch_type& c, const batch_type&, const batch_type&) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type& x, const batch_type& c, const batch_type&, const batch_type&) noexcept\n                 {\n                     return batch_type(1.) + x + x * c / (batch_type(2.) - c);\n                 }\n             };\n \n             template <exp_reduction_tag Tag, class A>\n-            inline batch<float, A> exp(batch<float, A> const& self) noexcept\n+            XSIMD_INLINE batch<float, A> exp(batch<float, A> const& self) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 using reducer_t = exp_reduction<float, A, Tag>;\n@@ -895,7 +895,7 @@ namespace xsimd\n             }\n \n             template <exp_reduction_tag Tag, class A>\n-            inline batch<double, A> exp(batch<double, A> const& self) noexcept\n+            XSIMD_INLINE batch<double, A> exp(batch<double, A> const& self) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 using reducer_t = exp_reduction<double, A, Tag>;\n@@ -910,13 +910,13 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp_tag>(self);\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             auto isincos = sincos(self.imag());\n@@ -925,14 +925,14 @@ namespace xsimd\n \n         // exp10\n         template <class A, class T>\n-        inline batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp10_tag>(self);\n         }\n \n         // exp2\n         template <class A, class T>\n-        inline batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp2_tag>(self);\n         }\n@@ -950,7 +950,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<float, A> expm1(const batch<float, A>& a) noexcept\n+            static XSIMD_INLINE batch<float, A> expm1(const batch<float, A>& a) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n@@ -974,7 +974,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> expm1(const batch<double, A>& a) noexcept\n+            static XSIMD_INLINE batch<double, A> expm1(const batch<double, A>& a) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n@@ -1005,7 +1005,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return select(self < constants::logeps<batch_type>(),\n@@ -1016,7 +1016,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -1029,22 +1029,22 @@ namespace xsimd\n \n         // polar\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> polar(const batch<T, A>& r, const batch<T, A>& theta, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> polar(const batch<T, A>& r, const batch<T, A>& theta, requires_arch<generic>) noexcept\n         {\n             auto sincosTheta = sincos(theta);\n             return { r * sincosTheta.second, r * sincosTheta.first };\n         }\n \n         // fdim\n         template <class A, class T>\n-        inline batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fmax(batch<T, A>(0), self - other);\n         }\n \n         // fmod\n         template <class A, class T>\n-        inline batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(trunc(self / other), other, self);\n         }\n@@ -1060,7 +1060,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             using int_type = as_integer_t<T>;\n@@ -1075,28 +1075,28 @@ namespace xsimd\n \n         // from bool\n         template <class A, class T>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch<T, A>(self.data) & batch<T, A>(1);\n         }\n \n         // horner\n         template <class T, class A, uint64_t... Coefs>\n-        inline batch<T, A> horner(const batch<T, A>& self) noexcept\n+        XSIMD_INLINE batch<T, A> horner(const batch<T, A>& self) noexcept\n         {\n             return detail::horner<batch<T, A>, Coefs...>(self);\n         }\n \n         // hypot\n         template <class A, class T>\n-        inline batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sqrt(fma(self, self, other * other));\n         }\n \n         // ipow\n         template <class A, class T, class ITy>\n-        inline batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) noexcept\n         {\n             return ::xsimd::detail::ipow(self, other);\n         }\n@@ -1112,7 +1112,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             using itype = as_integer_t<batch_type>;\n@@ -1123,7 +1123,7 @@ namespace xsimd\n \n         // lgamma\n         template <class A, class T>\n-        inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n@@ -1137,7 +1137,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<float, A> gammalnB(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammalnB(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0x3ed87730, //    4.227843421859038E-001\n@@ -1152,7 +1152,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> gammalnC(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammalnC(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0xbf13c468, //   -5.772156501719101E-001\n@@ -1167,7 +1167,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> gammaln2(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammaln2(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0x3daaaa94, //   8.333316229807355E-002f\n@@ -1177,7 +1177,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> gammaln1(const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> gammaln1(const batch<double, A>& x) noexcept\n             {\n                 return horner<batch<double, A>,\n                               0xc12a0c675418055eull, //  -8.53555664245765465627E5\n@@ -1199,7 +1199,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> gammalnA(const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> gammalnA(const batch<double, A>& x) noexcept\n             {\n                 return horner<batch<double, A>,\n                               0x3fb555555555554bull, //    8.33333333333331927722E-2\n@@ -1226,7 +1226,7 @@ namespace xsimd\n             struct lgamma_impl<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& a) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& a) noexcept\n                 {\n                     auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n                     batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n@@ -1248,7 +1248,7 @@ namespace xsimd\n                 }\n \n             private:\n-                static inline batch_type negative(const batch_type& q, const batch_type& w) noexcept\n+                static XSIMD_INLINE batch_type negative(const batch_type& q, const batch_type& w) noexcept\n                 {\n                     batch_type p = floor(q);\n                     batch_type z = q - p;\n@@ -1258,7 +1258,7 @@ namespace xsimd\n                     return -log(constants::invpi<batch_type>() * abs(z)) - w;\n                 }\n \n-                static inline batch_type other(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type other(const batch_type& x) noexcept\n                 {\n                     auto xlt650 = (x < batch_type(6.5));\n                     batch_type r0x = x;\n@@ -1347,7 +1347,7 @@ namespace xsimd\n             {\n                 using batch_type = batch<double, A>;\n \n-                static inline batch_type compute(const batch_type& a) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& a) noexcept\n                 {\n                     auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n                     batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n@@ -1369,6 +1369,8 @@ namespace xsimd\n                 }\n \n             private:\n+                // FIXME: cannot mark this one as XSIMD_INLINE because there's a\n+                // recursive loop on `lgamma'.\n                 static inline batch_type large_negative(const batch_type& q) noexcept\n                 {\n                     batch_type w = lgamma(q);\n@@ -1381,7 +1383,7 @@ namespace xsimd\n                     return constants::logpi<batch_type>() - log(z) - w;\n                 }\n \n-                static inline batch_type other(const batch_type& xx) noexcept\n+                static XSIMD_INLINE batch_type other(const batch_type& xx) noexcept\n                 {\n                     batch_type x = xx;\n                     auto test = (x < batch_type(13.));\n@@ -1424,7 +1426,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::lgamma_impl<batch<T, A>>::compute(self);\n         }\n@@ -1440,7 +1442,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1480,7 +1482,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1523,14 +1525,14 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             return batch<std::complex<T>, A>(log(abs(z)), atan2(z.imag(), z.real()));\n         }\n \n         // log2\n         template <class A>\n-        inline batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1570,7 +1572,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1620,7 +1622,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base) noexcept\n+            XSIMD_INLINE batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 using rv_type = typename batch_type::value_type;\n@@ -1629,7 +1631,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::logN_complex_impl(self, std::log(2));\n         }\n@@ -1647,7 +1649,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             const batch_type\n@@ -1698,7 +1700,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             const batch_type\n@@ -1752,7 +1754,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             return detail::logN_complex_impl(z, std::log(10));\n         }\n@@ -1768,7 +1770,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1800,7 +1802,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1833,7 +1835,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -1848,7 +1850,7 @@ namespace xsimd\n \n         // mod\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x % y; },\n@@ -1857,14 +1859,14 @@ namespace xsimd\n \n         // nearbyint\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> nearbyintf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> nearbyintf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 batch_type s = bitofsign(self);\n@@ -1884,26 +1886,26 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::nearbyintf(self);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::nearbyintf(self);\n         }\n \n         // nearbyint_as_int\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> nearbyint_as_int(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint_as_int(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<as_integer_t<float>, A>\n+        XSIMD_INLINE batch<as_integer_t<float>, A>\n         nearbyint_as_int(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using U = as_integer_t<float>;\n@@ -1913,7 +1915,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<as_integer_t<double>, A>\n+        XSIMD_INLINE batch<as_integer_t<double>, A>\n         nearbyint_as_int(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using U = as_integer_t<double>;\n@@ -1930,12 +1932,12 @@ namespace xsimd\n             {\n                 using batch_type = batch<T, A>;\n \n-                static inline batch_type next(batch_type const& b) noexcept\n+                static XSIMD_INLINE batch_type next(batch_type const& b) noexcept\n                 {\n                     return b;\n                 }\n \n-                static inline batch_type prev(batch_type const& b) noexcept\n+                static XSIMD_INLINE batch_type prev(batch_type const& b) noexcept\n                 {\n                     return b;\n                 }\n@@ -1963,21 +1965,21 @@ namespace xsimd\n                 using int_batch = typename bitwise_cast_batch<T, A>::type;\n                 using int_type = typename int_batch::value_type;\n \n-                static inline batch_type next(const batch_type& b) noexcept\n+                static XSIMD_INLINE batch_type next(const batch_type& b) noexcept\n                 {\n                     batch_type n = ::xsimd::bitwise_cast<T>(::xsimd::bitwise_cast<int_type>(b) + int_type(1));\n                     return select(b == constants::infinity<batch_type>(), b, n);\n                 }\n \n-                static inline batch_type prev(const batch_type& b) noexcept\n+                static XSIMD_INLINE batch_type prev(const batch_type& b) noexcept\n                 {\n                     batch_type p = ::xsimd::bitwise_cast<T>(::xsimd::bitwise_cast<int_type>(b) - int_type(1));\n                     return select(b == constants::minusinfinity<batch_type>(), b, p);\n                 }\n             };\n         }\n         template <class A, class T>\n-        inline batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) noexcept\n         {\n             using kernel = detail::nextafter_kernel<T, A>;\n             return select(from == to, from,\n@@ -1995,7 +1997,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const auto zero = batch_type(0.);\n@@ -2010,7 +2012,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using cplx_batch = batch<std::complex<T>, A>;\n             using real_batch = typename cplx_batch::real_batch;\n@@ -2029,16 +2031,16 @@ namespace xsimd\n \n         // reciprocal\n         template <class T, class A, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch<T, A> reciprocal(batch<T, A> const& self,\n-                                      requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(batch<T, A> const& self,\n+                                            requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return div(batch_type(1), self);\n         }\n \n         // reduce_add\n         template <class A, class T>\n-        inline std::complex<T> reduce_add(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE std::complex<T> reduce_add(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { reduce_add(self.real()), reduce_add(self.imag()) };\n         }\n@@ -2055,13 +2057,13 @@ namespace xsimd\n             };\n \n             template <class Op, class A, class T>\n-            inline T reduce(Op, batch<T, A> const& self, std::integral_constant<unsigned, 1>) noexcept\n+            XSIMD_INLINE T reduce(Op, batch<T, A> const& self, std::integral_constant<unsigned, 1>) noexcept\n             {\n                 return self.get(0);\n             }\n \n             template <class Op, class A, class T, unsigned Lvl>\n-            inline T reduce(Op op, batch<T, A> const& self, std::integral_constant<unsigned, Lvl>) noexcept\n+            XSIMD_INLINE T reduce(Op op, batch<T, A> const& self, std::integral_constant<unsigned, Lvl>) noexcept\n             {\n                 using index_type = as_unsigned_integer_t<T>;\n                 batch<T, A> split = swizzle(self, make_batch_constant<index_type, A, split_high<index_type, Lvl / 2>>());\n@@ -2071,7 +2073,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::reduce([](batch<T, A> const& x, batch<T, A> const& y)\n                                   { return max(x, y); },\n@@ -2080,7 +2082,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::reduce([](batch<T, A> const& x, batch<T, A> const& y)\n                                   { return min(x, y); },\n@@ -2089,32 +2091,32 @@ namespace xsimd\n \n         // remainder\n         template <class A>\n-        inline batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(nearbyint(self / other), other, self);\n         }\n         template <class A>\n-        inline batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(nearbyint(self / other), other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             auto mod = self % other;\n             return select(mod <= other / 2, mod, mod - other);\n         }\n \n         // select\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) noexcept\n         {\n             return { select(cond, true_br.real(), false_br.real()), select(cond, true_br.imag(), false_br.imag()) };\n         }\n \n         // sign\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type res = select(self > batch_type(0), batch_type(1), batch_type(0)) - select(self < batch_type(0), batch_type(1), batch_type(0));\n@@ -2124,7 +2126,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> signf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> signf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 batch_type res = select(self > batch_type(0.f), batch_type(1.f), batch_type(0.f)) - select(self < batch_type(0.f), batch_type(1.f), batch_type(0.f));\n@@ -2137,17 +2139,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signf(self);\n         }\n         template <class A>\n-        inline batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signf(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -2160,7 +2162,7 @@ namespace xsimd\n \n         // signnz\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return (self >> (sizeof(T) * 8 - 1)) | batch_type(1.);\n@@ -2169,7 +2171,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> signnzf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> signnzf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n #ifndef XSIMD_NO_NANS\n@@ -2181,19 +2183,19 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signnzf(self);\n         }\n         template <class A>\n-        inline batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signnzf(self);\n         }\n \n         // sqrt\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n \n             constexpr T csqrt_scale_factor = std::is_same<T, float>::value ? 6.7108864e7f : 1.8014398509481984e16;\n@@ -2248,7 +2250,7 @@ namespace xsimd\n             struct stirling_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3daaaaab,\n@@ -2257,12 +2259,12 @@ namespace xsimd\n                                   0xb970b359>(x);\n                 }\n \n-                static inline batch_type split_limit() noexcept\n+                static XSIMD_INLINE batch_type split_limit() noexcept\n                 {\n                     return batch_type(bit_cast<float>(uint32_t(0x41d628f6)));\n                 }\n \n-                static inline batch_type large_limit() noexcept\n+                static XSIMD_INLINE batch_type large_limit() noexcept\n                 {\n                     return batch_type(bit_cast<float>(uint32_t(0x420c28f3)));\n                 }\n@@ -2272,7 +2274,7 @@ namespace xsimd\n             struct stirling_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3fb5555555555986ull, //   8.33333333333482257126E-2\n@@ -2283,12 +2285,12 @@ namespace xsimd\n                                   >(x);\n                 }\n \n-                static inline batch_type split_limit() noexcept\n+                static XSIMD_INLINE batch_type split_limit() noexcept\n                 {\n                     return batch_type(bit_cast<double>(uint64_t(0x4061e083ba3443d4)));\n                 }\n \n-                static inline batch_type large_limit() noexcept\n+                static XSIMD_INLINE batch_type large_limit() noexcept\n                 {\n                     return batch_type(bit_cast<double>(uint64_t(0x4065800000000000)));\n                 }\n@@ -2304,7 +2306,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class T, class A>\n-            inline batch<T, A> stirling(const batch<T, A>& a) noexcept\n+            XSIMD_INLINE batch<T, A> stirling(const batch<T, A>& a) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 const batch_type stirlingsplitlim = stirling_kernel<batch_type>::split_limit();\n@@ -2342,7 +2344,7 @@ namespace xsimd\n             struct tgamma_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3f800000UL, //  9.999999757445841E-01\n@@ -2361,7 +2363,7 @@ namespace xsimd\n             struct tgamma_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3ff0000000000000ULL, // 9.99999999999999996796E-1\n@@ -2395,7 +2397,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B>\n-            inline B tgamma_large_negative(const B& a) noexcept\n+            XSIMD_INLINE B tgamma_large_negative(const B& a) noexcept\n             {\n                 B st = stirling(a);\n                 B p = floor(a);\n@@ -2409,7 +2411,7 @@ namespace xsimd\n             }\n \n             template <class B, class BB>\n-            inline B tgamma_other(const B& a, const BB& test) noexcept\n+            XSIMD_INLINE B tgamma_other(const B& a, const BB& test) noexcept\n             {\n                 B x = select(test, B(2.), a);\n #ifndef XSIMD_NO_INFINITIES\n@@ -2448,7 +2450,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             auto nan_result = (self < batch_type(0.) && is_flint(self));\n--- include/xsimd/arch/generic/xsimd_generic_memory.hpp\n@@ -36,7 +36,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class IT, class A, class I, size_t... Is>\n-            inline batch<IT, A> create_compress_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n+            XSIMD_INLINE batch<IT, A> create_compress_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n             {\n                 batch<IT, A> swizzle_mask(IT(0));\n                 alignas(A::alignment()) IT mask_buffer[batch<IT, A>::size] = { Is... };\n@@ -49,7 +49,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         compress(batch<T, A> const& x, batch_bool<T, A> const& mask,\n                  kernel::requires_arch<generic>) noexcept\n         {\n@@ -65,7 +65,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class IT, class A, class I, size_t... Is>\n-            inline batch<IT, A> create_expand_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n+            XSIMD_INLINE batch<IT, A> create_expand_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n             {\n                 batch<IT, A> swizzle_mask(IT(0));\n                 IT j = 0;\n@@ -75,7 +75,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         expand(batch<T, A> const& x, batch_bool<T, A> const& mask,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -88,7 +88,7 @@ namespace xsimd\n \n         // extract_pair\n         template <class A, class T>\n-        inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(i < size && \"index in bounds\");\n@@ -115,6 +115,7 @@ namespace xsimd\n         // gather\n         namespace detail\n         {\n+            // Not using XSIMD_INLINE here as it makes msvc hand got ever on avx512\n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N == 0, int>::type = 0>\n             inline batch<T, A> gather(U const* src, batch<V, A> const& index,\n                                       ::xsimd::index<N> I) noexcept\n@@ -134,7 +135,7 @@ namespace xsimd\n         } // namespace detail\n \n         template <typename T, typename A, typename V>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         gather(batch<T, A> const&, T const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -146,7 +147,7 @@ namespace xsimd\n \n         // Gather with runtime indexes and mismatched strides.\n         template <typename T, typename A, typename U, typename V>\n-        inline detail::sizes_mismatch_t<T, U, batch<T, A>>\n+        XSIMD_INLINE detail::sizes_mismatch_t<T, U, batch<T, A>>\n         gather(batch<T, A> const&, U const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -158,7 +159,7 @@ namespace xsimd\n \n         // Gather with runtime indexes and matching strides.\n         template <typename T, typename A, typename U, typename V>\n-        inline detail::stride_match_t<T, U, batch<T, A>>\n+        XSIMD_INLINE detail::stride_match_t<T, U, batch<T, A>>\n         gather(batch<T, A> const&, U const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -170,7 +171,7 @@ namespace xsimd\n \n         // insert\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept\n         {\n             struct index_mask\n             {\n@@ -185,47 +186,47 @@ namespace xsimd\n \n         // get\n         template <class A, size_t I, class T>\n-        inline T get(batch<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, size_t I, class T>\n-        inline T get(batch_bool<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch_bool<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch_bool<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, size_t I, class T>\n-        inline auto get(batch<std::complex<T>, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n+        XSIMD_INLINE auto get(batch<std::complex<T>, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n         {\n             alignas(A::alignment()) T buffer[batch<std::complex<T>, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, class T>\n-        inline T get(batch<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[i];\n         }\n \n         template <class A, class T>\n-        inline T get(batch_bool<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch_bool<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[i];\n         }\n \n         template <class A, class T>\n-        inline auto get(batch<std::complex<T>, A> const& self, std::size_t i, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n+        XSIMD_INLINE auto get(batch<std::complex<T>, A> const& self, std::size_t i, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n         {\n             using T2 = typename batch<std::complex<T>, A>::value_type;\n             alignas(A::alignment()) T2 buffer[batch<std::complex<T>, A>::size];\n@@ -237,14 +238,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 using batch_type_in = batch<T_in, A>;\n                 using batch_type_out = batch<T_out, A>;\n                 return fast_cast(batch_type_in::load_aligned(mem), batch_type_out(), A {});\n             }\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n                 using batch_type_out = batch<T_out, A>;\n@@ -254,7 +255,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T_in, class T_out>\n-        inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n         {\n             return detail::load_aligned<A>(mem, cvt, A {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n@@ -263,29 +264,29 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 using batch_type_in = batch<T_in, A>;\n                 using batch_type_out = batch<T_out, A>;\n                 return fast_cast(batch_type_in::load_unaligned(mem), batch_type_out(), A {});\n             }\n \n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n                 return load_aligned<A>(mem, cvt, generic {}, with_slow_conversion {});\n             }\n         }\n         template <class A, class T_in, class T_out>\n-        inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n         {\n             return detail::load_unaligned<A>(mem, cvt, generic {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n \n         // rotate_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> rotate_left(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_left(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             struct rotate_generator\n             {\n@@ -299,14 +300,14 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<std::complex<T>, A> rotate_left(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> rotate_left(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { rotate_left<N>(self.real()), rotate_left<N>(self.imag()) };\n         }\n \n         // rotate_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> rotate_right(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             struct rotate_generator\n             {\n@@ -320,7 +321,7 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<std::complex<T>, A> rotate_right(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> rotate_right(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { rotate_right<N>(self.real()), rotate_right<N>(self.imag()) };\n         }\n@@ -329,15 +330,15 @@ namespace xsimd\n         namespace detail\n         {\n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N == 0, int>::type = 0>\n-            inline void scatter(batch<T, A> const& src, U* dst,\n-                                batch<V, A> const& index,\n-                                ::xsimd::index<N> I) noexcept\n+            XSIMD_INLINE void scatter(batch<T, A> const& src, U* dst,\n+                                      batch<V, A> const& index,\n+                                      ::xsimd::index<N> I) noexcept\n             {\n                 dst[index.get(I)] = static_cast<U>(src.get(I));\n             }\n \n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N != 0, int>::type = 0>\n-            inline void\n+            XSIMD_INLINE void\n             scatter(batch<T, A> const& src, U* dst, batch<V, A> const& index,\n                     ::xsimd::index<N> I) noexcept\n             {\n@@ -350,7 +351,7 @@ namespace xsimd\n         } // namespace detail\n \n         template <typename A, typename T, typename V>\n-        inline void\n+        XSIMD_INLINE void\n         scatter(batch<T, A> const& src, T* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -362,7 +363,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T, typename U, typename V>\n-        inline detail::sizes_mismatch_t<T, U, void>\n+        XSIMD_INLINE detail::sizes_mismatch_t<T, U, void>\n         scatter(batch<T, A> const& src, U* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -374,7 +375,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T, typename U, typename V>\n-        inline detail::stride_match_t<T, U, void>\n+        XSIMD_INLINE detail::stride_match_t<T, U, void>\n         scatter(batch<T, A> const& src, U* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -455,7 +456,7 @@ namespace xsimd\n         }\n \n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept\n         {\n             constexpr size_t bsize = sizeof...(Indices);\n \n@@ -512,7 +513,7 @@ namespace xsimd\n \n         // store\n         template <class T, class A>\n-        inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             constexpr auto size = batch_bool<T, A>::size;\n@@ -524,7 +525,7 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T_in, class T_out>\n-        inline void store_aligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_aligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n             alignas(A::alignment()) T_in buffer[batch<T_in, A>::size];\n@@ -534,21 +535,21 @@ namespace xsimd\n \n         // store_unaligned\n         template <class A, class T_in, class T_out>\n-        inline void store_unaligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_unaligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n             return store_aligned<A>(mem, self, generic {});\n         }\n \n         // swizzle\n         template <class A, class T, class ITy, ITy... Vs>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch_constant<ITy, A, Vs...> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch_constant<ITy, A, Vs...> mask, requires_arch<generic>) noexcept\n         {\n             return { swizzle(self.real(), mask), swizzle(self.imag(), mask) };\n         }\n \n         template <class A, class T, class ITy>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             alignas(A::alignment()) T self_buffer[size];\n@@ -564,7 +565,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class ITy>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n         {\n             return { swizzle(self.real(), mask), swizzle(self.imag(), mask) };\n         }\n@@ -573,26 +574,26 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"load_complex not implemented for the required architecture\");\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"complex_high not implemented for the required architecture\");\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"complex_low not implemented for the required architecture\");\n             }\n         }\n \n         template <class A, class T_out, class T_in>\n-        inline batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_out, A>;\n             T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n@@ -603,7 +604,7 @@ namespace xsimd\n \n         // load_complex_unaligned\n         template <class A, class T_out, class T_in>\n-        inline batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_out, A>;\n             T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n@@ -614,7 +615,7 @@ namespace xsimd\n \n         // store_complex_aligned\n         template <class A, class T_out, class T_in>\n-        inline void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_in, A>;\n             real_batch hi = detail::complex_high(src, A {});\n@@ -626,7 +627,7 @@ namespace xsimd\n \n         // store_compelx_unaligned\n         template <class A, class T_out, class T_in>\n-        inline void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_in, A>;\n             real_batch hi = detail::complex_high(src, A {});\n--- include/xsimd/arch/generic/xsimd_generic_rounding.hpp\n@@ -24,23 +24,23 @@ namespace xsimd\n \n         // ceil\n         template <class A, class T>\n-        inline batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             batch<T, A> truncated_self = trunc(self);\n             return select(truncated_self < self, truncated_self + 1, truncated_self);\n         }\n \n         // floor\n         template <class A, class T>\n-        inline batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             batch<T, A> truncated_self = trunc(self);\n             return select(truncated_self > self, truncated_self - 1, truncated_self);\n         }\n \n         // round\n         template <class A, class T>\n-        inline batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             auto v = abs(self);\n             auto c = ceil(v);\n@@ -50,17 +50,17 @@ namespace xsimd\n \n         // trunc\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return select(abs(self) < constants::maxflint<batch<float, A>>(), to_float(to_int(self)), self);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return select(abs(self) < constants::maxflint<batch<double, A>>(), to_float(to_int(self)), self);\n         }\n--- include/xsimd/arch/generic/xsimd_generic_trigo.hpp\n@@ -35,7 +35,7 @@ namespace xsimd\n \n         // acos\n         template <class A, class T>\n-        inline batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -47,7 +47,7 @@ namespace xsimd\n             return select(x_larger_05, x, constants::pio2<batch_type>() - x);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -66,7 +66,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = self - batch_type(1.);\n@@ -76,7 +76,7 @@ namespace xsimd\n             return select(test, l1pz + constants::log_2<batch_type>(), l1pz);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = acos(z);\n@@ -86,7 +86,7 @@ namespace xsimd\n \n         // asin\n         template <class A>\n-        inline batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -105,7 +105,7 @@ namespace xsimd\n             return z ^ sign;\n         }\n         template <class A>\n-        inline batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -127,7 +127,7 @@ namespace xsimd\n                               ^ bitofsign(self));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -159,32 +159,32 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-            inline batch<T, A>\n+            XSIMD_INLINE batch<T, A>\n             average(const batch<T, A>& x1, const batch<T, A>& x2) noexcept\n             {\n                 return (x1 & x2) + ((x1 ^ x2) >> 1);\n             }\n \n             template <class A, class T>\n-            inline batch<T, A>\n+            XSIMD_INLINE batch<T, A>\n             averagef(const batch<T, A>& x1, const batch<T, A>& x2) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 return fma(x1, batch_type(0.5), x2 * batch_type(0.5));\n             }\n             template <class A>\n-            inline batch<float, A> average(batch<float, A> const& x1, batch<float, A> const& x2) noexcept\n+            XSIMD_INLINE batch<float, A> average(batch<float, A> const& x1, batch<float, A> const& x2) noexcept\n             {\n                 return averagef(x1, x2);\n             }\n             template <class A>\n-            inline batch<double, A> average(batch<double, A> const& x1, batch<double, A> const& x2) noexcept\n+            XSIMD_INLINE batch<double, A> average(batch<double, A> const& x1, batch<double, A> const& x2) noexcept\n             {\n                 return averagef(x1, x2);\n             }\n         }\n         template <class A>\n-        inline batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -212,7 +212,7 @@ namespace xsimd\n #endif\n         }\n         template <class A>\n-        inline batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -226,7 +226,7 @@ namespace xsimd\n             return bitofsign(self) ^ z;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = asin(batch_type(-z.imag(), z.real()));\n@@ -238,7 +238,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            static inline batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx) noexcept\n+            static XSIMD_INLINE batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 const auto flag1 = x < constants::tan3pio8<batch_type>();\n@@ -259,7 +259,7 @@ namespace xsimd\n                 return yy + z1;\n             }\n             template <class A>\n-            static inline batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx) noexcept\n+            static XSIMD_INLINE batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 const auto flag1 = x < constants::tan3pio8<batch_type>();\n@@ -288,15 +288,15 @@ namespace xsimd\n             }\n         }\n         template <class A, class T>\n-        inline batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type absa = abs(self);\n             const batch_type x = detail::kernel_atan(absa, batch_type(1.) / absa);\n             return x ^ bitofsign(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -327,7 +327,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -338,7 +338,7 @@ namespace xsimd\n             return bitofsign(self) ^ (batch_type(0.5) * log1p(select(test, fma(t, tmp, t), tmp)));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = atan(batch_type(-z.imag(), z.real()));\n@@ -348,7 +348,7 @@ namespace xsimd\n \n         // atan2\n         template <class A, class T>\n-        inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type q = abs(self / other);\n@@ -360,19 +360,19 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> quadrant(const batch<T, A>& x) noexcept\n+            XSIMD_INLINE batch<T, A> quadrant(const batch<T, A>& x) noexcept\n             {\n                 return x & batch<T, A>(3);\n             }\n \n             template <class A>\n-            inline batch<float, A> quadrant(const batch<float, A>& x) noexcept\n+            XSIMD_INLINE batch<float, A> quadrant(const batch<float, A>& x) noexcept\n             {\n                 return to_float(quadrant(to_int(x)));\n             }\n \n             template <class A>\n-            inline batch<double, A> quadrant(const batch<double, A>& x) noexcept\n+            XSIMD_INLINE batch<double, A> quadrant(const batch<double, A>& x) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type a = x * batch_type(0.25);\n@@ -389,7 +389,7 @@ namespace xsimd\n              */\n \n             template <class A>\n-            inline batch<float, A> cos_eval(const batch<float, A>& z) noexcept\n+            XSIMD_INLINE batch<float, A> cos_eval(const batch<float, A>& z) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -400,7 +400,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x) noexcept\n+            XSIMD_INLINE batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -411,7 +411,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> base_tancot_eval(const batch<float, A>& z) noexcept\n+            static XSIMD_INLINE batch<float, A> base_tancot_eval(const batch<float, A>& z) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type zz = z * z;\n@@ -426,15 +426,15 @@ namespace xsimd\n             }\n \n             template <class A, class BB>\n-            static inline batch<float, A> tan_eval(const batch<float, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<float, A> tan_eval(const batch<float, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = base_tancot_eval(z);\n                 return select(test, y, -batch_type(1.) / y);\n             }\n \n             template <class A, class BB>\n-            static inline batch<float, A> cot_eval(const batch<float, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<float, A> cot_eval(const batch<float, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = base_tancot_eval(z);\n@@ -451,7 +451,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<double, A> cos_eval(const batch<double, A>& z) noexcept\n+            static XSIMD_INLINE batch<double, A> cos_eval(const batch<double, A>& z) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -466,7 +466,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -480,7 +480,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> base_tancot_eval(const batch<double, A>& z) noexcept\n+            static XSIMD_INLINE batch<double, A> base_tancot_eval(const batch<double, A>& z) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type zz = z * z;\n@@ -497,15 +497,15 @@ namespace xsimd\n             }\n \n             template <class A, class BB>\n-            static inline batch<double, A> tan_eval(const batch<double, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<double, A> tan_eval(const batch<double, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = base_tancot_eval(z);\n                 return select(test, y, -batch_type(1.) / y);\n             }\n \n             template <class A, class BB>\n-            static inline batch<double, A> cot_eval(const batch<double, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<double, A> cot_eval(const batch<double, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = base_tancot_eval(z);\n@@ -531,7 +531,7 @@ namespace xsimd\n             template <class B, class Tag = trigo_radian_tag>\n             struct trigo_reducer\n             {\n-                static inline B reduce(const B& x, B& xr) noexcept\n+                static XSIMD_INLINE B reduce(const B& x, B& xr) noexcept\n                 {\n                     if (all(x <= constants::pio4<B>()))\n                     {\n@@ -606,7 +606,7 @@ namespace xsimd\n             template <class B>\n             struct trigo_reducer<B, trigo_pi_tag>\n             {\n-                static inline B reduce(const B& x, B& xr) noexcept\n+                static XSIMD_INLINE B reduce(const B& x, B& xr) noexcept\n                 {\n                     B xi = nearbyint(x * B(2.));\n                     B x2 = x - xi * B(0.5);\n@@ -617,7 +617,7 @@ namespace xsimd\n \n         }\n         template <class A, class T>\n-        inline batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -634,7 +634,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return { cos(z.real()) * cosh(z.imag()), -sin(z.real()) * sinh(z.imag()) };\n         }\n@@ -652,7 +652,7 @@ namespace xsimd\n          */\n \n         template <class A, class T>\n-        inline batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -663,7 +663,7 @@ namespace xsimd\n             return select(test1, tmp1 * tmp, detail::average(tmp, batch_type(1.) / tmp));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             auto x = z.real();\n             auto y = z.imag();\n@@ -674,7 +674,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class Tag = trigo_radian_tag>\n-            inline batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) noexcept\n+            XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n@@ -692,20 +692,20 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::sin(self);\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return { sin(z.real()) * cosh(z.imag()), cos(z.real()) * sinh(z.imag()) };\n         }\n \n         // sincos\n         template <class A, class T>\n-        inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -724,7 +724,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n+        XSIMD_INLINE std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n         sincos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n@@ -749,7 +749,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            inline batch<float, A> sinh_kernel(batch<float, A> const& self) noexcept\n+            XSIMD_INLINE batch<float, A> sinh_kernel(batch<float, A> const& self) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type sqr_self = self * self;\n@@ -763,7 +763,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> sinh_kernel(batch<double, A> const& self) noexcept\n+            XSIMD_INLINE batch<double, A> sinh_kernel(batch<double, A> const& self) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type sqrself = self * self;\n@@ -792,7 +792,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type half(0.5);\n@@ -814,7 +814,7 @@ namespace xsimd\n             return select(lt1, z, r) ^ bts;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             auto x = z.real();\n             auto y = z.imag();\n@@ -823,7 +823,7 @@ namespace xsimd\n \n         // tan\n         template <class A, class T>\n-        inline batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -836,7 +836,7 @@ namespace xsimd\n             return y ^ bitofsign(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -867,7 +867,7 @@ namespace xsimd\n             struct tanh_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type tanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type tanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     return fma(detail::horner<batch_type,\n@@ -881,7 +881,7 @@ namespace xsimd\n                                x, x);\n                 }\n \n-                static inline batch_type cotanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type cotanh(const batch_type& x) noexcept\n                 {\n                     return batch_type(1.) / tanh(x);\n                 }\n@@ -891,20 +891,20 @@ namespace xsimd\n             struct tanh_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type tanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type tanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     return fma(sqrx * p(sqrx) / q(sqrx), x, x);\n                 }\n \n-                static inline batch_type cotanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type cotanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     batch_type qval = q(sqrx);\n                     return qval / (x * fma(p(sqrx), sqrx, qval));\n                 }\n \n-                static inline batch_type p(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type p(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0xc0993ac030580563, // -1.61468768441708447952E3\n@@ -913,7 +913,7 @@ namespace xsimd\n                                           >(x);\n                 }\n \n-                static inline batch_type q(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type q(const batch_type& x) noexcept\n                 {\n                     return detail::horner1<batch_type,\n                                            0x40b2ec102442040c, //  4.84406305325125486048E3\n@@ -934,7 +934,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type one(1.);\n@@ -952,7 +952,7 @@ namespace xsimd\n             return select(test, z, r) ^ bts;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using real_batch = typename batch<std::complex<T>, A>::real_batch;\n             auto x = z.real();\n--- include/xsimd/arch/xsimd_avx.hpp\n@@ -27,39 +27,39 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n-            inline void split_avx(__m256i val, __m128i& low, __m128i& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256i val, __m128i& low, __m128i& high) noexcept\n             {\n                 low = _mm256_castsi256_si128(val);\n                 high = _mm256_extractf128_si256(val, 1);\n             }\n-            inline void split_avx(__m256 val, __m128& low, __m128& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256 val, __m128& low, __m128& high) noexcept\n             {\n                 low = _mm256_castps256_ps128(val);\n                 high = _mm256_extractf128_ps(val, 1);\n             }\n-            inline void split_avx(__m256d val, __m128d& low, __m128d& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256d val, __m128d& low, __m128d& high) noexcept\n             {\n                 low = _mm256_castpd256_pd128(val);\n                 high = _mm256_extractf128_pd(val, 1);\n             }\n-            inline __m256i merge_sse(__m128i low, __m128i high) noexcept\n+            XSIMD_INLINE __m256i merge_sse(__m128i low, __m128i high) noexcept\n             {\n                 return _mm256_insertf128_si256(_mm256_castsi128_si256(low), high, 1);\n             }\n-            inline __m256 merge_sse(__m128 low, __m128 high) noexcept\n+            XSIMD_INLINE __m256 merge_sse(__m128 low, __m128 high) noexcept\n             {\n                 return _mm256_insertf128_ps(_mm256_castps128_ps256(low), high, 1);\n             }\n-            inline __m256d merge_sse(__m128d low, __m128d high) noexcept\n+            XSIMD_INLINE __m256d merge_sse(__m128d low, __m128d high) noexcept\n             {\n                 return _mm256_insertf128_pd(_mm256_castpd128_pd256(low), high, 1);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self) noexcept\n             {\n                 __m128i self_low, self_high;\n                 split_avx(self, self_low, self_high);\n@@ -68,7 +68,7 @@ namespace xsimd\n                 return merge_sse(res_low, res_high);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self, __m256i other) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self, __m256i other) noexcept\n             {\n                 __m128i self_low, self_high, other_low, other_high;\n                 split_avx(self, self_low, self_high);\n@@ -78,7 +78,7 @@ namespace xsimd\n                 return merge_sse(res_low, res_high);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self, int32_t other) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self, int32_t other) noexcept\n             {\n                 __m128i self_low, self_high;\n                 split_avx(self, self_low, self_high);\n@@ -90,110 +90,110 @@ namespace xsimd\n \n         // abs\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m256 sign_mask = _mm256_set1_ps(-0.f); // -0.f = 1 << 31\n             return _mm256_andnot_ps(sign_mask, self);\n         }\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m256d sign_mask = _mm256_set1_pd(-0.f); // -0.f = 1 << 31\n             return _mm256_andnot_pd(sign_mask, self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return add(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_add_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_add_pd(self, other);\n         }\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_ps(self, batch_bool<float, A>(true)) != 0;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_pd(self, batch_bool<double, A>(true)) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_si256(self, batch_bool<T, A>(true)) != 0;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_ps(self, self);\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_pd(self, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_si256(self, self);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_pd(self, other);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -202,36 +202,36 @@ namespace xsimd\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_pd(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_pd(other, self);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -240,7 +240,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, int32_t o) noexcept\n                                       { return bitwise_lshift(batch<T, sse4_2>(s), o, sse4_2 {}); },\n@@ -249,14 +249,14 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s) noexcept\n                                       { return bitwise_not(batch<T, sse4_2>(s), sse4_2 {}); },\n                                       self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s) noexcept\n                                       { return bitwise_not(batch_bool<T, sse4_2>(s), sse4_2 {}); },\n@@ -265,34 +265,34 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_or(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_or(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o)); },\n@@ -301,7 +301,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, int32_t o) noexcept\n                                       { return bitwise_rshift(batch<T, sse4_2>(s), o, sse4_2 {}); },\n@@ -310,34 +310,34 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_xor(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2 {}); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_xor(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o), sse4_2 {}); },\n@@ -346,66 +346,66 @@ namespace xsimd\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castsi256_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castsi256_pd(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_pd(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_si256(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castpd_si256(self);\n         }\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n         }\n \n         // broadcast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -430,24 +430,24 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> broadcast(float val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float val, requires_arch<avx>) noexcept\n         {\n             return _mm256_set1_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<avx>) noexcept\n         {\n             return _mm256_set1_pd(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_ceil_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_ceil_pd(self);\n         }\n@@ -457,7 +457,7 @@ namespace xsimd\n             // On clang, _mm256_extractf128_ps is built upon build_shufflevector\n             // which require index parameter to be a constant\n             template <int index, class B>\n-            inline B get_half_complex_f(const B& real, const B& imag) noexcept\n+            XSIMD_INLINE B get_half_complex_f(const B& real, const B& imag) noexcept\n             {\n                 __m128 tmp0 = _mm256_extractf128_ps(real, index);\n                 __m128 tmp1 = _mm256_extractf128_ps(imag, index);\n@@ -469,7 +469,7 @@ namespace xsimd\n                 return res;\n             }\n             template <int index, class B>\n-            inline B get_half_complex_d(const B& real, const B& imag) noexcept\n+            XSIMD_INLINE B get_half_complex_d(const B& real, const B& imag) noexcept\n             {\n                 __m128d tmp0 = _mm256_extractf128_pd(real, index);\n                 __m128d tmp1 = _mm256_extractf128_pd(imag, index);\n@@ -483,24 +483,24 @@ namespace xsimd\n \n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_f<0>(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_d<0>(self.real(), self.imag());\n             }\n \n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_f<1>(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_d<1>(self.real(), self.imag());\n             }\n@@ -510,87 +510,87 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n             {\n                 return _mm256_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) noexcept\n             {\n                 return _mm256_cvttps_epi32(self);\n             }\n         }\n \n         // decr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_div_pd(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return eq(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2 {}); },\n                                       self, other);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_floor_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_floor_pd(self);\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut32[] = {\n                 0x0000000000000000ul,\n@@ -602,7 +602,7 @@ namespace xsimd\n             return _mm256_castsi256_ps(_mm256_setr_epi64x(lut32[mask & 0x3], lut32[(mask >> 2) & 0x3], lut32[(mask >> 4) & 0x3], lut32[mask >> 6]));\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul, 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -626,7 +626,7 @@ namespace xsimd\n             return _mm256_castsi256_pd(_mm256_load_si256((const __m256i*)lut64[mask]));\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut32[] = {\n                 0x00000000,\n@@ -689,7 +689,7 @@ namespace xsimd\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) noexcept\n         {\n             // row = (a,b,c,d,e,f,g,h)\n             // tmp0 = (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)\n@@ -715,7 +715,7 @@ namespace xsimd\n             return _mm256_add_ps(tmp0, tmp1);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx>) noexcept\n         {\n             // row = (a,b,c,d)\n             // tmp0 = (a0+a1, b0+b1, a2+a3, b2+b3)\n@@ -731,14 +731,14 @@ namespace xsimd\n \n         // incr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<avx>) noexcept\n         {\n #if !defined(_MSC_VER) || _MSC_VER > 1900\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n@@ -763,41 +763,41 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, self, _CMP_UNORD_Q);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, self, _CMP_UNORD_Q);\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_LE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_LE_OQ);\n         }\n \n         // load_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_si256((__m256i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_pd(mem);\n         }\n@@ -806,7 +806,7 @@ namespace xsimd\n         {\n             // load_complex\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 __m128 tmp0 = _mm256_extractf128_ps(hi, 0);\n@@ -825,7 +825,7 @@ namespace xsimd\n                 return { real, imag };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx>) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 __m128d tmp0 = _mm256_extractf128_pd(hi, 0);\n@@ -845,35 +845,35 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_si256((__m256i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_pd(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_LT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_LT_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return lt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -882,7 +882,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -905,86 +905,86 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_movemask_ps(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_movemask_pd(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_max_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_max_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_min_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_min_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_mul_pd(self, other);\n         }\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<avx>) noexcept\n         {\n             return _mm256_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return 0 - self;\n         }\n@@ -994,55 +994,55 @@ namespace xsimd\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)));\n         }\n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)));\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_si256(_mm256_xor_ps(_mm256_castsi256_ps(self.data), _mm256_castsi256_ps(other.data)));\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self,\n-                                          kernel::requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self,\n+                                                kernel::requires_arch<avx>) noexcept\n         {\n             return _mm256_rcp_ps(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx>) noexcept\n         {\n             // Warning about _mm256_hadd_ps:\n             // _mm256_hadd_ps(a,b) gives\n@@ -1060,7 +1060,7 @@ namespace xsimd\n             return _mm_cvtss_f32(_mm256_extractf128_ps(tmp, 0));\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& rhs, requires_arch<avx>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& rhs, requires_arch<avx>) noexcept\n         {\n             // rhs = (x0, x1, x2, x3)\n             // tmp = (x2, x3, x0, x1)\n@@ -1072,7 +1072,7 @@ namespace xsimd\n             return _mm_cvtsd_f64(_mm256_extractf128_pd(tmp, 0));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m128i low, high;\n             detail::split_avx(self, low, high);\n@@ -1082,7 +1082,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(1, 0);\n             batch<T, A> step = _mm256_permute2f128_si256(self, self, mask);\n@@ -1093,7 +1093,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(1, 0);\n             batch<T, A> step = _mm256_permute2f128_si256(self, self, mask);\n@@ -1104,19 +1104,19 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_rsqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_cvtps_pd(_mm_rsqrt_ps(_mm256_cvtpd_ps(val)));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1135,17 +1135,17 @@ namespace xsimd\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return _mm256_blendv_ps(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return _mm256_blendv_pd(false_br, true_br, cond);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             __m128i cond_low, cond_hi;\n             detail::split_avx(cond, cond_low, cond_hi);\n@@ -1161,84 +1161,84 @@ namespace xsimd\n             return detail::merge_sse(res_low, res_hi);\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, avx2 {});\n         }\n \n         template <class A, bool... Values>\n-        inline batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = batch_bool_constant<float, A, Values...>::mask();\n             return _mm256_blend_ps(false_br, true_br, mask);\n         }\n \n         template <class A, bool... Values>\n-        inline batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = batch_bool_constant<double, A, Values...>::mask();\n             return _mm256_blend_pd(false_br, true_br, mask);\n         }\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return _mm256_setr_ps(values...);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return _mm256_setr_pd(values...);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return _mm256_set_epi64x(v3, v2, v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm256_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm256_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n             return _mm256_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return _mm256_castsi256_ps(set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return _mm256_castsi256_pd(set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1, I2, I3);\n             // shuffle within lane\n@@ -1253,7 +1253,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<avx>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x1) | ((I1 & 0x1) << 1) | ((I2 & 0x1) << 2) | ((I3 & 0x1) << 3);\n             // shuffle within lane\n@@ -1269,7 +1269,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -1310,7 +1310,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -1350,19 +1350,19 @@ namespace xsimd\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_sqrt_pd(val);\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1377,70 +1377,70 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_si256((__m256i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_si256((__m256i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_si256((__m256i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_si256((__m256i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return sub(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_sub_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_sub_pd(self, other);\n         }\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256 hi = _mm256_castps128_ps256(_mm256_extractf128_ps(self, 1));\n@@ -1464,7 +1464,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256d hi = _mm256_castpd128_pd256(_mm256_extractf128_pd(self, 1));\n@@ -1488,14 +1488,14 @@ namespace xsimd\n         }\n \n         template <class A, typename T, detail::enable_sized_integral_t<T, 4> = 0>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch<uint32_t, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch<uint32_t, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n                 swizzle(bitwise_cast<float>(self), mask));\n         }\n \n         template <class A, typename T, detail::enable_sized_integral_t<T, 8> = 0>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         swizzle(batch<T, A> const& self, batch<uint64_t, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n@@ -1504,7 +1504,7 @@ namespace xsimd\n \n         // swizzle (constant mask)\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256 hi = _mm256_castps128_ps256(_mm256_extractf128_ps(self, 1));\n@@ -1529,7 +1529,7 @@ namespace xsimd\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256d hi = _mm256_castpd128_pd256(_mm256_extractf128_pd(self, 1));\n@@ -1563,17 +1563,17 @@ namespace xsimd\n                   uint32_t V6,\n                   uint32_t V7,\n                   detail::enable_sized_integral_t<T, 4> = 0>\n-        inline batch<T, A> swizzle(batch<T, A> const& self,\n-                                   batch_constant<uint32_t, A,\n-                                                  V0,\n-                                                  V1,\n-                                                  V2,\n-                                                  V3,\n-                                                  V4,\n-                                                  V5,\n-                                                  V6,\n-                                                  V7> const& mask,\n-                                   requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self,\n+                                         batch_constant<uint32_t, A,\n+                                                        V0,\n+                                                        V1,\n+                                                        V2,\n+                                                        V3,\n+                                                        V4,\n+                                                        V5,\n+                                                        V6,\n+                                                        V7> const& mask,\n+                                         requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n                 swizzle(bitwise_cast<float>(self), mask));\n@@ -1586,7 +1586,7 @@ namespace xsimd\n                   uint64_t V2,\n                   uint64_t V3,\n                   detail::enable_sized_integral_t<T, 8> = 0>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         swizzle(batch<T, A> const& self,\n                 batch_constant<uint64_t, A, V0, V1, V2, V3> const& mask,\n                 requires_arch<avx>) noexcept\n@@ -1597,19 +1597,19 @@ namespace xsimd\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_ps(self, _MM_FROUND_TO_ZERO);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_pd(self, _MM_FROUND_TO_ZERO);\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -1656,14 +1656,14 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_ps(self, other);\n             auto hi = _mm256_unpackhi_ps(self, other);\n             return _mm256_permute2f128_ps(lo, hi, 0x31);\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_pd(self, other);\n             auto hi = _mm256_unpackhi_pd(self, other);\n@@ -1672,7 +1672,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -1720,14 +1720,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_ps(self, other);\n             auto hi = _mm256_unpackhi_ps(self, other);\n             return _mm256_insertf128_ps(lo, _mm256_castps256_ps128(hi), 1);\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_pd(self, other);\n             auto hi = _mm256_unpackhi_pd(self, other);\n--- include/xsimd/arch/xsimd_avx2.hpp\n@@ -26,7 +26,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -52,7 +52,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -78,7 +78,7 @@ namespace xsimd\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -96,7 +96,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -116,43 +116,43 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_and_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_and_si256(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_andnot_si256(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_andnot_si256(other, self);\n         }\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -173,7 +173,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -191,19 +191,19 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_or_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_or_si256(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -253,7 +253,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -285,19 +285,19 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, other);\n         }\n \n         // complex_low\n         template <class A>\n-        inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n         {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 1, 1, 0));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(1, 2, 0, 0));\n@@ -306,7 +306,7 @@ namespace xsimd\n \n         // complex_high\n         template <class A>\n-        inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n         {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 3, 1, 2));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(3, 2, 2, 0));\n@@ -318,7 +318,7 @@ namespace xsimd\n         {\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to avx\n@@ -332,7 +332,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to avx\n@@ -349,7 +349,7 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -375,55 +375,55 @@ namespace xsimd\n \n         // gather\n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 4> = 0, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i32gather_epi32(reinterpret_cast<const int*>(src), index, sizeof(T));\n         }\n \n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 8> = 0, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i64gather_epi64(reinterpret_cast<const long long int*>(src), index, sizeof(T));\n         }\n \n         template <class A, class U,\n                   detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, float const* src,\n-                                      batch<U, A> const& index,\n-                                      kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, float const* src,\n+                                            batch<U, A> const& index,\n+                                            kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i32gather_ps(src, index, sizeof(float));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<double, A> gather(batch<double, A> const&, double const* src,\n-                                       batch<U, A> const& index,\n-                                       requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> gather(batch<double, A> const&, double const* src,\n+                                             batch<U, A> const& index,\n+                                             requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i64gather_pd(src, index, sizeof(double));\n         }\n \n         // gather: handmade conversions\n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, double const* src,\n-                                      batch<V, A> const& index,\n-                                      requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, double const* src,\n+                                            batch<V, A> const& index,\n+                                            requires_arch<avx2>) noexcept\n         {\n             const batch<double, A> low(_mm256_i32gather_pd(src, _mm256_castsi256_si128(index.data), sizeof(double)));\n             const batch<double, A> high(_mm256_i32gather_pd(src, _mm256_extractf128_si256(index.data, 1), sizeof(double)));\n             return detail::merge_sse(_mm256_cvtpd_ps(low.data), _mm256_cvtpd_ps(high.data));\n         }\n \n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n-                                        batch<V, A> const& index,\n-                                        requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n+                                              batch<V, A> const& index,\n+                                              requires_arch<avx2>) noexcept\n         {\n             const batch<double, A> low(_mm256_i32gather_pd(src, _mm256_castsi256_si128(index.data), sizeof(double)));\n             const batch<double, A> high(_mm256_i32gather_pd(src, _mm256_extractf128_si256(index.data, 1), sizeof(double)));\n@@ -432,7 +432,7 @@ namespace xsimd\n \n         // lt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -465,7 +465,7 @@ namespace xsimd\n \n         // load_complex\n         template <class A>\n-        inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type real = _mm256_castpd_ps(\n@@ -479,7 +479,7 @@ namespace xsimd\n             return { real, imag };\n         }\n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx2>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type real = _mm256_permute4x64_pd(_mm256_unpacklo_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n@@ -488,7 +488,7 @@ namespace xsimd\n         }\n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -507,7 +507,7 @@ namespace xsimd\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -551,7 +551,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -595,7 +595,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -623,7 +623,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -657,19 +657,19 @@ namespace xsimd\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), avx2 {}));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -705,7 +705,7 @@ namespace xsimd\n \n         // select\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -729,7 +729,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<T, A, Values...>::mask();\n             // FIXME: for some reason mask here is not considered as an immediate,\n@@ -752,7 +752,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx2>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -783,7 +783,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx2>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -814,7 +814,7 @@ namespace xsimd\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -850,7 +850,7 @@ namespace xsimd\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -876,79 +876,79 @@ namespace xsimd\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_ps(self, mask);\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             batch<uint32_t, A> broadcaster = { 0, 1, 0, 1, 0, 1, 0, 1 };\n             constexpr uint64_t comb = 0x0000000100000001ul * 2;\n             return bitwise_cast<double>(swizzle(bitwise_cast<float>(self), bitwise_cast<uint32_t>(mask * comb) + broadcaster, avx2 {}));\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<uint64_t>(swizzle(bitwise_cast<double>(self), mask, avx2 {}));\n         }\n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<double>(self), mask, avx2 {}));\n         }\n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_epi32(self, mask);\n         }\n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx2 {}));\n         }\n \n         // swizzle (constant mask)\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_ps(self, mask.as_batch());\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(V0, V1, V2, V3);\n             return _mm256_permute4x64_pd(self, mask);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(V0, V1, V2, V3);\n             return _mm256_permute4x64_epi64(self, mask);\n         }\n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, avx2 {}));\n         }\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_epi32(self, mask.as_batch());\n         }\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx2 {}));\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -983,7 +983,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n--- include/xsimd/arch/xsimd_avx512bw.hpp\n@@ -27,7 +27,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, int Cmp>\n-            inline batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 using register_type = typename batch_bool<T, A>::register_type;\n                 if (std::is_signed<T>::value)\n@@ -73,7 +73,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n             {\n@@ -96,7 +96,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -114,7 +114,7 @@ namespace xsimd\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -132,7 +132,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -152,7 +152,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n         {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n@@ -172,7 +172,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -226,42 +226,42 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_EQ>(self, other);\n         }\n \n         // ge\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GE>(self, other);\n         }\n \n         // gt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GT>(self, other);\n         }\n \n         // le\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LE>(self, other);\n         }\n \n         // lt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LT>(self, other);\n         }\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -297,7 +297,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -333,7 +333,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -353,26 +353,26 @@ namespace xsimd\n \n         // neq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_NE>(self, other);\n         }\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), avx2 {}));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -408,7 +408,7 @@ namespace xsimd\n \n         // select\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -446,7 +446,7 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -505,7 +505,7 @@ namespace xsimd\n             }\n         }\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -538,7 +538,7 @@ namespace xsimd\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -574,7 +574,7 @@ namespace xsimd\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -592,57 +592,57 @@ namespace xsimd\n \n         // swizzle (dynamic version)\n         template <class A>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_permutexvar_epi16(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, avx512bw {}));\n         }\n \n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_shuffle_epi8(self, mask);\n         }\n \n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int8_t>(swizzle(bitwise_cast<uint8_t>(self), mask, avx512bw {}));\n         }\n \n         // swizzle (static version)\n         template <class A, uint16_t... Vs>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint16_t... Vs>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint8_t... Vs>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint8_t... Vs>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             __m512i lo, hi;\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n@@ -670,7 +670,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             __m512i lo, hi;\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n--- include/xsimd/arch/xsimd_avx512dq.hpp\n@@ -23,74 +23,74 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_and_pd(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_andnot_pd(other, self);\n         }\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_ps(self, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_pd(self, _mm512_castsi512_pd(_mm512_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_or_pd(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_xor_pd(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512dq>) noexcept\n         {\n             // The following folds over the vector once:\n             // tmp1 = [a0..8, b0..8]\n@@ -152,35 +152,35 @@ namespace xsimd\n \n         // ldexp\n         template <class A>\n-        inline batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_scalef_pd(self, _mm512_cvtepi64_pd(other));\n         }\n \n         // mul\n         template <class A>\n-        inline batch<uint64_t, A> mul(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> mul(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_mullo_epi64(self, other);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> mul(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> mul(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_mullo_epi64(self, other);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n-                                                  requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n+                                                        requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_cvtpd_epi64(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m256 tmp1 = _mm512_extractf32x8_ps(rhs, 1);\n             __m256 tmp2 = _mm512_extractf32x8_ps(rhs, 0);\n@@ -192,13 +192,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx512dq>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx512dq>) noexcept\n             {\n                 return _mm512_cvtepi64_pd(self);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<avx512dq>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<avx512dq>) noexcept\n             {\n                 return _mm512_cvttpd_epi64(self);\n             }\n--- include/xsimd/arch/xsimd_avx512f.hpp\n@@ -27,30 +27,30 @@ namespace xsimd\n \n         namespace detail\n         {\n-            inline void split_avx512(__m512 val, __m256& low, __m256& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512 val, __m256& low, __m256& high) noexcept\n             {\n                 low = _mm512_castps512_ps256(val);\n                 high = _mm512_extractf32x8_ps(val, 1);\n             }\n-            inline void split_avx512(__m512d val, __m256d& low, __m256d& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512d val, __m256d& low, __m256d& high) noexcept\n             {\n                 low = _mm512_castpd512_pd256(val);\n                 high = _mm512_extractf64x4_pd(val, 1);\n             }\n-            inline void split_avx512(__m512i val, __m256i& low, __m256i& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512i val, __m256i& low, __m256i& high) noexcept\n             {\n                 low = _mm512_castsi512_si256(val);\n                 high = _mm512_extracti64x4_epi64(val, 1);\n             }\n-            inline __m512i merge_avx(__m256i low, __m256i high) noexcept\n+            XSIMD_INLINE __m512i merge_avx(__m256i low, __m256i high) noexcept\n             {\n                 return _mm512_inserti64x4(_mm512_castsi256_si512(low), high, 1);\n             }\n-            inline __m512 merge_avx(__m256 low, __m256 high) noexcept\n+            XSIMD_INLINE __m512 merge_avx(__m256 low, __m256 high) noexcept\n             {\n                 return _mm512_castpd_ps(_mm512_insertf64x4(_mm512_castpd256_pd512(_mm256_castps_pd(low)), _mm256_castps_pd(high), 1));\n             }\n-            inline __m512d merge_avx(__m256d low, __m256d high) noexcept\n+            XSIMD_INLINE __m512d merge_avx(__m256d low, __m256d high) noexcept\n             {\n                 return _mm512_insertf64x4(_mm512_castpd256_pd512(low), high, 1);\n             }\n@@ -86,7 +86,7 @@ namespace xsimd\n         namespace detail\n         {\n \n-            inline uint32_t morton(uint16_t x, uint16_t y) noexcept\n+            XSIMD_INLINE uint32_t morton(uint16_t x, uint16_t y) noexcept\n             {\n \n                 static const unsigned short MortonTable256[256] = {\n@@ -129,7 +129,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int Cmp>\n-            inline batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 using register_type = typename batch_bool<T, A>::register_type;\n                 if (std::is_signed<T>::value)\n@@ -217,15 +217,15 @@ namespace xsimd\n \n         // abs\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m512 self_asf = (__m512)self;\n             __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asf);\n             __m512i res_asi = _mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF), self_asi);\n             return *reinterpret_cast<__m512*>(&res_asi);\n         }\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m512d self_asd = (__m512d)self;\n             __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asd);\n@@ -234,7 +234,7 @@ namespace xsimd\n             return *reinterpret_cast<__m512d*>(&res_asi);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n             {\n@@ -270,7 +270,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -299,42 +299,42 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_add_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_add_pd(self, other);\n         }\n \n         // all\n         template <class A, class T>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return self.data == register_type(-1);\n         }\n \n         // any\n         template <class A, class T>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return self.data != register_type(0);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return self.data;\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n #if defined(_MSC_VER)\n             return _mm512_and_ps(self, other);\n@@ -343,52 +343,52 @@ namespace xsimd\n #endif\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_and_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_and_si512(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data & other.data);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_andnot_si512(_mm512_castps_si512(other), _mm512_castps_si512(self)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_andnot_si512(_mm512_castpd_si512(other), _mm512_castpd_si512(self)));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_andnot_si512(other, self);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data & ~other.data);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -433,56 +433,56 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_si512(self, _mm512_set1_epi32(-1));\n         }\n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(~self.data);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_or_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_or_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_or_si512(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -552,69 +552,69 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_si512(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castps_pd(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castps_si512(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castpd_si512(self);\n         }\n \n         // broadcast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -639,56 +639,56 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> broadcast(float val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_set1_ps(val);\n         }\n         template <class A>\n-        batch<double, A> inline broadcast(double val, requires_arch<avx512f>) noexcept\n+        batch<double, A> XSIMD_INLINE broadcast(double val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_set1_pd(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_ps(self, _MM_FROUND_TO_POS_INF);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_pd(self, _MM_FROUND_TO_POS_INF);\n         }\n \n         // compress\n         template <class A>\n-        inline batch<float, A> compress(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> compress(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_ps(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<double, A> compress(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> compress(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_pd(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int32_t, A> compress(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> compress(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint32_t, A> compress(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> compress(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int64_t, A> compress(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> compress(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi64(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint64_t, A> compress(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> compress(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi64(mask.mask(), self);\n         }\n@@ -697,19 +697,19 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvttps_epi32(self);\n             }\n \n             template <class A>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvtepu32_ps(self);\n             }\n@@ -725,27 +725,27 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi32(0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);\n                 return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi64(0, 8, 1, 9, 2, 10, 3, 11);\n                 return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n             }\n \n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi32(8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);\n                 return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi64(4, 12, 5, 13, 6, 14, 7, 15);\n                 return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n@@ -754,162 +754,162 @@ namespace xsimd\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_div_pd(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_EQ_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_EQ>(self, other);\n         }\n         template <class A, class T>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(~self.data ^ other.data);\n         }\n \n         // expand\n         template <class A>\n-        inline batch<float, A> expand(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> expand(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_ps(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<double, A> expand(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> expand(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_pd(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int32_t, A> expand(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> expand(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint32_t, A> expand(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> expand(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int64_t, A> expand(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> expand(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi64(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint64_t, A> expand(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> expand(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi64(mask.mask(), self);\n         }\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_ps(self, _MM_FROUND_TO_NEG_INF);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_pd(self, _MM_FROUND_TO_NEG_INF);\n         }\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fnmadd_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmsub_pd(x, y, z);\n         }\n \n         // from bool\n         template <class A, class T>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return select(self, batch<T, A>(1), batch<T, A>(0));\n         }\n \n         // from_mask\n         template <class T, class A>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx512f>) noexcept\n         {\n             return static_cast<typename batch_bool<T, A>::register_type>(mask);\n         }\n \n         // gather\n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 4> = 0, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i32gather_epi32(index, static_cast<const void*>(src), sizeof(T));\n         }\n \n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 8> = 0, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i64gather_epi64(index, static_cast<const void*>(src), sizeof(T));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, float const* src,\n-                                      batch<U, A> const& index,\n-                                      kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, float const* src,\n+                                            batch<U, A> const& index,\n+                                            kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i32gather_ps(index, src, sizeof(float));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         gather(batch<double, A> const&, double const* src, batch<U, A> const& index,\n                kernel::requires_arch<avx512f>) noexcept\n         {\n@@ -918,19 +918,19 @@ namespace xsimd\n \n         // gather: handmade conversions\n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, double const* src,\n-                                      batch<V, A> const& index,\n-                                      requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, double const* src,\n+                                            batch<V, A> const& index,\n+                                            requires_arch<avx512f>) noexcept\n         {\n             const batch<double, A> low(_mm512_i32gather_pd(_mm512_castsi512_si256(index.data), src, sizeof(double)));\n             const batch<double, A> high(_mm512_i32gather_pd(_mm256_castpd_si256(_mm512_extractf64x4_pd(_mm512_castsi512_pd(index.data), 1)), src, sizeof(double)));\n             return detail::merge_avx(_mm512_cvtpd_ps(low.data), _mm512_cvtpd_ps(high.data));\n         }\n \n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n-                                        batch<V, A> const& index,\n-                                        requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n+                                              batch<V, A> const& index,\n+                                              requires_arch<avx512f>) noexcept\n         {\n             const batch<double, A> low(_mm512_i32gather_pd(_mm512_castsi512_si256(index.data), src, sizeof(double)));\n             const batch<double, A> high(_mm512_i32gather_pd(_mm256_castpd_si256(_mm512_extractf64x4_pd(_mm512_castsi512_pd(index.data), 1)), src, sizeof(double)));\n@@ -939,41 +939,41 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_GE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_GE_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_GE>(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_GT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_GT_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_GT>(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) noexcept\n         {\n             // The following folds over the vector once:\n             // tmp1 = [a0..8, b0..8]\n@@ -1034,7 +1034,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx512f>) noexcept\n         {\n #define step1(I, a, b)                                                   \\\n     batch<double, avx512f> res##I;                                       \\\n@@ -1069,25 +1069,25 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, self, _CMP_UNORD_Q);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, self, _CMP_UNORD_Q);\n         }\n \n         // ldexp\n         template <class A>\n-        inline batch<float, A> ldexp(const batch<float, A>& self, const batch<as_integer_t<float>, A>& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> ldexp(const batch<float, A>& self, const batch<as_integer_t<float>, A>& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_scalef_ps(self, _mm512_cvtepi32_ps(other));\n         }\n \n         template <class A>\n-        inline batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512f>) noexcept\n         {\n             // FIXME: potential data loss here when converting other elements to\n             // int32 before converting them back to double.\n@@ -1097,34 +1097,34 @@ namespace xsimd\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_LE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_LE_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_LE>(self, other);\n         }\n \n         // load_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_si512((__m512i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_pd(mem);\n         }\n@@ -1133,7 +1133,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) noexcept\n             {\n                 __m512i real_idx = _mm512_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);\n                 __m512i imag_idx = _mm512_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);\n@@ -1142,7 +1142,7 @@ namespace xsimd\n                 return { real, imag };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx512f>) noexcept\n             {\n                 __m512i real_idx = _mm512_setr_epi64(0, 2, 4, 6, 8, 10, 12, 14);\n                 __m512i imag_idx = _mm512_setr_epi64(1, 3, 5, 7, 9, 11, 13, 15);\n@@ -1154,59 +1154,59 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_si512((__m512i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_pd(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_LT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_LT_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_LT>(self, other);\n         }\n \n         // mask\n         template <class A, class T>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return self.data;\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_max_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_max_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1246,17 +1246,17 @@ namespace xsimd\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_min_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_min_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1296,17 +1296,17 @@ namespace xsimd\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mul_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1322,66 +1322,66 @@ namespace xsimd\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return 0 - self;\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data ^ other.data);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         reciprocal(batch<float, A> const& self,\n                    kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rcp14_ps(self);\n         }\n \n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         reciprocal(batch<double, A> const& self,\n                    kernel::requires_arch<avx512f>) noexcept\n         {\n@@ -1390,7 +1390,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m128 tmp1 = _mm512_extractf32x4_ps(rhs, 0);\n             __m128 tmp2 = _mm512_extractf32x4_ps(rhs, 1);\n@@ -1402,15 +1402,15 @@ namespace xsimd\n             return reduce_add(batch<float, sse4_2>(res3), sse4_2 {});\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m256d tmp1 = _mm512_extractf64x4_pd(rhs, 1);\n             __m256d tmp2 = _mm512_extractf64x4_pd(rhs, 0);\n             __m256d res1 = _mm256_add_pd(tmp1, tmp2);\n             return reduce_add(batch<double, avx2>(res1), avx2 {});\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m256i low, high;\n             detail::split_avx512(self, low, high);\n@@ -1420,7 +1420,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) == 1), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             constexpr batch_constant<uint64_t, A, 5, 6, 7, 8, 0, 0, 0, 0> mask;\n             batch<T, A> step = _mm512_permutexvar_epi64(mask.as_batch(), self);\n@@ -1431,7 +1431,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) == 1), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             constexpr batch_constant<uint64_t, A, 5, 6, 7, 8, 0, 0, 0, 0> mask;\n             batch<T, A> step = _mm512_permutexvar_epi64(mask.as_batch(), self);\n@@ -1442,19 +1442,19 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rsqrt14_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rsqrt14_pd(val);\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1474,52 +1474,52 @@ namespace xsimd\n         // scatter\n         template <class A, class T,\n                   class = typename std::enable_if<std::is_same<uint32_t, T>::value || std::is_same<int32_t, T>::value, void>::type>\n-        inline void scatter(batch<T, A> const& src, T* dst,\n-                            batch<int32_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst,\n+                                  batch<int32_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i32scatter_epi32(dst, index, src, sizeof(T));\n         }\n \n         template <class A, class T,\n                   class = typename std::enable_if<std::is_same<uint64_t, T>::value || std::is_same<int64_t, T>::value, void>::type>\n-        inline void scatter(batch<T, A> const& src, T* dst,\n-                            batch<int64_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst,\n+                                  batch<int64_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i64scatter_epi64(dst, index, src, sizeof(T));\n         }\n \n         template <class A>\n-        inline void scatter(batch<float, A> const& src, float* dst,\n-                            batch<int32_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<float, A> const& src, float* dst,\n+                                  batch<int32_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i32scatter_ps(dst, index, src, sizeof(float));\n         }\n \n         template <class A>\n-        inline void scatter(batch<double, A> const& src, double* dst,\n-                            batch<int64_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<double, A> const& src, double* dst,\n+                                  batch<int64_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i64scatter_pd(dst, index, src, sizeof(double));\n         }\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mask_blend_ps(cond, false_br, true_br);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mask_blend_pd(cond, false_br, true_br);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1571,7 +1571,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, avx512f {});\n         }\n@@ -1589,32 +1589,32 @@ namespace xsimd\n \n         // set\n         template <class A>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) noexcept\n         {\n             return _mm512_setr_ps(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) noexcept\n         {\n             return _mm512_setr_pd(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm512_set_epi64(v7, v6, v5, v4, v3, v2, v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm512_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n         template <class A, class T, detail::enable_signed_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n #if defined(__clang__) || __GNUC__\n             return __extension__(__m512i)(__v32hi) {\n@@ -1628,10 +1628,10 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n #if defined(__clang__) || __GNUC__\n             return __extension__(__m512i)(__v32hu) {\n@@ -1645,14 +1645,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_signed_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n-                               T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n-                               T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n-                               T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n-                               T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n+                                     T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n+                                     T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n+                                     T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n+                                     T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n         {\n \n #if defined(__clang__) || __GNUC__\n@@ -1670,14 +1670,14 @@ namespace xsimd\n #endif\n         }\n         template <class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n-                               T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n-                               T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n-                               T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n-                               T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n+                                     T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n+                                     T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n+                                     T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n+                                     T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n         {\n \n #if defined(__clang__) || __GNUC__\n@@ -1696,7 +1696,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class... Values>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<T, A>::size, \"consistent init\");\n             using register_type = typename batch_bool<T, A>::register_type;\n@@ -1708,9 +1708,9 @@ namespace xsimd\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7, ITy I8, ITy I9, ITy I10, ITy I11, ITy I12, ITy I13, ITy I14, ITy I15>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y,\n-                                       batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7, I8, I9, I10, I11, I12, I13, I14, I15> mask,\n-                                       requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y,\n+                                             batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7, I8, I9, I10, I11, I12, I13, I14, I15> mask,\n+                                             requires_arch<avx512f>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x3) | ((I1 & 0x3) << 2) | ((I2 & 0x3) << 4) | ((I3 & 0x3) << 6);\n \n@@ -1726,7 +1726,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx512f>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x1) | ((I1 & 0x1) << 1) | ((I2 & 0x1) << 2) | ((I3 & 0x1) << 3) | ((I4 & 0x1) << 4) | ((I5 & 0x1) << 5) | ((I6 & 0x1) << 6) | ((I7 & 0x1) << 7);\n             // shuffle within lane\n@@ -1742,35 +1742,35 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             static_assert(N == 0xDEAD, \"not implemented yet\");\n             return {};\n         }\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             static_assert(N == 0xDEAD, \"not implemented yet\");\n             return {};\n         }\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sqrt_pd(val);\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1785,7 +1785,7 @@ namespace xsimd\n \n         // store\n         template <class T, class A>\n-        inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             constexpr auto size = batch_bool<T, A>::size;\n@@ -1795,51 +1795,51 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_si512((__m512i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_si512((__m512i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_si512((__m512i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_si512((__m512i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1868,86 +1868,86 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sub_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sub_pd(self, other);\n         }\n \n         // swizzle (dynamic version)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_ps(mask, self);\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_pd(mask, self);\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_epi64(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, avx512f {}));\n         }\n \n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_epi32(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx512f {}));\n         }\n \n         // swizzle (constant version)\n         template <class A, uint32_t... Vs>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint32_t... Vs>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint32_t... Vs>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n@@ -1980,14 +1980,14 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t... Idx, class _ = typename std::enable_if<detail::is_pair_of_contiguous_indices<uint16_t, A, Idx...>::value, void>::type>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Idx...>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Idx...>, requires_arch<avx512f>) noexcept\n         {\n             constexpr typename detail::fold_batch_constant<A, Idx...>::type mask32;\n             return _mm512_permutexvar_epi32(static_cast<batch<uint32_t, A>>(mask32), self);\n         }\n \n         template <class A>\n-        inline batch<uint16_t, A>\n+        XSIMD_INLINE batch<uint16_t, A>\n         swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, (uint16_t)1, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1>, requires_arch<avx512f>) noexcept\n         {\n             // FIXME: this sequence is very inefficient, but it's here to catch\n@@ -2004,29 +2004,29 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t... Vs>\n-        inline batch<int16_t, A>\n+        XSIMD_INLINE batch<int16_t, A>\n         swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, avx512f {}));\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         trunc(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         trunc(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             __m512i lo, hi;\n@@ -2064,7 +2064,7 @@ namespace xsimd\n                 1);\n         }\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_unpacklo_ps(self, other);\n@@ -2078,7 +2078,7 @@ namespace xsimd\n                 1);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_castpd_ps(_mm512_unpacklo_pd(self, other));\n@@ -2094,7 +2094,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             __m512i lo, hi;\n@@ -2132,7 +2132,7 @@ namespace xsimd\n                 2);\n         }\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_unpacklo_ps(self, other);\n@@ -2146,7 +2146,7 @@ namespace xsimd\n                 2);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_castpd_ps(_mm512_unpacklo_pd(self, other));\n--- include/xsimd/arch/xsimd_constants.hpp\n@@ -24,34 +24,34 @@ namespace xsimd\n \n #define XSIMD_DEFINE_CONSTANT(NAME, SINGLE, DOUBLE) \\\n     template <class T>                              \\\n-    inline T NAME() noexcept                        \\\n+    XSIMD_INLINE T NAME() noexcept                  \\\n     {                                               \\\n         return T(NAME<typename T::value_type>());   \\\n     }                                               \\\n     template <>                                     \\\n-    inline float NAME<float>() noexcept             \\\n+    XSIMD_INLINE float NAME<float>() noexcept       \\\n     {                                               \\\n         return SINGLE;                              \\\n     }                                               \\\n     template <>                                     \\\n-    inline double NAME<double>() noexcept           \\\n+    XSIMD_INLINE double NAME<double>() noexcept     \\\n     {                                               \\\n         return DOUBLE;                              \\\n     }\n \n #define XSIMD_DEFINE_CONSTANT_HEX(NAME, SINGLE, DOUBLE) \\\n     template <class T>                                  \\\n-    inline T NAME() noexcept                            \\\n+    XSIMD_INLINE T NAME() noexcept                      \\\n     {                                                   \\\n         return T(NAME<typename T::value_type>());       \\\n     }                                                   \\\n     template <>                                         \\\n-    inline float NAME<float>() noexcept                 \\\n+    XSIMD_INLINE float NAME<float>() noexcept           \\\n     {                                                   \\\n         return bit_cast<float>((uint32_t)SINGLE);       \\\n     }                                                   \\\n     template <>                                         \\\n-    inline double NAME<double>() noexcept               \\\n+    XSIMD_INLINE double NAME<double>() noexcept         \\\n     {                                                   \\\n         return bit_cast<double>((uint64_t)DOUBLE);      \\\n     }\n@@ -168,7 +168,7 @@ namespace xsimd\n         }\n \n         template <class T>\n-        inline constexpr T allbits() noexcept\n+        XSIMD_INLINE constexpr T allbits() noexcept\n         {\n             return T(detail::allbits_impl<typename T::value_type>::get_value());\n         }\n@@ -178,19 +178,19 @@ namespace xsimd\n          *****************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> mask1frexp() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> mask1frexp() noexcept\n         {\n             return as_integer_t<T>(mask1frexp<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t mask1frexp<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t mask1frexp<float>() noexcept\n         {\n             return 0x7f800000;\n         }\n \n         template <>\n-        inline constexpr int64_t mask1frexp<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t mask1frexp<double>() noexcept\n         {\n             return 0x7ff0000000000000;\n         }\n@@ -200,19 +200,19 @@ namespace xsimd\n          *****************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> mask2frexp() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> mask2frexp() noexcept\n         {\n             return as_integer_t<T>(mask2frexp<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t mask2frexp<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t mask2frexp<float>() noexcept\n         {\n             return 0x3f000000;\n         }\n \n         template <>\n-        inline constexpr int64_t mask2frexp<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t mask2frexp<double>() noexcept\n         {\n             return 0x3fe0000000000000;\n         }\n@@ -222,19 +222,19 @@ namespace xsimd\n          ******************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> maxexponent() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> maxexponent() noexcept\n         {\n             return as_integer_t<T>(maxexponent<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t maxexponent<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t maxexponent<float>() noexcept\n         {\n             return 127;\n         }\n \n         template <>\n-        inline constexpr int64_t maxexponent<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t maxexponent<double>() noexcept\n         {\n             return 1023;\n         }\n@@ -244,19 +244,19 @@ namespace xsimd\n          ******************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> maxexponentm1() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> maxexponentm1() noexcept\n         {\n             return as_integer_t<T>(maxexponentm1<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t maxexponentm1<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t maxexponentm1<float>() noexcept\n         {\n             return 126;\n         }\n \n         template <>\n-        inline constexpr int64_t maxexponentm1<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t maxexponentm1<double>() noexcept\n         {\n             return 1022;\n         }\n@@ -266,19 +266,19 @@ namespace xsimd\n          **********************/\n \n         template <class T>\n-        inline constexpr int32_t nmb() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb() noexcept\n         {\n             return nmb<typename T::value_type>();\n         }\n \n         template <>\n-        inline constexpr int32_t nmb<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb<float>() noexcept\n         {\n             return 23;\n         }\n \n         template <>\n-        inline constexpr int32_t nmb<double>() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb<double>() noexcept\n         {\n             return 52;\n         }\n@@ -288,7 +288,7 @@ namespace xsimd\n          ***********************/\n \n         template <class T>\n-        inline constexpr T zero() noexcept\n+        XSIMD_INLINE constexpr T zero() noexcept\n         {\n             return T(typename T::value_type(0));\n         }\n@@ -353,7 +353,7 @@ namespace xsimd\n             template <>\n             struct minvalue_impl<float>\n             {\n-                inline static float get_value() noexcept\n+                XSIMD_INLINE static float get_value() noexcept\n                 {\n                     return bit_cast<float>((uint32_t)0xff7fffff);\n                 }\n@@ -362,7 +362,7 @@ namespace xsimd\n             template <>\n             struct minvalue_impl<double>\n             {\n-                inline static double get_value() noexcept\n+                XSIMD_INLINE static double get_value() noexcept\n                 {\n                     return bit_cast<double>((uint64_t)0xffefffffffffffff);\n                 }\n--- include/xsimd/arch/xsimd_emulated.hpp\n@@ -28,7 +28,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -39,9 +39,9 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n@@ -66,7 +66,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::abs(v); },\n@@ -75,7 +75,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::add(v0, v1); },\n@@ -84,38 +84,38 @@ namespace xsimd\n \n         // all\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::all_of(self.data.begin(), self.data.end(), [](T v)\n                                { return bool(v); });\n         }\n \n         // any\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::any_of(self.data.begin(), self.data.end(), [](T v)\n                                { return bool(v); });\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, size_t N = 8 * sizeof(T_in) * batch<T_in, A>::size>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n         {\n             return { self.data };\n         }\n \n         // bitwise_and\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_and(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_and(v0, v1); },\n@@ -124,15 +124,15 @@ namespace xsimd\n \n         // bitwise_andnot\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_andnot(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_andnot(v0, v1); },\n@@ -141,7 +141,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([other](T v)\n                                           { return xsimd::bitwise_lshift(v, other); },\n@@ -150,15 +150,15 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::bitwise_not(v); },\n                                           self);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v)\n                                           { return xsimd::bitwise_not(v); },\n@@ -167,15 +167,15 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_or(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_or(v0, v1); },\n@@ -184,7 +184,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([other](T v)\n                                           { return xsimd::bitwise_rshift(v, other); },\n@@ -193,15 +193,15 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_xor(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_xor(v0, v1); },\n@@ -210,7 +210,7 @@ namespace xsimd\n \n         // bitwise_cast\n         template <class A, class T_in, class T_out, size_t N = 8 * sizeof(T_in) * batch<T_in, A>::size>\n-        inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T_out, A>::size;\n             std::array<T_out, size> result;\n@@ -222,7 +222,7 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        batch<T, A> inline broadcast(T val, requires_arch<emulated<N>>) noexcept\n+        batch<T, A> XSIMD_INLINE broadcast(T val, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> r;\n@@ -235,7 +235,7 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<T, A> complex_low(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_low(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> result;\n@@ -248,7 +248,7 @@ namespace xsimd\n             }\n             // complex_high\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<T, A> complex_high(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_high(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> result;\n@@ -263,14 +263,14 @@ namespace xsimd\n \n         // decr_if\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::div(v0, v1); },\n@@ -281,47 +281,47 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t N = 8 * sizeof(float) * batch<float, A>::size>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](int32_t v)\n                                               { return float(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(float) * batch<float, A>::size>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](uint32_t v)\n                                               { return float(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](int64_t v)\n                                               { return double(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](uint64_t v)\n                                               { return double(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(int32_t) * batch<int32_t, A>::size>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](float v)\n                                               { return int32_t(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](double v)\n                                               { return int64_t(v); },\n@@ -331,15 +331,15 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> eq(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> eq(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::eq(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch_bool<T, A>::size>\n-        inline batch_bool<T, emulated<N>> eq(batch_bool<T, emulated<N>> const& self, batch_bool<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> eq(batch_bool<T, emulated<N>> const& self, batch_bool<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::eq(v0, v1); },\n@@ -348,7 +348,7 @@ namespace xsimd\n \n         // from_bool\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v)\n                                           { return T(v); },\n@@ -357,7 +357,7 @@ namespace xsimd\n \n         // from_mask\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<bool, size> vmask;\n@@ -368,7 +368,7 @@ namespace xsimd\n \n         // ge\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> ge(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> ge(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::ge(v0, v1); },\n@@ -377,7 +377,7 @@ namespace xsimd\n \n         // gt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> gt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> gt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::gt(v0, v1); },\n@@ -386,7 +386,7 @@ namespace xsimd\n \n         // haddp\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> haddp(batch<T, A> const* row, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(batch<T, A> const* row, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> r;\n@@ -397,14 +397,14 @@ namespace xsimd\n \n         // incr_if\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<emulated<N>>) noexcept\n         {\n             batch<T, A> other = self;\n             other.data[I] = val;\n@@ -413,7 +413,7 @@ namespace xsimd\n \n         // isnan\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::isnan(v); },\n@@ -422,7 +422,7 @@ namespace xsimd\n \n         // load_aligned\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> res;\n@@ -432,7 +432,7 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> res;\n@@ -444,7 +444,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& hi, batch<T, A> const& lo, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& hi, batch<T, A> const& lo, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> real, imag;\n@@ -464,7 +464,7 @@ namespace xsimd\n \n         // le\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> le(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> le(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::le(v0, v1); },\n@@ -473,7 +473,7 @@ namespace xsimd\n \n         // lt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> lt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> lt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::lt(v0, v1); },\n@@ -482,7 +482,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             uint64_t res = 0;\n@@ -493,7 +493,7 @@ namespace xsimd\n \n         // max\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::max(v0, v1); },\n@@ -502,7 +502,7 @@ namespace xsimd\n \n         // min\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::min(v0, v1); },\n@@ -511,7 +511,7 @@ namespace xsimd\n \n         // mul\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::mul(v0, v1); },\n@@ -520,8 +520,8 @@ namespace xsimd\n \n         // nearbyint_as_int\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<as_integer_t<T>, A> nearbyint_as_int(batch<T, A> const& self,\n-                                                          requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<as_integer_t<T>, A> nearbyint_as_int(batch<T, A> const& self,\n+                                                                requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::nearbyint_as_int(v); },\n@@ -530,7 +530,7 @@ namespace xsimd\n \n         // neg\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::neg(v); },\n@@ -539,15 +539,15 @@ namespace xsimd\n \n         // neq\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::neq(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::neq(v0, v1); },\n@@ -556,7 +556,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> buffer;\n@@ -566,23 +566,23 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::accumulate(self.data.begin() + 1, self.data.end(), *self.data.begin(), [](T const& x, T const& y)\n                                    { return xsimd::max(x, y); });\n         }\n \n         // reduce_min\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::accumulate(self.data.begin() + 1, self.data.end(), *self.data.begin(), [](T const& x, T const& y)\n                                    { return xsimd::min(x, y); });\n         }\n \n         // rsqrt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> rsqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::rsqrt(v); },\n@@ -591,15 +591,15 @@ namespace xsimd\n \n         // select\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool c, T t, T f)\n                                           { return xsimd::select(c, t, f); },\n                                           cond, true_br, false_br);\n         }\n \n         template <class A, class T, bool... Values>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             static_assert(sizeof...(Values) == size, \"consistent init\");\n@@ -608,7 +608,7 @@ namespace xsimd\n \n         // shuffle\n         template <class A, typename T, class ITy, ITy... Is>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             batch<ITy, A> bmask = mask;\n@@ -620,7 +620,7 @@ namespace xsimd\n \n         // sqrt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::sqrt(v); },\n@@ -629,7 +629,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t M, class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> result;\n@@ -641,7 +641,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t M, class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> result;\n@@ -653,7 +653,7 @@ namespace xsimd\n \n         // sadd\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::sadd(v0, v1); },\n@@ -662,22 +662,22 @@ namespace xsimd\n \n         // set\n         template <class A, class T, size_t N, class... Values>\n-        inline batch<T, emulated<N>> set(batch<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n+        XSIMD_INLINE batch<T, emulated<N>> set(batch<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<T, emulated<N>>::size, \"consistent init\");\n             return { typename batch<T, emulated<N>>::register_type { static_cast<T>(values)... } };\n         }\n \n         template <class A, class T, size_t N, class... Values>\n-        inline batch_bool<T, emulated<N>> set(batch_bool<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> set(batch_bool<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<T, emulated<N>>::size, \"consistent init\");\n             return { std::array<bool, sizeof...(Values)> { static_cast<bool>(values)... } };\n         }\n \n         // ssub\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::ssub(v0, v1); },\n@@ -686,21 +686,21 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, size_t N>\n-        inline void store_aligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             std::copy(self.data.begin(), self.data.end(), mem);\n         }\n \n         // store_unaligned\n         template <class A, class T, size_t N>\n-        inline void store_unaligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             std::copy(self.data.begin(), self.data.end(), mem);\n         }\n \n         // sub\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::sub(v0, v1); },\n@@ -710,7 +710,7 @@ namespace xsimd\n         // swizzle\n \n         template <class A, typename T, class ITy, ITy... Is>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             batch<ITy, A> bmask = mask;\n@@ -722,7 +722,7 @@ namespace xsimd\n \n         // zip_hi\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             // Note: irregular behavior for odd numbers.\n@@ -742,7 +742,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             // Note: irregular behavior for odd numbers.\n--- include/xsimd/arch/xsimd_fma3_avx.hpp\n@@ -23,52 +23,52 @@ namespace xsimd\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmadd_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmsub_pd(x, y, z);\n         }\n--- include/xsimd/arch/xsimd_fma3_sse.hpp\n@@ -22,52 +22,52 @@ namespace xsimd\n         using namespace types;\n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmadd_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmsub_pd(x, y, z);\n         }\n--- include/xsimd/arch/xsimd_fma4.hpp\n@@ -23,52 +23,52 @@ namespace xsimd\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmacc_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmacc_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_macc_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_macc_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_msub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_msub_pd(x, y, z);\n         }\n--- include/xsimd/arch/xsimd_generic_fwd.hpp\n@@ -22,21 +22,21 @@ namespace xsimd\n     {\n         // forward declaration\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept;\n \n     }\n }\n--- include/xsimd/arch/xsimd_neon64.hpp\n@@ -33,25 +33,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vminvq_u32(arg) == ~0U;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u8(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u16(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64 {});\n         }\n@@ -61,25 +61,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vmaxvq_u32(arg) != 0;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u8(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u16(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64 {});\n         }\n@@ -90,13 +90,13 @@ namespace xsimd\n \n         // Required to avoid ambiguous call\n         template <class A, class T>\n-        inline batch<T, A> broadcast(T val, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<neon64>) noexcept\n         {\n             return broadcast<A>(val, neon {});\n         }\n \n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<neon64>) noexcept\n         {\n             return vdupq_n_f64(val);\n         }\n@@ -106,13 +106,13 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1) noexcept\n         {\n             return float64x2_t { d0, d1 };\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1) noexcept\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<double>;\n@@ -125,7 +125,7 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        inline batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vandq_u64(arg, vreinterpretq_u64_f64(vdupq_n_f64(1.))));\n         }\n@@ -142,13 +142,13 @@ namespace xsimd\n #endif\n \n         template <class A>\n-        inline batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n         {\n             return xsimd_aligned_load(vld1q_f64, double*, src);\n         }\n \n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n         {\n             return vld1q_f64(src);\n         }\n@@ -159,13 +159,13 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n         {\n             vst1q_f64(dst, src);\n         }\n \n         template <class A>\n-        inline void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n         {\n             return store_aligned<A>(dst, src, A {});\n         }\n@@ -175,7 +175,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>) noexcept\n         {\n             using real_batch = batch<double, A>;\n             const double* buf = reinterpret_cast<const double*>(mem);\n@@ -186,7 +186,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>) noexcept\n         {\n             return load_complex_aligned<A>(mem, cvt, A {});\n         }\n@@ -196,7 +196,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        inline void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n         {\n             float64x2x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -206,7 +206,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n         {\n             store_complex_aligned(dst, src, A {});\n         }\n@@ -216,19 +216,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_u64_s64(vnegq_s64(vreinterpretq_s64_u64(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vnegq_s64(rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vnegq_f64(rhs);\n         }\n@@ -238,7 +238,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vaddq_f64(lhs, rhs);\n         }\n@@ -248,7 +248,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return add(lhs, rhs, neon64 {});\n         }\n@@ -258,7 +258,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vsubq_f64(lhs, rhs);\n         }\n@@ -268,7 +268,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return sub(lhs, rhs, neon64 {});\n         }\n@@ -278,7 +278,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vmulq_f64(lhs, rhs);\n         }\n@@ -289,19 +289,19 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcvtq_u64_f64(vcvtq_f64_u64(lhs) / vcvtq_f64_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcvtq_s64_f64(vcvtq_f64_s64(lhs) / vcvtq_f64_s64(rhs));\n         }\n #endif\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vdivq_f64(lhs, rhs);\n         }\n@@ -311,37 +311,37 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_f64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n@@ -352,25 +352,25 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_f64_s64(x);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_f64_u64(x);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& x, batch<int64_t, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& x, batch<int64_t, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_s64_f64(x);\n             }\n \n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<double, A> const& x, batch<uint64_t, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<double, A> const& x, batch<uint64_t, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_u64_f64(x);\n             }\n@@ -382,19 +382,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_f64(lhs, rhs);\n         }\n@@ -404,19 +404,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_f64(lhs, rhs);\n         }\n@@ -426,19 +426,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_f64(lhs, rhs);\n         }\n@@ -448,19 +448,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_f64(lhs, rhs);\n         }\n@@ -470,7 +470,7 @@ namespace xsimd\n          *******************/\n \n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch_bool<T_out, A>::register_type;\n             return register_type(self);\n@@ -481,14 +481,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vandq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vandq_u64(lhs, rhs);\n         }\n@@ -498,14 +498,14 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vorrq_u64(lhs, rhs);\n         }\n@@ -515,14 +515,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(veorq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return veorq_u64(lhs, rhs);\n         }\n@@ -532,7 +532,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return bitwise_xor(lhs, rhs, A {});\n         }\n@@ -542,13 +542,13 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u32(vmvnq_u32(vreinterpretq_u32_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return detail::bitwise_not_u64(rhs);\n         }\n@@ -558,14 +558,14 @@ namespace xsimd\n          ******************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vbicq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vbicq_u64(lhs, rhs);\n         }\n@@ -575,7 +575,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vminq_f64(lhs, rhs);\n         }\n@@ -585,7 +585,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vmaxq_f64(lhs, rhs);\n         }\n@@ -595,34 +595,34 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return rhs;\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vabsq_s64(rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vabsq_f64(rhs);\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<neon64>) noexcept\n         {\n             return vcvtnq_s32_f32(self);\n         }\n \n #if !defined(__GNUC__)\n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n-                                                  requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n+                                                        requires_arch<neon64>) noexcept\n         {\n             return vcvtnq_s64_f64(self);\n         }\n@@ -633,7 +633,7 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         reciprocal(const batch<double, A>& x,\n                    kernel::requires_arch<neon64>) noexcept\n         {\n@@ -645,7 +645,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vrsqrteq_f64(rhs);\n         }\n@@ -655,7 +655,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vsqrtq_f64(rhs);\n         }\n@@ -666,13 +666,13 @@ namespace xsimd\n \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n         {\n             return vfmaq_f64(z, x, y);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n         {\n             return vfmaq_f64(-z, x, y);\n         }\n@@ -683,7 +683,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>) noexcept\n         {\n             return vpaddq_f64(row[0], row[1]);\n         }\n@@ -693,7 +693,7 @@ namespace xsimd\n          **********/\n \n         template <class A, size_t I>\n-        inline batch<double, A> insert(batch<double, A> const& self, double val, index<I>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> insert(batch<double, A> const& self, double val, index<I>, requires_arch<neon64>) noexcept\n         {\n             return vsetq_lane_f64(val, self, I);\n         }\n@@ -705,60 +705,60 @@ namespace xsimd\n         // Wrap reducer intrinsics so we can pass them as function pointers\n         // - OP: intrinsics name prefix, e.g., vorrq\n \n-#define WRAP_REDUCER_INT_EXCLUDING_64(OP)               \\\n-    namespace wrap                                      \\\n-    {                                                   \\\n-        inline uint8_t OP##_u8(uint8x16_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_u8(a);                        \\\n-        }                                               \\\n-        inline int8_t OP##_s8(int8x16_t a) noexcept     \\\n-        {                                               \\\n-            return ::OP##_s8(a);                        \\\n-        }                                               \\\n-        inline uint16_t OP##_u16(uint16x8_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u16(a);                       \\\n-        }                                               \\\n-        inline int16_t OP##_s16(int16x8_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s16(a);                       \\\n-        }                                               \\\n-        inline uint32_t OP##_u32(uint32x4_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u32(a);                       \\\n-        }                                               \\\n-        inline int32_t OP##_s32(int32x4_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s32(a);                       \\\n-        }                                               \\\n+#define WRAP_REDUCER_INT_EXCLUDING_64(OP)                     \\\n+    namespace wrap                                            \\\n+    {                                                         \\\n+        XSIMD_INLINE uint8_t OP##_u8(uint8x16_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_u8(a);                              \\\n+        }                                                     \\\n+        XSIMD_INLINE int8_t OP##_s8(int8x16_t a) noexcept     \\\n+        {                                                     \\\n+            return ::OP##_s8(a);                              \\\n+        }                                                     \\\n+        XSIMD_INLINE uint16_t OP##_u16(uint16x8_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u16(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int16_t OP##_s16(int16x8_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s16(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE uint32_t OP##_u32(uint32x4_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u32(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int32_t OP##_s32(int32x4_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s32(a);                             \\\n+        }                                                     \\\n     }\n \n-#define WRAP_REDUCER_INT(OP)                            \\\n-    WRAP_REDUCER_INT_EXCLUDING_64(OP)                   \\\n-    namespace wrap                                      \\\n-    {                                                   \\\n-        inline uint64_t OP##_u64(uint64x2_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u64(a);                       \\\n-        }                                               \\\n-        inline int64_t OP##_s64(int64x2_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s64(a);                       \\\n-        }                                               \\\n+#define WRAP_REDUCER_INT(OP)                                  \\\n+    WRAP_REDUCER_INT_EXCLUDING_64(OP)                         \\\n+    namespace wrap                                            \\\n+    {                                                         \\\n+        XSIMD_INLINE uint64_t OP##_u64(uint64x2_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u64(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int64_t OP##_s64(int64x2_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s64(a);                             \\\n+        }                                                     \\\n     }\n \n-#define WRAP_REDUCER_FLOAT(OP)                         \\\n-    namespace wrap                                     \\\n-    {                                                  \\\n-        inline float OP##_f32(float32x4_t a) noexcept  \\\n-        {                                              \\\n-            return ::OP##_f32(a);                      \\\n-        }                                              \\\n-        inline double OP##_f64(float64x2_t a) noexcept \\\n-        {                                              \\\n-            return ::OP##_f64(a);                      \\\n-        }                                              \\\n+#define WRAP_REDUCER_FLOAT(OP)                               \\\n+    namespace wrap                                           \\\n+    {                                                        \\\n+        XSIMD_INLINE float OP##_f32(float32x4_t a) noexcept  \\\n+        {                                                    \\\n+            return ::OP##_f32(a);                            \\\n+        }                                                    \\\n+        XSIMD_INLINE double OP##_f64(float64x2_t a) noexcept \\\n+        {                                                    \\\n+            return ::OP##_f64(a);                            \\\n+        }                                                    \\\n     }\n \n         namespace detail\n@@ -852,7 +852,7 @@ namespace xsimd\n         WRAP_REDUCER_FLOAT(vaddvq)\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_add(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_add(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -872,19 +872,19 @@ namespace xsimd\n \n         namespace wrap\n         {\n-            inline uint64_t vmaxvq_u64(uint64x2_t a) noexcept\n+            XSIMD_INLINE uint64_t vmaxvq_u64(uint64x2_t a) noexcept\n             {\n                 return std::max(vdupd_laneq_u64(a, 0), vdupd_laneq_u64(a, 1));\n             }\n \n-            inline int64_t vmaxvq_s64(int64x2_t a) noexcept\n+            XSIMD_INLINE int64_t vmaxvq_s64(int64x2_t a) noexcept\n             {\n                 return std::max(vdupd_laneq_s64(a, 0), vdupd_laneq_s64(a, 1));\n             }\n         }\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_max(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_max(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -904,19 +904,19 @@ namespace xsimd\n \n         namespace wrap\n         {\n-            inline uint64_t vminvq_u64(uint64x2_t a) noexcept\n+            XSIMD_INLINE uint64_t vminvq_u64(uint64x2_t a) noexcept\n             {\n                 return std::min(vdupd_laneq_u64(a, 0), vdupd_laneq_u64(a, 1));\n             }\n \n-            inline int64_t vminvq_s64(int64x2_t a) noexcept\n+            XSIMD_INLINE int64_t vminvq_s64(int64x2_t a) noexcept\n             {\n                 return std::min(vdupd_laneq_s64(a, 0), vdupd_laneq_s64(a, 1));\n             }\n         }\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_min(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_min(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -936,78 +936,78 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>) noexcept\n         {\n             return vbslq_f64(cond, a, b);\n         }\n \n         template <class A, bool... b>\n-        inline batch<double, A> select(batch_bool_constant<double, A, b...> const&,\n-                                       batch<double, A> const& true_br,\n-                                       batch<double, A> const& false_br,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, b...> const&,\n+                                             batch<double, A> const& true_br,\n+                                             batch<double, A> const& false_br,\n+                                             requires_arch<neon64>) noexcept\n         {\n             return select(batch_bool<double, A> { b... }, true_br, false_br, neon64 {});\n         }\n         /**********\n          * zip_lo *\n          **********/\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_f32(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_f64(lhs, rhs);\n         }\n@@ -1017,61 +1017,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_f32(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_f64(lhs, rhs);\n         }\n@@ -1083,8 +1083,8 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t I, size_t... Is>\n-            inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n-                                                 ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n+                                                       ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (n == I)\n                 {\n@@ -1098,7 +1098,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>) noexcept\n         {\n             constexpr std::size_t size = batch<double, A>::size;\n             assert(n < size && \"index in bounds\");\n@@ -1110,25 +1110,25 @@ namespace xsimd\n          ******************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n         {\n             return bitwise_rshift<A>(lhs, n, neon {});\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vshlq_u64(lhs, vnegq_s64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n         {\n             return bitwise_rshift<A>(lhs, n, neon {});\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vshlq_s64(lhs, vnegq_s64(rhs));\n         }\n@@ -1137,17 +1137,17 @@ namespace xsimd\n          * bitwise_cast *\n          ****************/\n \n-#define WRAP_CAST(SUFFIX, TYPE)                                          \\\n-    namespace wrap                                                       \\\n-    {                                                                    \\\n-        inline float64x2_t vreinterpretq_f64_##SUFFIX(TYPE a) noexcept   \\\n-        {                                                                \\\n-            return ::vreinterpretq_f64_##SUFFIX(a);                      \\\n-        }                                                                \\\n-        inline TYPE vreinterpretq_##SUFFIX##_f64(float64x2_t a) noexcept \\\n-        {                                                                \\\n-            return ::vreinterpretq_##SUFFIX##_f64(a);                    \\\n-        }                                                                \\\n+#define WRAP_CAST(SUFFIX, TYPE)                                                \\\n+    namespace wrap                                                             \\\n+    {                                                                          \\\n+        XSIMD_INLINE float64x2_t vreinterpretq_f64_##SUFFIX(TYPE a) noexcept   \\\n+        {                                                                      \\\n+            return ::vreinterpretq_f64_##SUFFIX(a);                            \\\n+        }                                                                      \\\n+        XSIMD_INLINE TYPE vreinterpretq_##SUFFIX##_f64(float64x2_t a) noexcept \\\n+        {                                                                      \\\n+            return ::vreinterpretq_##SUFFIX##_f64(a);                          \\\n+        }                                                                      \\\n     }\n \n         WRAP_CAST(u8, uint8x16_t)\n@@ -1163,7 +1163,7 @@ namespace xsimd\n #undef WRAP_CAST\n \n         template <class A, class T>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n         {\n             using caster_type = detail::bitwise_caster_impl<float64x2_t,\n                                                             uint8x16_t, int8x16_t,\n@@ -1199,7 +1199,7 @@ namespace xsimd\n         }\n \n         template <class A, class R>\n-        inline batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>) noexcept\n         {\n             using caster_type = detail::bitwise_caster_neon64<float64x2_t,\n                                                               uint8x16_t, int8x16_t,\n@@ -1218,7 +1218,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n         {\n             return arg;\n         }\n@@ -1228,7 +1228,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return !(arg == arg);\n         }\n@@ -1237,7 +1237,7 @@ namespace xsimd\n          * rotate_right *\n          ****************/\n         template <size_t N, class A>\n-        inline batch<double, A> rotate_right(batch<double, A> const& a, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> rotate_right(batch<double, A> const& a, requires_arch<neon64>) noexcept\n         {\n             return vextq_f64(a, a, N);\n         }\n@@ -1252,23 +1252,23 @@ namespace xsimd\n          * swizzle (dynamic) *\n          *********************/\n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_u8(self, idx);\n         }\n \n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_s8(self, idx);\n         }\n \n         template <class A>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n-                                          batch<uint16_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n+                                                batch<uint16_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1278,17 +1278,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n-                                         batch<uint16_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n+                                               batch<uint16_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n-                                          batch<uint32_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n+                                                batch<uint32_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1298,17 +1298,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n-                                         batch<uint32_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n+                                               batch<uint32_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n-                                          batch<uint64_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n+                                                batch<uint64_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1318,25 +1318,25 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n-                                         batch<uint64_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n+                                               batch<uint64_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self,\n-                                       batch<uint32_t, A> idx,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self,\n+                                             batch<uint32_t, A> idx,\n+                                             requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<float>(swizzle(bitwise_cast<uint32_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self,\n-                                        batch<uint64_t, A> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self,\n+                                              batch<uint64_t, A> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<double>(swizzle(bitwise_cast<uint64_t>(self), idx, neon64 {}));\n         }\n@@ -1388,114 +1388,114 @@ namespace xsimd\n             using index_burst_t = typename index_burst<B, T>::type;\n \n             template <typename T, class B>\n-            inline index_burst_t<B, T> burst_index(B)\n+            XSIMD_INLINE index_burst_t<B, T> burst_index(B)\n             {\n                 return index_burst_t<B, T>();\n             }\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self,\n-                                         batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self,\n+                                               batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_u8(self, batch<uint8_t, A>(idx));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self,\n-                                        batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self,\n+                                              batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_s8(self, batch<uint8_t, A>(idx));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n-                                          batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n+                                                batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u16_u8(swizzle<A>(batch_type(vreinterpretq_u8_u16(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n-                                         batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n+                                               batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s16_s8(swizzle<A>(batch_type(vreinterpretq_s8_s16(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n-                                          batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n+                                                batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u32_u8(swizzle<A>(batch_type(vreinterpretq_u8_u32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n-                                         batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n+                                               batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s32_s8(swizzle<A>(batch_type(vreinterpretq_s8_s32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n-                                          batch_constant<uint64_t, A, V0, V1> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n+                                                batch_constant<uint64_t, A, V0, V1> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u64_u8(swizzle<A>(batch_type(vreinterpretq_u8_u64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n-                                         batch_constant<uint64_t, A, V0, V1> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n+                                               batch_constant<uint64_t, A, V0, V1> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s64_s8(swizzle<A>(batch_type(vreinterpretq_s8_s64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self,\n-                                       batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self,\n+                                             batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                             requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_f32_u8(swizzle<A>(batch_type(vreinterpretq_u8_f32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self,\n-                                        batch_constant<uint64_t, A, V0, V1> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self,\n+                                              batch_constant<uint64_t, A, V0, V1> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_f64_u8(swizzle<A>(batch_type(vreinterpretq_u8_f64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<std::complex<float>, A> swizzle(batch<std::complex<float>, A> const& self,\n-                                                     batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                                     requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<float>, A> swizzle(batch<std::complex<float>, A> const& self,\n+                                                           batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                                           requires_arch<neon64>) noexcept\n         {\n             return batch<std::complex<float>>(swizzle(self.real(), idx, A()), swizzle(self.imag(), idx, A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<std::complex<double>, A> swizzle(batch<std::complex<double>, A> const& self,\n-                                                      batch_constant<uint64_t, A, V0, V1> idx,\n-                                                      requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> swizzle(batch<std::complex<double>, A> const& self,\n+                                                            batch_constant<uint64_t, A, V0, V1> idx,\n+                                                            requires_arch<neon64>) noexcept\n         {\n             return batch<std::complex<double>>(swizzle(self.real(), idx, A()), swizzle(self.imag(), idx, A()));\n         }\n--- include/xsimd/arch/xsimd_rvv.hpp\n@@ -384,7 +384,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, class U = as_unsigned_integer_t<T>>\n-            inline batch<U, A> rvv_to_unsigned_batch(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvv_to_unsigned_batch(batch<T, A> const& arg) noexcept\n             {\n                 return rvvreinterpret<U>(arg.data);\n             }\n@@ -413,18 +413,18 @@ namespace xsimd\n                                , size_t(bvec));\n \n             template <class T, size_t Width>\n-            inline rvv_bool_t<T, Width> pmask8(uint8_t mask) noexcept\n+            XSIMD_INLINE rvv_bool_t<T, Width> pmask8(uint8_t mask) noexcept\n             {\n                 return rvv_bool_t<T, Width>(mask);\n             }\n             template <class T, size_t Width>\n-            inline rvv_bool_t<T, Width> pmask(uint64_t mask) noexcept\n+            XSIMD_INLINE rvv_bool_t<T, Width> pmask(uint64_t mask) noexcept\n             {\n                 return rvv_bool_t<T, Width>(mask);\n             }\n \n             template <class A, class T, size_t offset = 0, int shift = 0>\n-            inline rvv_reg_t<T, A::width> vindex() noexcept\n+            XSIMD_INLINE rvv_reg_t<T, A::width> vindex() noexcept\n             {\n                 auto index = rvvid(T {});\n                 if (shift < 0)\n@@ -462,7 +462,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, size_t Width>\n-            inline detail::rvv_reg_t<T, Width> broadcast(T arg) noexcept\n+            XSIMD_INLINE detail::rvv_reg_t<T, Width> broadcast(T arg) noexcept\n             {\n                 // A bit of a dance, here, because rvvmv_splat has no other\n                 // argument from which to deduce type, and T=char is not\n@@ -475,7 +475,7 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T>\n-        inline batch<T, A> broadcast(T arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<rvv>) noexcept\n         {\n             return detail::broadcast<T, A::width>(arg);\n         }\n@@ -491,13 +491,13 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvle(reinterpret_cast<detail::rvv_fix_char_t<T> const*>(src));\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n         {\n             return load_aligned<A>(src, convert<T>(), rvv {});\n         }\n@@ -506,14 +506,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, size_t W, typename std::enable_if<W >= types::detail::rvv_width_m1, int>::type = 0>\n-            inline rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n+            XSIMD_INLINE rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n             {\n                 typename rvv_reg_t<T, W * 2>::register_type tmp;\n                 tmp = __riscv_vset(tmp, 0, lo);\n                 return __riscv_vset(tmp, 1, hi);\n             }\n \n-            template <class T, size_t W, typename std::enable_if<W<types::detail::rvv_width_m1, int>::type = 0> inline rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n+            template <class T, size_t W, typename std::enable_if<W<types::detail::rvv_width_m1, int>::type = 0> XSIMD_INLINE rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n             {\n                 return __riscv_vslideup(lo, hi, lo.vl, lo.vl * 2);\n             }\n@@ -544,7 +544,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<rvv>) noexcept\n             {\n                 const auto real_index = vindex<A, as_unsigned_integer_t<T>, 0, 1>();\n                 const auto imag_index = vindex<A, as_unsigned_integer_t<T>, 1, 1>();\n@@ -561,13 +561,13 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_aligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n         {\n             detail::rvvse(reinterpret_cast<detail::rvv_fix_char_t<T>*>(dst), src);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n         {\n             store_aligned<A>(dst, src, rvv {});\n         }\n@@ -590,7 +590,7 @@ namespace xsimd\n \n         // scatter\n         template <class A, class T, class U, detail::rvv_enable_sg_t<T, U> = 0>\n-        inline void scatter(batch<T, A> const& vals, T* dst, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& vals, T* dst, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n         {\n             using UU = as_unsigned_integer_t<U>;\n             const auto uindex = detail::rvv_to_unsigned_batch(index);\n@@ -602,7 +602,7 @@ namespace xsimd\n \n         // gather\n         template <class A, class T, class U, detail::rvv_enable_sg_t<T, U> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n         {\n             using UU = as_unsigned_integer_t<U>;\n             const auto uindex = detail::rvv_to_unsigned_batch(index);\n@@ -698,63 +698,63 @@ namespace xsimd\n \n         // add\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvadd(lhs, rhs);\n         }\n \n         // sadd\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsadd(lhs, rhs);\n         }\n \n         // sub\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsub(lhs, rhs);\n         }\n \n         // ssub\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvssub(lhs, rhs);\n         }\n \n         // mul\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmul(lhs, rhs);\n         }\n \n         // div\n         template <class A, class T, typename detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvdiv(lhs, rhs);\n         }\n \n         // max\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmax(lhs, rhs);\n         }\n \n         // min\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmin(lhs, rhs);\n         }\n \n         // neg\n         template <class A, class T, detail::rvv_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             using S = as_signed_integer_t<T>;\n             const auto as_signed = detail::rvvreinterpret<S>(arg);\n@@ -763,43 +763,43 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvneg(arg);\n         }\n \n         // abs\n         template <class A, class T, detail::rvv_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return arg;\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvabs(arg);\n         }\n \n         // fma: x * y + z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also detail::rvvmadd(x, y, z);\n             return detail::rvvmacc(z, x, y);\n         }\n \n         // fnma: z - x * y\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also detail::rvvnmsub(x, y, z);\n             return detail::rvvnmsac(z, x, y);\n         }\n \n         // fms: x * y - z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also vfmsac(z, x, y), but lacking integer version\n             // also vfmsub(x, y, z), but lacking integer version\n@@ -808,7 +808,7 @@ namespace xsimd\n \n         // fnms: - x * y - z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also vfnmacc(z, x, y), but lacking integer version\n             // also vfnmadd(x, y, z), but lacking integer version\n@@ -835,13 +835,13 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvand(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -850,21 +850,21 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmand(lhs, rhs);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto not_rhs = detail::rvvnot(rhs);\n             return detail::rvvand(lhs, not_rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -874,20 +874,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmandn(lhs, rhs);\n         }\n \n         // bitwise_or\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvor(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -896,20 +896,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmor(lhs, rhs);\n         }\n \n         // bitwise_xor\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvxor(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -918,28 +918,28 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmxor(lhs, rhs);\n         }\n \n         // bitwise_not\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvnot(arg);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto arg_bits = detail::rvv_to_unsigned_batch(arg);\n             const auto result_bits = detail::rvvnot(arg_bits);\n             return detail::rvvreinterpret<T>(result_bits);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmnot(arg);\n         }\n@@ -962,30 +962,30 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n         {\n             constexpr size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<size_t>(n) < size && \"index in bounds\");\n             return detail::rvvsll_splat(arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsll(lhs, detail::rvv_to_unsigned_batch<A, T>(rhs));\n         }\n \n         // bitwise_rshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n         {\n             constexpr size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<size_t>(n) < size && \"index in bounds\");\n             return detail::rvvsr_splat(arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsr(lhs, detail::rvv_to_unsigned_batch<A, T>(rhs));\n         }\n@@ -1019,14 +1019,14 @@ namespace xsimd\n                                 (__riscv_vfslide1down), , vec(vec, T))\n \n             template <class A, class T>\n-            inline T reduce_scalar(rvv_reg_t<T, types::detail::rvv_width_m1> const& arg)\n+            XSIMD_INLINE T reduce_scalar(rvv_reg_t<T, types::detail::rvv_width_m1> const& arg)\n             {\n                 return detail::rvvmv_lane0(rvv_reg_t<T, A::width>(arg.get_bytes(), types::detail::XSIMD_RVV_BITCAST));\n             }\n         }\n         // reduce_add\n         template <class A, class T, class V = typename batch<T, A>::value_type, detail::rvv_enable_all_t<T> = 0>\n-        inline V reduce_add(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE V reduce_add(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = detail::broadcast<T, types::detail::rvv_width_m1>(T(0));\n             const auto r = detail::rvvredsum(arg, zero);\n@@ -1035,7 +1035,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T reduce_max(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto lowest = detail::broadcast<T, types::detail::rvv_width_m1>(std::numeric_limits<T>::lowest());\n             const auto r = detail::rvvredmax(arg, lowest);\n@@ -1044,7 +1044,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T reduce_min(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto max = detail::broadcast<T, types::detail::rvv_width_m1>(std::numeric_limits<T>::max());\n             const auto r = detail::rvvredmin(arg, max);\n@@ -1053,7 +1053,7 @@ namespace xsimd\n \n         // haddp\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> haddp(const batch<T, A>* row, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(const batch<T, A>* row, requires_arch<rvv>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             T sums[size];\n@@ -1071,55 +1071,55 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmseq(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto neq_result = detail::rvvmxor(lhs, rhs);\n             return detail::rvvmnot(neq_result);\n         }\n \n         // neq\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsne(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmxor(lhs, rhs);\n         }\n \n         // lt\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmslt(lhs, rhs);\n         }\n \n         // le\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsle(lhs, rhs);\n         }\n \n         // gt\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsgt(lhs, rhs);\n         }\n \n         // ge\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsge(lhs, rhs);\n         }\n@@ -1133,7 +1133,7 @@ namespace xsimd\n         }\n         // compress\n         template <class A, class T>\n-        inline batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcompress(x, mask);\n         }\n@@ -1150,17 +1150,17 @@ namespace xsimd\n \n         // swizzle\n         template <class A, class T, class I, I... idx>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...>, requires_arch<rvv>) noexcept\n         {\n             static_assert(batch<T, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             const batch<I, A> indices { idx... };\n             return detail::rvvrgather(arg, indices);\n         }\n \n         template <class A, class T, class I, I... idx>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n-                                                 batch_constant<I, A, idx...>,\n-                                                 requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n+                                                       batch_constant<I, A, idx...>,\n+                                                       requires_arch<rvv>) noexcept\n         {\n             const auto real = swizzle(self.real(), batch_constant<I, A, idx...> {}, rvv {});\n             const auto imag = swizzle(self.imag(), batch_constant<I, A, idx...> {}, rvv {});\n@@ -1174,28 +1174,28 @@ namespace xsimd\n         // extract_pair\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, size_t n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, size_t n, requires_arch<rvv>) noexcept\n         {\n             const auto tmp = detail::rvvslidedown(rhs, n);\n             return detail::rvvslideup(tmp, lhs, lhs.size - n);\n         }\n \n         // select\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmerge(b, a, cond);\n         }\n \n         template <class A, class T, bool... b>\n-        inline batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<rvv>) noexcept\n         {\n             return select(batch_bool<T, A> { b... }, true_br, false_br, rvv {});\n         }\n \n         // zip_lo\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto index = detail::vindex<A, as_unsigned_integer_t<T>, 0, -1>();\n             const auto mask = detail::pmask8<T, A::width>(0xaa);\n@@ -1206,7 +1206,7 @@ namespace xsimd\n \n         // zip_hi\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto index = detail::vindex<A, as_unsigned_integer_t<T>, batch<T, A>::size / 2, -1>();\n             const auto mask = detail::pmask8<T, A::width>(0xaa);\n@@ -1217,7 +1217,7 @@ namespace xsimd\n \n         // store_complex\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n         {\n             const auto lo = zip_lo(src.real(), src.imag());\n             const auto hi = zip_hi(src.real(), src.imag());\n@@ -1227,7 +1227,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n         {\n             store_complex_aligned(dst, src, rvv {});\n         }\n@@ -1245,7 +1245,7 @@ namespace xsimd\n \n         // rsqrt\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             auto approx = detail::rvvfrsqrt7(arg);\n             approx = approx * (1.5 - (0.5 * arg * approx * approx));\n@@ -1254,14 +1254,14 @@ namespace xsimd\n \n         // sqrt\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvfsqrt(arg);\n         }\n \n         // reciprocal\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvfrec7(arg);\n         }\n@@ -1293,12 +1293,12 @@ namespace xsimd\n             using rvv_enable_itof_t = typename std::enable_if<(sizeof(T) == sizeof(U) && !std::is_floating_point<T>::value && std::is_floating_point<U>::value), int>::type;\n \n             template <class A, class T, class U, rvv_enable_ftoi_t<T, U> = 0>\n-            inline batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n             {\n                 return rvvfcvt_rtz(U {}, arg);\n             }\n             template <class A, class T, class U, rvv_enable_itof_t<T, U> = 0>\n-            inline batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n             {\n                 return rvvfcvt_f(arg);\n             }\n@@ -1310,22 +1310,22 @@ namespace xsimd\n \n         // set\n         template <class A, class T, class... Args>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n         {\n             const std::array<T, batch<T, A>::size> tmp { args... };\n             return load_unaligned<A>(tmp.data(), convert<T>(), rvv {});\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<rvv>,\n-                                             Args... args_complex) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<rvv>,\n+                                                   Args... args_complex) noexcept\n         {\n             return batch<std::complex<T>>(set(batch<T, rvv> {}, rvv {}, args_complex.real()...),\n                                           set(batch<T, rvv> {}, rvv {}, args_complex.imag()...));\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n         {\n             using U = as_unsigned_integer_t<T>;\n             const auto values = set(batch<U, rvv> {}, rvv {}, static_cast<U>(args)...);\n@@ -1336,22 +1336,22 @@ namespace xsimd\n \n         // insert\n         template <class A, class T, size_t I, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<rvv>) noexcept\n         {\n             const auto mask = detail::pmask<T, A::width>(uint64_t(1) << I);\n             return detail::rvvmerge_splat(arg, val, mask);\n         }\n \n         // get\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T get(batch<T, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n         {\n             const auto tmp = detail::rvvslidedown(arg, i);\n             return detail::rvvmv_lane0(tmp);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline std::complex<T> get(batch<std::complex<T>, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE std::complex<T> get(batch<std::complex<T>, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n         {\n             const auto tmpr = detail::rvvslidedown(arg.real(), i);\n             const auto tmpi = detail::rvvslidedown(arg.imag(), i);\n@@ -1360,36 +1360,36 @@ namespace xsimd\n \n         // all\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcpop(arg) == batch_bool<T, A>::size;\n         }\n \n         // any\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcpop(arg) > 0;\n         }\n \n         // bitwise_cast\n         template <class A, class T, class R, detail::rvv_enable_all_t<T> = 0, detail::rvv_enable_all_t<R> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<rvv>) noexcept\n         {\n             return detail::rvv_reg_t<R, A::width>(arg.data.get_bytes(), types::detail::XSIMD_RVV_BITCAST);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, detail::rvv_enable_all_t<T_in> = 0>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<rvv>) noexcept\n         {\n             using intermediate_t = typename detail::rvv_bool_t<T_out>;\n             return intermediate_t(arg.data);\n         }\n \n         // from_bool\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = broadcast<A>(T(0), rvv {});\n             return detail::rvvmerge_splat(zero, T(1), arg);\n@@ -1398,26 +1398,26 @@ namespace xsimd\n         namespace detail\n         {\n             template <size_t Width>\n-            inline vuint8m1_t rvvslidedownbytes(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes(vuint8m1_t arg, size_t i)\n             {\n                 return __riscv_vslidedown(arg, i, types::detail::rvv_width_m1 / 8);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf2>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf2>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf2(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf2 / 8);\n                 return __riscv_vlmul_ext_u8m1(result);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf4>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf4>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf4(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf4 / 8);\n                 return __riscv_vlmul_ext_u8m1(result);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf8>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf8>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf8(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf8 / 8);\n@@ -1427,7 +1427,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = broadcast<A>(uint8_t(0), rvv {});\n             const auto bytes = arg.data.get_bytes();\n@@ -1436,7 +1436,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             using reg_t = detail::rvv_reg_t<T, A::width>;\n             const auto bytes = arg.data.get_bytes();\n@@ -1445,7 +1445,7 @@ namespace xsimd\n \n         // isnan\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return !(arg == arg);\n         }\n@@ -1456,29 +1456,29 @@ namespace xsimd\n             using rvv_as_signed_integer_t = as_signed_integer_t<as_unsigned_integer_t<T>>;\n \n             template <class A, class T, class U = rvv_as_signed_integer_t<T>>\n-            inline batch<U, A> rvvfcvt_default(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvvfcvt_default(batch<T, A> const& arg) noexcept\n             {\n                 return rvvfcvt_rne(U {}, arg);\n             }\n \n             template <class A, class T, class U = rvv_as_signed_integer_t<T>>\n-            inline batch<U, A> rvvfcvt_afz(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvvfcvt_afz(batch<T, A> const& arg) noexcept\n             {\n                 return rvvfcvt_rmm(U {}, arg);\n             }\n         }\n \n         // nearbyint_as_int\n         template <class A, class T, class U = detail::rvv_as_signed_integer_t<T>>\n-        inline batch<U, A> nearbyint_as_int(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<U, A> nearbyint_as_int(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Reference rounds ties to nearest even\n             return detail::rvvfcvt_default(arg);\n         }\n \n         // round\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> round(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> round(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Round ties away from zero.\n             const auto mask = abs(arg) < constants::maxflint<batch<T, A>>();\n@@ -1487,7 +1487,7 @@ namespace xsimd\n \n         // nearbyint\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Round according to current rounding mode.\n             const auto mask = abs(arg) < constants::maxflint<batch<T, A>>();\n--- include/xsimd/arch/xsimd_scalar.hpp\n@@ -20,6 +20,8 @@\n #include <limits>\n #include <type_traits>\n \n+#include \"xsimd/config/xsimd_inline.hpp\"\n+\n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n #include \"xtl/xcomplex.hpp\"\n #endif\n@@ -86,7 +88,7 @@ namespace xsimd\n     using std::tgamma;\n     using std::trunc;\n \n-    inline signed char abs(signed char v)\n+    XSIMD_INLINE signed char abs(signed char v)\n     {\n         return v < 0 ? -v : v;\n     }\n@@ -96,43 +98,43 @@ namespace xsimd\n         // Use templated type here to prevent automatic instantiation that may\n         // ends up in a warning\n         template <typename char_type>\n-        inline char abs(char_type v, std::true_type)\n+        XSIMD_INLINE char abs(char_type v, std::true_type)\n         {\n             return v;\n         }\n         template <typename char_type>\n-        inline char abs(char_type v, std::false_type)\n+        XSIMD_INLINE char abs(char_type v, std::false_type)\n         {\n             return v < 0 ? -v : v;\n         }\n     }\n \n-    inline char abs(char v)\n+    XSIMD_INLINE char abs(char v)\n     {\n         return detail::abs(v, std::is_unsigned<char>::type {});\n     }\n \n-    inline short abs(short v)\n+    XSIMD_INLINE short abs(short v)\n     {\n         return v < 0 ? -v : v;\n     }\n-    inline unsigned char abs(unsigned char v)\n+    XSIMD_INLINE unsigned char abs(unsigned char v)\n     {\n         return v;\n     }\n-    inline unsigned short abs(unsigned short v)\n+    XSIMD_INLINE unsigned short abs(unsigned short v)\n     {\n         return v;\n     }\n-    inline unsigned int abs(unsigned int v)\n+    XSIMD_INLINE unsigned int abs(unsigned int v)\n     {\n         return v;\n     }\n-    inline unsigned long abs(unsigned long v)\n+    XSIMD_INLINE unsigned long abs(unsigned long v)\n     {\n         return v;\n     }\n-    inline unsigned long long abs(unsigned long long v)\n+    XSIMD_INLINE unsigned long long abs(unsigned long long v)\n     {\n         return v;\n     }\n@@ -145,56 +147,56 @@ namespace xsimd\n \n     // Windows defines catch all templates\n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isfinite(T var) noexcept\n     {\n         return std::isfinite(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isfinite(T var) noexcept\n     {\n         return isfinite(double(var));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isinf(T var) noexcept\n     {\n         return std::isinf(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isinf(T var) noexcept\n     {\n         return isinf(double(var));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isnan(T var) noexcept\n     {\n         return std::isnan(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isnan(T var) noexcept\n     {\n         return isnan(double(var));\n     }\n #endif\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type add(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type add(T const& x, Tp const& y) noexcept\n     {\n         return x + y;\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type avg(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type avg(T const& x, Tp const& y) noexcept\n     {\n         using common_type = typename std::common_type<T, Tp>::type;\n         if (std::is_floating_point<common_type>::value)\n@@ -215,7 +217,7 @@ namespace xsimd\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type avgr(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type avgr(T const& x, Tp const& y) noexcept\n     {\n         using common_type = typename std::common_type<T, Tp>::type;\n         if (std::is_floating_point<common_type>::value)\n@@ -227,49 +229,49 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline T incr(T const& x) noexcept\n+    XSIMD_INLINE T incr(T const& x) noexcept\n     {\n         return x + T(1);\n     }\n \n     template <class T>\n-    inline T incr_if(T const& x, bool mask) noexcept\n+    XSIMD_INLINE T incr_if(T const& x, bool mask) noexcept\n     {\n         return x + T(mask ? 1 : 0);\n     }\n \n-    inline bool all(bool mask)\n+    XSIMD_INLINE bool all(bool mask)\n     {\n         return mask;\n     }\n \n-    inline bool any(bool mask)\n+    XSIMD_INLINE bool any(bool mask)\n     {\n         return mask;\n     }\n \n-    inline bool none(bool mask)\n+    XSIMD_INLINE bool none(bool mask)\n     {\n         return !mask;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_and(T x, T y) noexcept\n     {\n         return x & y;\n     }\n \n     template <class T_out, class T_in>\n-    inline T_out bitwise_cast(T_in x) noexcept\n+    XSIMD_INLINE T_out bitwise_cast(T_in x) noexcept\n     {\n         static_assert(sizeof(T_in) == sizeof(T_out), \"bitwise_cast between types of the same size\");\n         T_out r;\n         std::memcpy((void*)&r, (void*)&x, sizeof(T_in));\n         return r;\n     }\n \n-    inline float bitwise_and(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_and(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -280,7 +282,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_and(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_and(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -292,32 +294,32 @@ namespace xsimd\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     bitwise_lshift(T0 x, T1 shift) noexcept\n     {\n         return x << shift;\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     bitwise_rshift(T0 x, T1 shift) noexcept\n     {\n         return x >> shift;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_not(T x) noexcept\n     {\n         return ~x;\n     }\n \n-    inline bool bitwise_not(bool x) noexcept\n+    XSIMD_INLINE bool bitwise_not(bool x) noexcept\n     {\n         return !x;\n     }\n \n-    inline float bitwise_not(float x) noexcept\n+    XSIMD_INLINE float bitwise_not(float x) noexcept\n     {\n         uint32_t ix;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -327,7 +329,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_not(double x) noexcept\n+    XSIMD_INLINE double bitwise_not(double x) noexcept\n     {\n         uint64_t ix;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -338,19 +340,19 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_scalar<T>::value, T>::type bitwise_andnot(T x, T y) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_scalar<T>::value, T>::type bitwise_andnot(T x, T y) noexcept\n     {\n         return bitwise_and(x, bitwise_not(y));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_or(T x, T y) noexcept\n     {\n         return x | y;\n     }\n \n-    inline float bitwise_or(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_or(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -361,7 +363,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_or(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_or(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -373,13 +375,13 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_xor(T x, T y) noexcept\n     {\n         return x ^ y;\n     }\n \n-    inline float bitwise_xor(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_xor(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -390,7 +392,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_xor(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_xor(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -402,75 +404,75 @@ namespace xsimd\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type div(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type div(T const& x, Tp const& y) noexcept\n     {\n         return x / y;\n     }\n \n     template <class T, class Tp>\n-    inline auto mod(T const& x, Tp const& y) noexcept -> decltype(x % y)\n+    XSIMD_INLINE auto mod(T const& x, Tp const& y) noexcept -> decltype(x % y)\n     {\n         return x % y;\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type mul(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type mul(T const& x, Tp const& y) noexcept\n     {\n         return x * y;\n     }\n \n     template <class T>\n-    inline T neg(T const& x) noexcept\n+    XSIMD_INLINE T neg(T const& x) noexcept\n     {\n         return -x;\n     }\n \n     template <class T>\n-    inline auto pos(T const& x) noexcept -> decltype(+x)\n+    XSIMD_INLINE auto pos(T const& x) noexcept -> decltype(+x)\n     {\n         return +x;\n     }\n \n-    inline float reciprocal(float const& x) noexcept\n+    XSIMD_INLINE float reciprocal(float const& x) noexcept\n     {\n         return 1.f / x;\n     }\n \n-    inline double reciprocal(double const& x) noexcept\n+    XSIMD_INLINE double reciprocal(double const& x) noexcept\n     {\n         return 1. / x;\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     rotl(T0 x, T1 shift) noexcept\n     {\n         constexpr auto N = std::numeric_limits<T0>::digits;\n         return (x << shift) | (x >> (N - shift));\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     rotr(T0 x, T1 shift) noexcept\n     {\n         constexpr auto N = std::numeric_limits<T0>::digits;\n         return (x >> shift) | (x << (N - shift));\n     }\n \n     template <class T>\n-    inline bool isnan(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isnan(std::complex<T> var) noexcept\n     {\n         return std::isnan(std::real(var)) || std::isnan(std::imag(var));\n     }\n \n     template <class T>\n-    inline bool isinf(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isinf(std::complex<T> var) noexcept\n     {\n         return std::isinf(std::real(var)) || std::isinf(std::imag(var));\n     }\n \n     template <class T>\n-    inline bool isfinite(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isfinite(std::complex<T> var) noexcept\n     {\n         return std::isfinite(std::real(var)) && std::isfinite(std::imag(var));\n     }\n@@ -499,138 +501,138 @@ namespace xsimd\n #endif\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T clip(const T& val, const T& low, const T& hi) noexcept\n+    XSIMD_INLINE T clip(const T& val, const T& low, const T& hi) noexcept\n     {\n         assert(low <= hi && \"ordered clipping bounds\");\n         return low > val ? low : (hi < val ? hi : val);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_flint(const T& x) noexcept\n+    XSIMD_INLINE bool is_flint(const T& x) noexcept\n     {\n         return std::isnan(x - x) ? false : (x - std::trunc(x)) == T(0);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_even(const T& x) noexcept\n+    XSIMD_INLINE bool is_even(const T& x) noexcept\n     {\n         return is_flint(x * T(0.5));\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_odd(const T& x) noexcept\n+    XSIMD_INLINE bool is_odd(const T& x) noexcept\n     {\n         return is_even(x - 1.);\n     }\n \n-    inline int32_t nearbyint_as_int(float var) noexcept\n+    XSIMD_INLINE int32_t nearbyint_as_int(float var) noexcept\n     {\n         return static_cast<int32_t>(std::nearbyint(var));\n     }\n \n-    inline int64_t nearbyint_as_int(double var) noexcept\n+    XSIMD_INLINE int64_t nearbyint_as_int(double var) noexcept\n     {\n         return static_cast<int64_t>(std::nearbyint(var));\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool eq(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool eq(const T& x0, const T& x1) noexcept\n     {\n         return x0 == x1;\n     }\n \n     template <class T>\n-    inline bool eq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n+    XSIMD_INLINE bool eq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n     {\n         return x0 == x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool ge(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool ge(const T& x0, const T& x1) noexcept\n     {\n         return x0 >= x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool gt(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool gt(const T& x0, const T& x1) noexcept\n     {\n         return x0 > x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool le(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool le(const T& x0, const T& x1) noexcept\n     {\n         return x0 <= x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool lt(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool lt(const T& x0, const T& x1) noexcept\n     {\n         return x0 < x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool neq(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool neq(const T& x0, const T& x1) noexcept\n     {\n         return x0 != x1;\n     }\n \n     template <class T>\n-    inline bool neq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n+    XSIMD_INLINE bool neq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n     {\n         return !(x0 == x1);\n     }\n \n #if defined(__APPLE__) && (MAC_OS_X_VERSION_MIN_REQUIRED > 1080)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return __exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return __exp10(x);\n     }\n #elif defined(__GLIBC__)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return ::exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return ::exp10(x);\n     }\n #elif !defined(__clang__) && defined(__GNUC__) && (__GNUC__ >= 5)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return __builtin_exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return __builtin_exp10(x);\n     }\n #elif defined(_WIN32)\n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T exp10(const T& x) noexcept\n+    XSIMD_INLINE T exp10(const T& x) noexcept\n     {\n         // Very inefficient but other implementations give incorrect results\n         // on Windows\n         return std::pow(T(10), x);\n     }\n #else\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         const float ln10 = std::log(10.f);\n         return std::exp(ln10 * x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         const double ln10 = std::log(10.);\n         return std::exp(ln10 * x);\n     }\n #endif\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline auto rsqrt(const T& x) noexcept -> decltype(std::sqrt(x))\n+    XSIMD_INLINE auto rsqrt(const T& x) noexcept -> decltype(std::sqrt(x))\n     {\n         using float_type = decltype(std::sqrt(x));\n         return static_cast<float_type>(1) / std::sqrt(x);\n@@ -639,7 +641,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C expm1_complex_scalar_impl(const C& val) noexcept\n+        XSIMD_INLINE C expm1_complex_scalar_impl(const C& val) noexcept\n         {\n             using T = typename C::value_type;\n             T isin = std::sin(val.imag());\n@@ -651,14 +653,14 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> expm1(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> expm1(const std::complex<T>& val) noexcept\n     {\n         return detail::expm1_complex_scalar_impl(val);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return detail::expm1_complex_scalar_impl(val);\n     }\n@@ -667,7 +669,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C log1p_complex_scalar_impl(const C& val) noexcept\n+        XSIMD_INLINE C log1p_complex_scalar_impl(const C& val) noexcept\n         {\n             using T = typename C::value_type;\n             C u = C(1.) + val;\n@@ -676,19 +678,19 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> log1p(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> log1p(const std::complex<T>& val) noexcept\n     {\n         return detail::log1p_complex_scalar_impl(val);\n     }\n \n     template <class T>\n-    inline std::complex<T> log2(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> log2(const std::complex<T>& val) noexcept\n     {\n         return log(val) / std::log(T(2));\n     }\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T sadd(const T& lhs, const T& rhs) noexcept\n+    XSIMD_INLINE T sadd(const T& lhs, const T& rhs) noexcept\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -719,7 +721,7 @@ namespace xsimd\n     }\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T ssub(const T& lhs, const T& rhs) noexcept\n+    XSIMD_INLINE T ssub(const T& lhs, const T& rhs) noexcept\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -755,7 +757,7 @@ namespace xsimd\n         using value_type_or_type = typename value_type_or_type_helper<T>::type;\n \n         template <class T0, class T1>\n-        inline typename std::enable_if<std::is_integral<T1>::value, T0>::type\n+        XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, T0>::type\n         ipow(const T0& x, const T1& n) noexcept\n         {\n             static_assert(std::is_integral<T1>::value, \"second argument must be an integer\");\n@@ -781,61 +783,61 @@ namespace xsimd\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, T0>::type\n     pow(const T0& x, const T1& n) noexcept\n     {\n         return detail::ipow(x, n);\n     }\n \n     template <class T0, class T1>\n-    inline auto\n+    XSIMD_INLINE auto\n     pow(const T0& t0, const T1& t1) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_floating_point<T1>::value, decltype(std::pow(t0, t1))>::type\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type\n     pow(const std::complex<T0>& t0, const T1& t1) noexcept\n     {\n         return detail::ipow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type\n+    XSIMD_INLINE typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type\n     pow(const std::complex<T0>& t0, const T1& t1) noexcept\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline auto\n+    XSIMD_INLINE auto\n     pow(const T0& t0, const std::complex<T1>& t1) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value, decltype(std::pow(t0, t1))>::type\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T bitofsign(T const& x) noexcept\n+    XSIMD_INLINE T bitofsign(T const& x) noexcept\n     {\n         return T(x < T(0));\n     }\n \n     template <class T>\n-    inline auto signbit(T const& v) noexcept -> decltype(bitofsign(v))\n+    XSIMD_INLINE auto signbit(T const& v) noexcept -> decltype(bitofsign(v))\n     {\n         return bitofsign(v);\n     }\n \n-    inline double sign(bool const& v) noexcept\n+    XSIMD_INLINE double sign(bool const& v) noexcept\n     {\n         return v;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T sign(const T& v) noexcept\n+    XSIMD_INLINE T sign(const T& v) noexcept\n     {\n         return v < T(0) ? T(-1.) : v == T(0) ? T(0.)\n                                              : T(1.);\n@@ -844,7 +846,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C sign_complex_scalar_impl(const C& v) noexcept\n+        XSIMD_INLINE C sign_complex_scalar_impl(const C& v) noexcept\n         {\n             using value_type = typename C::value_type;\n             if (v.real())\n@@ -859,66 +861,66 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> sign(const std::complex<T>& v) noexcept\n+    XSIMD_INLINE std::complex<T> sign(const std::complex<T>& v) noexcept\n     {\n         return detail::sign_complex_scalar_impl(v);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v) noexcept\n     {\n         return detail::sign_complex_scalar_impl(v);\n     }\n #endif\n \n-    inline double signnz(bool const&) noexcept\n+    XSIMD_INLINE double signnz(bool const&) noexcept\n     {\n         return 1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T signnz(const T& v) noexcept\n+    XSIMD_INLINE T signnz(const T& v) noexcept\n     {\n         return v < T(0) ? T(-1.) : T(1.);\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type sub(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type sub(T const& x, Tp const& y) noexcept\n     {\n         return x - y;\n     }\n \n     template <class T>\n-    inline T decr(T const& x) noexcept\n+    XSIMD_INLINE T decr(T const& x) noexcept\n     {\n         return x - T(1);\n     }\n \n     template <class T>\n-    inline T decr_if(T const& x, bool mask) noexcept\n+    XSIMD_INLINE T decr_if(T const& x, bool mask) noexcept\n     {\n         return x - T(mask ? 1 : 0);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return log(val) / log(T(2));\n     }\n #endif\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return detail::log1p_complex_scalar_impl(val);\n     }\n #endif\n \n     template <class T0, class T1>\n-    inline auto min(T0 const& self, T1 const& other) noexcept\n+    XSIMD_INLINE auto min(T0 const& self, T1 const& other) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,\n                                    typename std::decay<decltype(self > other ? other : self)>::type>::type\n     {\n@@ -927,14 +929,14 @@ namespace xsimd\n \n     // numpy defines minimum operator on complex using lexical comparison\n     template <class T0, class T1>\n-    inline std::complex<typename std::common_type<T0, T1>::type>\n+    XSIMD_INLINE std::complex<typename std::common_type<T0, T1>::type>\n     min(std::complex<T0> const& self, std::complex<T1> const& other) noexcept\n     {\n         return (self.real() < other.real()) ? (self) : (self.real() == other.real() ? (self.imag() < other.imag() ? self : other) : other);\n     }\n \n     template <class T0, class T1>\n-    inline auto max(T0 const& self, T1 const& other) noexcept\n+    XSIMD_INLINE auto max(T0 const& self, T1 const& other) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,\n                                    typename std::decay<decltype(self > other ? other : self)>::type>::type\n     {\n@@ -943,49 +945,49 @@ namespace xsimd\n \n     // numpy defines maximum operator on complex using lexical comparison\n     template <class T0, class T1>\n-    inline std::complex<typename std::common_type<T0, T1>::type>\n+    XSIMD_INLINE std::complex<typename std::common_type<T0, T1>::type>\n     max(std::complex<T0> const& self, std::complex<T1> const& other) noexcept\n     {\n         return (self.real() > other.real()) ? (self) : (self.real() == other.real() ? (self.imag() > other.imag() ? self : other) : other);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n     {\n         return a * b + c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n     {\n         return std::fma(a, b, c);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_scalar<T>::value, T>::type fms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_scalar<T>::value, T>::type fms(const T& a, const T& b, const T& c) noexcept\n     {\n         return a * b - c;\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.real(), b.real(), fms(a.imag(), b.imag(), c.real())),\n                      fma(a.real(), b.imag(), fma(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fma_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fma_complex_scalar_impl(a, b, c);\n     }\n@@ -994,109 +996,109 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C fms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.real(), b.real(), fma(a.imag(), b.imag(), c.real())),\n                      fma(a.real(), b.imag(), fms(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fms_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fms_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n     {\n         return -(a * b) + c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n     {\n         return std::fma(-a, b, c);\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fnma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fnma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.imag(), b.imag(), fms(a.real(), b.real(), c.real())),\n                      -fma(a.real(), b.imag(), fms(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fnma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fnma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fnma_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fnma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fnma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fnma_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n     {\n         return -(a * b) - c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n     {\n         return -std::fma(a, b, c);\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fnms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fnms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.imag(), b.imag(), fma(a.real(), b.real(), c.real())),\n                      -fma(a.real(), b.imag(), fma(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fnms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fnms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fnms_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fnms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fnms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fnms_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     namespace detail\n     {\n-#define XSIMD_HASSINCOS_TRAIT(func)                                                                                                     \\\n-    template <class S>                                                                                                                  \\\n-    struct has##func                                                                                                                    \\\n-    {                                                                                                                                   \\\n-        template <class T>                                                                                                              \\\n-        static inline auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type {}); \\\n-        static inline std::false_type get(...);                                                                                         \\\n-        static constexpr bool value = decltype(get((S*)nullptr))::value;                                                                \\\n+#define XSIMD_HASSINCOS_TRAIT(func)                                                                                                           \\\n+    template <class S>                                                                                                                        \\\n+    struct has##func                                                                                                                          \\\n+    {                                                                                                                                         \\\n+        template <class T>                                                                                                                    \\\n+        static XSIMD_INLINE auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type {}); \\\n+        static XSIMD_INLINE std::false_type get(...);                                                                                         \\\n+        static constexpr bool value = decltype(get((S*)nullptr))::value;                                                                      \\\n     }\n \n #define XSIMD_HASSINCOS(func, T) has##func<T>::value\n@@ -1109,21 +1111,21 @@ namespace xsimd\n         struct generic_sincosf\n         {\n             template <class T>\n-            inline typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 sincosf(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 __sincosf(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 s = std::sin(val);\n@@ -1134,21 +1136,21 @@ namespace xsimd\n         struct generic_sincos\n         {\n             template <class T>\n-            inline typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 sincos(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 __sincos(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 s = std::sin(val);\n@@ -1160,43 +1162,43 @@ namespace xsimd\n #undef XSIMD_HASSINCOS\n     }\n \n-    inline std::pair<float, float> sincos(float val) noexcept\n+    XSIMD_INLINE std::pair<float, float> sincos(float val) noexcept\n     {\n         float s, c;\n         detail::generic_sincosf {}(val, s, c);\n         return std::make_pair(s, c);\n     }\n \n-    inline std::pair<double, double> sincos(double val) noexcept\n+    XSIMD_INLINE std::pair<double, double> sincos(double val) noexcept\n     {\n         double s, c;\n         detail::generic_sincos {}(val, s, c);\n         return std::make_pair(s, c);\n     }\n \n     template <class T>\n-    inline std::pair<std::complex<T>, std::complex<T>>\n+    XSIMD_INLINE std::pair<std::complex<T>, std::complex<T>>\n     sincos(const std::complex<T>& val) noexcept\n     {\n         return std::make_pair(std::sin(val), std::cos(val));\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T>\n-    inline std::pair<xtl::xcomplex<T>, xtl::xcomplex<T>> sincos(const xtl::xcomplex<T>& val) noexcept\n+    XSIMD_INLINE std::pair<xtl::xcomplex<T>, xtl::xcomplex<T>> sincos(const xtl::xcomplex<T>& val) noexcept\n     {\n         return std::make_pair(sin(val), cos(val));\n     }\n #endif\n \n     template <class T, class _ = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-    inline T frexp(T const& val, int& exp) noexcept\n+    XSIMD_INLINE T frexp(T const& val, int& exp) noexcept\n     {\n         return std::frexp(val, &exp);\n     }\n \n     template <class T>\n-    inline T select(bool cond, T const& true_br, T const& false_br) noexcept\n+    XSIMD_INLINE T select(bool cond, T const& true_br, T const& false_br) noexcept\n     {\n         return cond ? true_br : false_br;\n     }\n--- include/xsimd/arch/xsimd_sse2.hpp\n@@ -24,7 +24,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -57,31 +57,31 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avgr(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n \n         // abs\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128d sign_mask = _mm_set1_pd(-0.f); // -0.f = 1 << 31\n             return _mm_andnot_pd(sign_mask, self);\n         }\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128 sign_mask = _mm_set1_ps(-0.f); // -0.f = 1 << 31\n             return _mm_andnot_ps(sign_mask, self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -107,54 +107,54 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_ps(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_pd(self, other);\n         }\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self) == 0x0F;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self) == 0x03;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_epi8(self) == 0xFFFF;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self) != 0;\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_epi8(self) != 0;\n         }\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -172,7 +172,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -192,83 +192,83 @@ namespace xsimd\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<sse2>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_si128(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_si128(self, other);\n         }\n \n         template <class A>\n-        batch<double, A> inline bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        batch<double, A> XSIMD_INLINE bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_pd(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_ps(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_ps(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_si128(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_si128(other, self);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_pd(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_pd(other, self);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -295,73 +295,73 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, _mm_set1_epi32(-1));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, _mm_set1_epi32(-1));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -423,81 +423,81 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_ps(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<sse2>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_si128(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_pd(self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_pd(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castpd_si128(self);\n         }\n \n         // broadcast\n         template <class A>\n-        batch<float, A> inline broadcast(float val, requires_arch<sse2>) noexcept\n+        batch<float, A> XSIMD_INLINE broadcast(float val, requires_arch<sse2>) noexcept\n         {\n             return _mm_set1_ps(val);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -522,7 +522,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<sse2>) noexcept\n         {\n             return _mm_set1_pd(val);\n         }\n@@ -533,43 +533,43 @@ namespace xsimd\n             // Override these methods in SSE-based archs, no need to override store_aligned / store_unaligned\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpacklo_ps(self.real(), self.imag());\n             }\n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpackhi_ps(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpacklo_pd(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpackhi_pd(self.real(), self.imag());\n             }\n         }\n \n         // decr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_div_pd(self, other);\n         }\n@@ -578,13 +578,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n             {\n                 return _mm_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to sse2\n@@ -597,7 +597,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to sse2\n@@ -611,25 +611,25 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) noexcept\n             {\n                 return _mm_cvttps_epi32(self);\n             }\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpeq_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_castps_si128(self), _mm_castps_si128(other)));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -658,24 +658,24 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpeq_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_pd(_mm_cmpeq_epi32(_mm_castpd_si128(self), _mm_castpd_si128(other)));\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut[][4] = {\n                 { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },\n@@ -699,7 +699,7 @@ namespace xsimd\n             return _mm_castsi128_ps(_mm_load_si128((const __m128i*)lut[mask]));\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -711,7 +711,7 @@ namespace xsimd\n             return _mm_castsi128_pd(_mm_load_si128((const __m128i*)lut[mask]));\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[] = {\n                 0x0000000000000000,\n@@ -771,24 +771,24 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpge_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpge_pd(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpgt_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -816,14 +816,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpgt_pd(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) noexcept\n         {\n             __m128 tmp0 = _mm_unpacklo_ps(row[0], row[1]);\n             __m128 tmp1 = _mm_unpackhi_ps(row[0], row[1]);\n@@ -836,22 +836,22 @@ namespace xsimd\n             return _mm_add_ps(tmp0, tmp2);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_pd(_mm_unpacklo_pd(row[0], row[1]),\n                               _mm_unpackhi_pd(row[0], row[1]));\n         }\n \n         // incr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -865,46 +865,46 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpunord_ps(self, self);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpunord_pd(self, self);\n         }\n \n         // load_aligned\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_ps(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_si128((__m128i const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_pd(mem);\n         }\n \n         // load_unaligned\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_ps(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_si128((__m128i const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_pd(mem);\n         }\n@@ -914,37 +914,37 @@ namespace xsimd\n         {\n             // Redefine these methods in the SSE-based archs if required\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) noexcept\n             {\n                 return { _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(2, 0, 2, 0)), _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(3, 1, 3, 1)) };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) noexcept\n             {\n                 return { _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1)) };\n             }\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmple_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmple_pd(self, other);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmplt_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1011,7 +1011,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmplt_pd(self, other);\n         }\n@@ -1021,7 +1021,7 @@ namespace xsimd\n          */\n         namespace detail\n         {\n-            inline int mask_lut(int mask)\n+            XSIMD_INLINE int mask_lut(int mask)\n             {\n                 // clang-format off\n                 static const int mask_lut[256] = {\n@@ -1049,7 +1049,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1075,149 +1075,149 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_max_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_max_pd(self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_min_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_min_pd(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mul_pd(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<int16_t, A> mul(batch<int16_t, A> const& self, batch<int16_t, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> mul(batch<int16_t, A> const& self, batch<int16_t, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mullo_epi16(self, other);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return 0 - self;\n         }\n         template <class A>\n-        inline batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(0x80000000)));\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(\n                 self, _mm_castsi128_pd(_mm_setr_epi32(0, 0x80000000, 0, 0x80000000)));\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpneq_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return ~(self == other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_si128(_mm_xor_ps(_mm_castsi128_ps(self.data), _mm_castsi128_ps(other.data)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpneq_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self,\n-                                          kernel::requires_arch<sse2>)\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self,\n+                                                kernel::requires_arch<sse2>)\n         {\n             return _mm_rcp_ps(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128 tmp0 = _mm_add_ps(self, _mm_movehl_ps(self, self));\n             __m128 tmp1 = _mm_add_ss(tmp0, _mm_shuffle_ps(tmp0, tmp0, 1));\n             return _mm_cvtss_f32(tmp1);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1248,14 +1248,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtsd_f64(_mm_add_sd(self, _mm_unpackhi_pd(self, self)));\n         }\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             constexpr auto mask0 = detail::shuffle(2, 3, 0, 0);\n             batch<T, A> step0 = _mm_shuffle_epi32(self, mask0);\n@@ -1277,7 +1277,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             constexpr auto mask0 = detail::shuffle(2, 3, 0, 0);\n             batch<T, A> step0 = _mm_shuffle_epi32(self, mask0);\n@@ -1299,42 +1299,42 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_rsqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtps_pd(_mm_rsqrt_ps(_mm_cvtpd_ps(val)));\n         }\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(_mm_and_ps(cond, true_br), _mm_andnot_ps(cond, false_br));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(_mm_and_si128(cond, true_br), _mm_andnot_si128(cond, false_br));\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, sse2 {});\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(_mm_and_pd(cond, true_br), _mm_andnot_pd(cond, false_br));\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1, I2, I3);\n             // shuffle within lane\n@@ -1348,7 +1348,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1> mask, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1);\n             // shuffle within lane\n@@ -1363,34 +1363,34 @@ namespace xsimd\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_sqrt_pd(val);\n         }\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<sse2>) noexcept\n         {\n             return _mm_slli_si128(x, N);\n         }\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<sse2>) noexcept\n         {\n             return _mm_srli_si128(x, N);\n         }\n \n         // sadd\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1426,55 +1426,55 @@ namespace xsimd\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return _mm_setr_ps(values...);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) noexcept\n         {\n             return _mm_set_epi64x(v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return _mm_setr_epi32(v0, v1, v2, v3);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return _mm_setr_pd(values...);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return _mm_castsi128_ps(set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return _mm_castsi128_pd(set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data);\n@@ -1483,7 +1483,7 @@ namespace xsimd\n         // ssub\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1519,56 +1519,56 @@ namespace xsimd\n \n         // store_aligned\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_ps(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_si128((__m128i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_si128((__m128i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_ps(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_si128((__m128i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_si128((__m128i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_sub_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1593,61 +1593,61 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_sub_pd(self, other);\n         }\n \n         // swizzle\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1, V2, V3);\n             return _mm_shuffle_ps(self, self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1);\n             return _mm_shuffle_pd(self, self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(2 * V0, 2 * V0 + 1, 2 * V1, 2 * V1 + 1);\n             return _mm_shuffle_epi32(self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<sse2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, sse2 {}));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1, V2, V3);\n             return _mm_shuffle_epi32(self, index);\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<sse2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, sse2 {}));\n         }\n \n         // zip_hi\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpackhi_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1672,19 +1672,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpackhi_pd(self, other);\n         }\n \n         // zip_lo\n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpacklo_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1709,7 +1709,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpacklo_pd(self, other);\n         }\n--- include/xsimd/arch/xsimd_sse3.hpp\n@@ -24,34 +24,34 @@ namespace xsimd\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) noexcept\n         {\n             return _mm_hadd_ps(_mm_hadd_ps(row[0], row[1]),\n                                _mm_hadd_ps(row[2], row[3]));\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse3>) noexcept\n         {\n             return _mm_hadd_pd(row[0], row[1]);\n         }\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) noexcept\n         {\n             return _mm_lddqu_si128((__m128i const*)mem);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<sse3>) noexcept\n         {\n             __m128 tmp0 = _mm_hadd_ps(self, self);\n             __m128 tmp1 = _mm_hadd_ps(tmp0, tmp0);\n             return _mm_cvtss_f32(tmp1);\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<sse3>) noexcept\n         {\n             __m128d tmp0 = _mm_hadd_pd(self, self);\n             return _mm_cvtsd_f64(tmp0);\n--- include/xsimd/arch/xsimd_sse4_1.hpp\n@@ -24,18 +24,18 @@ namespace xsimd\n         using namespace types;\n         // any\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch<T, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE bool any(batch<T, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return !_mm_testz_si128(self, self);\n         }\n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_ceil_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_ceil_pd(self);\n         }\n@@ -44,7 +44,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 __m128i xH = _mm_srai_epi32(x, 16);\n@@ -56,7 +56,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 __m128i xH = _mm_srli_epi64(x, 32);\n@@ -69,7 +69,7 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 8)\n             {\n@@ -83,19 +83,19 @@ namespace xsimd\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_floor_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_floor_pd(self);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -124,7 +124,7 @@ namespace xsimd\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -168,7 +168,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -212,7 +212,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -247,12 +247,12 @@ namespace xsimd\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n@@ -261,30 +261,30 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T>\n-            inline constexpr T interleave(T const& cond) noexcept\n+            XSIMD_INLINE constexpr T interleave(T const& cond) noexcept\n             {\n                 return (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 49) & 0x5555) | (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 48) & 0xAAAA);\n             }\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_epi8(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_ps(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_pd(false_br, true_br, cond);\n         }\n \n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<T, A, Values...>::mask();\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n@@ -308,26 +308,26 @@ namespace xsimd\n             }\n         }\n         template <class A, bool... Values>\n-        inline batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<float, A, Values...>::mask();\n             return _mm_blend_ps(false_br, true_br, mask);\n         }\n         template <class A, bool... Values>\n-        inline batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<double, A, Values...>::mask();\n             return _mm_blend_pd(false_br, true_br, mask);\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_ps(self, _MM_FROUND_TO_ZERO);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_pd(self, _MM_FROUND_TO_ZERO);\n         }\n--- include/xsimd/arch/xsimd_sse4_2.hpp\n@@ -25,12 +25,12 @@ namespace xsimd\n \n         // lt\n         template <class A>\n-        inline batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) noexcept\n+        XSIMD_INLINE batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) noexcept\n         {\n             return _mm_cmpgt_epi64(other, self);\n         }\n         template <class A>\n-        inline batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) noexcept\n+        XSIMD_INLINE batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) noexcept\n         {\n             auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n             auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n--- include/xsimd/arch/xsimd_ssse3.hpp\n@@ -27,7 +27,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -57,13 +57,13 @@ namespace xsimd\n         {\n \n             template <class T, class A>\n-            inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n+            XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n             {\n                 return other;\n             }\n \n             template <class T, class A, std::size_t I, std::size_t... Is>\n-            inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (i == I)\n                 {\n@@ -75,7 +75,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class _ = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(0 <= i && i < size && \"index in bounds\");\n@@ -84,7 +84,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -107,30 +107,30 @@ namespace xsimd\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             return _mm_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), ssse3 {}));\n         }\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n         {\n             return _mm_shuffle_epi8(self, mask);\n         }\n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n         {\n             return _mm_shuffle_epi8(self, mask);\n         }\n \n         template <class A, class T, class IT>\n-        inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+        XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n         swizzle(batch<T, A> const& self, batch<IT, A> mask, requires_arch<ssse3>) noexcept\n         {\n             constexpr auto pikes = static_cast<as_unsigned_integer_t<T>>(0x0706050403020100ul);\n@@ -140,7 +140,7 @@ namespace xsimd\n \n         // swizzle (constant mask)\n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<ssse3>) noexcept\n         {\n             constexpr batch_constant<uint8_t, A, 2 * V0, 2 * V0 + 1, 2 * V1, 2 * V1 + 1, 2 * V2, 2 * V2 + 1, 2 * V3, 2 * V3 + 1,\n                                      2 * V4, 2 * V4 + 1, 2 * V5, 2 * V5 + 1, 2 * V6, 2 * V6 + 1, 2 * V7, 2 * V7 + 1>\n@@ -149,21 +149,21 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<ssse3>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, ssse3 {}));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), ssse3 {});\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), ssse3 {});\n         }\n--- include/xsimd/arch/xsimd_sve.hpp\n@@ -31,22 +31,22 @@ namespace xsimd\n             using xsimd::types::detail::sve_vector_type;\n \n             // predicate creation\n-            inline svbool_t sve_ptrue_impl(index<1>) noexcept { return svptrue_b8(); }\n-            inline svbool_t sve_ptrue_impl(index<2>) noexcept { return svptrue_b16(); }\n-            inline svbool_t sve_ptrue_impl(index<4>) noexcept { return svptrue_b32(); }\n-            inline svbool_t sve_ptrue_impl(index<8>) noexcept { return svptrue_b64(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<1>) noexcept { return svptrue_b8(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<2>) noexcept { return svptrue_b16(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<4>) noexcept { return svptrue_b32(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<8>) noexcept { return svptrue_b64(); }\n \n             template <class T>\n             svbool_t sve_ptrue() noexcept { return sve_ptrue_impl(index<sizeof(T)> {}); }\n \n             // count active lanes in a predicate\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<1>) noexcept { return svcntp_b8(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<2>) noexcept { return svcntp_b16(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<4>) noexcept { return svcntp_b32(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<8>) noexcept { return svcntp_b64(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<1>) noexcept { return svcntp_b8(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<2>) noexcept { return svcntp_b16(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<4>) noexcept { return svcntp_b32(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<8>) noexcept { return svcntp_b64(p, p); }\n \n             template <class T>\n-            inline uint64_t sve_pcount(svbool_t p) noexcept { return sve_pcount_impl(p, index<sizeof(T)> {}); }\n+            XSIMD_INLINE uint64_t sve_pcount(svbool_t p) noexcept { return sve_pcount_impl(p, index<sizeof(T)> {}); }\n \n             // enable for signed integers\n             template <class T>\n@@ -84,20 +84,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n         {\n             return svld1(detail::sve_ptrue<T>(), reinterpret_cast<detail::sve_fix_char_t<T> const*>(src));\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n         {\n             return load_aligned<A>(src, convert<T>(), sve {});\n         }\n \n         // load_complex\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<std::complex<T>, A> load_complex_aligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> load_complex_aligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n         {\n             const T* buf = reinterpret_cast<const T*>(mem);\n             const auto tmp = svld2(detail::sve_ptrue<T>(), buf);\n@@ -107,7 +107,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<std::complex<T>, A> load_complex_unaligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> load_complex_unaligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n         {\n             return load_complex_aligned<A>(mem, convert<std::complex<T>> {}, sve {});\n         }\n@@ -117,20 +117,20 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_aligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n         {\n             svst1(detail::sve_ptrue<T>(), reinterpret_cast<detail::sve_fix_char_t<T>*>(dst), src);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n         {\n             store_aligned<A>(dst, src, sve {});\n         }\n \n         // store_complex\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n         {\n             using v2type = typename std::conditional<(sizeof(T) == 4), svfloat32x2_t, svfloat64x2_t>::type;\n             v2type tmp {};\n@@ -141,7 +141,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n         {\n             store_complex_aligned(dst, src, sve {});\n         }\n@@ -158,14 +158,14 @@ namespace xsimd\n \n         // scatter\n         template <class A, class T, class U, detail::sve_enable_sg_t<T, U> = 0>\n-        inline void scatter(batch<T, A> const& src, T* dst, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n         {\n             svst1_scatter_index(detail::sve_ptrue<T>(), dst, index.data, src.data);\n         }\n \n         // gather\n         template <class A, class T, class U, detail::sve_enable_sg_t<T, U> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n         {\n             return svld1_gather_index(detail::sve_ptrue<T>(), src, index.data);\n         }\n@@ -176,67 +176,67 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u8(uint8_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s8(int8_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u16(uint16_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s16(int16_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u32(uint32_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s32(int32_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u64(uint64_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s64(int64_t(arg));\n         }\n \n         template <class A>\n-        inline batch<float, A> broadcast(float arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_f32(arg);\n         }\n \n         template <class A>\n-        inline batch<double, A> broadcast(double arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_f64(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> broadcast(T val, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<sve>) noexcept\n         {\n             return broadcast<sve>(val, sve {});\n         }\n@@ -247,128 +247,128 @@ namespace xsimd\n \n         // add\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svadd_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // sadd\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svqadd(lhs, rhs);\n         }\n \n         // sub\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svsub_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // ssub\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svqsub(lhs, rhs);\n         }\n \n         // mul\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmul_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // div\n         template <class A, class T, typename std::enable_if<sizeof(T) >= 4, int>::type = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svdiv_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // max\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmax_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // min\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmin_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // neg\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u8(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s8(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u16(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s16(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u32(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s32(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u64(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s64(arg)));\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svneg_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // abs\n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return arg;\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svabs_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // fma: x * y + z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return svmad_x(detail::sve_ptrue<T>(), x, y, z);\n         }\n \n         // fnma: z - x * y\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return svmsb_x(detail::sve_ptrue<T>(), x, y, z);\n         }\n \n         // fms: x * y - z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return -fnma(x, y, z, sve {});\n         }\n \n         // fnms: - x * y - z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return -fma(x, y, z, sve {});\n         }\n@@ -379,13 +379,13 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svand_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -394,7 +394,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -403,20 +403,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svand_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svbic_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -425,7 +425,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -434,20 +434,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svbic_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_or\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svorr_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -456,7 +456,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -465,20 +465,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svorr_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_xor\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -487,7 +487,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -496,36 +496,36 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_not\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svnot_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto arg_bits = svreinterpret_u32(arg);\n             const auto result_bits = svnot_x(detail::sve_ptrue<float>(), arg_bits);\n             return svreinterpret_f32(result_bits);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto arg_bits = svreinterpret_u64(arg);\n             const auto result_bits = svnot_x(detail::sve_ptrue<double>(), arg_bits);\n             return svreinterpret_f64(result_bits);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svnot_z(detail::sve_ptrue<T>(), arg);\n         }\n@@ -537,76 +537,76 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<1>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<1>) noexcept\n             {\n                 return svreinterpret_u8(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<2>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<2>) noexcept\n             {\n                 return svreinterpret_u16(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<4>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<4>) noexcept\n             {\n                 return svreinterpret_u32(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<8>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<8>) noexcept\n             {\n                 return svreinterpret_u64(arg);\n             }\n \n             template <class A, class T, class U = as_unsigned_integer_t<T>>\n-            inline batch<U, A> sve_to_unsigned_batch(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch(batch<T, A> const& arg) noexcept\n             {\n                 return sve_to_unsigned_batch_impl<A, T, U>(arg, index<sizeof(T)> {});\n             }\n         } // namespace detail\n \n         // bitwise_lshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svlsl_x(detail::sve_ptrue<T>(), arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svlsl_x(detail::sve_ptrue<T>(), lhs, detail::sve_to_unsigned_batch<A, T>(rhs));\n         }\n \n         // bitwise_rshift\n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svlsr_x(detail::sve_ptrue<T>(), arg, static_cast<T>(n));\n         }\n \n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svlsr_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svasr_x(detail::sve_ptrue<T>(), arg, static_cast<as_unsigned_integer_t<T>>(n));\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svasr_x(detail::sve_ptrue<T>(), lhs, detail::sve_to_unsigned_batch<A, T>(rhs));\n         }\n@@ -617,29 +617,29 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class V = typename batch<T, A>::value_type, detail::sve_enable_all_t<T> = 0>\n-        inline V reduce_add(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE V reduce_add(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             // sve integer reduction results are promoted to 64 bits\n             return static_cast<V>(svaddv(detail::sve_ptrue<T>(), arg));\n         }\n \n         // reduce_max\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline T reduce_max(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svmaxv(detail::sve_ptrue<T>(), arg);\n         }\n \n         // reduce_min\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline T reduce_min(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svminv(detail::sve_ptrue<T>(), arg);\n         }\n \n         // haddp\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> haddp(const batch<T, A>* row, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(const batch<T, A>* row, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             T sums[size];\n@@ -656,55 +656,55 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpeq(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto neq_result = sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n             return svnot_z(detail::sve_ptrue<T>(), neq_result);\n         }\n \n         // neq\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpne(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // lt\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmplt(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // le\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmple(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // gt\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpgt(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // ge\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpge(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n@@ -715,22 +715,22 @@ namespace xsimd\n \n         //  rotate_right\n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> rotate_right(batch<T, A> const& a, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& a, requires_arch<sve>) noexcept\n         {\n             return svext(a, a, N);\n         }\n \n         // swizzle (dynamic)\n         template <class A, class T, class I>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch<I, A> indices, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch<I, A> indices, requires_arch<sve>) noexcept\n         {\n             return svtbl(arg, indices);\n         }\n \n         template <class A, class T, class I>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n-                                                 batch<I, A> indices,\n-                                                 requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n+                                                       batch<I, A> indices,\n+                                                       requires_arch<sve>) noexcept\n         {\n             const auto real = swizzle(self.real(), indices, sve {});\n             const auto imag = swizzle(self.imag(), indices, sve {});\n@@ -739,16 +739,16 @@ namespace xsimd\n \n         // swizzle (static)\n         template <class A, class T, class I, I... idx>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...> indices, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...> indices, requires_arch<sve>) noexcept\n         {\n             static_assert(batch<T, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             return swizzle(arg, indices.as_batch(), sve {});\n         }\n \n         template <class A, class T, class I, I... idx>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& arg,\n-                                                 batch_constant<I, A, idx...> indices,\n-                                                 requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& arg,\n+                                                       batch_constant<I, A, idx...> indices,\n+                                                       requires_arch<sve>) noexcept\n         {\n             static_assert(batch<std::complex<T>, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             return swizzle(arg, indices.as_batch(), sve {});\n@@ -762,14 +762,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> sve_extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n             {\n                 assert(false && \"extract_pair out of bounds\");\n                 return batch<T, A> {};\n             }\n \n             template <class A, class T, size_t I, size_t... Is>\n-            inline batch<T, A> sve_extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (n == I)\n                 {\n@@ -782,7 +782,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t... Is>\n-            inline batch<T, A> sve_extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>) noexcept\n             {\n                 if (n == 0)\n                 {\n@@ -796,7 +796,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(n < size && \"index in bounds\");\n@@ -805,27 +805,27 @@ namespace xsimd\n \n         // select\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<sve>) noexcept\n         {\n             return svsel(cond, a, b);\n         }\n \n         template <class A, class T, bool... b>\n-        inline batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sve>) noexcept\n         {\n             return select(batch_bool<T, A> { b... }, true_br, false_br, sve {});\n         }\n \n         // zip_lo\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svzip1(lhs, rhs);\n         }\n \n         // zip_hi\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svzip2(lhs, rhs);\n         }\n@@ -836,21 +836,21 @@ namespace xsimd\n \n         // rsqrt\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svrsqrte(arg);\n         }\n \n         // sqrt\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svsqrt_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // reciprocal\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<sve>) noexcept\n         {\n             return svrecpe(arg);\n         }\n@@ -863,37 +863,37 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, detail::enable_sized_integral_t<T, 4> = 0>\n-            inline batch<float, A> fast_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_f32_x(detail::sve_ptrue<T>(), arg);\n             }\n \n             template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-            inline batch<double, A> fast_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_f64_x(detail::sve_ptrue<T>(), arg);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& arg, batch<int32_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& arg, batch<int32_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_s32_x(detail::sve_ptrue<float>(), arg);\n             }\n \n             template <class A>\n-            inline batch<uint32_t, A> fast_cast(batch<float, A> const& arg, batch<uint32_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<float, A> const& arg, batch<uint32_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_u32_x(detail::sve_ptrue<float>(), arg);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& arg, batch<int64_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& arg, batch<int64_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_s64_x(detail::sve_ptrue<double>(), arg);\n             }\n \n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<double, A> const& arg, batch<uint64_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<double, A> const& arg, batch<uint64_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_u64_x(detail::sve_ptrue<double>(), arg);\n             }\n@@ -905,21 +905,21 @@ namespace xsimd\n \n         // set\n         template <class A, class T, class... Args>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sve>, Args... args) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sve>, Args... args) noexcept\n         {\n             return detail::sve_vector_type<T> { args... };\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<sve>,\n-                                             Args... args_complex) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<sve>,\n+                                                   Args... args_complex) noexcept\n         {\n             return batch<std::complex<T>>(detail::sve_vector_type<T> { args_complex.real()... },\n                                           detail::sve_vector_type<T> { args_complex.imag()... });\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sve>, Args... args) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sve>, Args... args) noexcept\n         {\n             using U = as_unsigned_integer_t<T>;\n             const auto values = detail::sve_vector_type<U> { static_cast<U>(args)... };\n@@ -931,17 +931,17 @@ namespace xsimd\n         namespace detail\n         {\n             // generate index sequence (iota)\n-            inline svuint8_t sve_iota_impl(index<1>) noexcept { return svindex_u8(0, 1); }\n-            inline svuint16_t sve_iota_impl(index<2>) noexcept { return svindex_u16(0, 1); }\n-            inline svuint32_t sve_iota_impl(index<4>) noexcept { return svindex_u32(0, 1); }\n-            inline svuint64_t sve_iota_impl(index<8>) noexcept { return svindex_u64(0, 1); }\n+            XSIMD_INLINE svuint8_t sve_iota_impl(index<1>) noexcept { return svindex_u8(0, 1); }\n+            XSIMD_INLINE svuint16_t sve_iota_impl(index<2>) noexcept { return svindex_u16(0, 1); }\n+            XSIMD_INLINE svuint32_t sve_iota_impl(index<4>) noexcept { return svindex_u32(0, 1); }\n+            XSIMD_INLINE svuint64_t sve_iota_impl(index<8>) noexcept { return svindex_u64(0, 1); }\n \n             template <class T, class V = sve_vector_type<as_unsigned_integer_t<T>>>\n-            inline V sve_iota() noexcept { return sve_iota_impl(index<sizeof(T)> {}); }\n+            XSIMD_INLINE V sve_iota() noexcept { return sve_iota_impl(index<sizeof(T)> {}); }\n         } // namespace detail\n \n         template <class A, class T, size_t I, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<sve>) noexcept\n         {\n             // create a predicate with only the I-th lane activated\n             const auto iota = detail::sve_iota<T>();\n@@ -951,89 +951,89 @@ namespace xsimd\n \n         // all\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_pcount<T>(arg) == batch_bool<T, A>::size;\n         }\n \n         // any\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svptest_any(arg, arg);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 1> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u8(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 1> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s8(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 2> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u16(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 2> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s16(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 4> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u32(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 4> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s32(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 8> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u64(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 8> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s64(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_f32(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_f64(arg);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, detail::sve_enable_all_t<T_in> = 0>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<sve>) noexcept\n         {\n             return arg.data;\n         }\n \n         // from_bool\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return select(arg, batch<T, A>(1), batch<T, A>(0));\n         }\n@@ -1045,7 +1045,7 @@ namespace xsimd\n             struct sve_slider_left\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     using u8_vector = batch<uint8_t, A>;\n                     const auto left = svdup_n_u8(0);\n@@ -1059,15 +1059,15 @@ namespace xsimd\n             struct sve_slider_left<0>\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     return arg;\n                 }\n             };\n         } // namespace detail\n \n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_slider_left<N>()(arg);\n         }\n@@ -1079,7 +1079,7 @@ namespace xsimd\n             struct sve_slider_right\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     using u8_vector = batch<uint8_t, A>;\n                     const auto left = bitwise_cast(arg, u8_vector {}, sve {}).data;\n@@ -1093,51 +1093,51 @@ namespace xsimd\n             struct sve_slider_right<batch<uint8_t, sve>::size>\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const&) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const&) noexcept\n                 {\n                     return batch<T, A> {};\n                 }\n             };\n         } // namespace detail\n \n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_slider_right<N>()(arg);\n         }\n \n         // isnan\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return !(arg == arg);\n         }\n \n         // nearbyint\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svrintx_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto nearest = svrintx_x(detail::sve_ptrue<float>(), arg);\n             return svcvt_s32_x(detail::sve_ptrue<float>(), nearest);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto nearest = svrintx_x(detail::sve_ptrue<double>(), arg);\n             return svcvt_s64_x(detail::sve_ptrue<double>(), nearest);\n         }\n \n         // ldexp\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& exp, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& exp, requires_arch<sve>) noexcept\n         {\n             return svscale_x(detail::sve_ptrue<T>(), x, exp);\n         }\n--- include/xsimd/arch/xsimd_wasm.hpp\n@@ -23,7 +23,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -34,15 +34,15 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n \n         // abs\n         template <class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -68,20 +68,20 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_abs(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_abs(self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -107,20 +107,20 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_add(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_add(self, other);\n         }\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -138,7 +138,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -158,94 +158,94 @@ namespace xsimd\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self) == 0x0F;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self) == 0x03;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_bitmask(self) == 0xFFFF;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self) != 0;\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_bitmask(self) != 0;\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<wasm>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A, class T>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_and(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_and(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A, class T>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_andnot(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_andnot(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class Tp>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<wasm>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n \n         // bitwise_or\n         template <class A, class T>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(self, other);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -272,7 +272,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -326,38 +326,38 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_not(self);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_not(self);\n         }\n \n         // bitwise_xor\n         template <class A, class T>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_xor(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_xor(self, other);\n         }\n \n         // broadcast\n         template <class A>\n-        batch<float, A> inline broadcast(float val, requires_arch<wasm>) noexcept\n+        batch<float, A> XSIMD_INLINE broadcast(float val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_splat(val);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -382,48 +382,48 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_splat(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ceil(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ceil(self);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_div(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_div(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_eq(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_eq(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -448,7 +448,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -473,12 +473,12 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_eq(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_eq(self, other);\n         }\n@@ -487,13 +487,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<wasm>) noexcept\n             {\n                 return wasm_f32x4_convert_i32x4(self);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to wasm\n@@ -506,7 +506,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to wasm\n@@ -520,7 +520,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_make(\n                     static_cast<int32_t>(wasm_f32x4_extract_lane(self, 0)),\n@@ -532,20 +532,20 @@ namespace xsimd\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_floor(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_floor(self);\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut[][4] = {\n                 { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },\n@@ -569,7 +569,7 @@ namespace xsimd\n             return wasm_v128_load((const v128_t*)lut[mask]);\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -581,7 +581,7 @@ namespace xsimd\n             return wasm_v128_load((const v128_t*)lut[mask]);\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[] = {\n                 0x0000000000000000,\n@@ -667,24 +667,24 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ge(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ge(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_gt(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -732,14 +732,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_gt(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_i32x4_shuffle(row[0], row[1], 0, 4, 1, 5);\n             v128_t tmp1 = wasm_i32x4_shuffle(row[0], row[1], 2, 6, 3, 7);\n@@ -752,20 +752,20 @@ namespace xsimd\n             return wasm_f32x4_add(tmp0, tmp2);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_add(wasm_i64x2_shuffle(row[0], row[1], 0, 2),\n                                   wasm_i64x2_shuffle(row[0], row[1], 1, 3));\n         }\n \n         // insert\n         template <class A, size_t I>\n-        inline batch<float, A> insert(batch<float, A> const& self, float val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> insert(batch<float, A> const& self, float val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_replace_lane(self, pos, val);\n         }\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -818,48 +818,48 @@ namespace xsimd\n         }\n \n         template <class A, size_t I>\n-        inline batch<double, A> insert(batch<double, A> const& self, double val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> insert(batch<double, A> const& self, double val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_replace_lane(self, pos, val);\n         }\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_f32x4_ne(self, self), wasm_f32x4_ne(self, self));\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_f64x2_ne(self, self), wasm_f64x2_ne(self, self));\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_le(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_le(self, other);\n         }\n \n         // load_aligned\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load((v128_t const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n@@ -868,42 +868,42 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<wasm>) noexcept\n             {\n                 return { wasm_i32x4_shuffle(hi, lo, 0, 2, 4, 6), wasm_i32x4_shuffle(hi, lo, 1, 3, 5, 7) };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<wasm>) noexcept\n             {\n                 return { wasm_i64x2_shuffle(hi, lo, 0, 2), wasm_i64x2_shuffle(hi, lo, 1, 3) };\n             }\n         }\n \n         // load_unaligned\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load((v128_t const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_lt(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -964,14 +964,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_lt(self, other);\n         }\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -996,66 +996,66 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_pmax(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_pmax(self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_pmin(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_pmin(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_mul(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_mul(self, other);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1081,67 +1081,67 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> neg(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> neg(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_neg(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_neg(self);\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ne(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return ~(self == other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ne(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ne(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ne(self, other);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f32x4_splat(1.0f);\n             return wasm_f32x4_div(one, self);\n         }\n         template <class A>\n-        inline batch<double, A> reciprocal(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> reciprocal(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f64x2_splat(1.0);\n             return wasm_f64x2_div(one, self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_f32x4_add(self, wasm_i32x4_shuffle(self, self, 6, 7, 2, 3));\n             v128_t tmp1 = wasm_i32x4_shuffle(tmp0, tmp0, 1, 0, 4, 4);\n@@ -1150,7 +1150,7 @@ namespace xsimd\n             return wasm_f32x4_extract_lane(tmp3, 0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1172,7 +1172,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_i64x2_shuffle(self, self, 1, 3);\n             v128_t tmp1 = wasm_f64x2_add(self, tmp0);\n@@ -1182,21 +1182,21 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f32x4_splat(1.0f);\n             return wasm_f32x4_div(one, wasm_f32x4_sqrt(self));\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f64x2_splat(1.0);\n             return wasm_f64x2_div(one, wasm_f64x2_sqrt(self));\n         }\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(\n                 wasm_i64x2_const(0, 0), x, ((N) & 0xF0) ? 0 : 16 - ((N) & 0xF),\n@@ -1212,7 +1212,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(\n                 x, wasm_i64x2_const(0, 0), ((N) & 0xF0) ? 16 : ((N) & 0xF) + 0,\n@@ -1228,7 +1228,7 @@ namespace xsimd\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1264,102 +1264,102 @@ namespace xsimd\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, wasm {});\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(x, y, I0, I1, I2, I3);\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(x, y, I0, I1);\n         }\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return wasm_f32x4_make(values...);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1) noexcept\n         {\n             return wasm_i64x2_make(v0, v1);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return wasm_i32x4_make(v0, v1, v2, v3);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return wasm_i16x8_make(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return wasm_i8x16_make(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return wasm_f64x2_make(values...);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data;\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1395,22 +1395,22 @@ namespace xsimd\n \n         // store_aligned\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n@@ -1420,58 +1420,58 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_shuffle(self.real(), self.imag(), 0, 4, 1, 5);\n             }\n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_shuffle(self.real(), self.imag(), 2, 6, 3, 7);\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i64x2_shuffle(self.real(), self.imag(), 0, 2);\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i64x2_shuffle(self.real(), self.imag(), 1, 3);\n             }\n         }\n \n         // store_unaligned\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n \n         // sub\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_sub(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1496,106 +1496,106 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_sub(self, other);\n         }\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_sqrt(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_sqrt(val);\n         }\n \n         // swizzle\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, self, V0, V1, V2, V3);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, self, V0, V1);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, self, V0, V1);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, self, V0, V1, V2, V3);\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i16x8_shuffle(self, self, V0, V1, V2, V3, V4, V5, V6, V7);\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(self, self, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15);\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int8_t>(swizzle(bitwise_cast<uint8_t>(self), mask, wasm {}));\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_trunc(self);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_trunc(self);\n         }\n \n         // zip_hi\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, other, 2, 6, 3, 7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1620,19 +1620,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, other, 1, 3);\n         }\n \n         // zip_lo\n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, other, 0, 4, 1, 5);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1657,7 +1657,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, other, 0, 2);\n         }\n--- include/xsimd/config/xsimd_arch.hpp\n@@ -57,13 +57,13 @@ namespace xsimd\n         };\n \n         template <typename T>\n-        inline constexpr T max_of(T value) noexcept\n+        XSIMD_INLINE constexpr T max_of(T value) noexcept\n         {\n             return value;\n         }\n \n         template <typename T, typename... Ts>\n-        inline constexpr T max_of(T head0, T head1, Ts... tail) noexcept\n+        XSIMD_INLINE constexpr T max_of(T head0, T head1, Ts... tail) noexcept\n         {\n             return max_of((head0 > head1 ? head0 : head1), tail...);\n         }\n@@ -104,7 +104,7 @@ namespace xsimd\n         }\n \n         template <class F>\n-        static inline void for_each(F&& f) noexcept\n+        static XSIMD_INLINE void for_each(F&& f) noexcept\n         {\n             (void)std::initializer_list<bool> { (f(Archs {}), true)... };\n         }\n@@ -196,14 +196,14 @@ namespace xsimd\n             F functor;\n \n             template <class Arch, class... Tys>\n-            inline auto walk_archs(arch_list<Arch>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto walk_archs(arch_list<Arch>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n             {\n                 assert(Arch::available() && \"At least one arch must be supported during dispatch\");\n                 return functor(Arch {}, std::forward<Tys>(args)...);\n             }\n \n             template <class Arch, class ArchNext, class... Archs, class... Tys>\n-            inline auto walk_archs(arch_list<Arch, ArchNext, Archs...>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto walk_archs(arch_list<Arch, ArchNext, Archs...>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n             {\n                 if (availables_archs.has(Arch {}))\n                     return functor(Arch {}, std::forward<Tys>(args)...);\n@@ -212,14 +212,14 @@ namespace xsimd\n             }\n \n         public:\n-            inline dispatcher(F f) noexcept\n+            XSIMD_INLINE dispatcher(F f) noexcept\n                 : availables_archs(available_architectures())\n                 , functor(f)\n             {\n             }\n \n             template <class... Tys>\n-            inline auto operator()(Tys&&... args) noexcept -> decltype(functor(default_arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto operator()(Tys&&... args) noexcept -> decltype(functor(default_arch {}, std::forward<Tys>(args)...))\n             {\n                 return walk_archs(ArchList {}, std::forward<Tys>(args)...);\n             }\n@@ -228,7 +228,7 @@ namespace xsimd\n \n     // Generic function dispatch, \u00e0 la ifunc\n     template <class ArchList = supported_architectures, class F>\n-    inline detail::dispatcher<F, ArchList> dispatch(F&& f) noexcept\n+    XSIMD_INLINE detail::dispatcher<F, ArchList> dispatch(F&& f) noexcept\n     {\n         return { std::forward<F>(f) };\n     }\n--- include/xsimd/config/xsimd_cpuid.hpp\n@@ -41,7 +41,7 @@ namespace xsimd\n \n #define ARCH_FIELD_EX(arch, field_name) \\\n     unsigned field_name;                \\\n-    inline bool has(::xsimd::arch) const { return this->field_name; }\n+    XSIMD_INLINE bool has(::xsimd::arch) const { return this->field_name; }\n #define ARCH_FIELD(name) ARCH_FIELD_EX(name, name)\n \n             ARCH_FIELD(sse2)\n@@ -78,7 +78,7 @@ namespace xsimd\n \n #undef ARCH_FIELD\n \n-            inline supported_arch() noexcept\n+            XSIMD_INLINE supported_arch() noexcept\n             {\n                 memset(this, 0, sizeof(supported_arch));\n \n@@ -191,7 +191,7 @@ namespace xsimd\n         };\n     } // namespace detail\n \n-    inline detail::supported_arch available_architectures() noexcept\n+    XSIMD_INLINE detail::supported_arch available_architectures() noexcept\n     {\n         static detail::supported_arch supported;\n         return supported;\n--- include/xsimd/config/xsimd_inline.hpp\n@@ -0,0 +1,23 @@\n+/***************************************************************************\n+ * Copyright (c) Johan Mabille, Sylvain Corlay, Wolf Vollprecht and         *\n+ * Martin Renou                                                             *\n+ * Copyright (c) QuantStack                                                 *\n+ * Copyright (c) Serge Guelton                                              *\n+ *                                                                          *\n+ * Distributed under the terms of the BSD 3-Clause License.                 *\n+ *                                                                          *\n+ * The full license is in the file LICENSE, distributed with this software. *\n+ ****************************************************************************/\n+\n+#ifndef XSIMD_INLINE_HPP\n+#define XSIMD_INLINE_HPP\n+\n+#if defined(__GNUC__)\n+#define XSIMD_INLINE inline __attribute__((always_inline))\n+#elif defined(_MSC_VER)\n+#define XSIMD_INLINE inline __forceinline\n+#else\n+#define XSIMD_INLINE inline\n+#endif\n+\n+#endif\n--- include/xsimd/math/xsimd_rem_pio2.hpp\n@@ -217,7 +217,7 @@ namespace xsimd\n          *\n          */\n \n-        inline int32_t __kernel_rem_pio2(double* x, double* y, int32_t e0, int32_t nx, int32_t prec, const int32_t* ipio2) noexcept\n+        XSIMD_INLINE int32_t __kernel_rem_pio2(double* x, double* y, int32_t e0, int32_t nx, int32_t prec, const int32_t* ipio2) noexcept\n         {\n             static const int32_t init_jk[] = { 2, 3, 4, 6 }; /* initial value for jk */\n \n@@ -450,7 +450,7 @@ namespace xsimd\n             return n & 7;\n         }\n \n-        inline std::int32_t __ieee754_rem_pio2(double x, double* y) noexcept\n+        XSIMD_INLINE std::int32_t __ieee754_rem_pio2(double x, double* y) noexcept\n         {\n             static const std::int32_t two_over_pi[] = {\n                 0xA2F983,\n--- include/xsimd/memory/xsimd_aligned_allocator.hpp\n@@ -59,43 +59,43 @@ namespace xsimd\n             using other = aligned_allocator<U, Align>;\n         };\n \n-        inline aligned_allocator() noexcept;\n-        inline aligned_allocator(const aligned_allocator& rhs) noexcept;\n+        XSIMD_INLINE aligned_allocator() noexcept;\n+        XSIMD_INLINE aligned_allocator(const aligned_allocator& rhs) noexcept;\n \n         template <class U>\n-        inline aligned_allocator(const aligned_allocator<U, Align>& rhs) noexcept;\n+        XSIMD_INLINE aligned_allocator(const aligned_allocator<U, Align>& rhs) noexcept;\n \n-        inline ~aligned_allocator();\n+        XSIMD_INLINE ~aligned_allocator();\n \n-        inline pointer address(reference) noexcept;\n-        inline const_pointer address(const_reference) const noexcept;\n+        XSIMD_INLINE pointer address(reference) noexcept;\n+        XSIMD_INLINE const_pointer address(const_reference) const noexcept;\n \n-        inline pointer allocate(size_type n, const void* hint = 0);\n-        inline void deallocate(pointer p, size_type n);\n+        XSIMD_INLINE pointer allocate(size_type n, const void* hint = 0);\n+        XSIMD_INLINE void deallocate(pointer p, size_type n);\n \n-        inline size_type max_size() const noexcept;\n-        inline size_type size_max() const noexcept;\n+        XSIMD_INLINE size_type max_size() const noexcept;\n+        XSIMD_INLINE size_type size_max() const noexcept;\n \n         template <class U, class... Args>\n-        inline void construct(U* p, Args&&... args);\n+        XSIMD_INLINE void construct(U* p, Args&&... args);\n \n         template <class U>\n-        inline void destroy(U* p);\n+        XSIMD_INLINE void destroy(U* p);\n     };\n \n     template <class T1, size_t Align1, class T2, size_t Align2>\n-    inline bool operator==(const aligned_allocator<T1, Align1>& lhs,\n-                           const aligned_allocator<T2, Align2>& rhs) noexcept;\n+    XSIMD_INLINE bool operator==(const aligned_allocator<T1, Align1>& lhs,\n+                                 const aligned_allocator<T2, Align2>& rhs) noexcept;\n \n     template <class T1, size_t Align1, class T2, size_t Align2>\n-    inline bool operator!=(const aligned_allocator<T1, Align1>& lhs,\n-                           const aligned_allocator<T2, Align2>& rhs) noexcept;\n+    XSIMD_INLINE bool operator!=(const aligned_allocator<T1, Align1>& lhs,\n+                                 const aligned_allocator<T2, Align2>& rhs) noexcept;\n \n-    inline void* aligned_malloc(size_t size, size_t alignment);\n-    inline void aligned_free(void* ptr);\n+    XSIMD_INLINE void* aligned_malloc(size_t size, size_t alignment);\n+    XSIMD_INLINE void aligned_free(void* ptr);\n \n     template <class T>\n-    inline size_t get_alignment_offset(const T* p, size_t size, size_t block_size);\n+    XSIMD_INLINE size_t get_alignment_offset(const T* p, size_t size, size_t block_size);\n \n     /************************************\n      * aligned_allocator implementation *\n@@ -105,15 +105,15 @@ namespace xsimd\n      * Default constructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::aligned_allocator() noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator() noexcept\n     {\n     }\n \n     /**\n      * Copy constructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::aligned_allocator(const aligned_allocator&) noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator(const aligned_allocator&) noexcept\n     {\n     }\n \n@@ -122,15 +122,15 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U>\n-    inline aligned_allocator<T, A>::aligned_allocator(const aligned_allocator<U, A>&) noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator(const aligned_allocator<U, A>&) noexcept\n     {\n     }\n \n     /**\n      * Destructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::~aligned_allocator()\n+    XSIMD_INLINE aligned_allocator<T, A>::~aligned_allocator()\n     {\n     }\n \n@@ -140,7 +140,7 @@ namespace xsimd\n      * @return the actual address of \\c r.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::address(reference r) noexcept -> pointer\n     {\n         return &r;\n@@ -152,7 +152,7 @@ namespace xsimd\n      * @return the actual address of \\c r.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::address(const_reference r) const noexcept -> const_pointer\n     {\n         return &r;\n@@ -167,7 +167,7 @@ namespace xsimd\n      * hold an array of \\c n objects of type \\c T.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::allocate(size_type n, const void*) -> pointer\n     {\n         pointer res = reinterpret_cast<pointer>(aligned_malloc(sizeof(T) * n, A));\n@@ -186,7 +186,7 @@ namespace xsimd\n      * @param n number of objects earlier passed to allocate().\n      */\n     template <class T, size_t A>\n-    inline void aligned_allocator<T, A>::deallocate(pointer p, size_type)\n+    XSIMD_INLINE void aligned_allocator<T, A>::deallocate(pointer p, size_type)\n     {\n         aligned_free(p);\n     }\n@@ -197,7 +197,7 @@ namespace xsimd\n      * @return the maximum supported allocated size.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::max_size() const noexcept -> size_type\n     {\n         return size_type(-1) / sizeof(T);\n@@ -207,7 +207,7 @@ namespace xsimd\n      * This method is deprecated, use max_size() instead\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::size_max() const noexcept -> size_type\n     {\n         return size_type(-1) / sizeof(T);\n@@ -221,7 +221,7 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U, class... Args>\n-    inline void aligned_allocator<T, A>::construct(U* p, Args&&... args)\n+    XSIMD_INLINE void aligned_allocator<T, A>::construct(U* p, Args&&... args)\n     {\n         new ((void*)p) U(std::forward<Args>(args)...);\n     }\n@@ -232,7 +232,7 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U>\n-    inline void aligned_allocator<T, A>::destroy(U* p)\n+    XSIMD_INLINE void aligned_allocator<T, A>::destroy(U* p)\n     {\n         p->~U();\n     }\n@@ -250,8 +250,8 @@ namespace xsimd\n      * @return true if the allocators have the same alignment.\n      */\n     template <class T1, size_t A1, class T2, size_t A2>\n-    inline bool operator==(const aligned_allocator<T1, A1>& lhs,\n-                           const aligned_allocator<T2, A2>& rhs) noexcept\n+    XSIMD_INLINE bool operator==(const aligned_allocator<T1, A1>& lhs,\n+                                 const aligned_allocator<T2, A2>& rhs) noexcept\n     {\n         return lhs.alignment == rhs.alignment;\n     }\n@@ -265,8 +265,8 @@ namespace xsimd\n      * @return true if the allocators have different alignments.\n      */\n     template <class T1, size_t A1, class T2, size_t A2>\n-    inline bool operator!=(const aligned_allocator<T1, A1>& lhs,\n-                           const aligned_allocator<T2, A2>& rhs) noexcept\n+    XSIMD_INLINE bool operator!=(const aligned_allocator<T1, A1>& lhs,\n+                                 const aligned_allocator<T2, A2>& rhs) noexcept\n     {\n         return !(lhs == rhs);\n     }\n@@ -277,7 +277,7 @@ namespace xsimd\n \n     namespace detail\n     {\n-        inline void* xaligned_malloc(size_t size, size_t alignment)\n+        XSIMD_INLINE void* xaligned_malloc(size_t size, size_t alignment)\n         {\n             assert(((alignment & (alignment - 1)) == 0) && \"alignment must be a power of two\");\n             assert((alignment >= sizeof(void*)) && \"alignment must be at least the size of a pointer\");\n@@ -293,7 +293,7 @@ namespace xsimd\n             return res;\n         }\n \n-        inline void xaligned_free(void* ptr)\n+        XSIMD_INLINE void xaligned_free(void* ptr)\n         {\n #ifdef _WIN32\n             _aligned_free(ptr);\n@@ -303,18 +303,18 @@ namespace xsimd\n         }\n     }\n \n-    inline void* aligned_malloc(size_t size, size_t alignment)\n+    XSIMD_INLINE void* aligned_malloc(size_t size, size_t alignment)\n     {\n         return detail::xaligned_malloc(size, alignment);\n     }\n \n-    inline void aligned_free(void* ptr)\n+    XSIMD_INLINE void aligned_free(void* ptr)\n     {\n         detail::xaligned_free(ptr);\n     }\n \n     template <class T>\n-    inline size_t get_alignment_offset(const T* p, size_t size, size_t block_size)\n+    XSIMD_INLINE size_t get_alignment_offset(const T* p, size_t size, size_t block_size)\n     {\n         // size_t block_size = simd_traits<T>::size;\n         if (block_size == 1)\n--- include/xsimd/memory/xsimd_alignment.hpp\n@@ -81,7 +81,7 @@ namespace xsimd\n      * @return true if the alignment requirements are met\n      */\n     template <class Arch = default_arch>\n-    inline bool is_aligned(void const* ptr)\n+    XSIMD_INLINE bool is_aligned(void const* ptr)\n     {\n         return (reinterpret_cast<uintptr_t>(ptr) % static_cast<uintptr_t>(Arch::alignment())) == 0;\n     }\n--- include/xsimd/types/xsimd_api.hpp\n@@ -53,7 +53,7 @@ namespace xsimd\n      * @return the absolute values of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> abs(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(x, A {});\n@@ -67,7 +67,7 @@ namespace xsimd\n      * @return the absolute values of \\c z.\n      */\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<std::complex<T>, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(z, A {});\n@@ -82,7 +82,7 @@ namespace xsimd\n      * @return the sum of \\c x and \\c y\n      */\n     template <class T, class A>\n-    inline auto add(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x + y)\n+    XSIMD_INLINE auto add(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x + y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x + y;\n@@ -96,7 +96,7 @@ namespace xsimd\n      * @return the arc cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> acos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> acos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::acos<A>(x, A {});\n@@ -110,7 +110,7 @@ namespace xsimd\n      * @return the inverse hyperbolic cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> acosh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> acosh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::acosh<A>(x, A {});\n@@ -124,7 +124,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::arg<A>(z, A {});\n@@ -138,7 +138,7 @@ namespace xsimd\n      * @return the arc sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> asin(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> asin(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::asin<A>(x, A {});\n@@ -152,7 +152,7 @@ namespace xsimd\n      * @return the inverse hyperbolic sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> asinh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> asinh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::asinh<A>(x, A {});\n@@ -166,7 +166,7 @@ namespace xsimd\n      * @return the arc tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> atan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atan<A>(x, A {});\n@@ -182,7 +182,7 @@ namespace xsimd\n      * @return the arc tangent of \\c x/y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atan2<A>(x, y, A {});\n@@ -196,7 +196,7 @@ namespace xsimd\n      * @return the inverse hyperbolic tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atanh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> atanh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atanh<A>(x, A {});\n@@ -211,7 +211,7 @@ namespace xsimd\n      * @return the average of elements between \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::avg<A>(x, y, A {});\n@@ -226,7 +226,7 @@ namespace xsimd\n      * @return the rounded average of elements between \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::avgr<A>(x, y, A {});\n@@ -240,7 +240,7 @@ namespace xsimd\n      * @return \\c x cast to \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_out, A>();\n         detail::static_check_supported_config<T_in, A>();\n@@ -256,7 +256,7 @@ namespace xsimd\n      * @return \\c x cast to \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> batch_cast(batch<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_out, A>();\n         detail::static_check_supported_config<T_in, A>();\n@@ -271,7 +271,7 @@ namespace xsimd\n      * @return bit of sign of \\c x\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitofsign(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitofsign<A>(x, A {});\n@@ -286,7 +286,7 @@ namespace xsimd\n      * @return the result of the bitwise and.\n      */\n     template <class T, class A>\n-    inline auto bitwise_and(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x & y)\n+    XSIMD_INLINE auto bitwise_and(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x & y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x & y;\n@@ -301,7 +301,7 @@ namespace xsimd\n      * @return the result of the bitwise and.\n      */\n     template <class T, class A>\n-    inline auto bitwise_and(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x & y)\n+    XSIMD_INLINE auto bitwise_and(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x & y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x & y;\n@@ -316,7 +316,7 @@ namespace xsimd\n      * @return the result of the bitwise and not.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_andnot<A>(x, y, A {});\n@@ -331,7 +331,7 @@ namespace xsimd\n      * @return the result of the bitwise and not.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_andnot<A>(x, y, A {});\n@@ -345,7 +345,7 @@ namespace xsimd\n      * @return \\c x reinterpreted as \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_in, A>();\n         detail::static_check_supported_config<T_out, A>();\n@@ -361,13 +361,13 @@ namespace xsimd\n      * @return shifted \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_lshift(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_lshift<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> bitwise_lshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_lshift<A>(x, shift, A {});\n@@ -381,7 +381,7 @@ namespace xsimd\n      * @return the result of the bitwise not.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_not(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(x, A {});\n@@ -395,7 +395,7 @@ namespace xsimd\n      * @return the result of the bitwise not.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(x, A {});\n@@ -410,7 +410,7 @@ namespace xsimd\n      * @return the result of the bitwise or.\n      */\n     template <class T, class A>\n-    inline auto bitwise_or(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x | y)\n+    XSIMD_INLINE auto bitwise_or(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x | y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x | y;\n@@ -425,7 +425,7 @@ namespace xsimd\n      * @return the result of the bitwise or.\n      */\n     template <class T, class A>\n-    inline auto bitwise_or(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x | y)\n+    XSIMD_INLINE auto bitwise_or(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x | y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x | y;\n@@ -440,13 +440,13 @@ namespace xsimd\n      * @return shifted \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_rshift(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_rshift<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> bitwise_rshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_rshift<A>(x, shift, A {});\n@@ -461,7 +461,7 @@ namespace xsimd\n      * @return the result of the bitwise xor.\n      */\n     template <class T, class A>\n-    inline auto bitwise_xor(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x ^ y)\n+    XSIMD_INLINE auto bitwise_xor(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x ^ y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x ^ y;\n@@ -476,7 +476,7 @@ namespace xsimd\n      * @return the result of the bitwise xor.\n      */\n     template <class T, class A>\n-    inline auto bitwise_xor(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x ^ y)\n+    XSIMD_INLINE auto bitwise_xor(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x ^ y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x ^ y;\n@@ -490,7 +490,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class T, class A = default_arch>\n-    inline batch<T, A> broadcast(T v) noexcept\n+    XSIMD_INLINE batch<T, A> broadcast(T v) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch<T, A>::broadcast(v);\n@@ -505,7 +505,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> broadcast_as(From v) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> broadcast_as(From v) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n@@ -523,7 +523,7 @@ namespace xsimd\n      * @return the cubic root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cbrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cbrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cbrt<A>(x, A {});\n@@ -538,7 +538,7 @@ namespace xsimd\n      * @return the batch of smallest integer values not less than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ceil(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> ceil(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ceil<A>(x, A {});\n@@ -554,7 +554,7 @@ namespace xsimd\n      * @return the result of the clipping.\n      */\n     template <class T, class A>\n-    inline batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) noexcept\n+    XSIMD_INLINE batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::clip(x, lo, hi, A {});\n@@ -567,7 +567,7 @@ namespace xsimd\n      * resulting vector, zeroing the remaining slots\n      */\n     template <class T, class A>\n-    inline batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::compress<A>(x, mask, A {});\n@@ -581,7 +581,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class A, class T>\n-    inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) noexcept\n     {\n         return kernel::conj(z, A {});\n     }\n@@ -597,7 +597,7 @@ namespace xsimd\n      * matches that of \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::copysign<A>(x, y, A {});\n@@ -611,7 +611,7 @@ namespace xsimd\n      * @return the cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cos<A>(x, A {});\n@@ -625,7 +625,7 @@ namespace xsimd\n      * @return the hyperbolic cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cosh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cosh<A>(x, A {});\n@@ -639,7 +639,7 @@ namespace xsimd\n      * @return the subtraction of \\c x and 1.\n      */\n     template <class T, class A>\n-    inline batch<T, A> decr(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> decr(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::decr<A>(x, A {});\n@@ -655,7 +655,7 @@ namespace xsimd\n      * @return the subtraction of \\c x and 1 when \\c mask is true.\n      */\n     template <class T, class A, class Mask>\n-    inline batch<T, A> decr_if(batch<T, A> const& x, Mask const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& x, Mask const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::decr_if<A>(x, mask, A {});\n@@ -670,7 +670,7 @@ namespace xsimd\n      * @return the result of the division.\n      */\n     template <class T, class A>\n-    inline auto div(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x / y)\n+    XSIMD_INLINE auto div(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x / y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x / y;\n@@ -685,7 +685,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto eq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x == y)\n+    XSIMD_INLINE auto eq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x == y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x == y;\n@@ -700,7 +700,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto eq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x == y)\n+    XSIMD_INLINE auto eq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x == y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x == y;\n@@ -714,7 +714,7 @@ namespace xsimd\n      * @return the natural exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp<A>(x, A {});\n@@ -728,7 +728,7 @@ namespace xsimd\n      * @return the base 10 exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp10(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp10(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp10<A>(x, A {});\n@@ -742,7 +742,7 @@ namespace xsimd\n      * @return the base 2 exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp2(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp2(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp2<A>(x, A {});\n@@ -755,7 +755,7 @@ namespace xsimd\n      * mask, zeroing the other slots\n      */\n     template <class T, class A>\n-    inline batch<T, A> expand(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> expand(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::expand<A>(x, mask, A {});\n@@ -769,7 +769,7 @@ namespace xsimd\n      * @return the natural exponential of \\c x, minus one.\n      */\n     template <class T, class A>\n-    inline batch<T, A> expm1(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> expm1(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::expm1<A>(x, A {});\n@@ -783,7 +783,7 @@ namespace xsimd\n      * @return the error function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> erf(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> erf(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::erf<A>(x, A {});\n@@ -797,7 +797,7 @@ namespace xsimd\n      * @return the error function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> erfc(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> erfc(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::erfc<A>(x, A {});\n@@ -814,7 +814,7 @@ namespace xsimd\n      * @return.\n      */\n     template <class T, class A>\n-    inline batch<T, A> extract_pair(batch<T, A> const& x, batch<T, A> const& y, std::size_t i) noexcept\n+    XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& x, batch<T, A> const& y, std::size_t i) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::extract_pair<A>(x, y, i, A {});\n@@ -828,7 +828,7 @@ namespace xsimd\n      * @return the absolute values of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fabs(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> fabs(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(x, A {});\n@@ -844,7 +844,7 @@ namespace xsimd\n      * @return the positive difference.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fdim<A>(x, y, A {});\n@@ -859,7 +859,7 @@ namespace xsimd\n      * @return the batch of largest integer values not greater than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> floor(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> floor(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::floor<A>(x, A {});\n@@ -875,7 +875,7 @@ namespace xsimd\n      * @return the result of the fused multiply-add operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fma<A>(x, y, z, A {});\n@@ -890,7 +890,7 @@ namespace xsimd\n      * @return a batch of the larger values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::max<A>(x, y, A {});\n@@ -905,7 +905,7 @@ namespace xsimd\n      * @return a batch of the smaller values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::min<A>(x, y, A {});\n@@ -920,7 +920,7 @@ namespace xsimd\n      * @return the result of the modulo.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fmod<A>(x, y, A {});\n@@ -936,7 +936,7 @@ namespace xsimd\n      * @return the result of the fused multiply-sub operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fms<A>(x, y, z, A {});\n@@ -952,7 +952,7 @@ namespace xsimd\n      * @return the result of the fused negated multiply-add operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fnma<A>(x, y, z, A {});\n@@ -968,7 +968,7 @@ namespace xsimd\n      * @return the result of the fused negated multiply-sub operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fnms<A>(x, y, z, A {});\n@@ -983,7 +983,7 @@ namespace xsimd\n      * @return the normalized fraction of x\n      */\n     template <class T, class A>\n-    inline batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) noexcept\n+    XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::frexp<A>(x, y, A {});\n@@ -999,7 +999,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x >= y;\n@@ -1015,7 +1015,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x > y;\n@@ -1031,7 +1031,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline batch<T, A> haddp(batch<T, A> const* row) noexcept\n+    XSIMD_INLINE batch<T, A> haddp(batch<T, A> const* row) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::haddp<A>(row, A {});\n@@ -1047,7 +1047,7 @@ namespace xsimd\n      * @return the square root of the sum of the squares of \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::hypot<A>(x, y, A {});\n@@ -1061,7 +1061,7 @@ namespace xsimd\n      * @return the argument of \\c x.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::imag<A>(x, A {});\n@@ -1075,7 +1075,7 @@ namespace xsimd\n      * @return the sum of \\c x and 1.\n      */\n     template <class T, class A>\n-    inline batch<T, A> incr(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> incr(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::incr<A>(x, A {});\n@@ -1091,7 +1091,7 @@ namespace xsimd\n      * @return the sum of \\c x and 1 when \\c mask is true.\n      */\n     template <class T, class A, class Mask>\n-    inline batch<T, A> incr_if(batch<T, A> const& x, Mask const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& x, Mask const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::incr_if<A>(x, mask, A {});\n@@ -1104,7 +1104,7 @@ namespace xsimd\n      * @return a batch of positive infinity\n      */\n     template <class B>\n-    inline B infinity()\n+    XSIMD_INLINE B infinity()\n     {\n         using T = typename B::value_type;\n         using A = typename B::arch_type;\n@@ -1122,7 +1122,7 @@ namespace xsimd\n      * @return copy of \\c x with position \\c pos set to \\c val\n      */\n     template <class T, class A, size_t I>\n-    inline batch<T, A> insert(batch<T, A> const& x, T val, index<I> pos) noexcept\n+    XSIMD_INLINE batch<T, A> insert(batch<T, A> const& x, T val, index<I> pos) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::insert<A>(x, val, pos, A {});\n@@ -1136,7 +1136,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_even(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_even<A>(x, A {});\n@@ -1150,7 +1150,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_flint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_flint<A>(x, A {});\n@@ -1164,7 +1164,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_odd(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_odd<A>(x, A {});\n@@ -1178,7 +1178,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isinf<A>(x, A {});\n@@ -1192,7 +1192,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isfinite<A>(x, A {});\n@@ -1206,7 +1206,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isnan<A>(x, A {});\n@@ -1221,7 +1221,7 @@ namespace xsimd\n      * @return a batch of floating point values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) noexcept\n+    XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ldexp<A>(x, y, A {});\n@@ -1236,7 +1236,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x <= y;\n@@ -1250,7 +1250,7 @@ namespace xsimd\n      * @return the natural logarithm of the gamma function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> lgamma(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::lgamma<A>(x, A {});\n@@ -1265,7 +1265,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> load_as(From const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> load_as(From const* ptr, aligned_mode) noexcept\n     {\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n         detail::static_check_supported_config<From, A>();\n@@ -1274,14 +1274,14 @@ namespace xsimd\n     }\n \n     template <class To, class A = default_arch>\n-    inline simd_return_type<bool, To, A> load_as(bool const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<bool, To, A> load_as(bool const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         return simd_return_type<bool, To, A>::load_aligned(ptr);\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         using batch_value_type = typename simd_return_type<std::complex<From>, To, A>::value_type;\n@@ -1290,7 +1290,7 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1307,7 +1307,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> load_as(From const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> load_as(From const* ptr, unaligned_mode) noexcept\n     {\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n         detail::static_check_supported_config<To, A>();\n@@ -1316,13 +1316,13 @@ namespace xsimd\n     }\n \n     template <class To, class A = default_arch>\n-    inline simd_return_type<bool, To, A> load_as(bool const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<bool, To, A> load_as(bool const* ptr, unaligned_mode) noexcept\n     {\n         return simd_return_type<bool, To, A>::load_unaligned(ptr);\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1332,7 +1332,7 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1349,7 +1349,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load(From const* ptr, aligned_mode = {}) noexcept\n+    XSIMD_INLINE batch<From, A> load(From const* ptr, aligned_mode = {}) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, aligned_mode {});\n@@ -1364,7 +1364,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load(From const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<From, A> load(From const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, unaligned_mode {});\n@@ -1379,7 +1379,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load_aligned(From const* ptr) noexcept\n+    XSIMD_INLINE batch<From, A> load_aligned(From const* ptr) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, aligned_mode {});\n@@ -1394,7 +1394,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load_unaligned(From const* ptr) noexcept\n+    XSIMD_INLINE batch<From, A> load_unaligned(From const* ptr) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, unaligned_mode {});\n@@ -1408,7 +1408,7 @@ namespace xsimd\n      * @return the natural logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log<A>(x, A {});\n@@ -1421,7 +1421,7 @@ namespace xsimd\n      * @return the base 2 logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log2(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log2(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log2<A>(x, A {});\n@@ -1434,7 +1434,7 @@ namespace xsimd\n      * @return the base 10 logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log10(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log10(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log10<A>(x, A {});\n@@ -1447,7 +1447,7 @@ namespace xsimd\n      * @return the natural logarithm of one plus \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log1p(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log1p(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log1p<A>(x, A {});\n@@ -1462,7 +1462,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x < y;\n@@ -1477,7 +1477,7 @@ namespace xsimd\n      * @return a batch of the larger values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::max<A>(x, y, A {});\n@@ -1492,7 +1492,7 @@ namespace xsimd\n      * @return a batch of the smaller values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::min<A>(x, y, A {});\n@@ -1505,7 +1505,7 @@ namespace xsimd\n      * @return a batch of positive infinity\n      */\n     template <class B>\n-    inline B minusinfinity() noexcept\n+    XSIMD_INLINE B minusinfinity() noexcept\n     {\n         using T = typename B::value_type;\n         using A = typename B::arch_type;\n@@ -1522,7 +1522,7 @@ namespace xsimd\n      * @return the result of the modulo.\n      */\n     template <class T, class A>\n-    inline auto mod(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x % y)\n+    XSIMD_INLINE auto mod(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x % y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x % y;\n@@ -1538,7 +1538,7 @@ namespace xsimd\n      * @return the result of the product.\n      */\n     template <class T, class A>\n-    inline auto mul(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x * y)\n+    XSIMD_INLINE auto mul(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x * y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x * y;\n@@ -1553,7 +1553,7 @@ namespace xsimd\n      * @return the batch of nearest integer values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> nearbyint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::nearbyint<A>(x, A {});\n@@ -1570,7 +1570,7 @@ namespace xsimd\n      * @warning For very large values the conversion to int silently overflows.\n      */\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A>\n+    XSIMD_INLINE batch<as_integer_t<T>, A>\n     nearbyint_as_int(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -1586,7 +1586,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto neq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x != y)\n+    XSIMD_INLINE auto neq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x != y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x != y;\n@@ -1601,7 +1601,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto neq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x != y)\n+    XSIMD_INLINE auto neq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x != y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x != y;\n@@ -1615,7 +1615,7 @@ namespace xsimd\n      * @return the opposite of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> neg(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> neg(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return -x;\n@@ -1631,7 +1631,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::nextafter<A>(x, y, A {});\n@@ -1645,7 +1645,7 @@ namespace xsimd\n      * @return the norm of \\c x.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::norm(x, A {});\n@@ -1660,7 +1660,7 @@ namespace xsimd\n      * @return \\c r exp(i * \\c theta).\n      */\n     template <class T, class A>\n-    inline complex_batch_type_t<batch<T, A>> polar(batch<T, A> const& r, batch<T, A> const& theta = batch<T, A> {}) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> polar(batch<T, A> const& r, batch<T, A> const& theta = batch<T, A> {}) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::polar<A>(r, theta, A {});\n@@ -1674,7 +1674,7 @@ namespace xsimd\n      * @return \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> pos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> pos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return +x;\n@@ -1690,7 +1690,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::pow<A>(x, y, A {});\n@@ -1706,7 +1706,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class ITy, class A, class = typename std::enable_if<std::is_integral<ITy>::value, void>::type>\n-    inline batch<T, A> pow(batch<T, A> const& x, ITy y) noexcept\n+    XSIMD_INLINE batch<T, A> pow(batch<T, A> const& x, ITy y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ipow<A>(x, y, A {});\n@@ -1720,7 +1720,7 @@ namespace xsimd\n      * @return the projection of \\c z.\n      */\n     template <class T, class A>\n-    inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::proj(z, A {});\n@@ -1734,7 +1734,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> real(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> real(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::real<A>(z, A {});\n@@ -1750,7 +1750,7 @@ namespace xsimd\n      * @return the reciprocal.\n      */\n     template <class T, class A, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-    inline batch<T, A> reciprocal(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> reciprocal(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reciprocal(x, A {});\n@@ -1765,7 +1765,7 @@ namespace xsimd\n      * @return the result of the reduction, as a scalar.\n      */\n     template <class T, class A, class F>\n-    inline T reduce(F&& f, batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce(F&& f, batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::detail::reduce(std::forward<F>(f), x, std::integral_constant<unsigned, batch<T, A>::size>());\n@@ -1779,7 +1779,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_add(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_add(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_add<A>(x, A {});\n@@ -1793,7 +1793,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_max(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_max(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_max<A>(x, A {});\n@@ -1807,7 +1807,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_min(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_min(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_min<A>(x, A {});\n@@ -1822,7 +1822,7 @@ namespace xsimd\n      * @return the result of the addition.\n      */\n     template <class T, class A>\n-    inline batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::remainder<A>(x, y, A {});\n@@ -1837,7 +1837,7 @@ namespace xsimd\n      * @return the batch of rounded values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return nearbyint(x);\n@@ -1855,7 +1855,7 @@ namespace xsimd\n      * @return rotated batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> rotate_left(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rotate_left(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotate_left<N, A>(x, A {});\n@@ -1873,7 +1873,7 @@ namespace xsimd\n      * @return rotated batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> rotate_right(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotate_right<N, A>(x, A {});\n@@ -1889,13 +1889,13 @@ namespace xsimd\n      * @return rotated \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rotl(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotl<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> rotl(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotl<A>(x, shift, A {});\n@@ -1911,13 +1911,13 @@ namespace xsimd\n      * @return rotated \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rotr(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotr<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> rotr(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotr<A>(x, shift, A {});\n@@ -1933,7 +1933,7 @@ namespace xsimd\n      * @return the batch of nearest integer values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> round(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> round(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::round<A>(x, A {});\n@@ -1951,7 +1951,7 @@ namespace xsimd\n      * @return the inverse square root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rsqrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rsqrt<A>(x, A {});\n@@ -1968,7 +1968,7 @@ namespace xsimd\n      * @return the result of the saturated addition.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sadd(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sadd<A>(x, y, A {});\n@@ -1989,7 +1989,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A>\n-    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2010,7 +2010,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2031,7 +2031,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A, bool... Values>\n-    inline batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2054,7 +2054,7 @@ namespace xsimd\n      * @return combined batch\n      */\n     template <class T, class A, class Vt, Vt... Values>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n@@ -2070,7 +2070,7 @@ namespace xsimd\n      * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n      */\n     template <class T, class A>\n-    inline batch<T, A> sign(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sign(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sign<A>(x, A {});\n@@ -2084,7 +2084,7 @@ namespace xsimd\n      * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n      */\n     template <class T, class A>\n-    inline batch<T, A> signnz(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::signnz<A>(x, A {});\n@@ -2098,7 +2098,7 @@ namespace xsimd\n      * @return the sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sin(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sin(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sin<A>(x, A {});\n@@ -2113,7 +2113,7 @@ namespace xsimd\n      * @return a pair containing the sine then the cosine of  batch \\c x\n      */\n     template <class T, class A>\n-    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sincos<A>(x, A {});\n@@ -2127,7 +2127,7 @@ namespace xsimd\n      * @return the hyperbolic sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sinh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sinh<A>(x, A {});\n@@ -2144,7 +2144,7 @@ namespace xsimd\n      * @return slided batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> slide_left(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x) noexcept\n     {\n         static_assert(std::is_integral<T>::value, \"can only slide batch of integers\");\n         detail::static_check_supported_config<T, A>();\n@@ -2162,7 +2162,7 @@ namespace xsimd\n      * @return slided batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> slide_right(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x) noexcept\n     {\n         static_assert(std::is_integral<T>::value, \"can only slide batch of integers\");\n         detail::static_check_supported_config<T, A>();\n@@ -2177,7 +2177,7 @@ namespace xsimd\n      * @return the square root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sqrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sqrt<A>(x, A {});\n@@ -2193,7 +2193,7 @@ namespace xsimd\n      * @return the result of the saturated difference.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ssub(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ssub<A>(x, y, A {});\n@@ -2208,29 +2208,29 @@ namespace xsimd\n      * @param src the batch to copy\n      */\n     template <class To, class A = default_arch, class From>\n-    inline void store_as(To* dst, batch<From, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(To* dst, batch<From, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store_aligned<A>(dst, src, A {});\n     }\n \n     template <class A = default_arch, class From>\n-    inline void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store<A>(src, dst, A {});\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         kernel::store_complex_aligned<A>(dst, src, A {});\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n     {\n         store_as(reinterpret_cast<std::complex<To>*>(dst), src, aligned_mode());\n     }\n@@ -2245,29 +2245,29 @@ namespace xsimd\n      * @param src the batch to copy\n      */\n     template <class To, class A = default_arch, class From>\n-    inline void store_as(To* dst, batch<From, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(To* dst, batch<From, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store_unaligned<A>(dst, src, A {});\n     }\n \n     template <class A = default_arch, class From>\n-    inline void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store<A>(src, dst, A {});\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         kernel::store_complex_unaligned<A>(dst, src, A {});\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         store_as(reinterpret_cast<std::complex<To>*>(dst), src, unaligned_mode());\n@@ -2283,7 +2283,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store(T* mem, batch<T, A> const& val, aligned_mode = {}) noexcept\n+    XSIMD_INLINE void store(T* mem, batch<T, A> const& val, aligned_mode = {}) noexcept\n     {\n         store_as<T, A>(mem, val, aligned_mode {});\n     }\n@@ -2297,7 +2297,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store(T* mem, batch<T, A> const& val, unaligned_mode) noexcept\n+    XSIMD_INLINE void store(T* mem, batch<T, A> const& val, unaligned_mode) noexcept\n     {\n         store_as<T, A>(mem, val, unaligned_mode {});\n     }\n@@ -2311,7 +2311,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store_aligned(T* mem, batch<T, A> const& val) noexcept\n+    XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& val) noexcept\n     {\n         store_as<T, A>(mem, val, aligned_mode {});\n     }\n@@ -2325,7 +2325,7 @@ namespace xsimd\n      * @param val the batch to copy\n      */\n     template <class A, class T>\n-    inline void store_unaligned(T* mem, batch<T, A> const& val) noexcept\n+    XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& val) noexcept\n     {\n         store_as<T, A>(mem, val, unaligned_mode {});\n     }\n@@ -2340,7 +2340,7 @@ namespace xsimd\n      * @return the difference between \\c x and \\c y\n      */\n     template <class T, class A>\n-    inline auto sub(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x - y)\n+    XSIMD_INLINE auto sub(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x - y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x - y;\n@@ -2356,15 +2356,15 @@ namespace xsimd\n      * @return swizzled batch\n      */\n     template <class T, class A, class Vt, Vt... Values>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     swizzle(batch<T, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n         return kernel::swizzle<A>(x, mask, A {});\n     }\n     template <class T, class A, class Vt, Vt... Values>\n-    inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n@@ -2381,7 +2381,7 @@ namespace xsimd\n      * @return swizzled batch\n      */\n     template <class T, class A, class Vt>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     swizzle(batch<T, A> const& x, batch<Vt, A> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n@@ -2390,7 +2390,7 @@ namespace xsimd\n     }\n \n     template <class T, class A, class Vt>\n-    inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch<Vt, A> mask) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch<Vt, A> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n@@ -2405,7 +2405,7 @@ namespace xsimd\n      * @return the tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tan<A>(x, A {});\n@@ -2419,7 +2419,7 @@ namespace xsimd\n      * @return the hyperbolic tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tanh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tanh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tanh<A>(x, A {});\n@@ -2433,7 +2433,7 @@ namespace xsimd\n      * @return the gamma function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tgamma(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tgamma(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tgamma<A>(x, A {});\n@@ -2448,7 +2448,7 @@ namespace xsimd\n      * @return \\c i converted to a value of an floating point type of the same size as \\c T\n      */\n     template <class T, class A>\n-    inline batch<as_float_t<T>, A> to_float(batch<T, A> const& i) noexcept\n+    XSIMD_INLINE batch<as_float_t<T>, A> to_float(batch<T, A> const& i) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch_cast<as_float_t<T>>(i);\n@@ -2463,7 +2463,7 @@ namespace xsimd\n      * @return \\c x converted to a value of an integer type of the same size as \\c T\n      */\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch_cast<as_integer_t<T>>(x);\n@@ -2478,7 +2478,7 @@ namespace xsimd\n      * @return the batch of nearest integer values not greater in magnitude than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> trunc(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::trunc<A>(x, A {});\n@@ -2494,7 +2494,7 @@ namespace xsimd\n      * @return a batch of the high part of shuffled values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::zip_hi<A>(x, y, A {});\n@@ -2510,7 +2510,7 @@ namespace xsimd\n      * @return a batch of the low part of shuffled values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::zip_lo<A>(x, y, A {});\n@@ -2527,15 +2527,15 @@ namespace xsimd\n      * @return \\c self cast to a \\c batch of \\c T\n      */\n     template <class T, class A, typename std::enable_if<std::is_integral<T>::value, int>::type = 3>\n-    inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n     {\n         T z(0);\n         detail::static_check_supported_config<T, A>();\n         return select(self, batch<T, A>(T(~z)), batch<T, A>(z));\n     }\n \n     template <class T, class A, typename std::enable_if<std::is_floating_point<T>::value, int>::type = 3>\n-    inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n     {\n         T z0(0), z1(0);\n         using int_type = as_unsigned_integer_t<T>;\n@@ -2554,7 +2554,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool all(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool all(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::all<A>(x, A {});\n@@ -2569,7 +2569,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool any(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool any(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::any<A>(x, A {});\n@@ -2584,7 +2584,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool none(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool none(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return !xsimd::any(x);\n@@ -2599,7 +2599,7 @@ namespace xsimd\n      * @return a reference to \\c o\n      */\n     template <class T, class A>\n-    inline std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) noexcept\n+    XSIMD_INLINE std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         constexpr auto size = batch<T, A>::size;\n@@ -2620,7 +2620,7 @@ namespace xsimd\n      * @return a reference to \\c o\n      */\n     template <class T, class A>\n-    inline std::ostream& operator<<(std::ostream& o, batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE std::ostream& operator<<(std::ostream& o, batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         constexpr auto size = batch_bool<T, A>::size;\n--- include/xsimd/types/xsimd_batch.hpp\n@@ -29,38 +29,38 @@ namespace xsimd\n         template <class T, class A>\n         struct integral_only_operators\n         {\n-            inline batch<T, A>& operator%=(batch<T, A> const& other) noexcept;\n-            inline batch<T, A>& operator>>=(int32_t other) noexcept;\n-            inline batch<T, A>& operator>>=(batch<T, A> const& other) noexcept;\n-            inline batch<T, A>& operator<<=(int32_t other) noexcept;\n-            inline batch<T, A>& operator<<=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator%=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator>>=(int32_t other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator>>=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator<<=(int32_t other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator<<=(batch<T, A> const& other) noexcept;\n \n             /** Shorthand for xsimd::mod() */\n-            friend inline batch<T, A> operator%(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator%(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) %= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_rshift() */\n-            friend inline batch<T, A> operator>>(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator>>(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) >>= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_lshift() */\n-            friend inline batch<T, A> operator<<(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator<<(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) <<= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_rshift() */\n-            friend inline batch<T, A> operator>>(batch<T, A> const& self, int32_t other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator>>(batch<T, A> const& self, int32_t other) noexcept\n             {\n                 return batch<T, A>(self) >>= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_lshift() */\n-            friend inline batch<T, A> operator<<(batch<T, A> const& self, int32_t other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator<<(batch<T, A> const& self, int32_t other) noexcept\n             {\n                 return batch<T, A>(self) <<= other;\n             }\n@@ -82,22 +82,22 @@ namespace xsimd\n         // with batch<T, A>. Their implementation must appear only once the\n         // kernel implementations have been included.\n         template <class T, class A>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n     }\n \n     /**\n@@ -123,152 +123,152 @@ namespace xsimd\n         using batch_bool_type = batch_bool<T, A>; ///< Associated batch type used to represented logical operations on this batch.\n \n         // constructors\n-        inline batch() = default; ///< Create a batch initialized with undefined values.\n-        inline batch(T val) noexcept;\n+        XSIMD_INLINE batch() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch(T val) noexcept;\n         template <class... Ts>\n-        inline batch(T val0, T val1, Ts... vals) noexcept;\n-        inline explicit batch(batch_bool_type const& b) noexcept;\n-        inline batch(register_type reg) noexcept;\n+        XSIMD_INLINE batch(T val0, T val1, Ts... vals) noexcept;\n+        XSIMD_INLINE explicit batch(batch_bool_type const& b) noexcept;\n+        XSIMD_INLINE batch(register_type reg) noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch broadcast(U val) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch broadcast(U val) noexcept;\n \n         // memory operators\n         template <class U>\n-        inline void store_aligned(U* mem) const noexcept;\n+        XSIMD_INLINE void store_aligned(U* mem) const noexcept;\n         template <class U>\n-        inline void store_unaligned(U* mem) const noexcept;\n+        XSIMD_INLINE void store_unaligned(U* mem) const noexcept;\n         template <class U>\n-        inline void store(U* mem, aligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, aligned_mode) const noexcept;\n         template <class U>\n-        inline void store(U* mem, unaligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, unaligned_mode) const noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load_aligned(U const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(U const* mem) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(U const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(U const* mem) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, aligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, aligned_mode) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, unaligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, unaligned_mode) noexcept;\n \n         template <class U, class V>\n-        XSIMD_NO_DISCARD static inline batch gather(U const* src, batch<V, arch_type> const& index) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch gather(U const* src, batch<V, arch_type> const& index) noexcept;\n         template <class U, class V>\n-        inline void scatter(U* dst, batch<V, arch_type> const& index) const noexcept;\n+        XSIMD_INLINE void scatter(U* dst, batch<V, arch_type> const& index) const noexcept;\n \n-        inline T get(std::size_t i) const noexcept;\n+        XSIMD_INLINE T get(std::size_t i) const noexcept;\n \n         // comparison operators. Defined as friend to enable automatic\n         // conversion of parameters from scalar to batch, at the cost of using a\n         // proxy implementation from details::.\n-        friend inline batch_bool<T, A> operator==(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator==(batch const& self, batch const& other) noexcept\n         {\n             return details::eq<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator!=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator!=(batch const& self, batch const& other) noexcept\n         {\n             return details::neq<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator>=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator>=(batch const& self, batch const& other) noexcept\n         {\n             return details::ge<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator<=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator<=(batch const& self, batch const& other) noexcept\n         {\n             return details::le<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator>(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator>(batch const& self, batch const& other) noexcept\n         {\n             return details::gt<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator<(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator<(batch const& self, batch const& other) noexcept\n         {\n             return details::lt<T, A>(self, other);\n         }\n \n         // Update operators\n-        inline batch& operator+=(batch const& other) noexcept;\n-        inline batch& operator-=(batch const& other) noexcept;\n-        inline batch& operator*=(batch const& other) noexcept;\n-        inline batch& operator/=(batch const& other) noexcept;\n-        inline batch& operator&=(batch const& other) noexcept;\n-        inline batch& operator|=(batch const& other) noexcept;\n-        inline batch& operator^=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator+=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator-=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator*=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator/=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator&=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator|=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator^=(batch const& other) noexcept;\n \n         // incr/decr operators\n-        inline batch& operator++() noexcept;\n-        inline batch& operator--() noexcept;\n-        inline batch operator++(int) noexcept;\n-        inline batch operator--(int) noexcept;\n+        XSIMD_INLINE batch& operator++() noexcept;\n+        XSIMD_INLINE batch& operator--() noexcept;\n+        XSIMD_INLINE batch operator++(int) noexcept;\n+        XSIMD_INLINE batch operator--(int) noexcept;\n \n         // unary operators\n-        inline batch_bool_type operator!() const noexcept;\n-        inline batch operator~() const noexcept;\n-        inline batch operator-() const noexcept;\n-        inline batch operator+() const noexcept;\n+        XSIMD_INLINE batch_bool_type operator!() const noexcept;\n+        XSIMD_INLINE batch operator~() const noexcept;\n+        XSIMD_INLINE batch operator-() const noexcept;\n+        XSIMD_INLINE batch operator+() const noexcept;\n \n         // arithmetic operators. They are defined as friend to enable automatic\n         // conversion of parameters from scalar to batch. Inline implementation\n         // is required to avoid warnings.\n \n         /** Shorthand for xsimd::add() */\n-        friend inline batch operator+(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator+(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) += other;\n         }\n \n         /** Shorthand for xsimd::sub() */\n-        friend inline batch operator-(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator-(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) -= other;\n         }\n \n         /** Shorthand for xsimd::mul() */\n-        friend inline batch operator*(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator*(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) *= other;\n         }\n \n         /** Shorthand for xsimd::div() */\n-        friend inline batch operator/(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator/(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) /= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_and() */\n-        friend inline batch operator&(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator&(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) &= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_or() */\n-        friend inline batch operator|(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator|(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) |= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_xor() */\n-        friend inline batch operator^(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator^(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) ^= other;\n         }\n \n         /** Shorthand for xsimd::logical_and() */\n-        friend inline batch operator&&(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator&&(batch const& self, batch const& other) noexcept\n         {\n             return batch(self).logical_and(other);\n         }\n \n         /** Shorthand for xsimd::logical_or() */\n-        friend inline batch operator||(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator||(batch const& self, batch const& other) noexcept\n         {\n             return batch(self).logical_or(other);\n         }\n \n     private:\n-        inline batch logical_and(batch const& other) const noexcept;\n-        inline batch logical_or(batch const& other) const noexcept;\n+        XSIMD_INLINE batch logical_and(batch const& other) const noexcept;\n+        XSIMD_INLINE batch logical_or(batch const& other) const noexcept;\n     };\n \n     template <class T, class A>\n@@ -297,51 +297,51 @@ namespace xsimd\n         using batch_type = batch<T, A>; ///< Associated batch type this batch represents logical operations for.\n \n         // constructors\n-        inline batch_bool() = default; ///< Create a batch initialized with undefined values.\n-        inline batch_bool(bool val) noexcept;\n-        inline batch_bool(register_type reg) noexcept;\n+        XSIMD_INLINE batch_bool() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch_bool(bool val) noexcept;\n+        XSIMD_INLINE batch_bool(register_type reg) noexcept;\n         template <class... Ts>\n-        inline batch_bool(bool val0, bool val1, Ts... vals) noexcept;\n+        XSIMD_INLINE batch_bool(bool val0, bool val1, Ts... vals) noexcept;\n \n         template <class Tp>\n-        inline batch_bool(Tp const*) = delete;\n+        XSIMD_INLINE batch_bool(Tp const*) = delete;\n \n         // memory operators\n-        inline void store_aligned(bool* mem) const noexcept;\n-        inline void store_unaligned(bool* mem) const noexcept;\n-        XSIMD_NO_DISCARD static inline batch_bool load_aligned(bool const* mem) noexcept;\n-        XSIMD_NO_DISCARD static inline batch_bool load_unaligned(bool const* mem) noexcept;\n+        XSIMD_INLINE void store_aligned(bool* mem) const noexcept;\n+        XSIMD_INLINE void store_unaligned(bool* mem) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch_bool load_aligned(bool const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch_bool load_unaligned(bool const* mem) noexcept;\n \n-        inline bool get(std::size_t i) const noexcept;\n+        XSIMD_INLINE bool get(std::size_t i) const noexcept;\n \n         // mask operations\n-        inline uint64_t mask() const noexcept;\n-        inline static batch_bool from_mask(uint64_t mask) noexcept;\n+        XSIMD_INLINE uint64_t mask() const noexcept;\n+        XSIMD_INLINE static batch_bool from_mask(uint64_t mask) noexcept;\n \n         // comparison operators\n-        inline batch_bool operator==(batch_bool const& other) const noexcept;\n-        inline batch_bool operator!=(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator==(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator!=(batch_bool const& other) const noexcept;\n \n         // logical operators\n-        inline batch_bool operator~() const noexcept;\n-        inline batch_bool operator!() const noexcept;\n-        inline batch_bool operator&(batch_bool const& other) const noexcept;\n-        inline batch_bool operator|(batch_bool const& other) const noexcept;\n-        inline batch_bool operator^(batch_bool const& other) const noexcept;\n-        inline batch_bool operator&&(batch_bool const& other) const noexcept;\n-        inline batch_bool operator||(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator~() const noexcept;\n+        XSIMD_INLINE batch_bool operator!() const noexcept;\n+        XSIMD_INLINE batch_bool operator&(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator|(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator^(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator&&(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator||(batch_bool const& other) const noexcept;\n \n         // update operators\n-        inline batch_bool& operator&=(batch_bool const& other) noexcept { return (*this) = (*this) & other; }\n-        inline batch_bool& operator|=(batch_bool const& other) noexcept { return (*this) = (*this) | other; }\n-        inline batch_bool& operator^=(batch_bool const& other) noexcept { return (*this) = (*this) ^ other; }\n+        XSIMD_INLINE batch_bool& operator&=(batch_bool const& other) noexcept { return (*this) = (*this) & other; }\n+        XSIMD_INLINE batch_bool& operator|=(batch_bool const& other) noexcept { return (*this) = (*this) | other; }\n+        XSIMD_INLINE batch_bool& operator^=(batch_bool const& other) noexcept { return (*this) = (*this) ^ other; }\n \n     private:\n         template <class U, class... V, size_t I, size_t... Is>\n-        static inline register_type make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept;\n+        static XSIMD_INLINE register_type make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept;\n \n         template <class... V>\n-        static inline register_type make_register(detail::index_sequence<>, V... v) noexcept;\n+        static XSIMD_INLINE register_type make_register(detail::index_sequence<>, V... v) noexcept;\n     };\n \n     template <class T, class A>\n@@ -367,106 +367,106 @@ namespace xsimd\n         static constexpr std::size_t size = real_batch::size; ///< Number of complex elements in this batch.\n \n         // constructors\n-        inline batch() = default; ///< Create a batch initialized with undefined values.\n-        inline batch(value_type const& val) noexcept;\n-        inline batch(real_batch const& real, real_batch const& imag) noexcept;\n+        XSIMD_INLINE batch() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch(value_type const& val) noexcept;\n+        XSIMD_INLINE batch(real_batch const& real, real_batch const& imag) noexcept;\n \n-        inline batch(real_batch const& real) noexcept;\n-        inline batch(T val) noexcept;\n+        XSIMD_INLINE batch(real_batch const& real) noexcept;\n+        XSIMD_INLINE batch(T val) noexcept;\n         template <class... Ts>\n-        inline batch(value_type val0, value_type val1, Ts... vals) noexcept;\n-        inline explicit batch(batch_bool_type const& b) noexcept;\n+        XSIMD_INLINE batch(value_type val0, value_type val1, Ts... vals) noexcept;\n+        XSIMD_INLINE explicit batch(batch_bool_type const& b) noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch broadcast(U val) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch broadcast(U val) noexcept;\n \n         // memory operators\n-        XSIMD_NO_DISCARD static inline batch load_aligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n-        inline void store_aligned(T* real_dst, T* imag_dst) const noexcept;\n-        inline void store_unaligned(T* real_dst, T* imag_dst) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n+        XSIMD_INLINE void store_aligned(T* real_dst, T* imag_dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(T* real_dst, T* imag_dst) const noexcept;\n \n-        XSIMD_NO_DISCARD static inline batch load_aligned(const value_type* src) noexcept;\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const value_type* src) noexcept;\n-        inline void store_aligned(value_type* dst) const noexcept;\n-        inline void store_unaligned(value_type* dst) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const value_type* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const value_type* src) noexcept;\n+        XSIMD_INLINE void store_aligned(value_type* dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(value_type* dst) const noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, aligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, aligned_mode) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, unaligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, unaligned_mode) noexcept;\n         template <class U>\n-        inline void store(U* mem, aligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, aligned_mode) const noexcept;\n         template <class U>\n-        inline void store(U* mem, unaligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, unaligned_mode) const noexcept;\n \n-        inline real_batch real() const noexcept;\n-        inline real_batch imag() const noexcept;\n+        XSIMD_INLINE real_batch real() const noexcept;\n+        XSIMD_INLINE real_batch imag() const noexcept;\n \n-        inline value_type get(std::size_t i) const noexcept;\n+        XSIMD_INLINE value_type get(std::size_t i) const noexcept;\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n         // xtl-related methods\n         template <bool i3ec>\n-        inline batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept;\n+        XSIMD_INLINE batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept;\n         template <bool i3ec, class... Ts>\n-        inline batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept;\n+        XSIMD_INLINE batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept;\n \n         template <bool i3ec>\n-        XSIMD_NO_DISCARD static inline batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n         template <bool i3ec>\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n         template <bool i3ec>\n-        inline void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n+        XSIMD_INLINE void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n         template <bool i3ec>\n-        inline void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n #endif\n \n         // comparison operators\n-        inline batch_bool<T, A> operator==(batch const& other) const noexcept;\n-        inline batch_bool<T, A> operator!=(batch const& other) const noexcept;\n+        XSIMD_INLINE batch_bool<T, A> operator==(batch const& other) const noexcept;\n+        XSIMD_INLINE batch_bool<T, A> operator!=(batch const& other) const noexcept;\n \n         // Update operators\n-        inline batch& operator+=(batch const& other) noexcept;\n-        inline batch& operator-=(batch const& other) noexcept;\n-        inline batch& operator*=(batch const& other) noexcept;\n-        inline batch& operator/=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator+=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator-=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator*=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator/=(batch const& other) noexcept;\n \n         // incr/decr operators\n-        inline batch& operator++() noexcept;\n-        inline batch& operator--() noexcept;\n-        inline batch operator++(int) noexcept;\n-        inline batch operator--(int) noexcept;\n+        XSIMD_INLINE batch& operator++() noexcept;\n+        XSIMD_INLINE batch& operator--() noexcept;\n+        XSIMD_INLINE batch operator++(int) noexcept;\n+        XSIMD_INLINE batch operator--(int) noexcept;\n \n         // unary operators\n-        inline batch_bool_type operator!() const noexcept;\n-        inline batch operator~() const noexcept;\n-        inline batch operator-() const noexcept;\n-        inline batch operator+() const noexcept;\n+        XSIMD_INLINE batch_bool_type operator!() const noexcept;\n+        XSIMD_INLINE batch operator~() const noexcept;\n+        XSIMD_INLINE batch operator-() const noexcept;\n+        XSIMD_INLINE batch operator+() const noexcept;\n \n         // arithmetic operators. They are defined as friend to enable automatic\n         // conversion of parameters from scalar to batch\n \n         /** Shorthand for xsimd::add() */\n-        friend inline batch operator+(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator+(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) += other;\n         }\n \n         /** Shorthand for xsimd::sub() */\n-        friend inline batch operator-(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator-(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) -= other;\n         }\n \n         /** Shorthand for xsimd::mul() */\n-        friend inline batch operator*(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator*(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) *= other;\n         }\n \n         /** Shorthand for xsimd::div() */\n-        friend inline batch operator/(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator/(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) /= other;\n         }\n@@ -500,7 +500,7 @@ namespace xsimd\n      * Create a batch with all element initialized to \\c val.\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(T val) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(T val) noexcept\n         : types::simd_register<T, A>(kernel::broadcast<A>(val, A {}))\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -512,7 +512,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class... Ts>\n-    inline batch<T, A>::batch(T val0, T val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(T val0, T val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<T>(vals)...))\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -525,7 +525,7 @@ namespace xsimd\n      * (resp. `false`).\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(batch_bool<T, A> const& b) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(batch_bool<T, A> const& b) noexcept\n         : batch(kernel::from_bool(b, A {}))\n     {\n     }\n@@ -535,7 +535,7 @@ namespace xsimd\n      * becomes handy when doing architecture-specific operations.\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(register_type reg) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(register_type reg) noexcept\n         : types::simd_register<T, A>({ reg })\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -546,7 +546,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    XSIMD_NO_DISCARD inline batch<T, A> batch<T, A>::broadcast(U val) noexcept\n+    XSIMD_NO_DISCARD XSIMD_INLINE batch<T, A> batch<T, A>::broadcast(U val) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch(static_cast<T>(val));\n@@ -562,7 +562,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store_aligned(U* mem) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store_aligned(U* mem) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         assert(((reinterpret_cast<uintptr_t>(mem) % A::alignment()) == 0)\n@@ -576,7 +576,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store_unaligned(U* mem) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store_unaligned(U* mem) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         kernel::store_unaligned<A>(mem, *this, A {});\n@@ -587,7 +587,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store(U* mem, aligned_mode) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store(U* mem, aligned_mode) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return store_aligned(mem);\n@@ -598,7 +598,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store(U* mem, unaligned_mode) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store(U* mem, unaligned_mode) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return store_unaligned(mem);\n@@ -610,7 +610,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load_aligned(U const* mem) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load_aligned(U const* mem) noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(mem) % A::alignment()) == 0)\n                && \"loaded pointer is not properly aligned\");\n@@ -624,7 +624,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load_unaligned(U const* mem) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load_unaligned(U const* mem) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::load_unaligned<A>(mem, kernel::convert<T> {}, A {});\n@@ -635,7 +635,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load(U const* mem, aligned_mode) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load(U const* mem, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return load_aligned(mem);\n@@ -646,7 +646,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return load_unaligned(mem);\n@@ -660,7 +660,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <typename U, typename V>\n-    inline batch<T, A> batch<T, A>::gather(U const* src, batch<V, A> const& index) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::gather(U const* src, batch<V, A> const& index) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         static_assert(std::is_convertible<T, U>::value, \"Can't convert from src to this batch's type!\");\n@@ -675,7 +675,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U, class V>\n-    inline void batch<T, A>::scatter(U* dst, batch<V, A> const& index) const noexcept\n+    XSIMD_INLINE void batch<T, A>::scatter(U* dst, batch<V, A> const& index) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         static_assert(std::is_convertible<T, U>::value, \"Can't convert from this batch's type to dst!\");\n@@ -688,7 +688,7 @@ namespace xsimd\n      * \\c warning This is very inefficient and should only be used for debugging purpose.\n      */\n     template <class T, class A>\n-    inline T batch<T, A>::get(std::size_t i) const noexcept\n+    XSIMD_INLINE T batch<T, A>::get(std::size_t i) const noexcept\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -702,7 +702,7 @@ namespace xsimd\n          * Shorthand for xsimd::eq()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::eq<A>(self, other, A {});\n@@ -712,7 +712,7 @@ namespace xsimd\n          * Shorthand for xsimd::neq()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::neq<A>(self, other, A {});\n@@ -722,7 +722,7 @@ namespace xsimd\n          * Shorthand for xsimd::ge()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::ge<A>(self, other, A {});\n@@ -732,7 +732,7 @@ namespace xsimd\n          * Shorthand for xsimd::le()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::le<A>(self, other, A {});\n@@ -742,7 +742,7 @@ namespace xsimd\n          * Shorthand for xsimd::gt()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::gt<A>(self, other, A {});\n@@ -752,7 +752,7 @@ namespace xsimd\n          * Shorthand for xsimd::lt()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::lt<A>(self, other, A {});\n@@ -764,84 +764,84 @@ namespace xsimd\n      **************************/\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::add<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::sub<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::mul<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::div<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& types::integral_only_operators<T, A>::operator%=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& types::integral_only_operators<T, A>::operator%=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::mod<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_and<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_or<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_xor<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_rshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_lshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(int32_t other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(int32_t other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_rshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(int32_t other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(int32_t other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_lshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n@@ -852,21 +852,21 @@ namespace xsimd\n      *****************************/\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator++() noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator++() noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return operator+=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator--() noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator--() noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return operator-=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator++(int) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator++(int) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         batch<T, A> copy(*this);\n@@ -875,7 +875,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator--(int) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator--(int) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         batch copy(*this);\n@@ -888,28 +888,28 @@ namespace xsimd\n      *************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<T, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<T, A>::operator!() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::eq<A>(*this, batch(0), A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator~() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator~() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(*this, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator-() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator-() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::neg<A>(*this, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator+() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator+() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this;\n@@ -920,13 +920,13 @@ namespace xsimd\n      ************************/\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const noexcept\n     {\n         return kernel::logical_and<A>(*this, other, A());\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const noexcept\n     {\n         return kernel::logical_or<A>(*this, other, A());\n     }\n@@ -936,14 +936,14 @@ namespace xsimd\n      ***************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A>::batch_bool(register_type reg) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(register_type reg) noexcept\n         : types::get_bool_simd_register_t<T, A>({ reg })\n     {\n     }\n \n     template <class T, class A>\n     template <class... Ts>\n-    inline batch_bool<T, A>::batch_bool(bool val0, bool val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(bool val0, bool val1, Ts... vals) noexcept\n         : batch_bool(kernel::set<A>(batch_bool {}, A {}, val0, val1, static_cast<bool>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"The constructor requires as many arguments as batch elements.\");\n@@ -954,19 +954,19 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    inline void batch_bool<T, A>::store_aligned(bool* mem) const noexcept\n+    XSIMD_INLINE void batch_bool<T, A>::store_aligned(bool* mem) const noexcept\n     {\n         kernel::store(*this, mem, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch_bool<T, A>::store_unaligned(bool* mem) const noexcept\n+    XSIMD_INLINE void batch_bool<T, A>::store_unaligned(bool* mem) const noexcept\n     {\n         store_aligned(mem);\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem) noexcept\n     {\n         batch_type ref(0);\n         alignas(A::alignment()) T buffer[size];\n@@ -976,7 +976,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem) noexcept\n     {\n         return load_aligned(mem);\n     }\n@@ -987,7 +987,7 @@ namespace xsimd\n      * @return bit mask\n      */\n     template <class T, class A>\n-    inline uint64_t batch_bool<T, A>::mask() const noexcept\n+    XSIMD_INLINE uint64_t batch_bool<T, A>::mask() const noexcept\n     {\n         return kernel::mask(*this, A {});\n     }\n@@ -998,13 +998,13 @@ namespace xsimd\n      * @return bit mask\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::from_mask(uint64_t mask) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::from_mask(uint64_t mask) noexcept\n     {\n         return kernel::from_mask(batch_bool<T, A>(), mask, A {});\n     }\n \n     template <class T, class A>\n-    inline bool batch_bool<T, A>::get(std::size_t i) const noexcept\n+    XSIMD_INLINE bool batch_bool<T, A>::get(std::size_t i) const noexcept\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -1014,13 +1014,13 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::eq<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::neq<A>(*this, other, A {}).data;\n     }\n@@ -1030,43 +1030,43 @@ namespace xsimd\n      ********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator~() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator~() const noexcept\n     {\n         return kernel::bitwise_not<A>(*this, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator!() const noexcept\n     {\n         return operator==(batch_bool(false));\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_and<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_or<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator^(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator^(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_xor<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const noexcept\n     {\n         return operator&(other);\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const noexcept\n     {\n         return operator|(other);\n     }\n@@ -1076,21 +1076,21 @@ namespace xsimd\n      ******************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A>::batch_bool(bool val) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(bool val) noexcept\n         : base_type { make_register(detail::make_index_sequence<size - 1>(), val) }\n     {\n     }\n \n     template <class T, class A>\n     template <class U, class... V, size_t I, size_t... Is>\n-    inline auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept -> register_type\n+    XSIMD_INLINE auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept -> register_type\n     {\n         return make_register(detail::index_sequence<Is...>(), u, u, v...);\n     }\n \n     template <class T, class A>\n     template <class... V>\n-    inline auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) noexcept -> register_type\n+    XSIMD_INLINE auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) noexcept -> register_type\n     {\n         return kernel::set<A>(batch_bool<T, A>(), A {}, v...).data;\n     }\n@@ -1100,51 +1100,51 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(value_type const& val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(value_type const& val) noexcept\n         : m_real(val.real())\n         , m_imag(val.imag())\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag) noexcept\n         : m_real(real)\n         , m_imag(imag)\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(real_batch const& real) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(real_batch const& real) noexcept\n         : m_real(real)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(T val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(T val) noexcept\n         : m_real(val)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n     template <class... Ts>\n-    inline batch<std::complex<T>, A>::batch(value_type val0, value_type val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(value_type val0, value_type val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<value_type>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"as many arguments as batch elements\");\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(batch_bool_type const& b) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(batch_bool_type const& b) noexcept\n         : m_real(b)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n     template <class U>\n-    XSIMD_NO_DISCARD inline batch<std::complex<T>, A> batch<std::complex<T>, A>::broadcast(U val) noexcept\n+    XSIMD_NO_DISCARD XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::broadcast(U val) noexcept\n     {\n         return batch(static_cast<std::complex<T>>(val));\n     }\n@@ -1154,100 +1154,100 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src) noexcept\n     {\n         return { batch<T, A>::load_aligned(real_src), imag_src ? batch<T, A>::load_aligned(imag_src) : batch<T, A>(0) };\n     }\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src) noexcept\n     {\n         return { batch<T, A>::load_unaligned(real_src), imag_src ? batch<T, A>::load_unaligned(imag_src) : batch<T, A>(0) };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src) noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(src) % A::alignment()) == 0)\n                && \"loaded pointer is not properly aligned\");\n         return kernel::load_complex_aligned<A>(src, kernel::convert<value_type> {}, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src) noexcept\n     {\n         return kernel::load_complex_unaligned<A>(src, kernel::convert<value_type> {}, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_aligned(value_type* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(value_type* dst) const noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(dst) % A::alignment()) == 0)\n                && \"store location is not properly aligned\");\n         return kernel::store_complex_aligned(dst, *this, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const noexcept\n     {\n         return kernel::store_complex_unaligned(dst, *this, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const noexcept\n     {\n         m_real.store_aligned(real_dst);\n         m_imag.store_aligned(imag_dst);\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const noexcept\n     {\n         m_real.store_unaligned(real_dst);\n         m_imag.store_unaligned(imag_dst);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode) noexcept\n     {\n         return load_aligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode) noexcept\n     {\n         return load_unaligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline void batch<std::complex<T>, A>::store(U* mem, aligned_mode) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store(U* mem, aligned_mode) const noexcept\n     {\n         return store_aligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline void batch<std::complex<T>, A>::store(U* mem, unaligned_mode) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store(U* mem, unaligned_mode) const noexcept\n     {\n         return store_unaligned(mem);\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::real() const noexcept -> real_batch\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::real() const noexcept -> real_batch\n     {\n         return m_real;\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::imag() const noexcept -> real_batch\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::imag() const noexcept -> real_batch\n     {\n         return m_imag;\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::get(std::size_t i) const noexcept -> value_type\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::get(std::size_t i) const noexcept -> value_type\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -1260,15 +1260,15 @@ namespace xsimd\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept\n         : m_real(val.real())\n         , m_imag(val.imag())\n     {\n     }\n \n     template <class T, class A>\n     template <bool i3ec, class... Ts>\n-    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<xtl::xcomplex<T, T, i3ec>>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"as many arguments as batch elements\");\n@@ -1280,28 +1280,28 @@ namespace xsimd\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n     {\n         return load_aligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n     {\n         return load_unaligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n     {\n         store_aligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n     {\n         store_unaligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n@@ -1313,13 +1313,13 @@ namespace xsimd\n      ***************************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const noexcept\n     {\n         return m_real == other.m_real && m_imag == other.m_imag;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const noexcept\n     {\n         return m_real != other.m_real || m_imag != other.m_imag;\n     }\n@@ -1329,23 +1329,23 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other) noexcept\n     {\n         m_real += other.m_real;\n         m_imag += other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other) noexcept\n     {\n         m_real -= other.m_real;\n         m_imag -= other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other) noexcept\n     {\n         real_batch new_real = fms(real(), other.real(), imag() * other.imag());\n         real_batch new_imag = fma(real(), other.imag(), imag() * other.real());\n@@ -1355,7 +1355,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other) noexcept\n     {\n         real_batch a = real();\n         real_batch b = imag();\n@@ -1372,27 +1372,27 @@ namespace xsimd\n      **************************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++() noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++() noexcept\n     {\n         return operator+=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--() noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--() noexcept\n     {\n         return operator-=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int) noexcept\n     {\n         batch copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int) noexcept\n     {\n         batch copy(*this);\n         operator-=(1);\n@@ -1404,25 +1404,25 @@ namespace xsimd\n      **********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator!() const noexcept\n     {\n         return operator==(batch(0));\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const noexcept\n     {\n         return { ~m_real, ~m_imag };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const noexcept\n     {\n         return { -m_real, -m_imag };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const noexcept\n     {\n         return { +m_real, +m_imag };\n     }\n--- include/xsimd/types/xsimd_batch_constant.hpp\n@@ -138,12 +138,12 @@ namespace xsimd\n         /**\n          * @brief Generate a batch of @p batch_type from this @p batch_constant\n          */\n-        inline batch_type as_batch() const noexcept { return { Values... }; }\n+        XSIMD_INLINE batch_type as_batch() const noexcept { return { Values... }; }\n \n         /**\n          * @brief Generate a batch of @p batch_type from this @p batch_constant\n          */\n-        inline operator batch_type() const noexcept { return as_batch(); }\n+        XSIMD_INLINE operator batch_type() const noexcept { return as_batch(); }\n \n         /**\n          * @brief Get the @p i th element of this @p batch_constant\n@@ -246,13 +246,13 @@ namespace xsimd\n     namespace detail\n     {\n         template <typename T, class A, class G, std::size_t... Is>\n-        inline constexpr auto make_batch_constant(detail::index_sequence<Is...>) noexcept\n+        XSIMD_INLINE constexpr auto make_batch_constant(detail::index_sequence<Is...>) noexcept\n             -> batch_constant<T, A, (T)G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n         }\n         template <typename T, class A, class G, std::size_t... Is>\n-        inline constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>) noexcept\n+        XSIMD_INLINE constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>) noexcept\n             -> batch_bool_constant<T, A, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n@@ -281,13 +281,13 @@ namespace xsimd\n      * @endcode\n      */\n     template <typename T, class A, class G>\n-    inline constexpr auto make_batch_constant() noexcept -> decltype(detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>()))\n+    XSIMD_INLINE constexpr auto make_batch_constant() noexcept -> decltype(detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>()))\n     {\n         return detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>());\n     }\n \n     template <typename T, class A, class G>\n-    inline constexpr auto make_batch_bool_constant() noexcept\n+    XSIMD_INLINE constexpr auto make_batch_bool_constant() noexcept\n         -> decltype(detail::make_batch_bool_constant<T, A, G>(\n             detail::make_index_sequence<batch<T, A>::size>()))\n     {\n--- include/xsimd/types/xsimd_emulated_register.hpp\n@@ -55,7 +55,7 @@ namespace xsimd\n             static_assert(N % (8 * sizeof(T)) == 0, \"bit width must be a multiple of scalar width\");\n             using register_type = std::array<T, N / (8 * sizeof(T))>;\n             register_type data;\n-            inline operator register_type() const noexcept\n+            XSIMD_INLINE operator register_type() const noexcept\n             {\n                 return data;\n             }\n--- include/xsimd/types/xsimd_register.hpp\n@@ -37,7 +37,7 @@ namespace xsimd\n     {                                                              \\\n         using register_type = VECTOR_TYPE;                         \\\n         register_type data;                                        \\\n-        inline operator register_type() const noexcept             \\\n+        XSIMD_INLINE operator register_type() const noexcept       \\\n         {                                                          \\\n             return data;                                           \\\n         }                                                          \\\n--- include/xsimd/types/xsimd_rvv_register.hpp\n@@ -88,14 +88,14 @@ namespace xsimd\n         using byte_type = XSIMD_RVV_TYPE(u, 8, vmul);                                     \\\n         using fixed_type = type __attribute__((riscv_rvv_vector_bits(width)));            \\\n         template <class U>                                                                \\\n-        static inline type bitcast(U x) noexcept                                          \\\n+        static XSIMD_INLINE type bitcast(U x) noexcept                                    \\\n         {                                                                                 \\\n             const auto words = XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, s, m, vmul)(x); \\\n             return XSIMD_RVV_JOINT5(__riscv_vreinterpret_, t, s, m, vmul)(words);         \\\n         }                                                                                 \\\n         template <>                                                                       \\\n-        inline type bitcast<type>(type x) noexcept { return x; }                          \\\n-        static inline byte_type as_bytes(type x) noexcept                                 \\\n+        XSIMD_INLINE type bitcast<type>(type x) noexcept { return x; }                    \\\n+        static XSIMD_INLINE byte_type as_bytes(type x) noexcept                           \\\n         {                                                                                 \\\n             const auto words = XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, s, m, vmul)(x); \\\n             return XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, 8, m, vmul)(words);         \\\n@@ -267,17 +267,17 @@ namespace xsimd\n             //\n             template <size_t>\n             struct rvv_bool_info;\n-#define XSIMD_RVV_MAKE_BOOL_TYPE(i)                                                       \\\n-    template <>                                                                           \\\n-    struct rvv_bool_info<i>                                                               \\\n-    {                                                                                     \\\n-        using type = XSIMD_RVV_JOINT(vbool, i, _t);                                       \\\n-        template <class T>                                                                \\\n-        static inline type bitcast(T value) noexcept                                      \\\n-        {                                                                                 \\\n-            return XSIMD_RVV_JOINT(__riscv_vreinterpret_b, i, )(value);                   \\\n-        }                                                                                 \\\n-        /*template <> static inline type bitcast(type value) noexcept { return value; }*/ \\\n+#define XSIMD_RVV_MAKE_BOOL_TYPE(i)                                                             \\\n+    template <>                                                                                 \\\n+    struct rvv_bool_info<i>                                                                     \\\n+    {                                                                                           \\\n+        using type = XSIMD_RVV_JOINT(vbool, i, _t);                                             \\\n+        template <class T>                                                                      \\\n+        static XSIMD_INLINE type bitcast(T value) noexcept                                      \\\n+        {                                                                                       \\\n+            return XSIMD_RVV_JOINT(__riscv_vreinterpret_b, i, )(value);                         \\\n+        }                                                                                       \\\n+        /*template <> static XSIMD_INLINE type bitcast(type value) noexcept { return value; }*/ \\\n     };\n             XSIMD_RVV_MAKE_BOOL_TYPE(1);\n             XSIMD_RVV_MAKE_BOOL_TYPE(2);\n--- include/xsimd/types/xsimd_traits.hpp\n@@ -86,7 +86,7 @@ namespace xsimd\n \n         // consistency checker\n         template <class T, class A>\n-        inline void static_check_supported_config()\n+        XSIMD_INLINE void static_check_supported_config()\n         {\n             (void)static_check_supported_config_emitter<T, A>();\n         }\n--- include/xsimd/xsimd.hpp\n@@ -51,6 +51,7 @@\n #endif\n \n #include \"config/xsimd_config.hpp\"\n+#include \"config/xsimd_inline.hpp\"\n \n #include \"arch/xsimd_scalar.hpp\"\n #include \"memory/xsimd_aligned_allocator.hpp\""
    ],
    "files_changed": [
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_arithmetic.hpp",
        "status": "modified",
        "additions": 25,
        "deletions": 25,
        "changes": 50,
        "patch": "@@ -28,7 +28,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x << y; },\n@@ -37,7 +37,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x >> y; },\n@@ -46,21 +46,21 @@ namespace xsimd\n \n         // decr\n         template <class A, class T>\n-        inline batch<T, A> decr(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> decr(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self - T(1);\n         }\n \n         // decr_if\n         template <class A, class T, class Mask>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n         {\n             return select(mask, decr(self), self);\n         }\n \n         // div\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x / y; },\n@@ -69,13 +69,13 @@ namespace xsimd\n \n         // fma\n         template <class A, class T>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return x * y + z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n             auto res_i = fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n@@ -84,13 +84,13 @@ namespace xsimd\n \n         // fms\n         template <class A, class T>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return x * y - z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n             auto res_i = fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n@@ -99,13 +99,13 @@ namespace xsimd\n \n         // fnma\n         template <class A, class T>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return -x * y + z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fnma(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = -fms(x.real(), y.real(), fma(x.imag(), y.imag(), z.real()));\n             auto res_i = -fma(x.real(), y.imag(), fms(x.imag(), y.real(), z.imag()));\n@@ -114,13 +114,13 @@ namespace xsimd\n \n         // fnms\n         template <class A, class T>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<generic>) noexcept\n         {\n             return -x * y - z;\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> fnms(batch<std::complex<T>, A> const& x, batch<std::complex<T>, A> const& y, batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             auto res_r = -fms(x.real(), y.real(), fms(x.imag(), y.imag(), z.real()));\n             auto res_i = -fma(x.real(), y.imag(), fma(x.imag(), y.real(), z.imag()));\n@@ -129,7 +129,7 @@ namespace xsimd\n \n         // hadd\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(buffer);\n@@ -143,21 +143,21 @@ namespace xsimd\n \n         // incr\n         template <class A, class T>\n-        inline batch<T, A> incr(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> incr(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self + T(1);\n         }\n \n         // incr_if\n         template <class A, class T, class Mask>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, Mask const& mask, requires_arch<generic>) noexcept\n         {\n             return select(mask, incr(self), self);\n         }\n \n         // mul\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x * y; },\n@@ -166,28 +166,28 @@ namespace xsimd\n \n         // rotl\n         template <class A, class T, class STy>\n-        inline batch<T, A> rotl(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n         {\n             constexpr auto N = std::numeric_limits<T>::digits;\n             return (self << other) | (self >> (N - other));\n         }\n \n         // rotr\n         template <class A, class T, class STy>\n-        inline batch<T, A> rotr(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& self, STy other, requires_arch<generic>) noexcept\n         {\n             constexpr auto N = std::numeric_limits<T>::digits;\n             return (self >> other) | (self << (N - other));\n         }\n \n         // sadd\n         template <class A>\n-        inline batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> sadd(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return add(self, other); // no saturated arithmetic on floating point numbers\n         }\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -204,19 +204,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> sadd(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return add(self, other); // no saturated arithmetic on floating point numbers\n         }\n \n         // ssub\n         template <class A>\n-        inline batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> ssub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sub(self, other); // no saturated arithmetic on floating point numbers\n         }\n         template <class A, class T, class /*=typename std::enable_if<std::is_integral<T>::value, void>::type*/>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -229,7 +229,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> ssub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sub(self, other); // no saturated arithmetic on floating point numbers\n         }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_complex.hpp",
        "status": "modified",
        "additions": 11,
        "deletions": 11,
        "changes": 22,
        "patch": "@@ -26,54 +26,54 @@ namespace xsimd\n \n         // real\n         template <class A, class T>\n-        inline batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> real(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> real(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self.real();\n         }\n \n         // imag\n         template <class A, class T>\n-        inline batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> imag(batch<T, A> const& /*self*/, requires_arch<generic>) noexcept\n         {\n             return batch<T, A>(T(0));\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> imag(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self.imag();\n         }\n \n         // arg\n         template <class A, class T>\n-        inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE real_batch_type_t<batch<T, A>> arg(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return atan2(imag(self), real(self));\n         }\n \n         // conj\n         template <class A, class T>\n-        inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { real(self), -imag(self) };\n         }\n \n         // norm\n         template <class A, class T>\n-        inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE real_batch_type_t<batch<T, A>> norm(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { fma(real(self), real(self), imag(self) * imag(self)) };\n         }\n \n         // proj\n         template <class A, class T>\n-        inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = complex_batch_type_t<batch<T, A>>;\n             using real_batch = typename batch_type::real_batch;\n@@ -86,19 +86,19 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isnan(self.real()) || isnan(self.imag()));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isinf(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isinf(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isinf(self.real()) || isinf(self.imag()));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> isfinite(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isfinite(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(isfinite(self.real()) && isfinite(self.imag()));\n         }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_details.hpp",
        "status": "modified",
        "additions": 57,
        "deletions": 57,
        "changes": 114,
        "patch": "@@ -23,89 +23,89 @@ namespace xsimd\n {\n     // Forward declaration. Should we put them in a separate file?\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<std::complex<T>, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& self) noexcept;\n     template <class T, class A>\n-    inline bool any(batch_bool<T, A> const& self) noexcept;\n+    XSIMD_INLINE bool any(batch_bool<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+    XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n     template <class A, class T_out, class T_in>\n-    inline batch<T_out, A> batch_cast(batch<T_in, A> const&, batch<T_out, A> const& out) noexcept;\n+    XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const&, batch<T_out, A> const& out) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> bitofsign(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& self) noexcept;\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> cos(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> cos(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> cosh(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> exp(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> exp(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n+    XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n+    XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n+    XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n     template <class T, class A, uint64_t... Coefs>\n-    inline batch<T, A> horner(const batch<T, A>& self) noexcept;\n+    XSIMD_INLINE batch<T, A> horner(const batch<T, A>& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> hypot(const batch<T, A>& self) noexcept;\n+    XSIMD_INLINE batch<T, A> hypot(const batch<T, A>& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_even(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_flint(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch_bool<T, A> is_odd(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n+    XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& e) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> log(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> log(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> nearbyint(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> nearbyint_as_int(const batch<T, A>& x) noexcept;\n+    XSIMD_INLINE batch<as_integer_t<T>, A> nearbyint_as_int(const batch<T, A>& x) noexcept;\n     template <class T, class A>\n-    inline T reduce_add(batch<T, A> const&) noexcept;\n+    XSIMD_INLINE T reduce_add(batch<T, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const&, batch<T, A> const&) noexcept;\n+    XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const&, batch<T, A> const&, batch<T, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const&, batch<std::complex<T>, A> const&) noexcept;\n+    XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const&, batch<std::complex<T>, A> const&, batch<std::complex<T>, A> const&) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sign(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sign(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> signnz(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sin(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sinh(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> sqrt(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> tan(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> tan(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_float_t<T>, A> to_float(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<as_float_t<T>, A> to_float(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<as_integer_t<T>, A> to_int(batch<T, A> const& self) noexcept;\n     template <class T, class A>\n-    inline batch<T, A> trunc(batch<T, A> const& self) noexcept;\n+    XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& self) noexcept;\n \n     namespace kernel\n     {\n \n         namespace detail\n         {\n             template <class F, class A, class T, class... Batches>\n-            inline batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch<T, A> apply(F&& func, batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 constexpr std::size_t size = batch<T, A>::size;\n                 alignas(A::alignment()) T self_buffer[size];\n@@ -120,7 +120,7 @@ namespace xsimd\n             }\n \n             template <class U, class F, class A, class T>\n-            inline batch<U, A> apply_transform(F&& func, batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<U, A> apply_transform(F&& func, batch<T, A> const& self) noexcept\n             {\n                 static_assert(batch<T, A>::size == batch<U, A>::size,\n                               \"Source and destination sizes must match\");\n@@ -141,50 +141,50 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<uint8_t, A> fast_cast(batch<int8_t, A> const& self, batch<uint8_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint8_t, A> fast_cast(batch<int8_t, A> const& self, batch<uint8_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint8_t>(self);\n             }\n             template <class A>\n-            inline batch<uint16_t, A> fast_cast(batch<int16_t, A> const& self, batch<uint16_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint16_t, A> fast_cast(batch<int16_t, A> const& self, batch<uint16_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint16_t>(self);\n             }\n             template <class A>\n-            inline batch<uint32_t, A> fast_cast(batch<int32_t, A> const& self, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<int32_t, A> const& self, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint32_t>(self);\n             }\n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<int64_t, A> const& self, batch<uint64_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<int64_t, A> const& self, batch<uint64_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<uint64_t>(self);\n             }\n             template <class A>\n-            inline batch<int8_t, A> fast_cast(batch<uint8_t, A> const& self, batch<int8_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int8_t, A> fast_cast(batch<uint8_t, A> const& self, batch<int8_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int8_t>(self);\n             }\n             template <class A>\n-            inline batch<int16_t, A> fast_cast(batch<uint16_t, A> const& self, batch<int16_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int16_t, A> fast_cast(batch<uint16_t, A> const& self, batch<int16_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int16_t>(self);\n             }\n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<uint32_t, A> const& self, batch<int32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<uint32_t, A> const& self, batch<int32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int32_t>(self);\n             }\n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<uint64_t, A> const& self, batch<int64_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<uint64_t, A> const& self, batch<int64_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 return bitwise_cast<int64_t>(self);\n             }\n \n             // Provide a generic uint32_t -> float cast only if we have a\n             // non-generic int32_t -> float fast_cast\n             template <class A, class _ = decltype(fast_cast(std::declval<batch<int32_t, A> const&>(), std::declval<batch<float, A> const&>(), A {}))>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& v, batch<float, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& v, batch<float, A> const&, requires_arch<generic>) noexcept\n             {\n                 // see https://stackoverflow.com/questions/34066228/how-to-perform-uint32-float-conversion-with-sse\n                 batch<uint32_t, A> msk_lo(0xFFFF);\n@@ -201,7 +201,7 @@ namespace xsimd\n             // Provide a generic float -> uint32_t cast only if we have a\n             // non-generic float -> int32_t fast_cast\n             template <class A, class _ = decltype(fast_cast(std::declval<batch<float, A> const&>(), std::declval<batch<int32_t, A> const&>(), A {}))>\n-            inline batch<uint32_t, A> fast_cast(batch<float, A> const& v, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<float, A> const& v, batch<uint32_t, A> const&, requires_arch<generic>) noexcept\n             {\n                 auto is_large = v >= batch<float, A>(1u << 31);\n                 auto small = bitwise_cast<float>(batch_cast<int32_t>(v));\n@@ -258,25 +258,25 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B, uint64_t c>\n-            inline B coef() noexcept\n+            XSIMD_INLINE B coef() noexcept\n             {\n                 using value_type = typename B::value_type;\n                 return B(bit_cast<value_type>(as_unsigned_integer_t<value_type>(c)));\n             }\n             template <class B>\n-            inline B horner(const B&) noexcept\n+            XSIMD_INLINE B horner(const B&) noexcept\n             {\n                 return B(typename B::value_type(0.));\n             }\n \n             template <class B, uint64_t c0>\n-            inline B horner(const B&) noexcept\n+            XSIMD_INLINE B horner(const B&) noexcept\n             {\n                 return coef<B, c0>();\n             }\n \n             template <class B, uint64_t c0, uint64_t c1, uint64_t... args>\n-            inline B horner(const B& self) noexcept\n+            XSIMD_INLINE B horner(const B& self) noexcept\n             {\n                 return fma(self, horner<B, c1, args...>(self), coef<B, c0>());\n             }\n@@ -291,19 +291,19 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B>\n-            inline B horner1(const B&) noexcept\n+            XSIMD_INLINE B horner1(const B&) noexcept\n             {\n                 return B(1.);\n             }\n \n             template <class B, uint64_t c0>\n-            inline B horner1(const B& x) noexcept\n+            XSIMD_INLINE B horner1(const B& x) noexcept\n             {\n                 return x + detail::coef<B, c0>();\n             }\n \n             template <class B, uint64_t c0, uint64_t c1, uint64_t... args>\n-            inline B horner1(const B& x) noexcept\n+            XSIMD_INLINE B horner1(const B& x) noexcept\n             {\n                 return fma(x, horner1<B, c1, args...>(x), detail::coef<B, c0>());\n             }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_logical.hpp",
        "status": "modified",
        "additions": 18,
        "deletions": 18,
        "changes": 36,
        "patch": "@@ -24,7 +24,7 @@ namespace xsimd\n \n         // from  mask\n         template <class A, class T>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             // This is inefficient but should never be called. It's just a\n@@ -36,98 +36,98 @@ namespace xsimd\n \n         // ge\n         template <class A, class T>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return other <= self;\n         }\n \n         // gt\n         template <class A, class T>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return other < self;\n         }\n \n         // is_even\n         template <class A, class T>\n-        inline batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return is_flint(self * T(0.5));\n         }\n \n         // is_flint\n         template <class A, class T>\n-        inline batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             auto frac = select(isnan(self - self), constants::nan<batch<T, A>>(), self - trunc(self));\n             return frac == T(0.);\n         }\n \n         // is_odd\n         template <class A, class T>\n-        inline batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return is_even(self - T(1.));\n         }\n \n         // isinf\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isinf(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isinf(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(false);\n         }\n         template <class A>\n-        inline batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isinf(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return abs(self) == std::numeric_limits<float>::infinity();\n         }\n         template <class A>\n-        inline batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isinf(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return abs(self) == std::numeric_limits<double>::infinity();\n         }\n \n         // isfinite\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isfinite(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isfinite(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(true);\n         }\n         template <class A>\n-        inline batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isfinite(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return (self - self) == 0.f;\n         }\n         template <class A>\n-        inline batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isfinite(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return (self - self) == 0.;\n         }\n \n         // isnan\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> isnan(batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return batch_bool<T, A>(false);\n         }\n \n         // le\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return (self < other) || (self == other);\n         }\n \n         // neq\n         template <class A, class T>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return !(other == self);\n         }\n \n         // logical_and\n         template <class A, class T>\n-        inline batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> logical_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x && y; },\n@@ -136,7 +136,7 @@ namespace xsimd\n \n         // logical_or\n         template <class A, class T>\n-        inline batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> logical_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept\n                                  { return x || y; },\n@@ -145,7 +145,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             self.store_aligned(buffer);"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_math.hpp",
        "status": "modified",
        "additions": 151,
        "deletions": 149,
        "changes": 300,
        "patch": "@@ -27,7 +27,7 @@ namespace xsimd\n         using namespace types;\n         // abs\n         template <class A, class T, class>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n                 return self;\n@@ -40,7 +40,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return hypot(z.real(), z.imag());\n         }\n@@ -49,13 +49,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::false_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::false_type) noexcept\n             {\n                 return (x & y) + ((x ^ y) >> 1);\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::true_type, std::true_type) noexcept\n             {\n                 // Inspired by\n                 // https://stackoverflow.com/questions/5697500/take-the-average-of-two-signed-numbers-in-c\n@@ -66,14 +66,14 @@ namespace xsimd\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::false_type, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, std::false_type, std::true_type) noexcept\n             {\n                 return (x + y) / 2;\n             }\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n         {\n             return detail::avg(x, y, typename std::is_integral<T>::type {}, typename std::is_signed<T>::type {});\n         }\n@@ -82,42 +82,42 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::true_type) noexcept\n+            XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::true_type) noexcept\n             {\n                 constexpr unsigned shift = 8 * sizeof(T) - 1;\n                 auto adj = std::is_signed<T>::value ? ((x ^ y) & 0x1) : (((x ^ y) << shift) >> shift);\n                 return ::xsimd::kernel::avg(x, y, A {}) + adj;\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::false_type) noexcept\n+            XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, std::false_type) noexcept\n             {\n                 return ::xsimd::kernel::avg(x, y, A {});\n             }\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y, requires_arch<generic>) noexcept\n         {\n             return detail::avgr(x, y, typename std::is_integral<T>::type {});\n         }\n \n         // batch_cast\n         template <class A, class T>\n-        inline batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> batch_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         namespace detail\n         {\n             template <class A, class T_out, class T_in>\n-            inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 return fast_cast(self, out, A {});\n             }\n             template <class A, class T_out, class T_in>\n-            inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be no conversion for this type combination\");\n                 using batch_type_in = batch<T_in, A>;\n@@ -133,14 +133,14 @@ namespace xsimd\n         }\n \n         template <class A, class T_out, class T_in>\n-        inline batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& self, batch<T_out, A> const& out, requires_arch<generic>) noexcept\n         {\n             return detail::batch_cast(self, out, A {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n \n         // bitofsign\n         template <class A, class T>\n-        inline batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(std::is_integral<T>::value, \"int type implementation\");\n             if (std::is_unsigned<T>::value)\n@@ -150,19 +150,19 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> bitofsign(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self & constants::signmask<batch<float, A>>();\n         }\n         template <class A>\n-        inline batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> bitofsign(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self & constants::signmask<batch<double, A>>();\n         }\n \n         // bitwise_cast\n         template <class A, class T>\n-        inline batch<T, A> bitwise_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<T, A> const& self, batch<T, A> const&, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n@@ -178,7 +178,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> cbrt(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type z = abs(self);\n@@ -225,7 +225,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> cbrt(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type z = abs(self);\n@@ -274,14 +274,14 @@ namespace xsimd\n \n         // clip\n         template <class A, class T>\n-        inline batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> clip(batch<T, A> const& self, batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<generic>) noexcept\n         {\n             return min(hi, max(self, lo));\n         }\n \n         // copysign\n         template <class A, class T, class _ = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> copysign(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return abs(self) | bitofsign(other);\n         }\n@@ -308,7 +308,7 @@ namespace xsimd\n                 using batch_type = batch<float, A>;\n                 // computes erf(a0)/a0\n                 // x is sqr(a0) and 0 <= abs(a0) <= 2/3\n-                static inline batch_type erf1(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erf1(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3f906eba, //   1.128379154774254e+00\n@@ -322,7 +322,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(sqr(x))\n                 // x >=  2/3\n-                static inline batch_type erfc2(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc2(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3f0a0e8b, //   5.392844046572836e-01\n@@ -339,7 +339,7 @@ namespace xsimd\n                                           >(x);\n                 }\n \n-                static inline batch_type erfc3(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc3(const batch_type& x) noexcept\n                 {\n                     return (batch_type(1.) - x) * detail::horner<batch_type,\n                                                                  0x3f7ffffe, //   9.9999988e-01\n@@ -361,7 +361,7 @@ namespace xsimd\n                 using batch_type = batch<double, A>;\n                 // computes erf(a0)/a0\n                 // x is sqr(a0) and 0 <= abs(a0) <= 0.65\n-                static inline batch_type erf1(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erf1(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3ff20dd750429b61ull, // 1.12837916709551\n@@ -381,7 +381,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(x*x)\n                 // 0.65 <= abs(x) <= 2.2\n-                static inline batch_type erfc2(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc2(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3feffffffbbb552bull, // 0.999999992049799\n@@ -405,7 +405,7 @@ namespace xsimd\n \n                 // computes erfc(x)*exp(x*x)\n                 // 2.2 <= abs(x) <= 6\n-                static inline batch_type erfc3(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc3(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0x3fefff5a9e697ae2ull, // 0.99992114009714\n@@ -429,7 +429,7 @@ namespace xsimd\n \n                 // computes erfc(rx)*exp(rx*rx)\n                 // x >=  6 rx = 1/x\n-                static inline batch_type erfc4(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type erfc4(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0xbc7e4ad1ec7d0000ll, // -2.627435221016534e-17\n@@ -461,7 +461,7 @@ namespace xsimd\n          */\n \n         template <class A>\n-        inline batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> erf(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -485,7 +485,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> erf(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -521,7 +521,7 @@ namespace xsimd\n \n         // erfc\n         template <class A>\n-        inline batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> erfc(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -546,7 +546,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> erfc(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -590,62 +590,62 @@ namespace xsimd\n                 B x;\n \n                 template <typename... Ts>\n-                inline B operator()(const Ts&... coefs) noexcept\n+                XSIMD_INLINE B operator()(const Ts&... coefs) noexcept\n                 {\n                     return eval(coefs...);\n                 }\n \n             private:\n-                inline B eval(const B& c0) noexcept\n+                XSIMD_INLINE B eval(const B& c0) noexcept\n                 {\n                     return c0;\n                 }\n \n-                inline B eval(const B& c0, const B& c1) noexcept\n+                XSIMD_INLINE B eval(const B& c0, const B& c1) noexcept\n                 {\n                     return fma(x, c1, c0);\n                 }\n \n                 template <size_t... Is, class Tuple>\n-                inline B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)\n+                XSIMD_INLINE B eval(::xsimd::detail::index_sequence<Is...>, const Tuple& tuple)\n                 {\n                     return estrin { x * x }(std::get<Is>(tuple)...);\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple) noexcept\n                 {\n                     return eval(::xsimd::detail::make_index_sequence<sizeof...(Args)>(), tuple);\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0))));\n                 }\n \n                 template <class... Args>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))));\n                 }\n \n                 template <class... Args, class... Ts>\n-                inline B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept\n+                XSIMD_INLINE B eval(const std::tuple<Args...>& tuple, const B& c0, const B& c1, const Ts&... coefs) noexcept\n                 {\n                     return eval(std::tuple_cat(tuple, std::make_tuple(eval(c0, c1))), coefs...);\n                 }\n \n                 template <class... Ts>\n-                inline B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept\n+                XSIMD_INLINE B eval(const B& c0, const B& c1, const Ts&... coefs) noexcept\n                 {\n                     return eval(std::make_tuple(eval(c0, c1)), coefs...);\n                 }\n             };\n         }\n \n         template <class T, class A, uint64_t... Coefs>\n-        inline batch<T, A> estrin(const batch<T, A>& self) noexcept\n+        XSIMD_INLINE batch<T, A> estrin(const batch<T, A>& self) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return detail::estrin<batch_type> { self }(detail::coef<batch_type, Coefs>()...);\n@@ -722,7 +722,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp_tag> : exp_reduction_base<batch<float, A>, exp_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type y = detail::horner<batch_type,\n                                                   0x3f000000, //  5.0000000e-01\n@@ -734,7 +734,7 @@ namespace xsimd\n                     return ++fma(y, x * x, x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n                     x = fnma(k, constants::log_2hi<batch_type>(), a);\n@@ -747,7 +747,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp10_tag> : exp_reduction_base<batch<float, A>, exp10_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     return ++(detail::horner<batch_type,\n                                              0x40135d8e, //    2.3025851e+00\n@@ -760,7 +760,7 @@ namespace xsimd\n                               * x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog10_2<batch_type>() * a);\n                     x = fnma(k, constants::log10_2hi<batch_type>(), a);\n@@ -773,7 +773,7 @@ namespace xsimd\n             struct exp_reduction<float, A, exp2_tag> : exp_reduction_base<batch<float, A>, exp2_tag>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type y = detail::horner<batch_type,\n                                                   0x3e75fdf1, //    2.4022652e-01\n@@ -785,7 +785,7 @@ namespace xsimd\n                     return ++fma(y, x * x, x * constants::log_2<batch_type>());\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(a);\n                     x = (a - k);\n@@ -797,7 +797,7 @@ namespace xsimd\n             struct exp_reduction<double, A, exp_tag> : exp_reduction_base<batch<double, A>, exp_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type t = x * x;\n                     return fnma(t,\n@@ -810,7 +810,7 @@ namespace xsimd\n                                 x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type& hi, batch_type& lo, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type& hi, batch_type& lo, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n                     hi = fnma(k, constants::log_2hi<batch_type>(), a);\n@@ -819,7 +819,7 @@ namespace xsimd\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type& x, const batch_type& c, const batch_type& hi, const batch_type& lo) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type& x, const batch_type& c, const batch_type& hi, const batch_type& lo) noexcept\n                 {\n                     return batch_type(1.) - (((lo - (x * c) / (batch_type(2.) - c)) - hi));\n                 }\n@@ -829,23 +829,23 @@ namespace xsimd\n             struct exp_reduction<double, A, exp10_tag> : exp_reduction_base<batch<double, A>, exp10_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type xx = x * x;\n                     batch_type px = x * detail::horner<batch_type, 0x40a2b4798e134a01ull, 0x40796b7a050349e4ull, 0x40277d9474c55934ull, 0x3fa4fd75f3062dd4ull>(xx);\n                     batch_type x2 = px / (detail::horner1<batch_type, 0x40a03f37650df6e2ull, 0x4093e05eefd67782ull, 0x405545fdce51ca08ull>(xx) - px);\n                     return ++(x2 + x2);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(constants::invlog10_2<batch_type>() * a);\n                     x = fnma(k, constants::log10_2hi<batch_type>(), a);\n                     x = fnma(k, constants::log10_2lo<batch_type>(), x);\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type&, const batch_type& c, const batch_type&, const batch_type&) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type&, const batch_type& c, const batch_type&, const batch_type&) noexcept\n                 {\n                     return c;\n                 }\n@@ -855,7 +855,7 @@ namespace xsimd\n             struct exp_reduction<double, A, exp2_tag> : exp_reduction_base<batch<double, A>, exp2_tag>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type approx(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type approx(const batch_type& x) noexcept\n                 {\n                     batch_type t = x * x;\n                     return fnma(t,\n@@ -868,21 +868,21 @@ namespace xsimd\n                                 x);\n                 }\n \n-                static inline batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type reduce(const batch_type& a, batch_type&, batch_type&, batch_type& x) noexcept\n                 {\n                     batch_type k = nearbyint(a);\n                     x = (a - k) * constants::log_2<batch_type>();\n                     return k;\n                 }\n \n-                static inline batch_type finalize(const batch_type& x, const batch_type& c, const batch_type&, const batch_type&) noexcept\n+                static XSIMD_INLINE batch_type finalize(const batch_type& x, const batch_type& c, const batch_type&, const batch_type&) noexcept\n                 {\n                     return batch_type(1.) + x + x * c / (batch_type(2.) - c);\n                 }\n             };\n \n             template <exp_reduction_tag Tag, class A>\n-            inline batch<float, A> exp(batch<float, A> const& self) noexcept\n+            XSIMD_INLINE batch<float, A> exp(batch<float, A> const& self) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 using reducer_t = exp_reduction<float, A, Tag>;\n@@ -895,7 +895,7 @@ namespace xsimd\n             }\n \n             template <exp_reduction_tag Tag, class A>\n-            inline batch<double, A> exp(batch<double, A> const& self) noexcept\n+            XSIMD_INLINE batch<double, A> exp(batch<double, A> const& self) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 using reducer_t = exp_reduction<double, A, Tag>;\n@@ -910,13 +910,13 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp_tag>(self);\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> exp(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             auto isincos = sincos(self.imag());\n@@ -925,14 +925,14 @@ namespace xsimd\n \n         // exp10\n         template <class A, class T>\n-        inline batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp10(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp10_tag>(self);\n         }\n \n         // exp2\n         template <class A, class T>\n-        inline batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> exp2(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::exp<detail::exp2_tag>(self);\n         }\n@@ -950,7 +950,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<float, A> expm1(const batch<float, A>& a) noexcept\n+            static XSIMD_INLINE batch<float, A> expm1(const batch<float, A>& a) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n@@ -974,7 +974,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> expm1(const batch<double, A>& a) noexcept\n+            static XSIMD_INLINE batch<double, A> expm1(const batch<double, A>& a) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type k = nearbyint(constants::invlog_2<batch_type>() * a);\n@@ -1005,7 +1005,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> expm1(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return select(self < constants::logeps<batch_type>(),\n@@ -1016,7 +1016,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> expm1(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -1029,22 +1029,22 @@ namespace xsimd\n \n         // polar\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> polar(const batch<T, A>& r, const batch<T, A>& theta, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> polar(const batch<T, A>& r, const batch<T, A>& theta, requires_arch<generic>) noexcept\n         {\n             auto sincosTheta = sincos(theta);\n             return { r * sincosTheta.second, r * sincosTheta.first };\n         }\n \n         // fdim\n         template <class A, class T>\n-        inline batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fdim(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fmax(batch<T, A>(0), self - other);\n         }\n \n         // fmod\n         template <class A, class T>\n-        inline batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> fmod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(trunc(self / other), other, self);\n         }\n@@ -1060,7 +1060,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& self, batch<as_integer_t<T>, A>& exp, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             using int_type = as_integer_t<T>;\n@@ -1075,28 +1075,28 @@ namespace xsimd\n \n         // from bool\n         template <class A, class T>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return batch<T, A>(self.data) & batch<T, A>(1);\n         }\n \n         // horner\n         template <class T, class A, uint64_t... Coefs>\n-        inline batch<T, A> horner(const batch<T, A>& self) noexcept\n+        XSIMD_INLINE batch<T, A> horner(const batch<T, A>& self) noexcept\n         {\n             return detail::horner<batch<T, A>, Coefs...>(self);\n         }\n \n         // hypot\n         template <class A, class T>\n-        inline batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> hypot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return sqrt(fma(self, self, other * other));\n         }\n \n         // ipow\n         template <class A, class T, class ITy>\n-        inline batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ipow(batch<T, A> const& self, ITy other, requires_arch<generic>) noexcept\n         {\n             return ::xsimd::detail::ipow(self, other);\n         }\n@@ -1112,7 +1112,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& self, const batch<as_integer_t<T>, A>& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             using itype = as_integer_t<batch_type>;\n@@ -1123,7 +1123,7 @@ namespace xsimd\n \n         // lgamma\n         template <class A, class T>\n-        inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n@@ -1137,7 +1137,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<float, A> gammalnB(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammalnB(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0x3ed87730, //    4.227843421859038E-001\n@@ -1152,7 +1152,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> gammalnC(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammalnC(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0xbf13c468, //   -5.772156501719101E-001\n@@ -1167,7 +1167,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> gammaln2(const batch<float, A>& x) noexcept\n+            static XSIMD_INLINE batch<float, A> gammaln2(const batch<float, A>& x) noexcept\n             {\n                 return horner<batch<float, A>,\n                               0x3daaaa94, //   8.333316229807355E-002f\n@@ -1177,7 +1177,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> gammaln1(const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> gammaln1(const batch<double, A>& x) noexcept\n             {\n                 return horner<batch<double, A>,\n                               0xc12a0c675418055eull, //  -8.53555664245765465627E5\n@@ -1199,7 +1199,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> gammalnA(const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> gammalnA(const batch<double, A>& x) noexcept\n             {\n                 return horner<batch<double, A>,\n                               0x3fb555555555554bull, //    8.33333333333331927722E-2\n@@ -1226,7 +1226,7 @@ namespace xsimd\n             struct lgamma_impl<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& a) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& a) noexcept\n                 {\n                     auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n                     batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n@@ -1248,7 +1248,7 @@ namespace xsimd\n                 }\n \n             private:\n-                static inline batch_type negative(const batch_type& q, const batch_type& w) noexcept\n+                static XSIMD_INLINE batch_type negative(const batch_type& q, const batch_type& w) noexcept\n                 {\n                     batch_type p = floor(q);\n                     batch_type z = q - p;\n@@ -1258,7 +1258,7 @@ namespace xsimd\n                     return -log(constants::invpi<batch_type>() * abs(z)) - w;\n                 }\n \n-                static inline batch_type other(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type other(const batch_type& x) noexcept\n                 {\n                     auto xlt650 = (x < batch_type(6.5));\n                     batch_type r0x = x;\n@@ -1347,7 +1347,7 @@ namespace xsimd\n             {\n                 using batch_type = batch<double, A>;\n \n-                static inline batch_type compute(const batch_type& a) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& a) noexcept\n                 {\n                     auto inf_result = (a <= batch_type(0.)) && is_flint(a);\n                     batch_type x = select(inf_result, constants::nan<batch_type>(), a);\n@@ -1369,6 +1369,8 @@ namespace xsimd\n                 }\n \n             private:\n+                // FIXME: cannot mark this one as XSIMD_INLINE because there's a\n+                // recursive loop on `lgamma'.\n                 static inline batch_type large_negative(const batch_type& q) noexcept\n                 {\n                     batch_type w = lgamma(q);\n@@ -1381,7 +1383,7 @@ namespace xsimd\n                     return constants::logpi<batch_type>() - log(z) - w;\n                 }\n \n-                static inline batch_type other(const batch_type& xx) noexcept\n+                static XSIMD_INLINE batch_type other(const batch_type& xx) noexcept\n                 {\n                     batch_type x = xx;\n                     auto test = (x < batch_type(13.));\n@@ -1424,7 +1426,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::lgamma_impl<batch<T, A>>::compute(self);\n         }\n@@ -1440,7 +1442,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1480,7 +1482,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1523,14 +1525,14 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             return batch<std::complex<T>, A>(log(abs(z)), atan2(z.imag(), z.real()));\n         }\n \n         // log2\n         template <class A>\n-        inline batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log2(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1570,7 +1572,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log2(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1620,7 +1622,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base) noexcept\n+            XSIMD_INLINE batch<T, A> logN_complex_impl(const batch<T, A>& z, typename batch<T, A>::value_type base) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 using rv_type = typename batch_type::value_type;\n@@ -1629,7 +1631,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log2(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::logN_complex_impl(self, std::log(2));\n         }\n@@ -1647,7 +1649,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log10(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             const batch_type\n@@ -1698,7 +1700,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log10(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             const batch_type\n@@ -1752,7 +1754,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log10(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             return detail::logN_complex_impl(z, std::log(10));\n         }\n@@ -1768,7 +1770,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A>\n-        inline batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> log1p(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             using int_type = as_integer_t<float>;\n@@ -1800,7 +1802,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> log1p(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             using int_type = as_integer_t<double>;\n@@ -1833,7 +1835,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> log1p(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -1848,7 +1850,7 @@ namespace xsimd\n \n         // mod\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> mod(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             return detail::apply([](T x, T y) noexcept -> T\n                                  { return x % y; },\n@@ -1857,14 +1859,14 @@ namespace xsimd\n \n         // nearbyint\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> nearbyintf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> nearbyintf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 batch_type s = bitofsign(self);\n@@ -1884,26 +1886,26 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::nearbyintf(self);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::nearbyintf(self);\n         }\n \n         // nearbyint_as_int\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> nearbyint_as_int(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint_as_int(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<as_integer_t<float>, A>\n+        XSIMD_INLINE batch<as_integer_t<float>, A>\n         nearbyint_as_int(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using U = as_integer_t<float>;\n@@ -1913,7 +1915,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<as_integer_t<double>, A>\n+        XSIMD_INLINE batch<as_integer_t<double>, A>\n         nearbyint_as_int(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using U = as_integer_t<double>;\n@@ -1930,12 +1932,12 @@ namespace xsimd\n             {\n                 using batch_type = batch<T, A>;\n \n-                static inline batch_type next(batch_type const& b) noexcept\n+                static XSIMD_INLINE batch_type next(batch_type const& b) noexcept\n                 {\n                     return b;\n                 }\n \n-                static inline batch_type prev(batch_type const& b) noexcept\n+                static XSIMD_INLINE batch_type prev(batch_type const& b) noexcept\n                 {\n                     return b;\n                 }\n@@ -1963,21 +1965,21 @@ namespace xsimd\n                 using int_batch = typename bitwise_cast_batch<T, A>::type;\n                 using int_type = typename int_batch::value_type;\n \n-                static inline batch_type next(const batch_type& b) noexcept\n+                static XSIMD_INLINE batch_type next(const batch_type& b) noexcept\n                 {\n                     batch_type n = ::xsimd::bitwise_cast<T>(::xsimd::bitwise_cast<int_type>(b) + int_type(1));\n                     return select(b == constants::infinity<batch_type>(), b, n);\n                 }\n \n-                static inline batch_type prev(const batch_type& b) noexcept\n+                static XSIMD_INLINE batch_type prev(const batch_type& b) noexcept\n                 {\n                     batch_type p = ::xsimd::bitwise_cast<T>(::xsimd::bitwise_cast<int_type>(b) - int_type(1));\n                     return select(b == constants::minusinfinity<batch_type>(), b, p);\n                 }\n             };\n         }\n         template <class A, class T>\n-        inline batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> nextafter(batch<T, A> const& from, batch<T, A> const& to, requires_arch<generic>) noexcept\n         {\n             using kernel = detail::nextafter_kernel<T, A>;\n             return select(from == to, from,\n@@ -1995,7 +1997,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> pow(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const auto zero = batch_type(0.);\n@@ -2010,7 +2012,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> pow(const batch<std::complex<T>, A>& a, const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using cplx_batch = batch<std::complex<T>, A>;\n             using real_batch = typename cplx_batch::real_batch;\n@@ -2029,16 +2031,16 @@ namespace xsimd\n \n         // reciprocal\n         template <class T, class A, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch<T, A> reciprocal(batch<T, A> const& self,\n-                                      requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(batch<T, A> const& self,\n+                                            requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return div(batch_type(1), self);\n         }\n \n         // reduce_add\n         template <class A, class T>\n-        inline std::complex<T> reduce_add(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE std::complex<T> reduce_add(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { reduce_add(self.real()), reduce_add(self.imag()) };\n         }\n@@ -2055,13 +2057,13 @@ namespace xsimd\n             };\n \n             template <class Op, class A, class T>\n-            inline T reduce(Op, batch<T, A> const& self, std::integral_constant<unsigned, 1>) noexcept\n+            XSIMD_INLINE T reduce(Op, batch<T, A> const& self, std::integral_constant<unsigned, 1>) noexcept\n             {\n                 return self.get(0);\n             }\n \n             template <class Op, class A, class T, unsigned Lvl>\n-            inline T reduce(Op op, batch<T, A> const& self, std::integral_constant<unsigned, Lvl>) noexcept\n+            XSIMD_INLINE T reduce(Op op, batch<T, A> const& self, std::integral_constant<unsigned, Lvl>) noexcept\n             {\n                 using index_type = as_unsigned_integer_t<T>;\n                 batch<T, A> split = swizzle(self, make_batch_constant<index_type, A, split_high<index_type, Lvl / 2>>());\n@@ -2071,7 +2073,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::reduce([](batch<T, A> const& x, batch<T, A> const& y)\n                                   { return max(x, y); },\n@@ -2080,7 +2082,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::reduce([](batch<T, A> const& x, batch<T, A> const& y)\n                                   { return min(x, y); },\n@@ -2089,32 +2091,32 @@ namespace xsimd\n \n         // remainder\n         template <class A>\n-        inline batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> remainder(batch<float, A> const& self, batch<float, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(nearbyint(self / other), other, self);\n         }\n         template <class A>\n-        inline batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> remainder(batch<double, A> const& self, batch<double, A> const& other, requires_arch<generic>) noexcept\n         {\n             return fnma(nearbyint(self / other), other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> remainder(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             auto mod = self % other;\n             return select(mod <= other / 2, mod, mod - other);\n         }\n \n         // select\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br, requires_arch<generic>) noexcept\n         {\n             return { select(cond, true_br.real(), false_br.real()), select(cond, true_br.imag(), false_br.imag()) };\n         }\n \n         // sign\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sign(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type res = select(self > batch_type(0), batch_type(1), batch_type(0)) - select(self < batch_type(0), batch_type(1), batch_type(0));\n@@ -2124,7 +2126,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> signf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> signf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 batch_type res = select(self > batch_type(0.f), batch_type(1.f), batch_type(0.f)) - select(self < batch_type(0.f), batch_type(1.f), batch_type(0.f));\n@@ -2137,17 +2139,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> sign(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signf(self);\n         }\n         template <class A>\n-        inline batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> sign(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signf(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sign(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -2160,7 +2162,7 @@ namespace xsimd\n \n         // signnz\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             return (self >> (sizeof(T) * 8 - 1)) | batch_type(1.);\n@@ -2169,7 +2171,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> signnzf(batch<T, A> const& self) noexcept\n+            XSIMD_INLINE batch<T, A> signnzf(batch<T, A> const& self) noexcept\n             {\n                 using batch_type = batch<T, A>;\n #ifndef XSIMD_NO_NANS\n@@ -2181,19 +2183,19 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> signnz(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signnzf(self);\n         }\n         template <class A>\n-        inline batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> signnz(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::signnzf(self);\n         }\n \n         // sqrt\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sqrt(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n \n             constexpr T csqrt_scale_factor = std::is_same<T, float>::value ? 6.7108864e7f : 1.8014398509481984e16;\n@@ -2248,7 +2250,7 @@ namespace xsimd\n             struct stirling_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3daaaaab,\n@@ -2257,12 +2259,12 @@ namespace xsimd\n                                   0xb970b359>(x);\n                 }\n \n-                static inline batch_type split_limit() noexcept\n+                static XSIMD_INLINE batch_type split_limit() noexcept\n                 {\n                     return batch_type(bit_cast<float>(uint32_t(0x41d628f6)));\n                 }\n \n-                static inline batch_type large_limit() noexcept\n+                static XSIMD_INLINE batch_type large_limit() noexcept\n                 {\n                     return batch_type(bit_cast<float>(uint32_t(0x420c28f3)));\n                 }\n@@ -2272,7 +2274,7 @@ namespace xsimd\n             struct stirling_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3fb5555555555986ull, //   8.33333333333482257126E-2\n@@ -2283,12 +2285,12 @@ namespace xsimd\n                                   >(x);\n                 }\n \n-                static inline batch_type split_limit() noexcept\n+                static XSIMD_INLINE batch_type split_limit() noexcept\n                 {\n                     return batch_type(bit_cast<double>(uint64_t(0x4061e083ba3443d4)));\n                 }\n \n-                static inline batch_type large_limit() noexcept\n+                static XSIMD_INLINE batch_type large_limit() noexcept\n                 {\n                     return batch_type(bit_cast<double>(uint64_t(0x4065800000000000)));\n                 }\n@@ -2304,7 +2306,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class T, class A>\n-            inline batch<T, A> stirling(const batch<T, A>& a) noexcept\n+            XSIMD_INLINE batch<T, A> stirling(const batch<T, A>& a) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 const batch_type stirlingsplitlim = stirling_kernel<batch_type>::split_limit();\n@@ -2342,7 +2344,7 @@ namespace xsimd\n             struct tgamma_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3f800000UL, //  9.999999757445841E-01\n@@ -2361,7 +2363,7 @@ namespace xsimd\n             struct tgamma_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type compute(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type compute(const batch_type& x) noexcept\n                 {\n                     return horner<batch_type,\n                                   0x3ff0000000000000ULL, // 9.99999999999999996796E-1\n@@ -2395,7 +2397,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class B>\n-            inline B tgamma_large_negative(const B& a) noexcept\n+            XSIMD_INLINE B tgamma_large_negative(const B& a) noexcept\n             {\n                 B st = stirling(a);\n                 B p = floor(a);\n@@ -2409,7 +2411,7 @@ namespace xsimd\n             }\n \n             template <class B, class BB>\n-            inline B tgamma_other(const B& a, const BB& test) noexcept\n+            XSIMD_INLINE B tgamma_other(const B& a, const BB& test) noexcept\n             {\n                 B x = select(test, B(2.), a);\n #ifndef XSIMD_NO_INFINITIES\n@@ -2448,7 +2450,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tgamma(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             auto nan_result = (self < batch_type(0.) && is_flint(self));"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_memory.hpp",
        "status": "modified",
        "additions": 47,
        "deletions": 46,
        "changes": 93,
        "patch": "@@ -36,7 +36,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class IT, class A, class I, size_t... Is>\n-            inline batch<IT, A> create_compress_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n+            XSIMD_INLINE batch<IT, A> create_compress_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n             {\n                 batch<IT, A> swizzle_mask(IT(0));\n                 alignas(A::alignment()) IT mask_buffer[batch<IT, A>::size] = { Is... };\n@@ -49,7 +49,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         compress(batch<T, A> const& x, batch_bool<T, A> const& mask,\n                  kernel::requires_arch<generic>) noexcept\n         {\n@@ -65,7 +65,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class IT, class A, class I, size_t... Is>\n-            inline batch<IT, A> create_expand_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n+            XSIMD_INLINE batch<IT, A> create_expand_swizzle_mask(I bitmask, ::xsimd::detail::index_sequence<Is...>)\n             {\n                 batch<IT, A> swizzle_mask(IT(0));\n                 IT j = 0;\n@@ -75,7 +75,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         expand(batch<T, A> const& x, batch_bool<T, A> const& mask,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -88,7 +88,7 @@ namespace xsimd\n \n         // extract_pair\n         template <class A, class T>\n-        inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<generic>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(i < size && \"index in bounds\");\n@@ -115,6 +115,7 @@ namespace xsimd\n         // gather\n         namespace detail\n         {\n+            // Not using XSIMD_INLINE here as it makes msvc hand got ever on avx512\n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N == 0, int>::type = 0>\n             inline batch<T, A> gather(U const* src, batch<V, A> const& index,\n                                       ::xsimd::index<N> I) noexcept\n@@ -134,7 +135,7 @@ namespace xsimd\n         } // namespace detail\n \n         template <typename T, typename A, typename V>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         gather(batch<T, A> const&, T const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -146,7 +147,7 @@ namespace xsimd\n \n         // Gather with runtime indexes and mismatched strides.\n         template <typename T, typename A, typename U, typename V>\n-        inline detail::sizes_mismatch_t<T, U, batch<T, A>>\n+        XSIMD_INLINE detail::sizes_mismatch_t<T, U, batch<T, A>>\n         gather(batch<T, A> const&, U const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -158,7 +159,7 @@ namespace xsimd\n \n         // Gather with runtime indexes and matching strides.\n         template <typename T, typename A, typename U, typename V>\n-        inline detail::stride_match_t<T, U, batch<T, A>>\n+        XSIMD_INLINE detail::stride_match_t<T, U, batch<T, A>>\n         gather(batch<T, A> const&, U const* src, batch<V, A> const& index,\n                kernel::requires_arch<generic>) noexcept\n         {\n@@ -170,7 +171,7 @@ namespace xsimd\n \n         // insert\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept\n         {\n             struct index_mask\n             {\n@@ -185,47 +186,47 @@ namespace xsimd\n \n         // get\n         template <class A, size_t I, class T>\n-        inline T get(batch<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, size_t I, class T>\n-        inline T get(batch_bool<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch_bool<T, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch_bool<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, size_t I, class T>\n-        inline auto get(batch<std::complex<T>, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n+        XSIMD_INLINE auto get(batch<std::complex<T>, A> const& self, ::xsimd::index<I>, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n         {\n             alignas(A::alignment()) T buffer[batch<std::complex<T>, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[I];\n         }\n \n         template <class A, class T>\n-        inline T get(batch<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) T buffer[batch<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[i];\n         }\n \n         template <class A, class T>\n-        inline T get(batch_bool<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n+        XSIMD_INLINE T get(batch_bool<T, A> const& self, std::size_t i, requires_arch<generic>) noexcept\n         {\n             alignas(A::alignment()) bool buffer[batch_bool<T, A>::size];\n             self.store_aligned(&buffer[0]);\n             return buffer[i];\n         }\n \n         template <class A, class T>\n-        inline auto get(batch<std::complex<T>, A> const& self, std::size_t i, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n+        XSIMD_INLINE auto get(batch<std::complex<T>, A> const& self, std::size_t i, requires_arch<generic>) noexcept -> typename batch<std::complex<T>, A>::value_type\n         {\n             using T2 = typename batch<std::complex<T>, A>::value_type;\n             alignas(A::alignment()) T2 buffer[batch<std::complex<T>, A>::size];\n@@ -237,14 +238,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 using batch_type_in = batch<T_in, A>;\n                 using batch_type_out = batch<T_out, A>;\n                 return fast_cast(batch_type_in::load_aligned(mem), batch_type_out(), A {});\n             }\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n                 using batch_type_out = batch<T_out, A>;\n@@ -254,7 +255,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T_in, class T_out>\n-        inline batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> load_aligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n         {\n             return detail::load_aligned<A>(mem, cvt, A {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n@@ -263,29 +264,29 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out>, requires_arch<generic>, with_fast_conversion) noexcept\n             {\n                 using batch_type_in = batch<T_in, A>;\n                 using batch_type_out = batch<T_out, A>;\n                 return fast_cast(batch_type_in::load_unaligned(mem), batch_type_out(), A {});\n             }\n \n             template <class A, class T_in, class T_out>\n-            inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) noexcept\n+            XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>, with_slow_conversion) noexcept\n             {\n                 static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct load for this type combination\");\n                 return load_aligned<A>(mem, cvt, generic {}, with_slow_conversion {});\n             }\n         }\n         template <class A, class T_in, class T_out>\n-        inline batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T_out, A> load_unaligned(T_in const* mem, convert<T_out> cvt, requires_arch<generic>) noexcept\n         {\n             return detail::load_unaligned<A>(mem, cvt, generic {}, detail::conversion_type<A, T_in, T_out> {});\n         }\n \n         // rotate_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> rotate_left(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_left(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             struct rotate_generator\n             {\n@@ -299,14 +300,14 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<std::complex<T>, A> rotate_left(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> rotate_left(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { rotate_left<N>(self.real()), rotate_left<N>(self.imag()) };\n         }\n \n         // rotate_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> rotate_right(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             struct rotate_generator\n             {\n@@ -320,7 +321,7 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<std::complex<T>, A> rotate_right(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> rotate_right(batch<std::complex<T>, A> const& self, requires_arch<generic>) noexcept\n         {\n             return { rotate_right<N>(self.real()), rotate_right<N>(self.imag()) };\n         }\n@@ -329,15 +330,15 @@ namespace xsimd\n         namespace detail\n         {\n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N == 0, int>::type = 0>\n-            inline void scatter(batch<T, A> const& src, U* dst,\n-                                batch<V, A> const& index,\n-                                ::xsimd::index<N> I) noexcept\n+            XSIMD_INLINE void scatter(batch<T, A> const& src, U* dst,\n+                                      batch<V, A> const& index,\n+                                      ::xsimd::index<N> I) noexcept\n             {\n                 dst[index.get(I)] = static_cast<U>(src.get(I));\n             }\n \n             template <size_t N, typename T, typename A, typename U, typename V, typename std::enable_if<N != 0, int>::type = 0>\n-            inline void\n+            XSIMD_INLINE void\n             scatter(batch<T, A> const& src, U* dst, batch<V, A> const& index,\n                     ::xsimd::index<N> I) noexcept\n             {\n@@ -350,7 +351,7 @@ namespace xsimd\n         } // namespace detail\n \n         template <typename A, typename T, typename V>\n-        inline void\n+        XSIMD_INLINE void\n         scatter(batch<T, A> const& src, T* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -362,7 +363,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T, typename U, typename V>\n-        inline detail::sizes_mismatch_t<T, U, void>\n+        XSIMD_INLINE detail::sizes_mismatch_t<T, U, void>\n         scatter(batch<T, A> const& src, U* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -374,7 +375,7 @@ namespace xsimd\n         }\n \n         template <typename A, typename T, typename U, typename V>\n-        inline detail::stride_match_t<T, U, void>\n+        XSIMD_INLINE detail::stride_match_t<T, U, void>\n         scatter(batch<T, A> const& src, U* dst,\n                 batch<V, A> const& index,\n                 kernel::requires_arch<generic>) noexcept\n@@ -455,7 +456,7 @@ namespace xsimd\n         }\n \n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept\n         {\n             constexpr size_t bsize = sizeof...(Indices);\n \n@@ -512,7 +513,7 @@ namespace xsimd\n \n         // store\n         template <class T, class A>\n-        inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store(batch_bool<T, A> const& self, bool* mem, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             constexpr auto size = batch_bool<T, A>::size;\n@@ -524,7 +525,7 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T_in, class T_out>\n-        inline void store_aligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_aligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n             alignas(A::alignment()) T_in buffer[batch<T_in, A>::size];\n@@ -534,21 +535,21 @@ namespace xsimd\n \n         // store_unaligned\n         template <class A, class T_in, class T_out>\n-        inline void store_unaligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_unaligned(T_out* mem, batch<T_in, A> const& self, requires_arch<generic>) noexcept\n         {\n             static_assert(!std::is_same<T_in, T_out>::value, \"there should be a direct store for this type combination\");\n             return store_aligned<A>(mem, self, generic {});\n         }\n \n         // swizzle\n         template <class A, class T, class ITy, ITy... Vs>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch_constant<ITy, A, Vs...> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch_constant<ITy, A, Vs...> mask, requires_arch<generic>) noexcept\n         {\n             return { swizzle(self.real(), mask), swizzle(self.imag(), mask) };\n         }\n \n         template <class A, class T, class ITy>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             alignas(A::alignment()) T self_buffer[size];\n@@ -564,7 +565,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class ITy>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self, batch<ITy, A> mask, requires_arch<generic>) noexcept\n         {\n             return { swizzle(self.real(), mask), swizzle(self.imag(), mask) };\n         }\n@@ -573,26 +574,26 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& /*hi*/, batch<T, A> const& /*lo*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"load_complex not implemented for the required architecture\");\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_high(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"complex_high not implemented for the required architecture\");\n             }\n \n             template <class A, class T>\n-            inline batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_low(batch<std::complex<T>, A> const& /*src*/, requires_arch<generic>) noexcept\n             {\n                 static_assert(std::is_same<T, void>::value, \"complex_low not implemented for the required architecture\");\n             }\n         }\n \n         template <class A, class T_out, class T_in>\n-        inline batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T_out>, A> load_complex_aligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_out, A>;\n             T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n@@ -603,7 +604,7 @@ namespace xsimd\n \n         // load_complex_unaligned\n         template <class A, class T_out, class T_in>\n-        inline batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T_out>, A> load_complex_unaligned(std::complex<T_in> const* mem, convert<std::complex<T_out>>, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_out, A>;\n             T_in const* buffer = reinterpret_cast<T_in const*>(mem);\n@@ -614,7 +615,7 @@ namespace xsimd\n \n         // store_complex_aligned\n         template <class A, class T_out, class T_in>\n-        inline void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_in, A>;\n             real_batch hi = detail::complex_high(src, A {});\n@@ -626,7 +627,7 @@ namespace xsimd\n \n         // store_compelx_unaligned\n         template <class A, class T_out, class T_in>\n-        inline void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T_out>* dst, batch<std::complex<T_in>, A> const& src, requires_arch<generic>) noexcept\n         {\n             using real_batch = batch<T_in, A>;\n             real_batch hi = detail::complex_high(src, A {});"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_rounding.hpp",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -24,23 +24,23 @@ namespace xsimd\n \n         // ceil\n         template <class A, class T>\n-        inline batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> ceil(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             batch<T, A> truncated_self = trunc(self);\n             return select(truncated_self < self, truncated_self + 1, truncated_self);\n         }\n \n         // floor\n         template <class A, class T>\n-        inline batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> floor(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             batch<T, A> truncated_self = trunc(self);\n             return select(truncated_self > self, truncated_self - 1, truncated_self);\n         }\n \n         // round\n         template <class A, class T>\n-        inline batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> round(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             auto v = abs(self);\n             auto c = ceil(v);\n@@ -50,17 +50,17 @@ namespace xsimd\n \n         // trunc\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return self;\n         }\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             return select(abs(self) < constants::maxflint<batch<float, A>>(), to_float(to_int(self)), self);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             return select(abs(self) < constants::maxflint<batch<double, A>>(), to_float(to_int(self)), self);\n         }"
      },
      {
        "filename": "include/xsimd/arch/generic/xsimd_generic_trigo.hpp",
        "status": "modified",
        "additions": 59,
        "deletions": 59,
        "changes": 118,
        "patch": "@@ -35,7 +35,7 @@ namespace xsimd\n \n         // acos\n         template <class A, class T>\n-        inline batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> acos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -47,7 +47,7 @@ namespace xsimd\n             return select(x_larger_05, x, constants::pio2<batch_type>() - x);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> acos(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -66,7 +66,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> acosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = self - batch_type(1.);\n@@ -76,7 +76,7 @@ namespace xsimd\n             return select(test, l1pz + constants::log_2<batch_type>(), l1pz);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> acosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = acos(z);\n@@ -86,7 +86,7 @@ namespace xsimd\n \n         // asin\n         template <class A>\n-        inline batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> asin(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -105,7 +105,7 @@ namespace xsimd\n             return z ^ sign;\n         }\n         template <class A>\n-        inline batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> asin(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -127,7 +127,7 @@ namespace xsimd\n                               ^ bitofsign(self));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> asin(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -159,32 +159,32 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-            inline batch<T, A>\n+            XSIMD_INLINE batch<T, A>\n             average(const batch<T, A>& x1, const batch<T, A>& x2) noexcept\n             {\n                 return (x1 & x2) + ((x1 ^ x2) >> 1);\n             }\n \n             template <class A, class T>\n-            inline batch<T, A>\n+            XSIMD_INLINE batch<T, A>\n             averagef(const batch<T, A>& x1, const batch<T, A>& x2) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 return fma(x1, batch_type(0.5), x2 * batch_type(0.5));\n             }\n             template <class A>\n-            inline batch<float, A> average(batch<float, A> const& x1, batch<float, A> const& x2) noexcept\n+            XSIMD_INLINE batch<float, A> average(batch<float, A> const& x1, batch<float, A> const& x2) noexcept\n             {\n                 return averagef(x1, x2);\n             }\n             template <class A>\n-            inline batch<double, A> average(batch<double, A> const& x1, batch<double, A> const& x2) noexcept\n+            XSIMD_INLINE batch<double, A> average(batch<double, A> const& x1, batch<double, A> const& x2) noexcept\n             {\n                 return averagef(x1, x2);\n             }\n         }\n         template <class A>\n-        inline batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<float, A> asinh(batch<float, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type x = abs(self);\n@@ -212,7 +212,7 @@ namespace xsimd\n #endif\n         }\n         template <class A>\n-        inline batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<double, A> asinh(batch<double, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type x = abs(self);\n@@ -226,7 +226,7 @@ namespace xsimd\n             return bitofsign(self) ^ z;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> asinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = asin(batch_type(-z.imag(), z.real()));\n@@ -238,7 +238,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            static inline batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx) noexcept\n+            static XSIMD_INLINE batch<float, A> kernel_atan(const batch<float, A>& x, const batch<float, A>& recx) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 const auto flag1 = x < constants::tan3pio8<batch_type>();\n@@ -259,7 +259,7 @@ namespace xsimd\n                 return yy + z1;\n             }\n             template <class A>\n-            static inline batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx) noexcept\n+            static XSIMD_INLINE batch<double, A> kernel_atan(const batch<double, A>& x, const batch<double, A>& recx) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 const auto flag1 = x < constants::tan3pio8<batch_type>();\n@@ -288,15 +288,15 @@ namespace xsimd\n             }\n         }\n         template <class A, class T>\n-        inline batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atan(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type absa = abs(self);\n             const batch_type x = detail::kernel_atan(absa, batch_type(1.) / absa);\n             return x ^ bitofsign(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> atan(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -327,7 +327,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -338,7 +338,7 @@ namespace xsimd\n             return bitofsign(self) ^ (batch_type(0.5) * log1p(select(test, fma(t, tmp, t), tmp)));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> atanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             batch_type w = atan(batch_type(-z.imag(), z.real()));\n@@ -348,7 +348,7 @@ namespace xsimd\n \n         // atan2\n         template <class A, class T>\n-        inline batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type q = abs(self / other);\n@@ -360,19 +360,19 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, class A>\n-            inline batch<T, A> quadrant(const batch<T, A>& x) noexcept\n+            XSIMD_INLINE batch<T, A> quadrant(const batch<T, A>& x) noexcept\n             {\n                 return x & batch<T, A>(3);\n             }\n \n             template <class A>\n-            inline batch<float, A> quadrant(const batch<float, A>& x) noexcept\n+            XSIMD_INLINE batch<float, A> quadrant(const batch<float, A>& x) noexcept\n             {\n                 return to_float(quadrant(to_int(x)));\n             }\n \n             template <class A>\n-            inline batch<double, A> quadrant(const batch<double, A>& x) noexcept\n+            XSIMD_INLINE batch<double, A> quadrant(const batch<double, A>& x) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type a = x * batch_type(0.25);\n@@ -389,7 +389,7 @@ namespace xsimd\n              */\n \n             template <class A>\n-            inline batch<float, A> cos_eval(const batch<float, A>& z) noexcept\n+            XSIMD_INLINE batch<float, A> cos_eval(const batch<float, A>& z) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -400,7 +400,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x) noexcept\n+            XSIMD_INLINE batch<float, A> sin_eval(const batch<float, A>& z, const batch<float, A>& x) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -411,7 +411,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<float, A> base_tancot_eval(const batch<float, A>& z) noexcept\n+            static XSIMD_INLINE batch<float, A> base_tancot_eval(const batch<float, A>& z) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type zz = z * z;\n@@ -426,15 +426,15 @@ namespace xsimd\n             }\n \n             template <class A, class BB>\n-            static inline batch<float, A> tan_eval(const batch<float, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<float, A> tan_eval(const batch<float, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = base_tancot_eval(z);\n                 return select(test, y, -batch_type(1.) / y);\n             }\n \n             template <class A, class BB>\n-            static inline batch<float, A> cot_eval(const batch<float, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<float, A> cot_eval(const batch<float, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type y = base_tancot_eval(z);\n@@ -451,7 +451,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            static inline batch<double, A> cos_eval(const batch<double, A>& z) noexcept\n+            static XSIMD_INLINE batch<double, A> cos_eval(const batch<double, A>& z) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -466,7 +466,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x) noexcept\n+            static XSIMD_INLINE batch<double, A> sin_eval(const batch<double, A>& z, const batch<double, A>& x) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = detail::horner<batch_type,\n@@ -480,7 +480,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            static inline batch<double, A> base_tancot_eval(const batch<double, A>& z) noexcept\n+            static XSIMD_INLINE batch<double, A> base_tancot_eval(const batch<double, A>& z) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type zz = z * z;\n@@ -497,15 +497,15 @@ namespace xsimd\n             }\n \n             template <class A, class BB>\n-            static inline batch<double, A> tan_eval(const batch<double, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<double, A> tan_eval(const batch<double, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = base_tancot_eval(z);\n                 return select(test, y, -batch_type(1.) / y);\n             }\n \n             template <class A, class BB>\n-            static inline batch<double, A> cot_eval(const batch<double, A>& z, const BB& test) noexcept\n+            static XSIMD_INLINE batch<double, A> cot_eval(const batch<double, A>& z, const BB& test) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type y = base_tancot_eval(z);\n@@ -531,7 +531,7 @@ namespace xsimd\n             template <class B, class Tag = trigo_radian_tag>\n             struct trigo_reducer\n             {\n-                static inline B reduce(const B& x, B& xr) noexcept\n+                static XSIMD_INLINE B reduce(const B& x, B& xr) noexcept\n                 {\n                     if (all(x <= constants::pio4<B>()))\n                     {\n@@ -606,7 +606,7 @@ namespace xsimd\n             template <class B>\n             struct trigo_reducer<B, trigo_pi_tag>\n             {\n-                static inline B reduce(const B& x, B& xr) noexcept\n+                static XSIMD_INLINE B reduce(const B& x, B& xr) noexcept\n                 {\n                     B xi = nearbyint(x * B(2.));\n                     B x2 = x - xi * B(0.5);\n@@ -617,7 +617,7 @@ namespace xsimd\n \n         }\n         template <class A, class T>\n-        inline batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> cos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -634,7 +634,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> cos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return { cos(z.real()) * cosh(z.imag()), -sin(z.real()) * sinh(z.imag()) };\n         }\n@@ -652,7 +652,7 @@ namespace xsimd\n          */\n \n         template <class A, class T>\n-        inline batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type x = abs(self);\n@@ -663,7 +663,7 @@ namespace xsimd\n             return select(test1, tmp1 * tmp, detail::average(tmp, batch_type(1.) / tmp));\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> cosh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             auto x = z.real();\n             auto y = z.imag();\n@@ -674,7 +674,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class Tag = trigo_radian_tag>\n-            inline batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) noexcept\n+            XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self, Tag = Tag()) noexcept\n             {\n                 using batch_type = batch<T, A>;\n                 const batch_type x = abs(self);\n@@ -692,20 +692,20 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sin(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             return detail::sin(self);\n         }\n \n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sin(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             return { sin(z.real()) * cosh(z.imag()), cos(z.real()) * sinh(z.imag()) };\n         }\n \n         // sincos\n         template <class A, class T>\n-        inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -724,7 +724,7 @@ namespace xsimd\n         }\n \n         template <class A, class T>\n-        inline std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n+        XSIMD_INLINE std::pair<batch<std::complex<T>, A>, batch<std::complex<T>, A>>\n         sincos(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n@@ -749,7 +749,7 @@ namespace xsimd\n              * ====================================================\n              */\n             template <class A>\n-            inline batch<float, A> sinh_kernel(batch<float, A> const& self) noexcept\n+            XSIMD_INLINE batch<float, A> sinh_kernel(batch<float, A> const& self) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 batch_type sqr_self = self * self;\n@@ -763,7 +763,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> sinh_kernel(batch<double, A> const& self) noexcept\n+            XSIMD_INLINE batch<double, A> sinh_kernel(batch<double, A> const& self) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 batch_type sqrself = self * self;\n@@ -792,7 +792,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& a, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type half(0.5);\n@@ -814,7 +814,7 @@ namespace xsimd\n             return select(lt1, z, r) ^ bts;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> sinh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             auto x = z.real();\n             auto y = z.imag();\n@@ -823,7 +823,7 @@ namespace xsimd\n \n         // tan\n         template <class A, class T>\n-        inline batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tan(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             const batch_type x = abs(self);\n@@ -836,7 +836,7 @@ namespace xsimd\n             return y ^ bitofsign(self);\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> tan(batch<std::complex<T>, A> const& z, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<std::complex<T>, A>;\n             using real_batch = typename batch_type::real_batch;\n@@ -867,7 +867,7 @@ namespace xsimd\n             struct tanh_kernel<batch<float, A>>\n             {\n                 using batch_type = batch<float, A>;\n-                static inline batch_type tanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type tanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     return fma(detail::horner<batch_type,\n@@ -881,7 +881,7 @@ namespace xsimd\n                                x, x);\n                 }\n \n-                static inline batch_type cotanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type cotanh(const batch_type& x) noexcept\n                 {\n                     return batch_type(1.) / tanh(x);\n                 }\n@@ -891,20 +891,20 @@ namespace xsimd\n             struct tanh_kernel<batch<double, A>>\n             {\n                 using batch_type = batch<double, A>;\n-                static inline batch_type tanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type tanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     return fma(sqrx * p(sqrx) / q(sqrx), x, x);\n                 }\n \n-                static inline batch_type cotanh(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type cotanh(const batch_type& x) noexcept\n                 {\n                     batch_type sqrx = x * x;\n                     batch_type qval = q(sqrx);\n                     return qval / (x * fma(p(sqrx), sqrx, qval));\n                 }\n \n-                static inline batch_type p(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type p(const batch_type& x) noexcept\n                 {\n                     return detail::horner<batch_type,\n                                           0xc0993ac030580563, // -1.61468768441708447952E3\n@@ -913,7 +913,7 @@ namespace xsimd\n                                           >(x);\n                 }\n \n-                static inline batch_type q(const batch_type& x) noexcept\n+                static XSIMD_INLINE batch_type q(const batch_type& x) noexcept\n                 {\n                     return detail::horner1<batch_type,\n                                            0x40b2ec102442040c, //  4.84406305325125486048E3\n@@ -934,7 +934,7 @@ namespace xsimd\n          * ====================================================\n          */\n         template <class A, class T>\n-        inline batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<T, A> tanh(batch<T, A> const& self, requires_arch<generic>) noexcept\n         {\n             using batch_type = batch<T, A>;\n             batch_type one(1.);\n@@ -952,7 +952,7 @@ namespace xsimd\n             return select(test, z, r) ^ bts;\n         }\n         template <class A, class T>\n-        inline batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> tanh(const batch<std::complex<T>, A>& z, requires_arch<generic>) noexcept\n         {\n             using real_batch = typename batch<std::complex<T>, A>::real_batch;\n             auto x = z.real();"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx.hpp",
        "status": "modified",
        "additions": 200,
        "deletions": 200,
        "changes": 400,
        "patch": "@@ -27,39 +27,39 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n-            inline void split_avx(__m256i val, __m128i& low, __m128i& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256i val, __m128i& low, __m128i& high) noexcept\n             {\n                 low = _mm256_castsi256_si128(val);\n                 high = _mm256_extractf128_si256(val, 1);\n             }\n-            inline void split_avx(__m256 val, __m128& low, __m128& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256 val, __m128& low, __m128& high) noexcept\n             {\n                 low = _mm256_castps256_ps128(val);\n                 high = _mm256_extractf128_ps(val, 1);\n             }\n-            inline void split_avx(__m256d val, __m128d& low, __m128d& high) noexcept\n+            XSIMD_INLINE void split_avx(__m256d val, __m128d& low, __m128d& high) noexcept\n             {\n                 low = _mm256_castpd256_pd128(val);\n                 high = _mm256_extractf128_pd(val, 1);\n             }\n-            inline __m256i merge_sse(__m128i low, __m128i high) noexcept\n+            XSIMD_INLINE __m256i merge_sse(__m128i low, __m128i high) noexcept\n             {\n                 return _mm256_insertf128_si256(_mm256_castsi128_si256(low), high, 1);\n             }\n-            inline __m256 merge_sse(__m128 low, __m128 high) noexcept\n+            XSIMD_INLINE __m256 merge_sse(__m128 low, __m128 high) noexcept\n             {\n                 return _mm256_insertf128_ps(_mm256_castps128_ps256(low), high, 1);\n             }\n-            inline __m256d merge_sse(__m128d low, __m128d high) noexcept\n+            XSIMD_INLINE __m256d merge_sse(__m128d low, __m128d high) noexcept\n             {\n                 return _mm256_insertf128_pd(_mm256_castpd128_pd256(low), high, 1);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self) noexcept\n             {\n                 __m128i self_low, self_high;\n                 split_avx(self, self_low, self_high);\n@@ -68,7 +68,7 @@ namespace xsimd\n                 return merge_sse(res_low, res_high);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self, __m256i other) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self, __m256i other) noexcept\n             {\n                 __m128i self_low, self_high, other_low, other_high;\n                 split_avx(self, self_low, self_high);\n@@ -78,7 +78,7 @@ namespace xsimd\n                 return merge_sse(res_low, res_high);\n             }\n             template <class F>\n-            inline __m256i fwd_to_sse(F f, __m256i self, int32_t other) noexcept\n+            XSIMD_INLINE __m256i fwd_to_sse(F f, __m256i self, int32_t other) noexcept\n             {\n                 __m128i self_low, self_high;\n                 split_avx(self, self_low, self_high);\n@@ -90,110 +90,110 @@ namespace xsimd\n \n         // abs\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m256 sign_mask = _mm256_set1_ps(-0.f); // -0.f = 1 << 31\n             return _mm256_andnot_ps(sign_mask, self);\n         }\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m256d sign_mask = _mm256_set1_pd(-0.f); // -0.f = 1 << 31\n             return _mm256_andnot_pd(sign_mask, self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return add(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_add_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_add_pd(self, other);\n         }\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_ps(self, batch_bool<float, A>(true)) != 0;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_pd(self, batch_bool<double, A>(true)) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_testc_si256(self, batch_bool<T, A>(true)) != 0;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_ps(self, self);\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_pd(self, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return !_mm256_testz_si256(self, self);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_and_pd(self, other);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_and(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -202,36 +202,36 @@ namespace xsimd\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_pd(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_andnot_pd(other, self);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_andnot(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -240,7 +240,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, int32_t o) noexcept\n                                       { return bitwise_lshift(batch<T, sse4_2>(s), o, sse4_2 {}); },\n@@ -249,14 +249,14 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s) noexcept\n                                       { return bitwise_not(batch<T, sse4_2>(s), sse4_2 {}); },\n                                       self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s) noexcept\n                                       { return bitwise_not(batch_bool<T, sse4_2>(s), sse4_2 {}); },\n@@ -265,34 +265,34 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_or_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_or(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_or(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o)); },\n@@ -301,7 +301,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, int32_t o) noexcept\n                                       { return bitwise_rshift(batch<T, sse4_2>(s), o, sse4_2 {}); },\n@@ -310,34 +310,34 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_xor(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2 {}); },\n                                       self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return bitwise_xor(batch_bool<T, sse4_2>(s), batch_bool<T, sse4_2>(o), sse4_2 {}); },\n@@ -346,66 +346,66 @@ namespace xsimd\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castsi256_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castsi256_pd(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_pd(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_si256(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx>) noexcept\n         {\n             return _mm256_castpd_si256(self);\n         }\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));\n         }\n \n         // broadcast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -430,24 +430,24 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> broadcast(float val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float val, requires_arch<avx>) noexcept\n         {\n             return _mm256_set1_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<avx>) noexcept\n         {\n             return _mm256_set1_pd(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_ceil_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_ceil_pd(self);\n         }\n@@ -457,7 +457,7 @@ namespace xsimd\n             // On clang, _mm256_extractf128_ps is built upon build_shufflevector\n             // which require index parameter to be a constant\n             template <int index, class B>\n-            inline B get_half_complex_f(const B& real, const B& imag) noexcept\n+            XSIMD_INLINE B get_half_complex_f(const B& real, const B& imag) noexcept\n             {\n                 __m128 tmp0 = _mm256_extractf128_ps(real, index);\n                 __m128 tmp1 = _mm256_extractf128_ps(imag, index);\n@@ -469,7 +469,7 @@ namespace xsimd\n                 return res;\n             }\n             template <int index, class B>\n-            inline B get_half_complex_d(const B& real, const B& imag) noexcept\n+            XSIMD_INLINE B get_half_complex_d(const B& real, const B& imag) noexcept\n             {\n                 __m128d tmp0 = _mm256_extractf128_pd(real, index);\n                 __m128d tmp1 = _mm256_extractf128_pd(imag, index);\n@@ -483,24 +483,24 @@ namespace xsimd\n \n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_f<0>(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_d<0>(self.real(), self.imag());\n             }\n \n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_f<1>(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx>) noexcept\n             {\n                 return get_half_complex_d<1>(self.real(), self.imag());\n             }\n@@ -510,87 +510,87 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx>) noexcept\n             {\n                 return _mm256_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx>) noexcept\n             {\n                 return _mm256_cvttps_epi32(self);\n             }\n         }\n \n         // decr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_div_pd(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return eq(batch<T, sse4_2>(s), batch<T, sse4_2>(o), sse4_2 {}); },\n                                       self, other);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self != other);\n         }\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_floor_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_floor_pd(self);\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut32[] = {\n                 0x0000000000000000ul,\n@@ -602,7 +602,7 @@ namespace xsimd\n             return _mm256_castsi256_ps(_mm256_setr_epi64x(lut32[mask & 0x3], lut32[(mask >> 2) & 0x3], lut32[(mask >> 4) & 0x3], lut32[mask >> 6]));\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul, 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -626,7 +626,7 @@ namespace xsimd\n             return _mm256_castsi256_pd(_mm256_load_si256((const __m256i*)lut64[mask]));\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut32[] = {\n                 0x00000000,\n@@ -689,7 +689,7 @@ namespace xsimd\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx>) noexcept\n         {\n             // row = (a,b,c,d,e,f,g,h)\n             // tmp0 = (a0+a1, a2+a3, b0+b1, b2+b3, a4+a5, a6+a7, b4+b5, b6+b7)\n@@ -715,7 +715,7 @@ namespace xsimd\n             return _mm256_add_ps(tmp0, tmp1);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx>) noexcept\n         {\n             // row = (a,b,c,d)\n             // tmp0 = (a0+a1, b0+b1, a2+a3, b2+b3)\n@@ -731,14 +731,14 @@ namespace xsimd\n \n         // incr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<avx>) noexcept\n         {\n #if !defined(_MSC_VER) || _MSC_VER > 1900\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n@@ -763,41 +763,41 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, self, _CMP_UNORD_Q);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, self, _CMP_UNORD_Q);\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_LE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_LE_OQ);\n         }\n \n         // load_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_si256((__m256i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n         {\n             return _mm256_load_pd(mem);\n         }\n@@ -806,7 +806,7 @@ namespace xsimd\n         {\n             // load_complex\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx>) noexcept\n             {\n                 using batch_type = batch<float, A>;\n                 __m128 tmp0 = _mm256_extractf128_ps(hi, 0);\n@@ -825,7 +825,7 @@ namespace xsimd\n                 return { real, imag };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx>) noexcept\n             {\n                 using batch_type = batch<double, A>;\n                 __m128d tmp0 = _mm256_extractf128_pd(hi, 0);\n@@ -845,35 +845,35 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_si256((__m256i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx>) noexcept\n         {\n             return _mm256_loadu_pd(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_LT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_LT_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return lt(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n@@ -882,7 +882,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -905,86 +905,86 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_movemask_ps(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_movemask_pd(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_max_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_max_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_min_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_min_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_mul_pd(self, other);\n         }\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<avx>) noexcept\n         {\n             return _mm256_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return 0 - self;\n         }\n@@ -994,55 +994,55 @@ namespace xsimd\n             return _mm256_xor_ps(self, _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)));\n         }\n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)));\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_ps(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_cmp_pd(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_castps_si256(_mm256_xor_ps(_mm256_castsi256_ps(self.data), _mm256_castsi256_ps(other.data)));\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self,\n-                                          kernel::requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self,\n+                                                kernel::requires_arch<avx>) noexcept\n         {\n             return _mm256_rcp_ps(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx>) noexcept\n         {\n             // Warning about _mm256_hadd_ps:\n             // _mm256_hadd_ps(a,b) gives\n@@ -1060,7 +1060,7 @@ namespace xsimd\n             return _mm_cvtss_f32(_mm256_extractf128_ps(tmp, 0));\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& rhs, requires_arch<avx>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& rhs, requires_arch<avx>) noexcept\n         {\n             // rhs = (x0, x1, x2, x3)\n             // tmp = (x2, x3, x0, x1)\n@@ -1072,7 +1072,7 @@ namespace xsimd\n             return _mm_cvtsd_f64(_mm256_extractf128_pd(tmp, 0));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             __m128i low, high;\n             detail::split_avx(self, low, high);\n@@ -1082,7 +1082,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(1, 0);\n             batch<T, A> step = _mm256_permute2f128_si256(self, self, mask);\n@@ -1093,7 +1093,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(1, 0);\n             batch<T, A> step = _mm256_permute2f128_si256(self, self, mask);\n@@ -1104,19 +1104,19 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_rsqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_cvtps_pd(_mm_rsqrt_ps(_mm256_cvtpd_ps(val)));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1135,17 +1135,17 @@ namespace xsimd\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return _mm256_blendv_ps(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return _mm256_blendv_pd(false_br, true_br, cond);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             __m128i cond_low, cond_hi;\n             detail::split_avx(cond, cond_low, cond_hi);\n@@ -1161,84 +1161,84 @@ namespace xsimd\n             return detail::merge_sse(res_low, res_hi);\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, avx2 {});\n         }\n \n         template <class A, bool... Values>\n-        inline batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = batch_bool_constant<float, A, Values...>::mask();\n             return _mm256_blend_ps(false_br, true_br, mask);\n         }\n \n         template <class A, bool... Values>\n-        inline batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx>) noexcept\n         {\n             constexpr auto mask = batch_bool_constant<double, A, Values...>::mask();\n             return _mm256_blend_pd(false_br, true_br, mask);\n         }\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return _mm256_setr_ps(values...);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return _mm256_setr_pd(values...);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return _mm256_set_epi64x(v3, v2, v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm256_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm256_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23, T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n             return _mm256_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26, v27, v28, v29, v30, v31);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return _mm256_castsi256_ps(set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<avx>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return _mm256_castsi256_pd(set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1, I2, I3);\n             // shuffle within lane\n@@ -1253,7 +1253,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<avx>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x1) | ((I1 & 0x1) << 1) | ((I2 & 0x1) << 2) | ((I3 & 0x1) << 3);\n             // shuffle within lane\n@@ -1269,7 +1269,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -1310,7 +1310,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -1350,19 +1350,19 @@ namespace xsimd\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx>) noexcept\n         {\n             return _mm256_sqrt_pd(val);\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1377,70 +1377,70 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_si256((__m256i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_si256((__m256i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_si256((__m256i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_si256((__m256i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             return detail::fwd_to_sse([](__m128i s, __m128i o) noexcept\n                                       { return sub(batch<T, sse4_2>(s), batch<T, sse4_2>(o)); },\n                                       self, other);\n         }\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_sub_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             return _mm256_sub_pd(self, other);\n         }\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256 hi = _mm256_castps128_ps256(_mm256_extractf128_ps(self, 1));\n@@ -1464,7 +1464,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256d hi = _mm256_castpd128_pd256(_mm256_extractf128_pd(self, 1));\n@@ -1488,14 +1488,14 @@ namespace xsimd\n         }\n \n         template <class A, typename T, detail::enable_sized_integral_t<T, 4> = 0>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch<uint32_t, A> const& mask, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch<uint32_t, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n                 swizzle(bitwise_cast<float>(self), mask));\n         }\n \n         template <class A, typename T, detail::enable_sized_integral_t<T, 8> = 0>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         swizzle(batch<T, A> const& self, batch<uint64_t, A> const& mask, requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n@@ -1504,7 +1504,7 @@ namespace xsimd\n \n         // swizzle (constant mask)\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256 hi = _mm256_castps128_ps256(_mm256_extractf128_ps(self, 1));\n@@ -1529,7 +1529,7 @@ namespace xsimd\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx>) noexcept\n         {\n             // duplicate low and high part of input\n             __m256d hi = _mm256_castpd128_pd256(_mm256_extractf128_pd(self, 1));\n@@ -1563,17 +1563,17 @@ namespace xsimd\n                   uint32_t V6,\n                   uint32_t V7,\n                   detail::enable_sized_integral_t<T, 4> = 0>\n-        inline batch<T, A> swizzle(batch<T, A> const& self,\n-                                   batch_constant<uint32_t, A,\n-                                                  V0,\n-                                                  V1,\n-                                                  V2,\n-                                                  V3,\n-                                                  V4,\n-                                                  V5,\n-                                                  V6,\n-                                                  V7> const& mask,\n-                                   requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self,\n+                                         batch_constant<uint32_t, A,\n+                                                        V0,\n+                                                        V1,\n+                                                        V2,\n+                                                        V3,\n+                                                        V4,\n+                                                        V5,\n+                                                        V6,\n+                                                        V7> const& mask,\n+                                         requires_arch<avx>) noexcept\n         {\n             return bitwise_cast<T>(\n                 swizzle(bitwise_cast<float>(self), mask));\n@@ -1586,7 +1586,7 @@ namespace xsimd\n                   uint64_t V2,\n                   uint64_t V3,\n                   detail::enable_sized_integral_t<T, 8> = 0>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         swizzle(batch<T, A> const& self,\n                 batch_constant<uint64_t, A, V0, V1, V2, V3> const& mask,\n                 requires_arch<avx>) noexcept\n@@ -1597,19 +1597,19 @@ namespace xsimd\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_ps(self, _MM_FROUND_TO_ZERO);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<avx>) noexcept\n         {\n             return _mm256_round_pd(self, _MM_FROUND_TO_ZERO);\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -1656,14 +1656,14 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_ps(self, other);\n             auto hi = _mm256_unpackhi_ps(self, other);\n             return _mm256_permute2f128_ps(lo, hi, 0x31);\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_pd(self, other);\n             auto hi = _mm256_unpackhi_pd(self, other);\n@@ -1672,7 +1672,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1 || sizeof(T) == 2)\n             {\n@@ -1720,14 +1720,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_ps(self, other);\n             auto hi = _mm256_unpackhi_ps(self, other);\n             return _mm256_insertf128_ps(lo, _mm256_castps256_ps128(hi), 1);\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx>) noexcept\n         {\n             auto lo = _mm256_unpacklo_pd(self, other);\n             auto hi = _mm256_unpackhi_pd(self, other);"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx2.hpp",
        "status": "modified",
        "additions": 70,
        "deletions": 70,
        "changes": 140,
        "patch": "@@ -26,7 +26,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -52,7 +52,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -78,7 +78,7 @@ namespace xsimd\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -96,7 +96,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -116,43 +116,43 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_and_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_and_si256(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_andnot_si256(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_andnot_si256(other, self);\n         }\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, _mm256_set1_epi32(-1));\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -173,7 +173,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -191,19 +191,19 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_or_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_or_si256(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -253,7 +253,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -285,19 +285,19 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             return _mm256_xor_si256(self, other);\n         }\n \n         // complex_low\n         template <class A>\n-        inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n         {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 1, 1, 0));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(1, 2, 0, 0));\n@@ -306,7 +306,7 @@ namespace xsimd\n \n         // complex_high\n         template <class A>\n-        inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx2>) noexcept\n         {\n             __m256d tmp0 = _mm256_permute4x64_pd(self.real(), _MM_SHUFFLE(3, 3, 1, 2));\n             __m256d tmp1 = _mm256_permute4x64_pd(self.imag(), _MM_SHUFFLE(3, 2, 2, 0));\n@@ -318,7 +318,7 @@ namespace xsimd\n         {\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to avx\n@@ -332,7 +332,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to avx\n@@ -349,7 +349,7 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -375,55 +375,55 @@ namespace xsimd\n \n         // gather\n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 4> = 0, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i32gather_epi32(reinterpret_cast<const int*>(src), index, sizeof(T));\n         }\n \n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 8> = 0, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i64gather_epi64(reinterpret_cast<const long long int*>(src), index, sizeof(T));\n         }\n \n         template <class A, class U,\n                   detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, float const* src,\n-                                      batch<U, A> const& index,\n-                                      kernel::requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, float const* src,\n+                                            batch<U, A> const& index,\n+                                            kernel::requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i32gather_ps(src, index, sizeof(float));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<double, A> gather(batch<double, A> const&, double const* src,\n-                                       batch<U, A> const& index,\n-                                       requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> gather(batch<double, A> const&, double const* src,\n+                                             batch<U, A> const& index,\n+                                             requires_arch<avx2>) noexcept\n         {\n             // scatter for this one is AVX512F+AVX512VL\n             return _mm256_i64gather_pd(src, index, sizeof(double));\n         }\n \n         // gather: handmade conversions\n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, double const* src,\n-                                      batch<V, A> const& index,\n-                                      requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, double const* src,\n+                                            batch<V, A> const& index,\n+                                            requires_arch<avx2>) noexcept\n         {\n             const batch<double, A> low(_mm256_i32gather_pd(src, _mm256_castsi256_si128(index.data), sizeof(double)));\n             const batch<double, A> high(_mm256_i32gather_pd(src, _mm256_extractf128_si256(index.data, 1), sizeof(double)));\n             return detail::merge_sse(_mm256_cvtpd_ps(low.data), _mm256_cvtpd_ps(high.data));\n         }\n \n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n-                                        batch<V, A> const& index,\n-                                        requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n+                                              batch<V, A> const& index,\n+                                              requires_arch<avx2>) noexcept\n         {\n             const batch<double, A> low(_mm256_i32gather_pd(src, _mm256_castsi256_si128(index.data), sizeof(double)));\n             const batch<double, A> high(_mm256_i32gather_pd(src, _mm256_extractf128_si256(index.data, 1), sizeof(double)));\n@@ -432,7 +432,7 @@ namespace xsimd\n \n         // lt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -465,7 +465,7 @@ namespace xsimd\n \n         // load_complex\n         template <class A>\n-        inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx2>) noexcept\n         {\n             using batch_type = batch<float, A>;\n             batch_type real = _mm256_castpd_ps(\n@@ -479,7 +479,7 @@ namespace xsimd\n             return { real, imag };\n         }\n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx2>) noexcept\n         {\n             using batch_type = batch<double, A>;\n             batch_type real = _mm256_permute4x64_pd(_mm256_unpacklo_pd(hi, lo), _MM_SHUFFLE(3, 1, 2, 0));\n@@ -488,7 +488,7 @@ namespace xsimd\n         }\n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -507,7 +507,7 @@ namespace xsimd\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -551,7 +551,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -595,7 +595,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -623,7 +623,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -657,19 +657,19 @@ namespace xsimd\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return _mm256_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), avx2 {}));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -705,7 +705,7 @@ namespace xsimd\n \n         // select\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -729,7 +729,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx2>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<T, A, Values...>::mask();\n             // FIXME: for some reason mask here is not considered as an immediate,\n@@ -752,7 +752,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx2>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -783,7 +783,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx2>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -814,7 +814,7 @@ namespace xsimd\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -850,7 +850,7 @@ namespace xsimd\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -876,79 +876,79 @@ namespace xsimd\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_ps(self, mask);\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             batch<uint32_t, A> broadcaster = { 0, 1, 0, 1, 0, 1, 0, 1 };\n             constexpr uint64_t comb = 0x0000000100000001ul * 2;\n             return bitwise_cast<double>(swizzle(bitwise_cast<float>(self), bitwise_cast<uint32_t>(mask * comb) + broadcaster, avx2 {}));\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<uint64_t>(swizzle(bitwise_cast<double>(self), mask, avx2 {}));\n         }\n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<double>(self), mask, avx2 {}));\n         }\n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_epi32(self, mask);\n         }\n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx2 {}));\n         }\n \n         // swizzle (constant mask)\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_ps(self, mask.as_batch());\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(V0, V1, V2, V3);\n             return _mm256_permute4x64_pd(self, mask);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3>, requires_arch<avx2>) noexcept\n         {\n             constexpr auto mask = detail::shuffle(V0, V1, V2, V3);\n             return _mm256_permute4x64_epi64(self, mask);\n         }\n         template <class A, uint64_t V0, uint64_t V1, uint64_t V2, uint64_t V3>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1, V2, V3> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, avx2 {}));\n         }\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return _mm256_permutevar8x32_epi32(self, mask.as_batch());\n         }\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3, uint32_t V4, uint32_t V5, uint32_t V6, uint32_t V7>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<avx2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx2 {}));\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -983,7 +983,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512bw.hpp",
        "status": "modified",
        "additions": 34,
        "deletions": 34,
        "changes": 68,
        "patch": "@@ -27,7 +27,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, int Cmp>\n-            inline batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch_bool<T, A> compare_int_avx512bw(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 using register_type = typename batch_bool<T, A>::register_type;\n                 if (std::is_signed<T>::value)\n@@ -73,7 +73,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n             {\n@@ -96,7 +96,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -114,7 +114,7 @@ namespace xsimd\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -132,7 +132,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -152,7 +152,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n         {\n #if defined(XSIMD_AVX512_SHIFT_INTRINSICS_IMM_ONLY)\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n@@ -172,7 +172,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -226,42 +226,42 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_EQ>(self, other);\n         }\n \n         // ge\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GE>(self, other);\n         }\n \n         // gt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_GT>(self, other);\n         }\n \n         // le\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LE>(self, other);\n         }\n \n         // lt\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_LT>(self, other);\n         }\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -297,7 +297,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -333,7 +333,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -353,26 +353,26 @@ namespace xsimd\n \n         // neq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             return detail::compare_int_avx512bw<A, T, _MM_CMPINT_NE>(self, other);\n         }\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), avx2 {}));\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -408,7 +408,7 @@ namespace xsimd\n \n         // select\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -446,7 +446,7 @@ namespace xsimd\n         }\n \n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -505,7 +505,7 @@ namespace xsimd\n             }\n         }\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<avx512bw>) noexcept\n         {\n             constexpr unsigned BitCount = N * 8;\n             if (BitCount == 0)\n@@ -538,7 +538,7 @@ namespace xsimd\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -574,7 +574,7 @@ namespace xsimd\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -592,57 +592,57 @@ namespace xsimd\n \n         // swizzle (dynamic version)\n         template <class A>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_permutexvar_epi16(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch<uint16_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, avx512bw {}));\n         }\n \n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return _mm512_shuffle_epi8(self, mask);\n         }\n \n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<avx512bw>) noexcept\n         {\n             return bitwise_cast<int8_t>(swizzle(bitwise_cast<uint8_t>(self), mask, avx512bw {}));\n         }\n \n         // swizzle (static version)\n         template <class A, uint16_t... Vs>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint16_t... Vs>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint8_t... Vs>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         template <class A, uint8_t... Vs>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, Vs...> mask, requires_arch<avx512bw>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512bw {});\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             __m512i lo, hi;\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n@@ -670,7 +670,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512bw>) noexcept\n         {\n             __m512i lo, hi;\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512dq.hpp",
        "status": "modified",
        "additions": 20,
        "deletions": 20,
        "changes": 40,
        "patch": "@@ -23,74 +23,74 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_and_pd(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_andnot_ps(other, self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_andnot_pd(other, self);\n         }\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_ps(self, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_pd(self, _mm512_castsi512_pd(_mm512_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_or_pd(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_xor_pd(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512dq>) noexcept\n         {\n             // The following folds over the vector once:\n             // tmp1 = [a0..8, b0..8]\n@@ -152,35 +152,35 @@ namespace xsimd\n \n         // ldexp\n         template <class A>\n-        inline batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_scalef_pd(self, _mm512_cvtepi64_pd(other));\n         }\n \n         // mul\n         template <class A>\n-        inline batch<uint64_t, A> mul(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> mul(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_mullo_epi64(self, other);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> mul(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> mul(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_mullo_epi64(self, other);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n-                                                  requires_arch<avx512dq>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n+                                                        requires_arch<avx512dq>) noexcept\n         {\n             return _mm512_cvtpd_epi64(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m256 tmp1 = _mm512_extractf32x8_ps(rhs, 1);\n             __m256 tmp2 = _mm512_extractf32x8_ps(rhs, 0);\n@@ -192,13 +192,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx512dq>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<avx512dq>) noexcept\n             {\n                 return _mm512_cvtepi64_pd(self);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<avx512dq>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<avx512dq>) noexcept\n             {\n                 return _mm512_cvttpd_epi64(self);\n             }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_avx512f.hpp",
        "status": "modified",
        "additions": 245,
        "deletions": 245,
        "changes": 490,
        "patch": "@@ -27,30 +27,30 @@ namespace xsimd\n \n         namespace detail\n         {\n-            inline void split_avx512(__m512 val, __m256& low, __m256& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512 val, __m256& low, __m256& high) noexcept\n             {\n                 low = _mm512_castps512_ps256(val);\n                 high = _mm512_extractf32x8_ps(val, 1);\n             }\n-            inline void split_avx512(__m512d val, __m256d& low, __m256d& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512d val, __m256d& low, __m256d& high) noexcept\n             {\n                 low = _mm512_castpd512_pd256(val);\n                 high = _mm512_extractf64x4_pd(val, 1);\n             }\n-            inline void split_avx512(__m512i val, __m256i& low, __m256i& high) noexcept\n+            XSIMD_INLINE void split_avx512(__m512i val, __m256i& low, __m256i& high) noexcept\n             {\n                 low = _mm512_castsi512_si256(val);\n                 high = _mm512_extracti64x4_epi64(val, 1);\n             }\n-            inline __m512i merge_avx(__m256i low, __m256i high) noexcept\n+            XSIMD_INLINE __m512i merge_avx(__m256i low, __m256i high) noexcept\n             {\n                 return _mm512_inserti64x4(_mm512_castsi256_si512(low), high, 1);\n             }\n-            inline __m512 merge_avx(__m256 low, __m256 high) noexcept\n+            XSIMD_INLINE __m512 merge_avx(__m256 low, __m256 high) noexcept\n             {\n                 return _mm512_castpd_ps(_mm512_insertf64x4(_mm512_castpd256_pd512(_mm256_castps_pd(low)), _mm256_castps_pd(high), 1));\n             }\n-            inline __m512d merge_avx(__m256d low, __m256d high) noexcept\n+            XSIMD_INLINE __m512d merge_avx(__m256d low, __m256d high) noexcept\n             {\n                 return _mm512_insertf64x4(_mm512_castpd256_pd512(low), high, 1);\n             }\n@@ -86,7 +86,7 @@ namespace xsimd\n         namespace detail\n         {\n \n-            inline uint32_t morton(uint16_t x, uint16_t y) noexcept\n+            XSIMD_INLINE uint32_t morton(uint16_t x, uint16_t y) noexcept\n             {\n \n                 static const unsigned short MortonTable256[256] = {\n@@ -129,7 +129,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, int Cmp>\n-            inline batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            XSIMD_INLINE batch_bool<T, A> compare_int_avx512f(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 using register_type = typename batch_bool<T, A>::register_type;\n                 if (std::is_signed<T>::value)\n@@ -217,15 +217,15 @@ namespace xsimd\n \n         // abs\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m512 self_asf = (__m512)self;\n             __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asf);\n             __m512i res_asi = _mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF), self_asi);\n             return *reinterpret_cast<__m512*>(&res_asi);\n         }\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m512d self_asd = (__m512d)self;\n             __m512i self_asi = *reinterpret_cast<__m512i*>(&self_asd);\n@@ -234,7 +234,7 @@ namespace xsimd\n             return *reinterpret_cast<__m512d*>(&res_asi);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_unsigned<T>::value)\n             {\n@@ -270,7 +270,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -299,42 +299,42 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_add_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_add_pd(self, other);\n         }\n \n         // all\n         template <class A, class T>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return self.data == register_type(-1);\n         }\n \n         // any\n         template <class A, class T>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return self.data != register_type(0);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return self.data;\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n #if defined(_MSC_VER)\n             return _mm512_and_ps(self, other);\n@@ -343,52 +343,52 @@ namespace xsimd\n #endif\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_and_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_and_si512(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data & other.data);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_andnot_si512(_mm512_castps_si512(other), _mm512_castps_si512(self)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_andnot_si512(_mm512_castpd_si512(other), _mm512_castpd_si512(self)));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_andnot_si512(other, self);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data & ~other.data);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -433,56 +433,56 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_si512(self, _mm512_set1_epi32(-1));\n         }\n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(~self.data);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_or_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_or_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_or_si512(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -552,69 +552,69 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(self), _mm512_castps_si512(other)));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(self), _mm512_castpd_si512(other)));\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data | other.data);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_xor_si512(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castsi512_pd(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castps_pd(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castps_si512(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_castpd_si512(self);\n         }\n \n         // broadcast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -639,56 +639,56 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> broadcast(float val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_set1_ps(val);\n         }\n         template <class A>\n-        batch<double, A> inline broadcast(double val, requires_arch<avx512f>) noexcept\n+        batch<double, A> XSIMD_INLINE broadcast(double val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_set1_pd(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_ps(self, _MM_FROUND_TO_POS_INF);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_pd(self, _MM_FROUND_TO_POS_INF);\n         }\n \n         // compress\n         template <class A>\n-        inline batch<float, A> compress(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> compress(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_ps(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<double, A> compress(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> compress(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_pd(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int32_t, A> compress(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> compress(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint32_t, A> compress(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> compress(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int64_t, A> compress(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> compress(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi64(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint64_t, A> compress(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> compress(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_compress_epi64(mask.mask(), self);\n         }\n@@ -697,19 +697,19 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvttps_epi32(self);\n             }\n \n             template <class A>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<avx512f>) noexcept\n             {\n                 return _mm512_cvtepu32_ps(self);\n             }\n@@ -725,27 +725,27 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi32(0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23);\n                 return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi64(0, 8, 1, 9, 2, 10, 3, 11);\n                 return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n             }\n \n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi32(8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31);\n                 return _mm512_permutex2var_ps(self.real(), idx, self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<avx512f>) noexcept\n             {\n                 __m512i idx = _mm512_setr_epi64(4, 12, 5, 13, 6, 14, 7, 15);\n                 return _mm512_permutex2var_pd(self.real(), idx, self.imag());\n@@ -754,162 +754,162 @@ namespace xsimd\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_div_pd(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_EQ_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_EQ_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_EQ>(self, other);\n         }\n         template <class A, class T>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(~self.data ^ other.data);\n         }\n \n         // expand\n         template <class A>\n-        inline batch<float, A> expand(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> expand(batch<float, A> const& self, batch_bool<float, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_ps(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<double, A> expand(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> expand(batch<double, A> const& self, batch_bool<double, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_pd(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int32_t, A> expand(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> expand(batch<int32_t, A> const& self, batch_bool<int32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint32_t, A> expand(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> expand(batch<uint32_t, A> const& self, batch_bool<uint32_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi32(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<int64_t, A> expand(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> expand(batch<int64_t, A> const& self, batch_bool<int64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi64(mask.mask(), self);\n         }\n         template <class A>\n-        inline batch<uint64_t, A> expand(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> expand(batch<uint64_t, A> const& self, batch_bool<uint64_t, A> const& mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_maskz_expand_epi64(mask.mask(), self);\n         }\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_ps(self, _MM_FROUND_TO_NEG_INF);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_pd(self, _MM_FROUND_TO_NEG_INF);\n         }\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fnmadd_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_fmsub_pd(x, y, z);\n         }\n \n         // from bool\n         template <class A, class T>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return select(self, batch<T, A>(1), batch<T, A>(0));\n         }\n \n         // from_mask\n         template <class T, class A>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<avx512f>) noexcept\n         {\n             return static_cast<typename batch_bool<T, A>::register_type>(mask);\n         }\n \n         // gather\n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 4> = 0, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i32gather_epi32(index, static_cast<const void*>(src), sizeof(T));\n         }\n \n         template <class T, class A, class U, detail::enable_sized_integral_t<T, 8> = 0, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n-                                  kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index,\n+                                        kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i64gather_epi64(index, static_cast<const void*>(src), sizeof(T));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, float const* src,\n-                                      batch<U, A> const& index,\n-                                      kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, float const* src,\n+                                            batch<U, A> const& index,\n+                                            kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_i32gather_ps(index, src, sizeof(float));\n         }\n \n         template <class A, class U, detail::enable_sized_integral_t<U, 8> = 0>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         gather(batch<double, A> const&, double const* src, batch<U, A> const& index,\n                kernel::requires_arch<avx512f>) noexcept\n         {\n@@ -918,19 +918,19 @@ namespace xsimd\n \n         // gather: handmade conversions\n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<float, A> gather(batch<float, A> const&, double const* src,\n-                                      batch<V, A> const& index,\n-                                      requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> gather(batch<float, A> const&, double const* src,\n+                                            batch<V, A> const& index,\n+                                            requires_arch<avx512f>) noexcept\n         {\n             const batch<double, A> low(_mm512_i32gather_pd(_mm512_castsi512_si256(index.data), src, sizeof(double)));\n             const batch<double, A> high(_mm512_i32gather_pd(_mm256_castpd_si256(_mm512_extractf64x4_pd(_mm512_castsi512_pd(index.data), 1)), src, sizeof(double)));\n             return detail::merge_avx(_mm512_cvtpd_ps(low.data), _mm512_cvtpd_ps(high.data));\n         }\n \n         template <class A, class V, detail::enable_sized_integral_t<V, 4> = 0>\n-        inline batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n-                                        batch<V, A> const& index,\n-                                        requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> gather(batch<int32_t, A> const&, double const* src,\n+                                              batch<V, A> const& index,\n+                                              requires_arch<avx512f>) noexcept\n         {\n             const batch<double, A> low(_mm512_i32gather_pd(_mm512_castsi512_si256(index.data), src, sizeof(double)));\n             const batch<double, A> high(_mm512_i32gather_pd(_mm256_castpd_si256(_mm512_extractf64x4_pd(_mm512_castsi512_pd(index.data), 1)), src, sizeof(double)));\n@@ -939,41 +939,41 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_GE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_GE_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_GE>(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_GT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_GT_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_GT>(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<avx512f>) noexcept\n         {\n             // The following folds over the vector once:\n             // tmp1 = [a0..8, b0..8]\n@@ -1034,7 +1034,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<avx512f>) noexcept\n         {\n #define step1(I, a, b)                                                   \\\n     batch<double, avx512f> res##I;                                       \\\n@@ -1069,25 +1069,25 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, self, _CMP_UNORD_Q);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, self, _CMP_UNORD_Q);\n         }\n \n         // ldexp\n         template <class A>\n-        inline batch<float, A> ldexp(const batch<float, A>& self, const batch<as_integer_t<float>, A>& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> ldexp(const batch<float, A>& self, const batch<as_integer_t<float>, A>& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_scalef_ps(self, _mm512_cvtepi32_ps(other));\n         }\n \n         template <class A>\n-        inline batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> ldexp(const batch<double, A>& self, const batch<as_integer_t<double>, A>& other, requires_arch<avx512f>) noexcept\n         {\n             // FIXME: potential data loss here when converting other elements to\n             // int32 before converting them back to double.\n@@ -1097,34 +1097,34 @@ namespace xsimd\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_LE_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_LE_OQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_LE>(self, other);\n         }\n \n         // load_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_si512((__m512i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_load_pd(mem);\n         }\n@@ -1133,7 +1133,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<avx512f>) noexcept\n             {\n                 __m512i real_idx = _mm512_setr_epi32(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30);\n                 __m512i imag_idx = _mm512_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31);\n@@ -1142,7 +1142,7 @@ namespace xsimd\n                 return { real, imag };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx512f>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<avx512f>) noexcept\n             {\n                 __m512i real_idx = _mm512_setr_epi64(0, 2, 4, 6, 8, 10, 12, 14);\n                 __m512i imag_idx = _mm512_setr_epi64(1, 3, 5, 7, 9, 11, 13, 15);\n@@ -1154,59 +1154,59 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_si512((__m512i const*)mem);\n         }\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_ps(mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_loadu_pd(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_LT_OQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_LT_OQ);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return detail::compare_int_avx512f<A, T, _MM_CMPINT_LT>(self, other);\n         }\n \n         // mask\n         template <class A, class T>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return self.data;\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_max_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_max_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1246,17 +1246,17 @@ namespace xsimd\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_min_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_min_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1296,17 +1296,17 @@ namespace xsimd\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mul_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1322,66 +1322,66 @@ namespace xsimd\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_NEAREST_INT, _MM_FROUND_CUR_DIRECTION);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return 0 - self;\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_ps_mask(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_cmp_pd_mask(self, other, _CMP_NEQ_UQ);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             return register_type(self.data ^ other.data);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         reciprocal(batch<float, A> const& self,\n                    kernel::requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rcp14_ps(self);\n         }\n \n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         reciprocal(batch<double, A> const& self,\n                    kernel::requires_arch<avx512f>) noexcept\n         {\n@@ -1390,7 +1390,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m128 tmp1 = _mm512_extractf32x4_ps(rhs, 0);\n             __m128 tmp2 = _mm512_extractf32x4_ps(rhs, 1);\n@@ -1402,15 +1402,15 @@ namespace xsimd\n             return reduce_add(batch<float, sse4_2>(res3), sse4_2 {});\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& rhs, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& rhs, requires_arch<avx512f>) noexcept\n         {\n             __m256d tmp1 = _mm512_extractf64x4_pd(rhs, 1);\n             __m256d tmp2 = _mm512_extractf64x4_pd(rhs, 0);\n             __m256d res1 = _mm256_add_pd(tmp1, tmp2);\n             return reduce_add(batch<double, avx2>(res1), avx2 {});\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             __m256i low, high;\n             detail::split_avx512(self, low, high);\n@@ -1420,7 +1420,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) == 1), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             constexpr batch_constant<uint64_t, A, 5, 6, 7, 8, 0, 0, 0, 0> mask;\n             batch<T, A> step = _mm512_permutexvar_epi64(mask.as_batch(), self);\n@@ -1431,7 +1431,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) == 1), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             constexpr batch_constant<uint64_t, A, 5, 6, 7, 8, 0, 0, 0, 0> mask;\n             batch<T, A> step = _mm512_permutexvar_epi64(mask.as_batch(), self);\n@@ -1442,19 +1442,19 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rsqrt14_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_rsqrt14_pd(val);\n         }\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1474,52 +1474,52 @@ namespace xsimd\n         // scatter\n         template <class A, class T,\n                   class = typename std::enable_if<std::is_same<uint32_t, T>::value || std::is_same<int32_t, T>::value, void>::type>\n-        inline void scatter(batch<T, A> const& src, T* dst,\n-                            batch<int32_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst,\n+                                  batch<int32_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i32scatter_epi32(dst, index, src, sizeof(T));\n         }\n \n         template <class A, class T,\n                   class = typename std::enable_if<std::is_same<uint64_t, T>::value || std::is_same<int64_t, T>::value, void>::type>\n-        inline void scatter(batch<T, A> const& src, T* dst,\n-                            batch<int64_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst,\n+                                  batch<int64_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i64scatter_epi64(dst, index, src, sizeof(T));\n         }\n \n         template <class A>\n-        inline void scatter(batch<float, A> const& src, float* dst,\n-                            batch<int32_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<float, A> const& src, float* dst,\n+                                  batch<int32_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i32scatter_ps(dst, index, src, sizeof(float));\n         }\n \n         template <class A>\n-        inline void scatter(batch<double, A> const& src, double* dst,\n-                            batch<int64_t, A> const& index,\n-                            kernel::requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void scatter(batch<double, A> const& src, double* dst,\n+                                  batch<int64_t, A> const& index,\n+                                  kernel::requires_arch<avx512f>) noexcept\n         {\n             _mm512_i64scatter_pd(dst, index, src, sizeof(double));\n         }\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mask_blend_ps(cond, false_br, true_br);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_mask_blend_pd(cond, false_br, true_br);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1571,7 +1571,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<avx512f>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, avx512f {});\n         }\n@@ -1589,32 +1589,32 @@ namespace xsimd\n \n         // set\n         template <class A>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<avx512f>, float v0, float v1, float v2, float v3, float v4, float v5, float v6, float v7, float v8, float v9, float v10, float v11, float v12, float v13, float v14, float v15) noexcept\n         {\n             return _mm512_setr_ps(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<avx512f>, double v0, double v1, double v2, double v3, double v4, double v5, double v6, double v7) noexcept\n         {\n             return _mm512_setr_pd(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm512_set_epi64(v7, v6, v5, v4, v3, v2, v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm512_setr_epi32(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n         template <class A, class T, detail::enable_signed_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n #if defined(__clang__) || __GNUC__\n             return __extension__(__m512i)(__v32hi) {\n@@ -1628,10 +1628,10 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31) noexcept\n         {\n #if defined(__clang__) || __GNUC__\n             return __extension__(__m512i)(__v32hu) {\n@@ -1645,14 +1645,14 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::enable_signed_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n-                               T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n-                               T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n-                               T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n-                               T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n+                                     T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n+                                     T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n+                                     T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n+                                     T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n         {\n \n #if defined(__clang__) || __GNUC__\n@@ -1670,14 +1670,14 @@ namespace xsimd\n #endif\n         }\n         template <class A, class T, detail::enable_unsigned_integer_t<T> = 0>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n-                               T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n-                               T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n-                               T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n-                               T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n-                               T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n-                               T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n-                               T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<avx512f>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7,\n+                                     T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15,\n+                                     T v16, T v17, T v18, T v19, T v20, T v21, T v22, T v23,\n+                                     T v24, T v25, T v26, T v27, T v28, T v29, T v30, T v31,\n+                                     T v32, T v33, T v34, T v35, T v36, T v37, T v38, T v39,\n+                                     T v40, T v41, T v42, T v43, T v44, T v45, T v46, T v47,\n+                                     T v48, T v49, T v50, T v51, T v52, T v53, T v54, T v55,\n+                                     T v56, T v57, T v58, T v59, T v60, T v61, T v62, T v63) noexcept\n         {\n \n #if defined(__clang__) || __GNUC__\n@@ -1696,7 +1696,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class... Values>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<avx512f>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<T, A>::size, \"consistent init\");\n             using register_type = typename batch_bool<T, A>::register_type;\n@@ -1708,9 +1708,9 @@ namespace xsimd\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7, ITy I8, ITy I9, ITy I10, ITy I11, ITy I12, ITy I13, ITy I14, ITy I15>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y,\n-                                       batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7, I8, I9, I10, I11, I12, I13, I14, I15> mask,\n-                                       requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y,\n+                                             batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7, I8, I9, I10, I11, I12, I13, I14, I15> mask,\n+                                             requires_arch<avx512f>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x3) | ((I1 & 0x3) << 2) | ((I2 & 0x3) << 4) | ((I3 & 0x3) << 6);\n \n@@ -1726,7 +1726,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3, ITy I4, ITy I5, ITy I6, ITy I7>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3, I4, I5, I6, I7> mask, requires_arch<avx512f>) noexcept\n         {\n             constexpr uint32_t smask = (I0 & 0x1) | ((I1 & 0x1) << 1) | ((I2 & 0x1) << 2) | ((I3 & 0x1) << 3) | ((I4 & 0x1) << 4) | ((I5 & 0x1) << 5) | ((I6 & 0x1) << 6) | ((I7 & 0x1) << 7);\n             // shuffle within lane\n@@ -1742,35 +1742,35 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             static_assert(N == 0xDEAD, \"not implemented yet\");\n             return {};\n         }\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const&, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const&, requires_arch<avx512f>) noexcept\n         {\n             static_assert(N == 0xDEAD, \"not implemented yet\");\n             return {};\n         }\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sqrt_pd(val);\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1785,7 +1785,7 @@ namespace xsimd\n \n         // store\n         template <class T, class A>\n-        inline void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store(batch_bool<T, A> const& self, bool* mem, requires_arch<avx512f>) noexcept\n         {\n             using register_type = typename batch_bool<T, A>::register_type;\n             constexpr auto size = batch_bool<T, A>::size;\n@@ -1795,51 +1795,51 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_si512((__m512i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_si512((__m512i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_si512((__m512i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_si512((__m512i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_ps(mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1868,86 +1868,86 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sub_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_sub_pd(self, other);\n         }\n \n         // swizzle (dynamic version)\n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_ps(mask, self);\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_pd(mask, self);\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_epi64(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch<uint64_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, avx512f {}));\n         }\n \n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_permutexvar_epi32(mask, self);\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch<uint32_t, A> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, avx512f {}));\n         }\n \n         // swizzle (constant version)\n         template <class A, uint32_t... Vs>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint64_t... Vs>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint32_t... Vs>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n \n         template <class A, uint32_t... Vs>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), avx512f {});\n         }\n@@ -1980,14 +1980,14 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t... Idx, class _ = typename std::enable_if<detail::is_pair_of_contiguous_indices<uint16_t, A, Idx...>::value, void>::type>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Idx...>, requires_arch<avx512f>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, Idx...>, requires_arch<avx512f>) noexcept\n         {\n             constexpr typename detail::fold_batch_constant<A, Idx...>::type mask32;\n             return _mm512_permutexvar_epi32(static_cast<batch<uint32_t, A>>(mask32), self);\n         }\n \n         template <class A>\n-        inline batch<uint16_t, A>\n+        XSIMD_INLINE batch<uint16_t, A>\n         swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, (uint16_t)1, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1, (uint16_t)0, (uint16_t)1>, requires_arch<avx512f>) noexcept\n         {\n             // FIXME: this sequence is very inefficient, but it's here to catch\n@@ -2004,29 +2004,29 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t... Vs>\n-        inline batch<int16_t, A>\n+        XSIMD_INLINE batch<int16_t, A>\n         swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, Vs...> mask, requires_arch<avx512f>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, avx512f {}));\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         trunc(batch<float, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_ps(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         trunc(batch<double, A> const& self, requires_arch<avx512f>) noexcept\n         {\n             return _mm512_roundscale_round_pd(self, _MM_FROUND_TO_ZERO, _MM_FROUND_CUR_DIRECTION);\n         }\n \n         // zip_hi\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             __m512i lo, hi;\n@@ -2064,7 +2064,7 @@ namespace xsimd\n                 1);\n         }\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_unpacklo_ps(self, other);\n@@ -2078,7 +2078,7 @@ namespace xsimd\n                 1);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_castpd_ps(_mm512_unpacklo_pd(self, other));\n@@ -2094,7 +2094,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A>\n+        XSIMD_INLINE batch<T, A>\n         zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             __m512i lo, hi;\n@@ -2132,7 +2132,7 @@ namespace xsimd\n                 2);\n         }\n         template <class A>\n-        inline batch<float, A>\n+        XSIMD_INLINE batch<float, A>\n         zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_unpacklo_ps(self, other);\n@@ -2146,7 +2146,7 @@ namespace xsimd\n                 2);\n         }\n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<avx512f>) noexcept\n         {\n             auto lo = _mm512_castpd_ps(_mm512_unpacklo_pd(self, other));"
      },
      {
        "filename": "include/xsimd/arch/xsimd_constants.hpp",
        "status": "modified",
        "additions": 25,
        "deletions": 25,
        "changes": 50,
        "patch": "@@ -24,34 +24,34 @@ namespace xsimd\n \n #define XSIMD_DEFINE_CONSTANT(NAME, SINGLE, DOUBLE) \\\n     template <class T>                              \\\n-    inline T NAME() noexcept                        \\\n+    XSIMD_INLINE T NAME() noexcept                  \\\n     {                                               \\\n         return T(NAME<typename T::value_type>());   \\\n     }                                               \\\n     template <>                                     \\\n-    inline float NAME<float>() noexcept             \\\n+    XSIMD_INLINE float NAME<float>() noexcept       \\\n     {                                               \\\n         return SINGLE;                              \\\n     }                                               \\\n     template <>                                     \\\n-    inline double NAME<double>() noexcept           \\\n+    XSIMD_INLINE double NAME<double>() noexcept     \\\n     {                                               \\\n         return DOUBLE;                              \\\n     }\n \n #define XSIMD_DEFINE_CONSTANT_HEX(NAME, SINGLE, DOUBLE) \\\n     template <class T>                                  \\\n-    inline T NAME() noexcept                            \\\n+    XSIMD_INLINE T NAME() noexcept                      \\\n     {                                                   \\\n         return T(NAME<typename T::value_type>());       \\\n     }                                                   \\\n     template <>                                         \\\n-    inline float NAME<float>() noexcept                 \\\n+    XSIMD_INLINE float NAME<float>() noexcept           \\\n     {                                                   \\\n         return bit_cast<float>((uint32_t)SINGLE);       \\\n     }                                                   \\\n     template <>                                         \\\n-    inline double NAME<double>() noexcept               \\\n+    XSIMD_INLINE double NAME<double>() noexcept         \\\n     {                                                   \\\n         return bit_cast<double>((uint64_t)DOUBLE);      \\\n     }\n@@ -168,7 +168,7 @@ namespace xsimd\n         }\n \n         template <class T>\n-        inline constexpr T allbits() noexcept\n+        XSIMD_INLINE constexpr T allbits() noexcept\n         {\n             return T(detail::allbits_impl<typename T::value_type>::get_value());\n         }\n@@ -178,19 +178,19 @@ namespace xsimd\n          *****************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> mask1frexp() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> mask1frexp() noexcept\n         {\n             return as_integer_t<T>(mask1frexp<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t mask1frexp<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t mask1frexp<float>() noexcept\n         {\n             return 0x7f800000;\n         }\n \n         template <>\n-        inline constexpr int64_t mask1frexp<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t mask1frexp<double>() noexcept\n         {\n             return 0x7ff0000000000000;\n         }\n@@ -200,19 +200,19 @@ namespace xsimd\n          *****************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> mask2frexp() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> mask2frexp() noexcept\n         {\n             return as_integer_t<T>(mask2frexp<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t mask2frexp<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t mask2frexp<float>() noexcept\n         {\n             return 0x3f000000;\n         }\n \n         template <>\n-        inline constexpr int64_t mask2frexp<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t mask2frexp<double>() noexcept\n         {\n             return 0x3fe0000000000000;\n         }\n@@ -222,19 +222,19 @@ namespace xsimd\n          ******************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> maxexponent() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> maxexponent() noexcept\n         {\n             return as_integer_t<T>(maxexponent<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t maxexponent<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t maxexponent<float>() noexcept\n         {\n             return 127;\n         }\n \n         template <>\n-        inline constexpr int64_t maxexponent<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t maxexponent<double>() noexcept\n         {\n             return 1023;\n         }\n@@ -244,19 +244,19 @@ namespace xsimd\n          ******************************/\n \n         template <class T>\n-        inline constexpr as_integer_t<T> maxexponentm1() noexcept\n+        XSIMD_INLINE constexpr as_integer_t<T> maxexponentm1() noexcept\n         {\n             return as_integer_t<T>(maxexponentm1<typename T::value_type>());\n         }\n \n         template <>\n-        inline constexpr int32_t maxexponentm1<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t maxexponentm1<float>() noexcept\n         {\n             return 126;\n         }\n \n         template <>\n-        inline constexpr int64_t maxexponentm1<double>() noexcept\n+        XSIMD_INLINE constexpr int64_t maxexponentm1<double>() noexcept\n         {\n             return 1022;\n         }\n@@ -266,19 +266,19 @@ namespace xsimd\n          **********************/\n \n         template <class T>\n-        inline constexpr int32_t nmb() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb() noexcept\n         {\n             return nmb<typename T::value_type>();\n         }\n \n         template <>\n-        inline constexpr int32_t nmb<float>() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb<float>() noexcept\n         {\n             return 23;\n         }\n \n         template <>\n-        inline constexpr int32_t nmb<double>() noexcept\n+        XSIMD_INLINE constexpr int32_t nmb<double>() noexcept\n         {\n             return 52;\n         }\n@@ -288,7 +288,7 @@ namespace xsimd\n          ***********************/\n \n         template <class T>\n-        inline constexpr T zero() noexcept\n+        XSIMD_INLINE constexpr T zero() noexcept\n         {\n             return T(typename T::value_type(0));\n         }\n@@ -353,7 +353,7 @@ namespace xsimd\n             template <>\n             struct minvalue_impl<float>\n             {\n-                inline static float get_value() noexcept\n+                XSIMD_INLINE static float get_value() noexcept\n                 {\n                     return bit_cast<float>((uint32_t)0xff7fffff);\n                 }\n@@ -362,7 +362,7 @@ namespace xsimd\n             template <>\n             struct minvalue_impl<double>\n             {\n-                inline static double get_value() noexcept\n+                XSIMD_INLINE static double get_value() noexcept\n                 {\n                     return bit_cast<double>((uint64_t)0xffefffffffffffff);\n                 }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_emulated.hpp",
        "status": "modified",
        "additions": 76,
        "deletions": 76,
        "changes": 152,
        "patch": "@@ -28,7 +28,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -39,9 +39,9 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n \n         namespace detail\n         {\n@@ -66,7 +66,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::abs(v); },\n@@ -75,7 +75,7 @@ namespace xsimd\n \n         // add\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::add(v0, v1); },\n@@ -84,38 +84,38 @@ namespace xsimd\n \n         // all\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::all_of(self.data.begin(), self.data.end(), [](T v)\n                                { return bool(v); });\n         }\n \n         // any\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::any_of(self.data.begin(), self.data.end(), [](T v)\n                                { return bool(v); });\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, size_t N = 8 * sizeof(T_in) * batch<T_in, A>::size>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n         {\n             return { self.data };\n         }\n \n         // bitwise_and\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_and(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_and(v0, v1); },\n@@ -124,15 +124,15 @@ namespace xsimd\n \n         // bitwise_andnot\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_andnot(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_andnot(v0, v1); },\n@@ -141,7 +141,7 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([other](T v)\n                                           { return xsimd::bitwise_lshift(v, other); },\n@@ -150,15 +150,15 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::bitwise_not(v); },\n                                           self);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v)\n                                           { return xsimd::bitwise_not(v); },\n@@ -167,15 +167,15 @@ namespace xsimd\n \n         // bitwise_or\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_or(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_or(v0, v1); },\n@@ -184,7 +184,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([other](T v)\n                                           { return xsimd::bitwise_rshift(v, other); },\n@@ -193,15 +193,15 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::bitwise_xor(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::bitwise_xor(v0, v1); },\n@@ -210,7 +210,7 @@ namespace xsimd\n \n         // bitwise_cast\n         template <class A, class T_in, class T_out, size_t N = 8 * sizeof(T_in) * batch<T_in, A>::size>\n-        inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& self, batch<T_out, A> const&, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T_out, A>::size;\n             std::array<T_out, size> result;\n@@ -222,7 +222,7 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        batch<T, A> inline broadcast(T val, requires_arch<emulated<N>>) noexcept\n+        batch<T, A> XSIMD_INLINE broadcast(T val, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> r;\n@@ -235,7 +235,7 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<T, A> complex_low(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_low(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> result;\n@@ -248,7 +248,7 @@ namespace xsimd\n             }\n             // complex_high\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<T, A> complex_high(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<T, A> complex_high(batch<std::complex<T>, A> const& self, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> result;\n@@ -263,14 +263,14 @@ namespace xsimd\n \n         // decr_if\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::div(v0, v1); },\n@@ -281,47 +281,47 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t N = 8 * sizeof(float) * batch<float, A>::size>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](int32_t v)\n                                               { return float(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(float) * batch<float, A>::size>\n-            inline batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<uint32_t, A> const& self, batch<float, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](uint32_t v)\n                                               { return float(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](int64_t v)\n                                               { return double(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& self, batch<double, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](uint64_t v)\n                                               { return double(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(int32_t) * batch<int32_t, A>::size>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](float v)\n                                               { return int32_t(v); },\n                                               self);\n             }\n \n             template <class A, size_t N = 8 * sizeof(double) * batch<double, A>::size>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& self, batch<int64_t, A> const&, requires_arch<emulated<N>>) noexcept\n             {\n                 return detail::emulated_apply([](double v)\n                                               { return int64_t(v); },\n@@ -331,15 +331,15 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> eq(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> eq(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::eq(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch_bool<T, A>::size>\n-        inline batch_bool<T, emulated<N>> eq(batch_bool<T, emulated<N>> const& self, batch_bool<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> eq(batch_bool<T, emulated<N>> const& self, batch_bool<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::eq(v0, v1); },\n@@ -348,7 +348,7 @@ namespace xsimd\n \n         // from_bool\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v)\n                                           { return T(v); },\n@@ -357,7 +357,7 @@ namespace xsimd\n \n         // from_mask\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<bool, size> vmask;\n@@ -368,7 +368,7 @@ namespace xsimd\n \n         // ge\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> ge(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> ge(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::ge(v0, v1); },\n@@ -377,7 +377,7 @@ namespace xsimd\n \n         // gt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> gt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> gt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::gt(v0, v1); },\n@@ -386,7 +386,7 @@ namespace xsimd\n \n         // haddp\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> haddp(batch<T, A> const* row, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(batch<T, A> const* row, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> r;\n@@ -397,14 +397,14 @@ namespace xsimd\n \n         // incr_if\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<emulated<N>>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<emulated<N>>) noexcept\n         {\n             batch<T, A> other = self;\n             other.data[I] = val;\n@@ -413,7 +413,7 @@ namespace xsimd\n \n         // isnan\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::isnan(v); },\n@@ -422,7 +422,7 @@ namespace xsimd\n \n         // load_aligned\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> res;\n@@ -432,7 +432,7 @@ namespace xsimd\n \n         // load_unaligned\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> res;\n@@ -444,7 +444,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& hi, batch<T, A> const& lo, requires_arch<emulated<N>>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& hi, batch<T, A> const& lo, requires_arch<emulated<N>>) noexcept\n             {\n                 constexpr size_t size = batch<T, A>::size;\n                 std::array<T, size> real, imag;\n@@ -464,7 +464,7 @@ namespace xsimd\n \n         // le\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> le(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> le(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::le(v0, v1); },\n@@ -473,7 +473,7 @@ namespace xsimd\n \n         // lt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, emulated<N>> lt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> lt(batch<T, emulated<N>> const& self, batch<T, emulated<N>> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::lt(v0, v1); },\n@@ -482,7 +482,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             uint64_t res = 0;\n@@ -493,7 +493,7 @@ namespace xsimd\n \n         // max\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::max(v0, v1); },\n@@ -502,7 +502,7 @@ namespace xsimd\n \n         // min\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::min(v0, v1); },\n@@ -511,7 +511,7 @@ namespace xsimd\n \n         // mul\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::mul(v0, v1); },\n@@ -520,8 +520,8 @@ namespace xsimd\n \n         // nearbyint_as_int\n         template <class A, typename T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<as_integer_t<T>, A> nearbyint_as_int(batch<T, A> const& self,\n-                                                          requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<as_integer_t<T>, A> nearbyint_as_int(batch<T, A> const& self,\n+                                                                requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::nearbyint_as_int(v); },\n@@ -530,7 +530,7 @@ namespace xsimd\n \n         // neg\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::neg(v); },\n@@ -539,15 +539,15 @@ namespace xsimd\n \n         // neq\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::neq(v0, v1); },\n                                           self, other);\n         }\n \n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool v0, bool v1)\n                                           { return xsimd::neq(v0, v1); },\n@@ -556,7 +556,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> buffer;\n@@ -566,23 +566,23 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::accumulate(self.data.begin() + 1, self.data.end(), *self.data.begin(), [](T const& x, T const& y)\n                                    { return xsimd::max(x, y); });\n         }\n \n         // reduce_min\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return std::accumulate(self.data.begin() + 1, self.data.end(), *self.data.begin(), [](T const& x, T const& y)\n                                    { return xsimd::min(x, y); });\n         }\n \n         // rsqrt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> rsqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::rsqrt(v); },\n@@ -591,15 +591,15 @@ namespace xsimd\n \n         // select\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](bool c, T t, T f)\n                                           { return xsimd::select(c, t, f); },\n                                           cond, true_br, false_br);\n         }\n \n         template <class A, class T, bool... Values>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             static_assert(sizeof...(Values) == size, \"consistent init\");\n@@ -608,7 +608,7 @@ namespace xsimd\n \n         // shuffle\n         template <class A, typename T, class ITy, ITy... Is>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             batch<ITy, A> bmask = mask;\n@@ -620,7 +620,7 @@ namespace xsimd\n \n         // sqrt\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v)\n                                           { return xsimd::sqrt(v); },\n@@ -629,7 +629,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t M, class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> result;\n@@ -641,7 +641,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t M, class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             std::array<T, size> result;\n@@ -653,7 +653,7 @@ namespace xsimd\n \n         // sadd\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::sadd(v0, v1); },\n@@ -662,22 +662,22 @@ namespace xsimd\n \n         // set\n         template <class A, class T, size_t N, class... Values>\n-        inline batch<T, emulated<N>> set(batch<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n+        XSIMD_INLINE batch<T, emulated<N>> set(batch<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<T, emulated<N>>::size, \"consistent init\");\n             return { typename batch<T, emulated<N>>::register_type { static_cast<T>(values)... } };\n         }\n \n         template <class A, class T, size_t N, class... Values>\n-        inline batch_bool<T, emulated<N>> set(batch_bool<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, emulated<N>> set(batch_bool<T, emulated<N>> const&, requires_arch<emulated<N>>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<T, emulated<N>>::size, \"consistent init\");\n             return { std::array<bool, sizeof...(Values)> { static_cast<bool>(values)... } };\n         }\n \n         // ssub\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::ssub(v0, v1); },\n@@ -686,21 +686,21 @@ namespace xsimd\n \n         // store_aligned\n         template <class A, class T, size_t N>\n-        inline void store_aligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             std::copy(self.data.begin(), self.data.end(), mem);\n         }\n \n         // store_unaligned\n         template <class A, class T, size_t N>\n-        inline void store_unaligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, emulated<N>> const& self, requires_arch<emulated<N>>) noexcept\n         {\n             std::copy(self.data.begin(), self.data.end(), mem);\n         }\n \n         // sub\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             return detail::emulated_apply([](T v0, T v1)\n                                           { return xsimd::sub(v0, v1); },\n@@ -710,7 +710,7 @@ namespace xsimd\n         // swizzle\n \n         template <class A, typename T, class ITy, ITy... Is>\n-        inline batch<T, A> swizzle(batch<T, A> const& self, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& self, batch_constant<ITy, A, Is...> mask, requires_arch<emulated<8 * sizeof(T) * batch<T, A>::size>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             batch<ITy, A> bmask = mask;\n@@ -722,7 +722,7 @@ namespace xsimd\n \n         // zip_hi\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             // Note: irregular behavior for odd numbers.\n@@ -742,7 +742,7 @@ namespace xsimd\n \n         // zip_lo\n         template <class A, class T, size_t N = 8 * sizeof(T) * batch<T, A>::size>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<emulated<N>>) noexcept\n         {\n             constexpr size_t size = batch<T, A>::size;\n             // Note: irregular behavior for odd numbers."
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma3_avx.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -23,52 +23,52 @@ namespace xsimd\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmadd_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fnmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<avx>>) noexcept\n         {\n             return _mm256_fmsub_pd(x, y, z);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma3_sse.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -22,52 +22,52 @@ namespace xsimd\n         using namespace types;\n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmadd_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fnmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmadd_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmadd_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(batch<float, A> const& x, batch<float, A> const& y, batch<float, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<fma3<sse4_2>>) noexcept\n         {\n             return _mm_fmsub_pd(x, y, z);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_fma4.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -23,52 +23,52 @@ namespace xsimd\n \n         // fnma\n         template <class A>\n-        inline batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fnma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmacc_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fnma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmacc_pd(x, y, z);\n         }\n \n         // fnms\n         template <class A>\n-        inline batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fnms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmsub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fnms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_nmsub_pd(x, y, z);\n         }\n \n         // fma\n         template <class A>\n-        inline batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fma(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_macc_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_macc_pd(x, y, z);\n         }\n \n         // fms\n         template <class A>\n-        inline batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<float, A> fms(simd_register<float, A> const& x, simd_register<float, A> const& y, simd_register<float, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_msub_ps(x, y, z);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(simd_register<double, A> const& x, simd_register<double, A> const& y, simd_register<double, A> const& z, requires_arch<fma4>) noexcept\n         {\n             return _mm_msub_pd(x, y, z);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_generic_fwd.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -22,21 +22,21 @@ namespace xsimd\n     {\n         // forward declaration\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<generic>) noexcept;\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE T hadd(batch<T, A> const& self, requires_arch<generic>) noexcept;\n \n     }\n }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_neon.hpp",
        "status": "modified",
        "additions": 403,
        "deletions": 403,
        "changes": 806,
        "patch": null
      },
      {
        "filename": "include/xsimd/arch/xsimd_neon64.hpp",
        "status": "modified",
        "additions": 252,
        "deletions": 252,
        "changes": 504,
        "patch": "@@ -33,25 +33,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vminvq_u32(arg) == ~0U;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u8(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u16(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return all(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64 {});\n         }\n@@ -61,25 +61,25 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_t<T, 4> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vmaxvq_u32(arg) != 0;\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 1> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u8(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 2> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u16(arg)), neon64 {});\n         }\n \n         template <class A, class T, detail::enable_sized_t<T, 8> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return any(batch_bool<uint32_t, A>(vreinterpretq_u32_u64(arg)), neon64 {});\n         }\n@@ -90,13 +90,13 @@ namespace xsimd\n \n         // Required to avoid ambiguous call\n         template <class A, class T>\n-        inline batch<T, A> broadcast(T val, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<neon64>) noexcept\n         {\n             return broadcast<A>(val, neon {});\n         }\n \n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<neon64>) noexcept\n         {\n             return vdupq_n_f64(val);\n         }\n@@ -106,13 +106,13 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<neon64>, double d0, double d1) noexcept\n         {\n             return float64x2_t { d0, d1 };\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<neon64>, bool b0, bool b1) noexcept\n         {\n             using register_type = typename batch_bool<double, A>::register_type;\n             using unsigned_type = as_unsigned_integer_t<double>;\n@@ -125,7 +125,7 @@ namespace xsimd\n          *************/\n \n         template <class A>\n-        inline batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> from_bool(batch_bool<double, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vandq_u64(arg, vreinterpretq_u64_f64(vdupq_n_f64(1.))));\n         }\n@@ -142,13 +142,13 @@ namespace xsimd\n #endif\n \n         template <class A>\n-        inline batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n         {\n             return xsimd_aligned_load(vld1q_f64, double*, src);\n         }\n \n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* src, convert<double>, requires_arch<neon64>) noexcept\n         {\n             return vld1q_f64(src);\n         }\n@@ -159,13 +159,13 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_aligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n         {\n             vst1q_f64(dst, src);\n         }\n \n         template <class A>\n-        inline void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* dst, batch<double, A> const& src, requires_arch<neon64>) noexcept\n         {\n             return store_aligned<A>(dst, src, A {});\n         }\n@@ -175,7 +175,7 @@ namespace xsimd\n          ****************/\n \n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex_aligned(std::complex<double> const* mem, convert<std::complex<double>>, requires_arch<neon64>) noexcept\n         {\n             using real_batch = batch<double, A>;\n             const double* buf = reinterpret_cast<const double*>(mem);\n@@ -186,7 +186,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> load_complex_unaligned(std::complex<double> const* mem, convert<std::complex<double>> cvt, requires_arch<neon64>) noexcept\n         {\n             return load_complex_aligned<A>(mem, cvt, A {});\n         }\n@@ -196,7 +196,7 @@ namespace xsimd\n          *****************/\n \n         template <class A>\n-        inline void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n         {\n             float64x2x2_t tmp;\n             tmp.val[0] = src.real();\n@@ -206,7 +206,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<double>* dst, batch<std::complex<double>, A> const& src, requires_arch<neon64>) noexcept\n         {\n             store_complex_aligned(dst, src, A {});\n         }\n@@ -216,19 +216,19 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_u64_s64(vnegq_s64(vreinterpretq_s64_u64(rhs)));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vnegq_s64(rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vnegq_f64(rhs);\n         }\n@@ -238,7 +238,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vaddq_f64(lhs, rhs);\n         }\n@@ -248,7 +248,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sadd(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return add(lhs, rhs, neon64 {});\n         }\n@@ -258,7 +258,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vsubq_f64(lhs, rhs);\n         }\n@@ -268,7 +268,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> ssub(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return sub(lhs, rhs, neon64 {});\n         }\n@@ -278,7 +278,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vmulq_f64(lhs, rhs);\n         }\n@@ -289,19 +289,19 @@ namespace xsimd\n \n #if defined(XSIMD_FAST_INTEGER_DIVISION)\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcvtq_u64_f64(vcvtq_f64_u64(lhs) / vcvtq_f64_u64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcvtq_s64_f64(vcvtq_f64_s64(lhs) / vcvtq_f64_s64(rhs));\n         }\n #endif\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vdivq_f64(lhs, rhs);\n         }\n@@ -311,37 +311,37 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_f64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vceqq_u64(lhs, rhs);\n         }\n@@ -352,25 +352,25 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_f64_s64(x);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_f64_u64(x);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& x, batch<int64_t, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& x, batch<int64_t, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_s64_f64(x);\n             }\n \n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<double, A> const& x, batch<uint64_t, A> const&, requires_arch<neon64>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<double, A> const& x, batch<uint64_t, A> const&, requires_arch<neon64>) noexcept\n             {\n                 return vcvtq_u64_f64(x);\n             }\n@@ -382,19 +382,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcltq_f64(lhs, rhs);\n         }\n@@ -404,19 +404,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcleq_f64(lhs, rhs);\n         }\n@@ -426,19 +426,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgtq_f64(lhs, rhs);\n         }\n@@ -448,19 +448,19 @@ namespace xsimd\n          ******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vcgeq_f64(lhs, rhs);\n         }\n@@ -470,7 +470,7 @@ namespace xsimd\n          *******************/\n \n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch_bool<T_out, A>::register_type;\n             return register_type(self);\n@@ -481,14 +481,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vandq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vandq_u64(lhs, rhs);\n         }\n@@ -498,14 +498,14 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vorrq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vorrq_u64(lhs, rhs);\n         }\n@@ -515,14 +515,14 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(veorq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return veorq_u64(lhs, rhs);\n         }\n@@ -532,7 +532,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return bitwise_xor(lhs, rhs, A {});\n         }\n@@ -542,13 +542,13 @@ namespace xsimd\n          ***************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u32(vmvnq_u32(vreinterpretq_u32_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return detail::bitwise_not_u64(rhs);\n         }\n@@ -558,14 +558,14 @@ namespace xsimd\n          ******************/\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vreinterpretq_f64_u64(vbicq_u64(vreinterpretq_u64_f64(lhs),\n                                                    vreinterpretq_u64_f64(rhs)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& lhs, batch_bool<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vbicq_u64(lhs, rhs);\n         }\n@@ -575,7 +575,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vminq_f64(lhs, rhs);\n         }\n@@ -585,7 +585,7 @@ namespace xsimd\n          *******/\n \n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vmaxq_f64(lhs, rhs);\n         }\n@@ -595,34 +595,34 @@ namespace xsimd\n          *******/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return rhs;\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vabsq_s64(rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vabsq_f64(rhs);\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<neon64>) noexcept\n         {\n             return vcvtnq_s32_f32(self);\n         }\n \n #if !defined(__GNUC__)\n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n-                                                  requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& self,\n+                                                        requires_arch<neon64>) noexcept\n         {\n             return vcvtnq_s64_f64(self);\n         }\n@@ -633,7 +633,7 @@ namespace xsimd\n          **************/\n \n         template <class A>\n-        inline batch<double, A>\n+        XSIMD_INLINE batch<double, A>\n         reciprocal(const batch<double, A>& x,\n                    kernel::requires_arch<neon64>) noexcept\n         {\n@@ -645,7 +645,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vrsqrteq_f64(rhs);\n         }\n@@ -655,7 +655,7 @@ namespace xsimd\n          ********/\n \n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vsqrtq_f64(rhs);\n         }\n@@ -666,13 +666,13 @@ namespace xsimd\n \n #ifdef __ARM_FEATURE_FMA\n         template <class A>\n-        inline batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> fma(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n         {\n             return vfmaq_f64(z, x, y);\n         }\n \n         template <class A>\n-        inline batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> fms(batch<double, A> const& x, batch<double, A> const& y, batch<double, A> const& z, requires_arch<neon64>) noexcept\n         {\n             return vfmaq_f64(-z, x, y);\n         }\n@@ -683,7 +683,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(const batch<double, A>* row, requires_arch<neon64>) noexcept\n         {\n             return vpaddq_f64(row[0], row[1]);\n         }\n@@ -693,7 +693,7 @@ namespace xsimd\n          **********/\n \n         template <class A, size_t I>\n-        inline batch<double, A> insert(batch<double, A> const& self, double val, index<I>, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> insert(batch<double, A> const& self, double val, index<I>, requires_arch<neon64>) noexcept\n         {\n             return vsetq_lane_f64(val, self, I);\n         }\n@@ -705,60 +705,60 @@ namespace xsimd\n         // Wrap reducer intrinsics so we can pass them as function pointers\n         // - OP: intrinsics name prefix, e.g., vorrq\n \n-#define WRAP_REDUCER_INT_EXCLUDING_64(OP)               \\\n-    namespace wrap                                      \\\n-    {                                                   \\\n-        inline uint8_t OP##_u8(uint8x16_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_u8(a);                        \\\n-        }                                               \\\n-        inline int8_t OP##_s8(int8x16_t a) noexcept     \\\n-        {                                               \\\n-            return ::OP##_s8(a);                        \\\n-        }                                               \\\n-        inline uint16_t OP##_u16(uint16x8_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u16(a);                       \\\n-        }                                               \\\n-        inline int16_t OP##_s16(int16x8_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s16(a);                       \\\n-        }                                               \\\n-        inline uint32_t OP##_u32(uint32x4_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u32(a);                       \\\n-        }                                               \\\n-        inline int32_t OP##_s32(int32x4_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s32(a);                       \\\n-        }                                               \\\n+#define WRAP_REDUCER_INT_EXCLUDING_64(OP)                     \\\n+    namespace wrap                                            \\\n+    {                                                         \\\n+        XSIMD_INLINE uint8_t OP##_u8(uint8x16_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_u8(a);                              \\\n+        }                                                     \\\n+        XSIMD_INLINE int8_t OP##_s8(int8x16_t a) noexcept     \\\n+        {                                                     \\\n+            return ::OP##_s8(a);                              \\\n+        }                                                     \\\n+        XSIMD_INLINE uint16_t OP##_u16(uint16x8_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u16(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int16_t OP##_s16(int16x8_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s16(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE uint32_t OP##_u32(uint32x4_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u32(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int32_t OP##_s32(int32x4_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s32(a);                             \\\n+        }                                                     \\\n     }\n \n-#define WRAP_REDUCER_INT(OP)                            \\\n-    WRAP_REDUCER_INT_EXCLUDING_64(OP)                   \\\n-    namespace wrap                                      \\\n-    {                                                   \\\n-        inline uint64_t OP##_u64(uint64x2_t a) noexcept \\\n-        {                                               \\\n-            return ::OP##_u64(a);                       \\\n-        }                                               \\\n-        inline int64_t OP##_s64(int64x2_t a) noexcept   \\\n-        {                                               \\\n-            return ::OP##_s64(a);                       \\\n-        }                                               \\\n+#define WRAP_REDUCER_INT(OP)                                  \\\n+    WRAP_REDUCER_INT_EXCLUDING_64(OP)                         \\\n+    namespace wrap                                            \\\n+    {                                                         \\\n+        XSIMD_INLINE uint64_t OP##_u64(uint64x2_t a) noexcept \\\n+        {                                                     \\\n+            return ::OP##_u64(a);                             \\\n+        }                                                     \\\n+        XSIMD_INLINE int64_t OP##_s64(int64x2_t a) noexcept   \\\n+        {                                                     \\\n+            return ::OP##_s64(a);                             \\\n+        }                                                     \\\n     }\n \n-#define WRAP_REDUCER_FLOAT(OP)                         \\\n-    namespace wrap                                     \\\n-    {                                                  \\\n-        inline float OP##_f32(float32x4_t a) noexcept  \\\n-        {                                              \\\n-            return ::OP##_f32(a);                      \\\n-        }                                              \\\n-        inline double OP##_f64(float64x2_t a) noexcept \\\n-        {                                              \\\n-            return ::OP##_f64(a);                      \\\n-        }                                              \\\n+#define WRAP_REDUCER_FLOAT(OP)                               \\\n+    namespace wrap                                           \\\n+    {                                                        \\\n+        XSIMD_INLINE float OP##_f32(float32x4_t a) noexcept  \\\n+        {                                                    \\\n+            return ::OP##_f32(a);                            \\\n+        }                                                    \\\n+        XSIMD_INLINE double OP##_f64(float64x2_t a) noexcept \\\n+        {                                                    \\\n+            return ::OP##_f64(a);                            \\\n+        }                                                    \\\n     }\n \n         namespace detail\n@@ -852,7 +852,7 @@ namespace xsimd\n         WRAP_REDUCER_FLOAT(vaddvq)\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_add(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_add(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -872,19 +872,19 @@ namespace xsimd\n \n         namespace wrap\n         {\n-            inline uint64_t vmaxvq_u64(uint64x2_t a) noexcept\n+            XSIMD_INLINE uint64_t vmaxvq_u64(uint64x2_t a) noexcept\n             {\n                 return std::max(vdupd_laneq_u64(a, 0), vdupd_laneq_u64(a, 1));\n             }\n \n-            inline int64_t vmaxvq_s64(int64x2_t a) noexcept\n+            XSIMD_INLINE int64_t vmaxvq_s64(int64x2_t a) noexcept\n             {\n                 return std::max(vdupd_laneq_s64(a, 0), vdupd_laneq_s64(a, 1));\n             }\n         }\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_max(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_max(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -904,19 +904,19 @@ namespace xsimd\n \n         namespace wrap\n         {\n-            inline uint64_t vminvq_u64(uint64x2_t a) noexcept\n+            XSIMD_INLINE uint64_t vminvq_u64(uint64x2_t a) noexcept\n             {\n                 return std::min(vdupd_laneq_u64(a, 0), vdupd_laneq_u64(a, 1));\n             }\n \n-            inline int64_t vminvq_s64(int64x2_t a) noexcept\n+            XSIMD_INLINE int64_t vminvq_s64(int64x2_t a) noexcept\n             {\n                 return std::min(vdupd_laneq_s64(a, 0), vdupd_laneq_s64(a, 1));\n             }\n         }\n \n         template <class A, class T, detail::enable_neon64_type_t<T> = 0>\n-        inline typename batch<T, A>::value_type reduce_min(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE typename batch<T, A>::value_type reduce_min(batch<T, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             using register_type = typename batch<T, A>::register_type;\n             const detail::neon_reducer_dispatcher::unary dispatcher = {\n@@ -936,78 +936,78 @@ namespace xsimd\n          **********/\n \n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& a, batch<double, A> const& b, requires_arch<neon64>) noexcept\n         {\n             return vbslq_f64(cond, a, b);\n         }\n \n         template <class A, bool... b>\n-        inline batch<double, A> select(batch_bool_constant<double, A, b...> const&,\n-                                       batch<double, A> const& true_br,\n-                                       batch<double, A> const& false_br,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, b...> const&,\n+                                             batch<double, A> const& true_br,\n+                                             batch<double, A> const& false_br,\n+                                             requires_arch<neon64>) noexcept\n         {\n             return select(batch_bool<double, A> { b... }, true_br, false_br, neon64 {});\n         }\n         /**********\n          * zip_lo *\n          **********/\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_f32(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip1q_f64(lhs, rhs);\n         }\n@@ -1017,61 +1017,61 @@ namespace xsimd\n          **********/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s8(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s16(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s32(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_u64(lhs, rhs);\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_s64(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_f32(lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vzip2q_f64(lhs, rhs);\n         }\n@@ -1083,8 +1083,8 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, size_t I, size_t... Is>\n-            inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n-                                                 ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n,\n+                                                       ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (n == I)\n                 {\n@@ -1098,7 +1098,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> extract_pair(batch<double, A> const& lhs, batch<double, A> const& rhs, std::size_t n, requires_arch<neon64>) noexcept\n         {\n             constexpr std::size_t size = batch<double, A>::size;\n             assert(n < size && \"index in bounds\");\n@@ -1110,25 +1110,25 @@ namespace xsimd\n          ******************/\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n         {\n             return bitwise_rshift<A>(lhs, n, neon {});\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<as_signed_integer_t<T>, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vshlq_u64(lhs, vnegq_s64(rhs));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, int n, requires_arch<neon64>) noexcept\n         {\n             return bitwise_rshift<A>(lhs, n, neon {});\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<neon64>) noexcept\n         {\n             return vshlq_s64(lhs, vnegq_s64(rhs));\n         }\n@@ -1137,17 +1137,17 @@ namespace xsimd\n          * bitwise_cast *\n          ****************/\n \n-#define WRAP_CAST(SUFFIX, TYPE)                                          \\\n-    namespace wrap                                                       \\\n-    {                                                                    \\\n-        inline float64x2_t vreinterpretq_f64_##SUFFIX(TYPE a) noexcept   \\\n-        {                                                                \\\n-            return ::vreinterpretq_f64_##SUFFIX(a);                      \\\n-        }                                                                \\\n-        inline TYPE vreinterpretq_##SUFFIX##_f64(float64x2_t a) noexcept \\\n-        {                                                                \\\n-            return ::vreinterpretq_##SUFFIX##_f64(a);                    \\\n-        }                                                                \\\n+#define WRAP_CAST(SUFFIX, TYPE)                                                \\\n+    namespace wrap                                                             \\\n+    {                                                                          \\\n+        XSIMD_INLINE float64x2_t vreinterpretq_f64_##SUFFIX(TYPE a) noexcept   \\\n+        {                                                                      \\\n+            return ::vreinterpretq_f64_##SUFFIX(a);                            \\\n+        }                                                                      \\\n+        XSIMD_INLINE TYPE vreinterpretq_##SUFFIX##_f64(float64x2_t a) noexcept \\\n+        {                                                                      \\\n+            return ::vreinterpretq_##SUFFIX##_f64(a);                          \\\n+        }                                                                      \\\n     }\n \n         WRAP_CAST(u8, uint8x16_t)\n@@ -1163,7 +1163,7 @@ namespace xsimd\n #undef WRAP_CAST\n \n         template <class A, class T>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n         {\n             using caster_type = detail::bitwise_caster_impl<float64x2_t,\n                                                             uint8x16_t, int8x16_t,\n@@ -1199,7 +1199,7 @@ namespace xsimd\n         }\n \n         template <class A, class R>\n-        inline batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<double, A> const& arg, batch<R, A> const&, requires_arch<neon64>) noexcept\n         {\n             using caster_type = detail::bitwise_caster_neon64<float64x2_t,\n                                                               uint8x16_t, int8x16_t,\n@@ -1218,7 +1218,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<double, A> const& arg, batch<double, A> const&, requires_arch<neon64>) noexcept\n         {\n             return arg;\n         }\n@@ -1228,7 +1228,7 @@ namespace xsimd\n          *********/\n \n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& arg, requires_arch<neon64>) noexcept\n         {\n             return !(arg == arg);\n         }\n@@ -1237,7 +1237,7 @@ namespace xsimd\n          * rotate_right *\n          ****************/\n         template <size_t N, class A>\n-        inline batch<double, A> rotate_right(batch<double, A> const& a, requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> rotate_right(batch<double, A> const& a, requires_arch<neon64>) noexcept\n         {\n             return vextq_f64(a, a, N);\n         }\n@@ -1252,23 +1252,23 @@ namespace xsimd\n          * swizzle (dynamic) *\n          *********************/\n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_u8(self, idx);\n         }\n \n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_s8(self, idx);\n         }\n \n         template <class A>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n-                                          batch<uint16_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n+                                                batch<uint16_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1278,17 +1278,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n-                                         batch<uint16_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n+                                               batch<uint16_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n-                                          batch<uint32_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n+                                                batch<uint32_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1298,17 +1298,17 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n-                                         batch<uint32_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n+                                               batch<uint32_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n-                                          batch<uint64_t, A> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n+                                                batch<uint64_t, A> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             using index_type = batch<uint8_t, A>;\n@@ -1318,25 +1318,25 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n-                                         batch<uint64_t, A> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n+                                               batch<uint64_t, A> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<float, A> swizzle(batch<float, A> const& self,\n-                                       batch<uint32_t, A> idx,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self,\n+                                             batch<uint32_t, A> idx,\n+                                             requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<float>(swizzle(bitwise_cast<uint32_t>(self), idx, neon64 {}));\n         }\n \n         template <class A>\n-        inline batch<double, A> swizzle(batch<double, A> const& self,\n-                                        batch<uint64_t, A> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self,\n+                                              batch<uint64_t, A> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return bitwise_cast<double>(swizzle(bitwise_cast<uint64_t>(self), idx, neon64 {}));\n         }\n@@ -1388,114 +1388,114 @@ namespace xsimd\n             using index_burst_t = typename index_burst<B, T>::type;\n \n             template <typename T, class B>\n-            inline index_burst_t<B, T> burst_index(B)\n+            XSIMD_INLINE index_burst_t<B, T> burst_index(B)\n             {\n                 return index_burst_t<B, T>();\n             }\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self,\n-                                         batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self,\n+                                               batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_u8(self, batch<uint8_t, A>(idx));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self,\n-                                        batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self,\n+                                              batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             return vqtbl1q_s8(self, batch<uint8_t, A>(idx));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n-                                          batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self,\n+                                                batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u16_u8(swizzle<A>(batch_type(vreinterpretq_u8_u16(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n-                                         batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self,\n+                                               batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s16_s8(swizzle<A>(batch_type(vreinterpretq_s8_s16(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n-                                          batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self,\n+                                                batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u32_u8(swizzle<A>(batch_type(vreinterpretq_u8_u32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n-                                         batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self,\n+                                               batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s32_s8(swizzle<A>(batch_type(vreinterpretq_s8_s32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n-                                          batch_constant<uint64_t, A, V0, V1> idx,\n-                                          requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self,\n+                                                batch_constant<uint64_t, A, V0, V1> idx,\n+                                                requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_u64_u8(swizzle<A>(batch_type(vreinterpretq_u8_u64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n-                                         batch_constant<uint64_t, A, V0, V1> idx,\n-                                         requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self,\n+                                               batch_constant<uint64_t, A, V0, V1> idx,\n+                                               requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<int8_t, A>;\n             return vreinterpretq_s64_s8(swizzle<A>(batch_type(vreinterpretq_s8_s64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self,\n-                                       batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                       requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self,\n+                                             batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                             requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_f32_u8(swizzle<A>(batch_type(vreinterpretq_u8_f32(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self,\n-                                        batch_constant<uint64_t, A, V0, V1> idx,\n-                                        requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self,\n+                                              batch_constant<uint64_t, A, V0, V1> idx,\n+                                              requires_arch<neon64>) noexcept\n         {\n             using batch_type = batch<uint8_t, A>;\n             return vreinterpretq_f64_u8(swizzle<A>(batch_type(vreinterpretq_u8_f64(self)), detail::burst_index<uint8_t>(idx), A()));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<std::complex<float>, A> swizzle(batch<std::complex<float>, A> const& self,\n-                                                     batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n-                                                     requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<float>, A> swizzle(batch<std::complex<float>, A> const& self,\n+                                                           batch_constant<uint32_t, A, V0, V1, V2, V3> idx,\n+                                                           requires_arch<neon64>) noexcept\n         {\n             return batch<std::complex<float>>(swizzle(self.real(), idx, A()), swizzle(self.imag(), idx, A()));\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<std::complex<double>, A> swizzle(batch<std::complex<double>, A> const& self,\n-                                                      batch_constant<uint64_t, A, V0, V1> idx,\n-                                                      requires_arch<neon64>) noexcept\n+        XSIMD_INLINE batch<std::complex<double>, A> swizzle(batch<std::complex<double>, A> const& self,\n+                                                            batch_constant<uint64_t, A, V0, V1> idx,\n+                                                            requires_arch<neon64>) noexcept\n         {\n             return batch<std::complex<double>>(swizzle(self.real(), idx, A()), swizzle(self.imag(), idx, A()));\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_rvv.hpp",
        "status": "modified",
        "additions": 104,
        "deletions": 104,
        "changes": 208,
        "patch": "@@ -384,7 +384,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, class U = as_unsigned_integer_t<T>>\n-            inline batch<U, A> rvv_to_unsigned_batch(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvv_to_unsigned_batch(batch<T, A> const& arg) noexcept\n             {\n                 return rvvreinterpret<U>(arg.data);\n             }\n@@ -413,18 +413,18 @@ namespace xsimd\n                                , size_t(bvec));\n \n             template <class T, size_t Width>\n-            inline rvv_bool_t<T, Width> pmask8(uint8_t mask) noexcept\n+            XSIMD_INLINE rvv_bool_t<T, Width> pmask8(uint8_t mask) noexcept\n             {\n                 return rvv_bool_t<T, Width>(mask);\n             }\n             template <class T, size_t Width>\n-            inline rvv_bool_t<T, Width> pmask(uint64_t mask) noexcept\n+            XSIMD_INLINE rvv_bool_t<T, Width> pmask(uint64_t mask) noexcept\n             {\n                 return rvv_bool_t<T, Width>(mask);\n             }\n \n             template <class A, class T, size_t offset = 0, int shift = 0>\n-            inline rvv_reg_t<T, A::width> vindex() noexcept\n+            XSIMD_INLINE rvv_reg_t<T, A::width> vindex() noexcept\n             {\n                 auto index = rvvid(T {});\n                 if (shift < 0)\n@@ -462,7 +462,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, size_t Width>\n-            inline detail::rvv_reg_t<T, Width> broadcast(T arg) noexcept\n+            XSIMD_INLINE detail::rvv_reg_t<T, Width> broadcast(T arg) noexcept\n             {\n                 // A bit of a dance, here, because rvvmv_splat has no other\n                 // argument from which to deduce type, and T=char is not\n@@ -475,7 +475,7 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T>\n-        inline batch<T, A> broadcast(T arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<rvv>) noexcept\n         {\n             return detail::broadcast<T, A::width>(arg);\n         }\n@@ -491,13 +491,13 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvle(reinterpret_cast<detail::rvv_fix_char_t<T> const*>(src));\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<rvv>) noexcept\n         {\n             return load_aligned<A>(src, convert<T>(), rvv {});\n         }\n@@ -506,14 +506,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T, size_t W, typename std::enable_if<W >= types::detail::rvv_width_m1, int>::type = 0>\n-            inline rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n+            XSIMD_INLINE rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n             {\n                 typename rvv_reg_t<T, W * 2>::register_type tmp;\n                 tmp = __riscv_vset(tmp, 0, lo);\n                 return __riscv_vset(tmp, 1, hi);\n             }\n \n-            template <class T, size_t W, typename std::enable_if<W<types::detail::rvv_width_m1, int>::type = 0> inline rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n+            template <class T, size_t W, typename std::enable_if<W<types::detail::rvv_width_m1, int>::type = 0> XSIMD_INLINE rvv_reg_t<T, W * 2> rvvabut(rvv_reg_t<T, W> const& lo, rvv_reg_t<T, W> const& hi) noexcept\n             {\n                 return __riscv_vslideup(lo, hi, lo.vl, lo.vl * 2);\n             }\n@@ -544,7 +544,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-            inline batch<std::complex<T>, A> load_complex(batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<std::complex<T>, A> load_complex(batch<T, A> const& lo, batch<T, A> const& hi, requires_arch<rvv>) noexcept\n             {\n                 const auto real_index = vindex<A, as_unsigned_integer_t<T>, 0, 1>();\n                 const auto imag_index = vindex<A, as_unsigned_integer_t<T>, 1, 1>();\n@@ -561,13 +561,13 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_aligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n         {\n             detail::rvvse(reinterpret_cast<detail::rvv_fix_char_t<T>*>(dst), src);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<rvv>) noexcept\n         {\n             store_aligned<A>(dst, src, rvv {});\n         }\n@@ -590,7 +590,7 @@ namespace xsimd\n \n         // scatter\n         template <class A, class T, class U, detail::rvv_enable_sg_t<T, U> = 0>\n-        inline void scatter(batch<T, A> const& vals, T* dst, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& vals, T* dst, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n         {\n             using UU = as_unsigned_integer_t<U>;\n             const auto uindex = detail::rvv_to_unsigned_batch(index);\n@@ -602,7 +602,7 @@ namespace xsimd\n \n         // gather\n         template <class A, class T, class U, detail::rvv_enable_sg_t<T, U> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<rvv>) noexcept\n         {\n             using UU = as_unsigned_integer_t<U>;\n             const auto uindex = detail::rvv_to_unsigned_batch(index);\n@@ -698,63 +698,63 @@ namespace xsimd\n \n         // add\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvadd(lhs, rhs);\n         }\n \n         // sadd\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsadd(lhs, rhs);\n         }\n \n         // sub\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsub(lhs, rhs);\n         }\n \n         // ssub\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvssub(lhs, rhs);\n         }\n \n         // mul\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmul(lhs, rhs);\n         }\n \n         // div\n         template <class A, class T, typename detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvdiv(lhs, rhs);\n         }\n \n         // max\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmax(lhs, rhs);\n         }\n \n         // min\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmin(lhs, rhs);\n         }\n \n         // neg\n         template <class A, class T, detail::rvv_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             using S = as_signed_integer_t<T>;\n             const auto as_signed = detail::rvvreinterpret<S>(arg);\n@@ -763,43 +763,43 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvneg(arg);\n         }\n \n         // abs\n         template <class A, class T, detail::rvv_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return arg;\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvabs(arg);\n         }\n \n         // fma: x * y + z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also detail::rvvmadd(x, y, z);\n             return detail::rvvmacc(z, x, y);\n         }\n \n         // fnma: z - x * y\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also detail::rvvnmsub(x, y, z);\n             return detail::rvvnmsac(z, x, y);\n         }\n \n         // fms: x * y - z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also vfmsac(z, x, y), but lacking integer version\n             // also vfmsub(x, y, z), but lacking integer version\n@@ -808,7 +808,7 @@ namespace xsimd\n \n         // fnms: - x * y - z\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<rvv>) noexcept\n         {\n             // also vfnmacc(z, x, y), but lacking integer version\n             // also vfnmadd(x, y, z), but lacking integer version\n@@ -835,13 +835,13 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvand(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -850,21 +850,21 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmand(lhs, rhs);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto not_rhs = detail::rvvnot(rhs);\n             return detail::rvvand(lhs, not_rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -874,20 +874,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmandn(lhs, rhs);\n         }\n \n         // bitwise_or\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvor(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -896,20 +896,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmor(lhs, rhs);\n         }\n \n         // bitwise_xor\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvxor(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto lhs_bits = detail::rvv_to_unsigned_batch(lhs);\n             const auto rhs_bits = detail::rvv_to_unsigned_batch(rhs);\n@@ -918,28 +918,28 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmxor(lhs, rhs);\n         }\n \n         // bitwise_not\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvnot(arg);\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto arg_bits = detail::rvv_to_unsigned_batch(arg);\n             const auto result_bits = detail::rvvnot(arg_bits);\n             return detail::rvvreinterpret<T>(result_bits);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmnot(arg);\n         }\n@@ -962,30 +962,30 @@ namespace xsimd\n \n         // bitwise_lshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n         {\n             constexpr size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<size_t>(n) < size && \"index in bounds\");\n             return detail::rvvsll_splat(arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsll(lhs, detail::rvv_to_unsigned_batch<A, T>(rhs));\n         }\n \n         // bitwise_rshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<rvv>) noexcept\n         {\n             constexpr size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<size_t>(n) < size && \"index in bounds\");\n             return detail::rvvsr_splat(arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvsr(lhs, detail::rvv_to_unsigned_batch<A, T>(rhs));\n         }\n@@ -1019,14 +1019,14 @@ namespace xsimd\n                                 (__riscv_vfslide1down), , vec(vec, T))\n \n             template <class A, class T>\n-            inline T reduce_scalar(rvv_reg_t<T, types::detail::rvv_width_m1> const& arg)\n+            XSIMD_INLINE T reduce_scalar(rvv_reg_t<T, types::detail::rvv_width_m1> const& arg)\n             {\n                 return detail::rvvmv_lane0(rvv_reg_t<T, A::width>(arg.get_bytes(), types::detail::XSIMD_RVV_BITCAST));\n             }\n         }\n         // reduce_add\n         template <class A, class T, class V = typename batch<T, A>::value_type, detail::rvv_enable_all_t<T> = 0>\n-        inline V reduce_add(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE V reduce_add(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = detail::broadcast<T, types::detail::rvv_width_m1>(T(0));\n             const auto r = detail::rvvredsum(arg, zero);\n@@ -1035,7 +1035,7 @@ namespace xsimd\n \n         // reduce_max\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T reduce_max(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto lowest = detail::broadcast<T, types::detail::rvv_width_m1>(std::numeric_limits<T>::lowest());\n             const auto r = detail::rvvredmax(arg, lowest);\n@@ -1044,7 +1044,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T reduce_min(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto max = detail::broadcast<T, types::detail::rvv_width_m1>(std::numeric_limits<T>::max());\n             const auto r = detail::rvvredmin(arg, max);\n@@ -1053,7 +1053,7 @@ namespace xsimd\n \n         // haddp\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> haddp(const batch<T, A>* row, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(const batch<T, A>* row, requires_arch<rvv>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             T sums[size];\n@@ -1071,55 +1071,55 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmseq(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto neq_result = detail::rvvmxor(lhs, rhs);\n             return detail::rvvmnot(neq_result);\n         }\n \n         // neq\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsne(lhs, rhs);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmxor(lhs, rhs);\n         }\n \n         // lt\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmslt(lhs, rhs);\n         }\n \n         // le\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsle(lhs, rhs);\n         }\n \n         // gt\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsgt(lhs, rhs);\n         }\n \n         // ge\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmsge(lhs, rhs);\n         }\n@@ -1133,7 +1133,7 @@ namespace xsimd\n         }\n         // compress\n         template <class A, class T>\n-        inline batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcompress(x, mask);\n         }\n@@ -1150,17 +1150,17 @@ namespace xsimd\n \n         // swizzle\n         template <class A, class T, class I, I... idx>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...>, requires_arch<rvv>) noexcept\n         {\n             static_assert(batch<T, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             const batch<I, A> indices { idx... };\n             return detail::rvvrgather(arg, indices);\n         }\n \n         template <class A, class T, class I, I... idx>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n-                                                 batch_constant<I, A, idx...>,\n-                                                 requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n+                                                       batch_constant<I, A, idx...>,\n+                                                       requires_arch<rvv>) noexcept\n         {\n             const auto real = swizzle(self.real(), batch_constant<I, A, idx...> {}, rvv {});\n             const auto imag = swizzle(self.imag(), batch_constant<I, A, idx...> {}, rvv {});\n@@ -1174,28 +1174,28 @@ namespace xsimd\n         // extract_pair\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, size_t n, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, size_t n, requires_arch<rvv>) noexcept\n         {\n             const auto tmp = detail::rvvslidedown(rhs, n);\n             return detail::rvvslideup(tmp, lhs, lhs.size - n);\n         }\n \n         // select\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvmerge(b, a, cond);\n         }\n \n         template <class A, class T, bool... b>\n-        inline batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<rvv>) noexcept\n         {\n             return select(batch_bool<T, A> { b... }, true_br, false_br, rvv {});\n         }\n \n         // zip_lo\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto index = detail::vindex<A, as_unsigned_integer_t<T>, 0, -1>();\n             const auto mask = detail::pmask8<T, A::width>(0xaa);\n@@ -1206,7 +1206,7 @@ namespace xsimd\n \n         // zip_hi\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<rvv>) noexcept\n         {\n             const auto index = detail::vindex<A, as_unsigned_integer_t<T>, batch<T, A>::size / 2, -1>();\n             const auto mask = detail::pmask8<T, A::width>(0xaa);\n@@ -1217,7 +1217,7 @@ namespace xsimd\n \n         // store_complex\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n         {\n             const auto lo = zip_lo(src.real(), src.imag());\n             const auto hi = zip_hi(src.real(), src.imag());\n@@ -1227,7 +1227,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<rvv>) noexcept\n         {\n             store_complex_aligned(dst, src, rvv {});\n         }\n@@ -1245,7 +1245,7 @@ namespace xsimd\n \n         // rsqrt\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             auto approx = detail::rvvfrsqrt7(arg);\n             approx = approx * (1.5 - (0.5 * arg * approx * approx));\n@@ -1254,14 +1254,14 @@ namespace xsimd\n \n         // sqrt\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvfsqrt(arg);\n         }\n \n         // reciprocal\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvfrec7(arg);\n         }\n@@ -1293,12 +1293,12 @@ namespace xsimd\n             using rvv_enable_itof_t = typename std::enable_if<(sizeof(T) == sizeof(U) && !std::is_floating_point<T>::value && std::is_floating_point<U>::value), int>::type;\n \n             template <class A, class T, class U, rvv_enable_ftoi_t<T, U> = 0>\n-            inline batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n             {\n                 return rvvfcvt_rtz(U {}, arg);\n             }\n             template <class A, class T, class U, rvv_enable_itof_t<T, U> = 0>\n-            inline batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n+            XSIMD_INLINE batch<U, A> fast_cast(batch<T, A> const& arg, batch<U, A> const&, requires_arch<rvv>) noexcept\n             {\n                 return rvvfcvt_f(arg);\n             }\n@@ -1310,22 +1310,22 @@ namespace xsimd\n \n         // set\n         template <class A, class T, class... Args>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n         {\n             const std::array<T, batch<T, A>::size> tmp { args... };\n             return load_unaligned<A>(tmp.data(), convert<T>(), rvv {});\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<rvv>,\n-                                             Args... args_complex) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<rvv>,\n+                                                   Args... args_complex) noexcept\n         {\n             return batch<std::complex<T>>(set(batch<T, rvv> {}, rvv {}, args_complex.real()...),\n                                           set(batch<T, rvv> {}, rvv {}, args_complex.imag()...));\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<rvv>, Args... args) noexcept\n         {\n             using U = as_unsigned_integer_t<T>;\n             const auto values = set(batch<U, rvv> {}, rvv {}, static_cast<U>(args)...);\n@@ -1336,22 +1336,22 @@ namespace xsimd\n \n         // insert\n         template <class A, class T, size_t I, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<rvv>) noexcept\n         {\n             const auto mask = detail::pmask<T, A::width>(uint64_t(1) << I);\n             return detail::rvvmerge_splat(arg, val, mask);\n         }\n \n         // get\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline T get(batch<T, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE T get(batch<T, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n         {\n             const auto tmp = detail::rvvslidedown(arg, i);\n             return detail::rvvmv_lane0(tmp);\n         }\n \n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline std::complex<T> get(batch<std::complex<T>, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE std::complex<T> get(batch<std::complex<T>, A> const& arg, size_t i, requires_arch<rvv>) noexcept\n         {\n             const auto tmpr = detail::rvvslidedown(arg.real(), i);\n             const auto tmpi = detail::rvvslidedown(arg.imag(), i);\n@@ -1360,36 +1360,36 @@ namespace xsimd\n \n         // all\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcpop(arg) == batch_bool<T, A>::size;\n         }\n \n         // any\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return detail::rvvcpop(arg) > 0;\n         }\n \n         // bitwise_cast\n         template <class A, class T, class R, detail::rvv_enable_all_t<T> = 0, detail::rvv_enable_all_t<R> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<rvv>) noexcept\n         {\n             return detail::rvv_reg_t<R, A::width>(arg.data.get_bytes(), types::detail::XSIMD_RVV_BITCAST);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, detail::rvv_enable_all_t<T_in> = 0>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<rvv>) noexcept\n         {\n             using intermediate_t = typename detail::rvv_bool_t<T_out>;\n             return intermediate_t(arg.data);\n         }\n \n         // from_bool\n         template <class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = broadcast<A>(T(0), rvv {});\n             return detail::rvvmerge_splat(zero, T(1), arg);\n@@ -1398,26 +1398,26 @@ namespace xsimd\n         namespace detail\n         {\n             template <size_t Width>\n-            inline vuint8m1_t rvvslidedownbytes(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes(vuint8m1_t arg, size_t i)\n             {\n                 return __riscv_vslidedown(arg, i, types::detail::rvv_width_m1 / 8);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf2>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf2>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf2(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf2 / 8);\n                 return __riscv_vlmul_ext_u8m1(result);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf4>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf4>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf4(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf4 / 8);\n                 return __riscv_vlmul_ext_u8m1(result);\n             }\n             template <>\n-            inline vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf8>(vuint8m1_t arg, size_t i)\n+            XSIMD_INLINE vuint8m1_t rvvslidedownbytes<types::detail::rvv_width_mf8>(vuint8m1_t arg, size_t i)\n             {\n                 const auto bytes = __riscv_vlmul_trunc_u8mf8(arg);\n                 const auto result = __riscv_vslidedown(bytes, i, types::detail::rvv_width_mf8 / 8);\n@@ -1427,7 +1427,7 @@ namespace xsimd\n \n         // slide_left\n         template <size_t N, class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             const auto zero = broadcast<A>(uint8_t(0), rvv {});\n             const auto bytes = arg.data.get_bytes();\n@@ -1436,7 +1436,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T, detail::rvv_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             using reg_t = detail::rvv_reg_t<T, A::width>;\n             const auto bytes = arg.data.get_bytes();\n@@ -1445,7 +1445,7 @@ namespace xsimd\n \n         // isnan\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             return !(arg == arg);\n         }\n@@ -1456,29 +1456,29 @@ namespace xsimd\n             using rvv_as_signed_integer_t = as_signed_integer_t<as_unsigned_integer_t<T>>;\n \n             template <class A, class T, class U = rvv_as_signed_integer_t<T>>\n-            inline batch<U, A> rvvfcvt_default(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvvfcvt_default(batch<T, A> const& arg) noexcept\n             {\n                 return rvvfcvt_rne(U {}, arg);\n             }\n \n             template <class A, class T, class U = rvv_as_signed_integer_t<T>>\n-            inline batch<U, A> rvvfcvt_afz(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> rvvfcvt_afz(batch<T, A> const& arg) noexcept\n             {\n                 return rvvfcvt_rmm(U {}, arg);\n             }\n         }\n \n         // nearbyint_as_int\n         template <class A, class T, class U = detail::rvv_as_signed_integer_t<T>>\n-        inline batch<U, A> nearbyint_as_int(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<U, A> nearbyint_as_int(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Reference rounds ties to nearest even\n             return detail::rvvfcvt_default(arg);\n         }\n \n         // round\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> round(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> round(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Round ties away from zero.\n             const auto mask = abs(arg) < constants::maxflint<batch<T, A>>();\n@@ -1487,7 +1487,7 @@ namespace xsimd\n \n         // nearbyint\n         template <class A, class T, detail::rvv_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<rvv>) noexcept\n         {\n             // Round according to current rounding mode.\n             const auto mask = abs(arg) < constants::maxflint<batch<T, A>>();"
      },
      {
        "filename": "include/xsimd/arch/xsimd_scalar.hpp",
        "status": "modified",
        "additions": 150,
        "deletions": 148,
        "changes": 298,
        "patch": "@@ -20,6 +20,8 @@\n #include <limits>\n #include <type_traits>\n \n+#include \"xsimd/config/xsimd_inline.hpp\"\n+\n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n #include \"xtl/xcomplex.hpp\"\n #endif\n@@ -86,7 +88,7 @@ namespace xsimd\n     using std::tgamma;\n     using std::trunc;\n \n-    inline signed char abs(signed char v)\n+    XSIMD_INLINE signed char abs(signed char v)\n     {\n         return v < 0 ? -v : v;\n     }\n@@ -96,43 +98,43 @@ namespace xsimd\n         // Use templated type here to prevent automatic instantiation that may\n         // ends up in a warning\n         template <typename char_type>\n-        inline char abs(char_type v, std::true_type)\n+        XSIMD_INLINE char abs(char_type v, std::true_type)\n         {\n             return v;\n         }\n         template <typename char_type>\n-        inline char abs(char_type v, std::false_type)\n+        XSIMD_INLINE char abs(char_type v, std::false_type)\n         {\n             return v < 0 ? -v : v;\n         }\n     }\n \n-    inline char abs(char v)\n+    XSIMD_INLINE char abs(char v)\n     {\n         return detail::abs(v, std::is_unsigned<char>::type {});\n     }\n \n-    inline short abs(short v)\n+    XSIMD_INLINE short abs(short v)\n     {\n         return v < 0 ? -v : v;\n     }\n-    inline unsigned char abs(unsigned char v)\n+    XSIMD_INLINE unsigned char abs(unsigned char v)\n     {\n         return v;\n     }\n-    inline unsigned short abs(unsigned short v)\n+    XSIMD_INLINE unsigned short abs(unsigned short v)\n     {\n         return v;\n     }\n-    inline unsigned int abs(unsigned int v)\n+    XSIMD_INLINE unsigned int abs(unsigned int v)\n     {\n         return v;\n     }\n-    inline unsigned long abs(unsigned long v)\n+    XSIMD_INLINE unsigned long abs(unsigned long v)\n     {\n         return v;\n     }\n-    inline unsigned long long abs(unsigned long long v)\n+    XSIMD_INLINE unsigned long long abs(unsigned long long v)\n     {\n         return v;\n     }\n@@ -145,56 +147,56 @@ namespace xsimd\n \n     // Windows defines catch all templates\n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isfinite(T var) noexcept\n     {\n         return std::isfinite(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isfinite(T var) noexcept\n     {\n         return isfinite(double(var));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isinf(T var) noexcept\n     {\n         return std::isinf(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isinf(T var) noexcept\n     {\n         return isinf(double(var));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, bool>::type\n     isnan(T var) noexcept\n     {\n         return std::isnan(var);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, bool>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, bool>::type\n     isnan(T var) noexcept\n     {\n         return isnan(double(var));\n     }\n #endif\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type add(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type add(T const& x, Tp const& y) noexcept\n     {\n         return x + y;\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type avg(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type avg(T const& x, Tp const& y) noexcept\n     {\n         using common_type = typename std::common_type<T, Tp>::type;\n         if (std::is_floating_point<common_type>::value)\n@@ -215,7 +217,7 @@ namespace xsimd\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type avgr(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type avgr(T const& x, Tp const& y) noexcept\n     {\n         using common_type = typename std::common_type<T, Tp>::type;\n         if (std::is_floating_point<common_type>::value)\n@@ -227,49 +229,49 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline T incr(T const& x) noexcept\n+    XSIMD_INLINE T incr(T const& x) noexcept\n     {\n         return x + T(1);\n     }\n \n     template <class T>\n-    inline T incr_if(T const& x, bool mask) noexcept\n+    XSIMD_INLINE T incr_if(T const& x, bool mask) noexcept\n     {\n         return x + T(mask ? 1 : 0);\n     }\n \n-    inline bool all(bool mask)\n+    XSIMD_INLINE bool all(bool mask)\n     {\n         return mask;\n     }\n \n-    inline bool any(bool mask)\n+    XSIMD_INLINE bool any(bool mask)\n     {\n         return mask;\n     }\n \n-    inline bool none(bool mask)\n+    XSIMD_INLINE bool none(bool mask)\n     {\n         return !mask;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_and(T x, T y) noexcept\n     {\n         return x & y;\n     }\n \n     template <class T_out, class T_in>\n-    inline T_out bitwise_cast(T_in x) noexcept\n+    XSIMD_INLINE T_out bitwise_cast(T_in x) noexcept\n     {\n         static_assert(sizeof(T_in) == sizeof(T_out), \"bitwise_cast between types of the same size\");\n         T_out r;\n         std::memcpy((void*)&r, (void*)&x, sizeof(T_in));\n         return r;\n     }\n \n-    inline float bitwise_and(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_and(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -280,7 +282,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_and(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_and(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -292,32 +294,32 @@ namespace xsimd\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     bitwise_lshift(T0 x, T1 shift) noexcept\n     {\n         return x << shift;\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     bitwise_rshift(T0 x, T1 shift) noexcept\n     {\n         return x >> shift;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_not(T x) noexcept\n     {\n         return ~x;\n     }\n \n-    inline bool bitwise_not(bool x) noexcept\n+    XSIMD_INLINE bool bitwise_not(bool x) noexcept\n     {\n         return !x;\n     }\n \n-    inline float bitwise_not(float x) noexcept\n+    XSIMD_INLINE float bitwise_not(float x) noexcept\n     {\n         uint32_t ix;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -327,7 +329,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_not(double x) noexcept\n+    XSIMD_INLINE double bitwise_not(double x) noexcept\n     {\n         uint64_t ix;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -338,19 +340,19 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_scalar<T>::value, T>::type bitwise_andnot(T x, T y) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_scalar<T>::value, T>::type bitwise_andnot(T x, T y) noexcept\n     {\n         return bitwise_and(x, bitwise_not(y));\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_or(T x, T y) noexcept\n     {\n         return x | y;\n     }\n \n-    inline float bitwise_or(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_or(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -361,7 +363,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_or(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_or(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -373,13 +375,13 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type\n     bitwise_xor(T x, T y) noexcept\n     {\n         return x ^ y;\n     }\n \n-    inline float bitwise_xor(float x, float y) noexcept\n+    XSIMD_INLINE float bitwise_xor(float x, float y) noexcept\n     {\n         uint32_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(float));\n@@ -390,7 +392,7 @@ namespace xsimd\n         return r;\n     }\n \n-    inline double bitwise_xor(double x, double y) noexcept\n+    XSIMD_INLINE double bitwise_xor(double x, double y) noexcept\n     {\n         uint64_t ix, iy;\n         std::memcpy((void*)&ix, (void*)&x, sizeof(double));\n@@ -402,75 +404,75 @@ namespace xsimd\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type div(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type div(T const& x, Tp const& y) noexcept\n     {\n         return x / y;\n     }\n \n     template <class T, class Tp>\n-    inline auto mod(T const& x, Tp const& y) noexcept -> decltype(x % y)\n+    XSIMD_INLINE auto mod(T const& x, Tp const& y) noexcept -> decltype(x % y)\n     {\n         return x % y;\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type mul(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type mul(T const& x, Tp const& y) noexcept\n     {\n         return x * y;\n     }\n \n     template <class T>\n-    inline T neg(T const& x) noexcept\n+    XSIMD_INLINE T neg(T const& x) noexcept\n     {\n         return -x;\n     }\n \n     template <class T>\n-    inline auto pos(T const& x) noexcept -> decltype(+x)\n+    XSIMD_INLINE auto pos(T const& x) noexcept -> decltype(+x)\n     {\n         return +x;\n     }\n \n-    inline float reciprocal(float const& x) noexcept\n+    XSIMD_INLINE float reciprocal(float const& x) noexcept\n     {\n         return 1.f / x;\n     }\n \n-    inline double reciprocal(double const& x) noexcept\n+    XSIMD_INLINE double reciprocal(double const& x) noexcept\n     {\n         return 1. / x;\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     rotl(T0 x, T1 shift) noexcept\n     {\n         constexpr auto N = std::numeric_limits<T0>::digits;\n         return (x << shift) | (x >> (N - shift));\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T0>::value && std::is_integral<T1>::value, T0>::type\n     rotr(T0 x, T1 shift) noexcept\n     {\n         constexpr auto N = std::numeric_limits<T0>::digits;\n         return (x >> shift) | (x << (N - shift));\n     }\n \n     template <class T>\n-    inline bool isnan(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isnan(std::complex<T> var) noexcept\n     {\n         return std::isnan(std::real(var)) || std::isnan(std::imag(var));\n     }\n \n     template <class T>\n-    inline bool isinf(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isinf(std::complex<T> var) noexcept\n     {\n         return std::isinf(std::real(var)) || std::isinf(std::imag(var));\n     }\n \n     template <class T>\n-    inline bool isfinite(std::complex<T> var) noexcept\n+    XSIMD_INLINE bool isfinite(std::complex<T> var) noexcept\n     {\n         return std::isfinite(std::real(var)) && std::isfinite(std::imag(var));\n     }\n@@ -499,138 +501,138 @@ namespace xsimd\n #endif\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T clip(const T& val, const T& low, const T& hi) noexcept\n+    XSIMD_INLINE T clip(const T& val, const T& low, const T& hi) noexcept\n     {\n         assert(low <= hi && \"ordered clipping bounds\");\n         return low > val ? low : (hi < val ? hi : val);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_flint(const T& x) noexcept\n+    XSIMD_INLINE bool is_flint(const T& x) noexcept\n     {\n         return std::isnan(x - x) ? false : (x - std::trunc(x)) == T(0);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_even(const T& x) noexcept\n+    XSIMD_INLINE bool is_even(const T& x) noexcept\n     {\n         return is_flint(x * T(0.5));\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool is_odd(const T& x) noexcept\n+    XSIMD_INLINE bool is_odd(const T& x) noexcept\n     {\n         return is_even(x - 1.);\n     }\n \n-    inline int32_t nearbyint_as_int(float var) noexcept\n+    XSIMD_INLINE int32_t nearbyint_as_int(float var) noexcept\n     {\n         return static_cast<int32_t>(std::nearbyint(var));\n     }\n \n-    inline int64_t nearbyint_as_int(double var) noexcept\n+    XSIMD_INLINE int64_t nearbyint_as_int(double var) noexcept\n     {\n         return static_cast<int64_t>(std::nearbyint(var));\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool eq(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool eq(const T& x0, const T& x1) noexcept\n     {\n         return x0 == x1;\n     }\n \n     template <class T>\n-    inline bool eq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n+    XSIMD_INLINE bool eq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n     {\n         return x0 == x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool ge(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool ge(const T& x0, const T& x1) noexcept\n     {\n         return x0 >= x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool gt(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool gt(const T& x0, const T& x1) noexcept\n     {\n         return x0 > x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool le(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool le(const T& x0, const T& x1) noexcept\n     {\n         return x0 <= x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool lt(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool lt(const T& x0, const T& x1) noexcept\n     {\n         return x0 < x1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline bool neq(const T& x0, const T& x1) noexcept\n+    XSIMD_INLINE bool neq(const T& x0, const T& x1) noexcept\n     {\n         return x0 != x1;\n     }\n \n     template <class T>\n-    inline bool neq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n+    XSIMD_INLINE bool neq(const std::complex<T>& x0, const std::complex<T>& x1) noexcept\n     {\n         return !(x0 == x1);\n     }\n \n #if defined(__APPLE__) && (MAC_OS_X_VERSION_MIN_REQUIRED > 1080)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return __exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return __exp10(x);\n     }\n #elif defined(__GLIBC__)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return ::exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return ::exp10(x);\n     }\n #elif !defined(__clang__) && defined(__GNUC__) && (__GNUC__ >= 5)\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         return __builtin_exp10f(x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         return __builtin_exp10(x);\n     }\n #elif defined(_WIN32)\n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T exp10(const T& x) noexcept\n+    XSIMD_INLINE T exp10(const T& x) noexcept\n     {\n         // Very inefficient but other implementations give incorrect results\n         // on Windows\n         return std::pow(T(10), x);\n     }\n #else\n-    inline float exp10(const float& x) noexcept\n+    XSIMD_INLINE float exp10(const float& x) noexcept\n     {\n         const float ln10 = std::log(10.f);\n         return std::exp(ln10 * x);\n     }\n-    inline double exp10(const double& x) noexcept\n+    XSIMD_INLINE double exp10(const double& x) noexcept\n     {\n         const double ln10 = std::log(10.);\n         return std::exp(ln10 * x);\n     }\n #endif\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline auto rsqrt(const T& x) noexcept -> decltype(std::sqrt(x))\n+    XSIMD_INLINE auto rsqrt(const T& x) noexcept -> decltype(std::sqrt(x))\n     {\n         using float_type = decltype(std::sqrt(x));\n         return static_cast<float_type>(1) / std::sqrt(x);\n@@ -639,7 +641,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C expm1_complex_scalar_impl(const C& val) noexcept\n+        XSIMD_INLINE C expm1_complex_scalar_impl(const C& val) noexcept\n         {\n             using T = typename C::value_type;\n             T isin = std::sin(val.imag());\n@@ -651,14 +653,14 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> expm1(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> expm1(const std::complex<T>& val) noexcept\n     {\n         return detail::expm1_complex_scalar_impl(val);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> expm1(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return detail::expm1_complex_scalar_impl(val);\n     }\n@@ -667,7 +669,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C log1p_complex_scalar_impl(const C& val) noexcept\n+        XSIMD_INLINE C log1p_complex_scalar_impl(const C& val) noexcept\n         {\n             using T = typename C::value_type;\n             C u = C(1.) + val;\n@@ -676,19 +678,19 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> log1p(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> log1p(const std::complex<T>& val) noexcept\n     {\n         return detail::log1p_complex_scalar_impl(val);\n     }\n \n     template <class T>\n-    inline std::complex<T> log2(const std::complex<T>& val) noexcept\n+    XSIMD_INLINE std::complex<T> log2(const std::complex<T>& val) noexcept\n     {\n         return log(val) / std::log(T(2));\n     }\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T sadd(const T& lhs, const T& rhs) noexcept\n+    XSIMD_INLINE T sadd(const T& lhs, const T& rhs) noexcept\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -719,7 +721,7 @@ namespace xsimd\n     }\n \n     template <typename T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T ssub(const T& lhs, const T& rhs) noexcept\n+    XSIMD_INLINE T ssub(const T& lhs, const T& rhs) noexcept\n     {\n         if (std::numeric_limits<T>::is_signed)\n         {\n@@ -755,7 +757,7 @@ namespace xsimd\n         using value_type_or_type = typename value_type_or_type_helper<T>::type;\n \n         template <class T0, class T1>\n-        inline typename std::enable_if<std::is_integral<T1>::value, T0>::type\n+        XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, T0>::type\n         ipow(const T0& x, const T1& n) noexcept\n         {\n             static_assert(std::is_integral<T1>::value, \"second argument must be an integer\");\n@@ -781,61 +783,61 @@ namespace xsimd\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T1>::value, T0>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, T0>::type\n     pow(const T0& x, const T1& n) noexcept\n     {\n         return detail::ipow(x, n);\n     }\n \n     template <class T0, class T1>\n-    inline auto\n+    XSIMD_INLINE auto\n     pow(const T0& t0, const T1& t1) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_floating_point<T1>::value, decltype(std::pow(t0, t1))>::type\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T1>::value, std::complex<T0>>::type\n     pow(const std::complex<T0>& t0, const T1& t1) noexcept\n     {\n         return detail::ipow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type\n+    XSIMD_INLINE typename std::enable_if<!std::is_integral<T1>::value, std::complex<T0>>::type\n     pow(const std::complex<T0>& t0, const T1& t1) noexcept\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T0, class T1>\n-    inline auto\n+    XSIMD_INLINE auto\n     pow(const T0& t0, const std::complex<T1>& t1) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value, decltype(std::pow(t0, t1))>::type\n     {\n         return std::pow(t0, t1);\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T bitofsign(T const& x) noexcept\n+    XSIMD_INLINE T bitofsign(T const& x) noexcept\n     {\n         return T(x < T(0));\n     }\n \n     template <class T>\n-    inline auto signbit(T const& v) noexcept -> decltype(bitofsign(v))\n+    XSIMD_INLINE auto signbit(T const& v) noexcept -> decltype(bitofsign(v))\n     {\n         return bitofsign(v);\n     }\n \n-    inline double sign(bool const& v) noexcept\n+    XSIMD_INLINE double sign(bool const& v) noexcept\n     {\n         return v;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T sign(const T& v) noexcept\n+    XSIMD_INLINE T sign(const T& v) noexcept\n     {\n         return v < T(0) ? T(-1.) : v == T(0) ? T(0.)\n                                              : T(1.);\n@@ -844,7 +846,7 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C sign_complex_scalar_impl(const C& v) noexcept\n+        XSIMD_INLINE C sign_complex_scalar_impl(const C& v) noexcept\n         {\n             using value_type = typename C::value_type;\n             if (v.real())\n@@ -859,66 +861,66 @@ namespace xsimd\n     }\n \n     template <class T>\n-    inline std::complex<T> sign(const std::complex<T>& v) noexcept\n+    XSIMD_INLINE std::complex<T> sign(const std::complex<T>& v) noexcept\n     {\n         return detail::sign_complex_scalar_impl(v);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> sign(const xtl::xcomplex<T, T, i3ec>& v) noexcept\n     {\n         return detail::sign_complex_scalar_impl(v);\n     }\n #endif\n \n-    inline double signnz(bool const&) noexcept\n+    XSIMD_INLINE double signnz(bool const&) noexcept\n     {\n         return 1;\n     }\n \n     template <class T, class = typename std::enable_if<std::is_scalar<T>::value>::type>\n-    inline T signnz(const T& v) noexcept\n+    XSIMD_INLINE T signnz(const T& v) noexcept\n     {\n         return v < T(0) ? T(-1.) : T(1.);\n     }\n \n     template <class T, class Tp>\n-    inline typename std::common_type<T, Tp>::type sub(T const& x, Tp const& y) noexcept\n+    XSIMD_INLINE typename std::common_type<T, Tp>::type sub(T const& x, Tp const& y) noexcept\n     {\n         return x - y;\n     }\n \n     template <class T>\n-    inline T decr(T const& x) noexcept\n+    XSIMD_INLINE T decr(T const& x) noexcept\n     {\n         return x - T(1);\n     }\n \n     template <class T>\n-    inline T decr_if(T const& x, bool mask) noexcept\n+    XSIMD_INLINE T decr_if(T const& x, bool mask) noexcept\n     {\n         return x - T(mask ? 1 : 0);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> log2(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return log(val) / log(T(2));\n     }\n #endif\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> log1p(const xtl::xcomplex<T, T, i3ec>& val) noexcept\n     {\n         return detail::log1p_complex_scalar_impl(val);\n     }\n #endif\n \n     template <class T0, class T1>\n-    inline auto min(T0 const& self, T1 const& other) noexcept\n+    XSIMD_INLINE auto min(T0 const& self, T1 const& other) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,\n                                    typename std::decay<decltype(self > other ? other : self)>::type>::type\n     {\n@@ -927,14 +929,14 @@ namespace xsimd\n \n     // numpy defines minimum operator on complex using lexical comparison\n     template <class T0, class T1>\n-    inline std::complex<typename std::common_type<T0, T1>::type>\n+    XSIMD_INLINE std::complex<typename std::common_type<T0, T1>::type>\n     min(std::complex<T0> const& self, std::complex<T1> const& other) noexcept\n     {\n         return (self.real() < other.real()) ? (self) : (self.real() == other.real() ? (self.imag() < other.imag() ? self : other) : other);\n     }\n \n     template <class T0, class T1>\n-    inline auto max(T0 const& self, T1 const& other) noexcept\n+    XSIMD_INLINE auto max(T0 const& self, T1 const& other) noexcept\n         -> typename std::enable_if<std::is_scalar<T0>::value && std::is_scalar<T1>::value,\n                                    typename std::decay<decltype(self > other ? other : self)>::type>::type\n     {\n@@ -943,49 +945,49 @@ namespace xsimd\n \n     // numpy defines maximum operator on complex using lexical comparison\n     template <class T0, class T1>\n-    inline std::complex<typename std::common_type<T0, T1>::type>\n+    XSIMD_INLINE std::complex<typename std::common_type<T0, T1>::type>\n     max(std::complex<T0> const& self, std::complex<T1> const& other) noexcept\n     {\n         return (self.real() > other.real()) ? (self) : (self.real() == other.real() ? (self.imag() > other.imag() ? self : other) : other);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n     {\n         return a * b + c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fma(const T& a, const T& b, const T& c) noexcept\n     {\n         return std::fma(a, b, c);\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_scalar<T>::value, T>::type fms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_scalar<T>::value, T>::type fms(const T& a, const T& b, const T& c) noexcept\n     {\n         return a * b - c;\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.real(), b.real(), fms(a.imag(), b.imag(), c.real())),\n                      fma(a.real(), b.imag(), fma(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fma_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fma_complex_scalar_impl(a, b, c);\n     }\n@@ -994,109 +996,109 @@ namespace xsimd\n     namespace detail\n     {\n         template <class C>\n-        inline C fms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.real(), b.real(), fma(a.imag(), b.imag(), c.real())),\n                      fma(a.real(), b.imag(), fms(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fms_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fms_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n     {\n         return -(a * b) + c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fnma(const T& a, const T& b, const T& c) noexcept\n     {\n         return std::fma(-a, b, c);\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fnma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fnma_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.imag(), b.imag(), fms(a.real(), b.real(), c.real())),\n                      -fma(a.real(), b.imag(), fms(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fnma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fnma(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fnma_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fnma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fnma(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fnma_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     template <class T>\n-    inline typename std::enable_if<std::is_integral<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_integral<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n     {\n         return -(a * b) - c;\n     }\n \n     template <class T>\n-    inline typename std::enable_if<std::is_floating_point<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n+    XSIMD_INLINE typename std::enable_if<std::is_floating_point<T>::value, T>::type fnms(const T& a, const T& b, const T& c) noexcept\n     {\n         return -std::fma(a, b, c);\n     }\n \n     namespace detail\n     {\n         template <class C>\n-        inline C fnms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n+        XSIMD_INLINE C fnms_complex_scalar_impl(const C& a, const C& b, const C& c) noexcept\n         {\n             return { fms(a.imag(), b.imag(), fma(a.real(), b.real(), c.real())),\n                      -fma(a.real(), b.imag(), fma(a.imag(), b.real(), c.imag())) };\n         }\n     }\n \n     template <class T>\n-    inline std::complex<T> fnms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n+    XSIMD_INLINE std::complex<T> fnms(const std::complex<T>& a, const std::complex<T>& b, const std::complex<T>& c) noexcept\n     {\n         return detail::fnms_complex_scalar_impl(a, b, c);\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T, bool i3ec>\n-    inline xtl::xcomplex<T, T, i3ec> fnms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n+    XSIMD_INLINE xtl::xcomplex<T, T, i3ec> fnms(const xtl::xcomplex<T, T, i3ec>& a, const xtl::xcomplex<T, T, i3ec>& b, const xtl::xcomplex<T, T, i3ec>& c) noexcept\n     {\n         return detail::fnms_complex_scalar_impl(a, b, c);\n     }\n #endif\n \n     namespace detail\n     {\n-#define XSIMD_HASSINCOS_TRAIT(func)                                                                                                     \\\n-    template <class S>                                                                                                                  \\\n-    struct has##func                                                                                                                    \\\n-    {                                                                                                                                   \\\n-        template <class T>                                                                                                              \\\n-        static inline auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type {}); \\\n-        static inline std::false_type get(...);                                                                                         \\\n-        static constexpr bool value = decltype(get((S*)nullptr))::value;                                                                \\\n+#define XSIMD_HASSINCOS_TRAIT(func)                                                                                                           \\\n+    template <class S>                                                                                                                        \\\n+    struct has##func                                                                                                                          \\\n+    {                                                                                                                                         \\\n+        template <class T>                                                                                                                    \\\n+        static XSIMD_INLINE auto get(T* ptr) -> decltype(func(std::declval<T>(), std::declval<T*>(), std::declval<T*>()), std::true_type {}); \\\n+        static XSIMD_INLINE std::false_type get(...);                                                                                         \\\n+        static constexpr bool value = decltype(get((S*)nullptr))::value;                                                                      \\\n     }\n \n #define XSIMD_HASSINCOS(func, T) has##func<T>::value\n@@ -1109,21 +1111,21 @@ namespace xsimd\n         struct generic_sincosf\n         {\n             template <class T>\n-            inline typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<XSIMD_HASSINCOS(sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 sincosf(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && XSIMD_HASSINCOS(__sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 __sincosf(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincosf, T) && !XSIMD_HASSINCOS(__sincosf, T), void>::type\n             operator()(float val, T& s, T& c)\n             {\n                 s = std::sin(val);\n@@ -1134,21 +1136,21 @@ namespace xsimd\n         struct generic_sincos\n         {\n             template <class T>\n-            inline typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<XSIMD_HASSINCOS(sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 sincos(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && XSIMD_HASSINCOS(__sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 __sincos(val, &s, &c);\n             }\n \n             template <class T>\n-            inline typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type\n+            XSIMD_INLINE typename std::enable_if<!XSIMD_HASSINCOS(sincos, T) && !XSIMD_HASSINCOS(__sincos, T), void>::type\n             operator()(double val, T& s, T& c)\n             {\n                 s = std::sin(val);\n@@ -1160,43 +1162,43 @@ namespace xsimd\n #undef XSIMD_HASSINCOS\n     }\n \n-    inline std::pair<float, float> sincos(float val) noexcept\n+    XSIMD_INLINE std::pair<float, float> sincos(float val) noexcept\n     {\n         float s, c;\n         detail::generic_sincosf {}(val, s, c);\n         return std::make_pair(s, c);\n     }\n \n-    inline std::pair<double, double> sincos(double val) noexcept\n+    XSIMD_INLINE std::pair<double, double> sincos(double val) noexcept\n     {\n         double s, c;\n         detail::generic_sincos {}(val, s, c);\n         return std::make_pair(s, c);\n     }\n \n     template <class T>\n-    inline std::pair<std::complex<T>, std::complex<T>>\n+    XSIMD_INLINE std::pair<std::complex<T>, std::complex<T>>\n     sincos(const std::complex<T>& val) noexcept\n     {\n         return std::make_pair(std::sin(val), std::cos(val));\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class T>\n-    inline std::pair<xtl::xcomplex<T>, xtl::xcomplex<T>> sincos(const xtl::xcomplex<T>& val) noexcept\n+    XSIMD_INLINE std::pair<xtl::xcomplex<T>, xtl::xcomplex<T>> sincos(const xtl::xcomplex<T>& val) noexcept\n     {\n         return std::make_pair(sin(val), cos(val));\n     }\n #endif\n \n     template <class T, class _ = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-    inline T frexp(T const& val, int& exp) noexcept\n+    XSIMD_INLINE T frexp(T const& val, int& exp) noexcept\n     {\n         return std::frexp(val, &exp);\n     }\n \n     template <class T>\n-    inline T select(bool cond, T const& true_br, T const& false_br) noexcept\n+    XSIMD_INLINE T select(bool cond, T const& true_br, T const& false_br) noexcept\n     {\n         return cond ? true_br : false_br;\n     }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse2.hpp",
        "status": "modified",
        "additions": 182,
        "deletions": 182,
        "changes": 364,
        "patch": "@@ -24,7 +24,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -57,31 +57,31 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avgr(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n \n         // abs\n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128d sign_mask = _mm_set1_pd(-0.f); // -0.f = 1 << 31\n             return _mm_andnot_pd(sign_mask, self);\n         }\n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128 sign_mask = _mm_set1_ps(-0.f); // -0.f = 1 << 31\n             return _mm_andnot_ps(sign_mask, self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -107,54 +107,54 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_ps(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_pd(self, other);\n         }\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self) == 0x0F;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self) == 0x03;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_epi8(self) == 0xFFFF;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self) != 0;\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_epi8(self) != 0;\n         }\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -172,7 +172,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -192,83 +192,83 @@ namespace xsimd\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<sse2>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_and(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_si128(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_si128(self, other);\n         }\n \n         template <class A>\n-        batch<double, A> inline bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        batch<double, A> XSIMD_INLINE bitwise_and(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_and(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_and_pd(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_ps(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_andnot(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_ps(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_si128(other, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_si128(other, self);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_pd(other, self);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_andnot(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_andnot_pd(other, self);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -295,73 +295,73 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_not(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, _mm_set1_epi32(-1));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, _mm_set1_epi32(-1));\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_not(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, _mm_castsi128_pd(_mm_set1_epi32(-1)));\n         }\n \n         // bitwise_or\n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_or(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(self, other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_or(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(self, other);\n         }\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -423,81 +423,81 @@ namespace xsimd\n \n         // bitwise_xor\n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> bitwise_xor(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> bitwise_xor(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_si128(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_ps(self);\n         }\n         template <class A, class T, class Tp, class = typename std::enable_if<std::is_integral<typename std::common_type<T, Tp>::type>::value, void>::type>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<sse2>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<float, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_si128(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_pd(self);\n         }\n         template <class A>\n-        inline batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<float, A> const& self, batch<double, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_pd(self);\n         }\n         template <class A>\n-        inline batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<double, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castpd_ps(self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_cast(batch<double, A> const& self, batch<T, A> const&, requires_arch<sse2>) noexcept\n         {\n             return _mm_castpd_si128(self);\n         }\n \n         // broadcast\n         template <class A>\n-        batch<float, A> inline broadcast(float val, requires_arch<sse2>) noexcept\n+        batch<float, A> XSIMD_INLINE broadcast(float val, requires_arch<sse2>) noexcept\n         {\n             return _mm_set1_ps(val);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -522,7 +522,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<sse2>) noexcept\n         {\n             return _mm_set1_pd(val);\n         }\n@@ -533,43 +533,43 @@ namespace xsimd\n             // Override these methods in SSE-based archs, no need to override store_aligned / store_unaligned\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpacklo_ps(self.real(), self.imag());\n             }\n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpackhi_ps(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpacklo_pd(self.real(), self.imag());\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<sse2>) noexcept\n             {\n                 return _mm_unpackhi_pd(self.real(), self.imag());\n             }\n         }\n \n         // decr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n         {\n             return self + batch<T, A>(mask.data);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_div_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_div_pd(self, other);\n         }\n@@ -578,13 +578,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<sse2>) noexcept\n             {\n                 return _mm_cvtepi32_ps(self);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to sse2\n@@ -597,7 +597,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse2>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to sse2\n@@ -611,25 +611,25 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<sse2>) noexcept\n             {\n                 return _mm_cvttps_epi32(self);\n             }\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpeq_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_castps_si128(self), _mm_castps_si128(other)));\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -658,24 +658,24 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return ~(self != other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpeq_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castsi128_pd(_mm_cmpeq_epi32(_mm_castpd_si128(self), _mm_castpd_si128(other)));\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut[][4] = {\n                 { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },\n@@ -699,7 +699,7 @@ namespace xsimd\n             return _mm_castsi128_ps(_mm_load_si128((const __m128i*)lut[mask]));\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -711,7 +711,7 @@ namespace xsimd\n             return _mm_castsi128_pd(_mm_load_si128((const __m128i*)lut[mask]));\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<sse2>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[] = {\n                 0x0000000000000000,\n@@ -771,24 +771,24 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpge_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpge_pd(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpgt_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -816,14 +816,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpgt_pd(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse2>) noexcept\n         {\n             __m128 tmp0 = _mm_unpacklo_ps(row[0], row[1]);\n             __m128 tmp1 = _mm_unpackhi_ps(row[0], row[1]);\n@@ -836,22 +836,22 @@ namespace xsimd\n             return _mm_add_ps(tmp0, tmp2);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse2>) noexcept\n         {\n             return _mm_add_pd(_mm_unpacklo_pd(row[0], row[1]),\n                               _mm_unpackhi_pd(row[0], row[1]));\n         }\n \n         // incr_if\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& self, batch_bool<T, A> const& mask, requires_arch<sse2>) noexcept\n         {\n             return self - batch<T, A>(mask.data);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -865,46 +865,46 @@ namespace xsimd\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpunord_ps(self, self);\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpunord_pd(self, self);\n         }\n \n         // load_aligned\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_ps(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_si128((__m128i const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n         {\n             return _mm_load_pd(mem);\n         }\n \n         // load_unaligned\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_ps(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_si128((__m128i const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<sse2>) noexcept\n         {\n             return _mm_loadu_pd(mem);\n         }\n@@ -914,37 +914,37 @@ namespace xsimd\n         {\n             // Redefine these methods in the SSE-based archs if required\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<sse2>) noexcept\n             {\n                 return { _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(2, 0, 2, 0)), _mm_shuffle_ps(hi, lo, _MM_SHUFFLE(3, 1, 3, 1)) };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<sse2>) noexcept\n             {\n                 return { _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(0, 0)), _mm_shuffle_pd(hi, lo, _MM_SHUFFLE2(1, 1)) };\n             }\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmple_ps(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmple_pd(self, other);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmplt_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1011,7 +1011,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmplt_pd(self, other);\n         }\n@@ -1021,7 +1021,7 @@ namespace xsimd\n          */\n         namespace detail\n         {\n-            inline int mask_lut(int mask)\n+            XSIMD_INLINE int mask_lut(int mask)\n             {\n                 // clang-format off\n                 static const int mask_lut[256] = {\n@@ -1049,7 +1049,7 @@ namespace xsimd\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1075,149 +1075,149 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_ps(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_movemask_pd(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_max_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_max_pd(self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_min_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_min_pd(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mul_ps(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mul_pd(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<int16_t, A> mul(batch<int16_t, A> const& self, batch<int16_t, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> mul(batch<int16_t, A> const& self, batch<int16_t, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_mullo_epi16(self, other);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n-                                                  requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& self,\n+                                                        requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtps_epi32(self);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return 0 - self;\n         }\n         template <class A>\n-        inline batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> neg(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, _mm_castsi128_ps(_mm_set1_epi32(0x80000000)));\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(\n                 self, _mm_castsi128_pd(_mm_setr_epi32(0, 0x80000000, 0, 0x80000000)));\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpneq_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return ~(self == other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_castps_si128(_mm_xor_ps(_mm_castsi128_ps(self.data), _mm_castsi128_ps(other.data)));\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_cmpneq_pd(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_xor_pd(self, other);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self,\n-                                          kernel::requires_arch<sse2>)\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self,\n+                                                kernel::requires_arch<sse2>)\n         {\n             return _mm_rcp_ps(self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             __m128 tmp0 = _mm_add_ps(self, _mm_movehl_ps(self, self));\n             __m128 tmp1 = _mm_add_ss(tmp0, _mm_shuffle_ps(tmp0, tmp0, 1));\n             return _mm_cvtss_f32(tmp1);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1248,14 +1248,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtsd_f64(_mm_add_sd(self, _mm_unpackhi_pd(self, self)));\n         }\n \n         // reduce_max\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_max(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             constexpr auto mask0 = detail::shuffle(2, 3, 0, 0);\n             batch<T, A> step0 = _mm_shuffle_epi32(self, mask0);\n@@ -1277,7 +1277,7 @@ namespace xsimd\n \n         // reduce_min\n         template <class A, class T, class _ = typename std::enable_if<(sizeof(T) <= 2), void>::type>\n-        inline T reduce_min(batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             constexpr auto mask0 = detail::shuffle(2, 3, 0, 0);\n             batch<T, A> step0 = _mm_shuffle_epi32(self, mask0);\n@@ -1299,42 +1299,42 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_rsqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_cvtps_pd(_mm_rsqrt_ps(_mm_cvtpd_ps(val)));\n         }\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_ps(_mm_and_ps(cond, true_br), _mm_andnot_ps(cond, false_br));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_si128(_mm_and_si128(cond, true_br), _mm_andnot_si128(cond, false_br));\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, sse2 {});\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse2>) noexcept\n         {\n             return _mm_or_pd(_mm_and_pd(cond, true_br), _mm_andnot_pd(cond, false_br));\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3> mask, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1, I2, I3);\n             // shuffle within lane\n@@ -1348,7 +1348,7 @@ namespace xsimd\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1> mask, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t smask = detail::mod_shuffle(I0, I1);\n             // shuffle within lane\n@@ -1363,34 +1363,34 @@ namespace xsimd\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_sqrt_ps(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<sse2>) noexcept\n         {\n             return _mm_sqrt_pd(val);\n         }\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<sse2>) noexcept\n         {\n             return _mm_slli_si128(x, N);\n         }\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<sse2>) noexcept\n         {\n             return _mm_srli_si128(x, N);\n         }\n \n         // sadd\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1426,55 +1426,55 @@ namespace xsimd\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return _mm_setr_ps(values...);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1) noexcept\n         {\n             return _mm_set_epi64x(v1, v0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return _mm_setr_epi32(v0, v1, v2, v3);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return _mm_setr_epi16(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sse2>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return _mm_setr_epi8(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return _mm_setr_pd(values...);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return _mm_castsi128_ps(set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data);\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<sse2>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return _mm_castsi128_pd(set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data);\n@@ -1483,7 +1483,7 @@ namespace xsimd\n         // ssub\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1519,56 +1519,56 @@ namespace xsimd\n \n         // store_aligned\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_ps(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_si128((__m128i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_si128((__m128i*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_store_pd(mem, self);\n         }\n \n         // store_unaligned\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_ps(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_si128((__m128i*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_si128((__m128i*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<sse2>) noexcept\n         {\n             return _mm_storeu_pd(mem, self);\n         }\n \n         // sub\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_sub_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1593,61 +1593,61 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_sub_pd(self, other);\n         }\n \n         // swizzle\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1, V2, V3);\n             return _mm_shuffle_ps(self, self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1);\n             return _mm_shuffle_pd(self, self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(2 * V0, 2 * V0 + 1, 2 * V1, 2 * V1 + 1);\n             return _mm_shuffle_epi32(self, index);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<sse2>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, sse2 {}));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<sse2>) noexcept\n         {\n             constexpr uint32_t index = detail::shuffle(V0, V1, V2, V3);\n             return _mm_shuffle_epi32(self, index);\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<sse2>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, sse2 {}));\n         }\n \n         // zip_hi\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpackhi_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1672,19 +1672,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpackhi_pd(self, other);\n         }\n \n         // zip_lo\n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpacklo_ps(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse2>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1709,7 +1709,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<sse2>) noexcept\n         {\n             return _mm_unpacklo_pd(self, other);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse3.hpp",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -24,34 +24,34 @@ namespace xsimd\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<sse3>) noexcept\n         {\n             return _mm_hadd_ps(_mm_hadd_ps(row[0], row[1]),\n                                _mm_hadd_ps(row[2], row[3]));\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<sse3>) noexcept\n         {\n             return _mm_hadd_pd(row[0], row[1]);\n         }\n \n         // load_unaligned\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<sse3>) noexcept\n         {\n             return _mm_lddqu_si128((__m128i const*)mem);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<sse3>) noexcept\n         {\n             __m128 tmp0 = _mm_hadd_ps(self, self);\n             __m128 tmp1 = _mm_hadd_ps(tmp0, tmp0);\n             return _mm_cvtss_f32(tmp1);\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<sse3>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<sse3>) noexcept\n         {\n             __m128d tmp0 = _mm_hadd_pd(self, self);\n             return _mm_cvtsd_f64(tmp0);"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse4_1.hpp",
        "status": "modified",
        "additions": 23,
        "deletions": 23,
        "changes": 46,
        "patch": "@@ -24,18 +24,18 @@ namespace xsimd\n         using namespace types;\n         // any\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch<T, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE bool any(batch<T, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return !_mm_testz_si128(self, self);\n         }\n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_ceil_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_ceil_pd(self);\n         }\n@@ -44,7 +44,7 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 __m128i xH = _mm_srai_epi32(x, 16);\n@@ -56,7 +56,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<sse4_1>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 __m128i xH = _mm_srli_epi64(x, 32);\n@@ -69,7 +69,7 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 8)\n             {\n@@ -83,19 +83,19 @@ namespace xsimd\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_floor_ps(self);\n         }\n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_floor_pd(self);\n         }\n \n         // insert\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -124,7 +124,7 @@ namespace xsimd\n \n         // max\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -168,7 +168,7 @@ namespace xsimd\n \n         // min\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -212,7 +212,7 @@ namespace xsimd\n \n         // mul\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& self, batch<T, A> const& other, requires_arch<sse4_1>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -247,12 +247,12 @@ namespace xsimd\n \n         // nearbyint\n         template <class A>\n-        inline batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> nearbyint(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_ps(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n         template <class A>\n-        inline batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> nearbyint(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_pd(self, _MM_FROUND_TO_NEAREST_INT);\n         }\n@@ -261,30 +261,30 @@ namespace xsimd\n         namespace detail\n         {\n             template <class T>\n-            inline constexpr T interleave(T const& cond) noexcept\n+            XSIMD_INLINE constexpr T interleave(T const& cond) noexcept\n             {\n                 return (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 49) & 0x5555) | (((cond * 0x0101010101010101ULL & 0x8040201008040201ULL) * 0x0102040810204081ULL >> 48) & 0xAAAA);\n             }\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_epi8(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_ps(false_br, true_br, cond);\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_blendv_pd(false_br, true_br, cond);\n         }\n \n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<T, A, Values...>::mask();\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n@@ -308,26 +308,26 @@ namespace xsimd\n             }\n         }\n         template <class A, bool... Values>\n-        inline batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool_constant<float, A, Values...> const&, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<float, A, Values...>::mask();\n             return _mm_blend_ps(false_br, true_br, mask);\n         }\n         template <class A, bool... Values>\n-        inline batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool_constant<double, A, Values...> const&, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<sse4_1>) noexcept\n         {\n             constexpr int mask = batch_bool_constant<double, A, Values...>::mask();\n             return _mm_blend_pd(false_br, true_br, mask);\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_ps(self, _MM_FROUND_TO_ZERO);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<sse4_1>) noexcept\n         {\n             return _mm_round_pd(self, _MM_FROUND_TO_ZERO);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sse4_2.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -25,12 +25,12 @@ namespace xsimd\n \n         // lt\n         template <class A>\n-        inline batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) noexcept\n+        XSIMD_INLINE batch_bool<int64_t, A> lt(batch<int64_t, A> const& self, batch<int64_t, A> const& other, requires_arch<sse4_2>) noexcept\n         {\n             return _mm_cmpgt_epi64(other, self);\n         }\n         template <class A>\n-        inline batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) noexcept\n+        XSIMD_INLINE batch_bool<uint64_t, A> lt(batch<uint64_t, A> const& self, batch<uint64_t, A> const& other, requires_arch<sse4_2>) noexcept\n         {\n             auto xself = _mm_xor_si128(self, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));\n             auto xother = _mm_xor_si128(other, _mm_set1_epi64x(std::numeric_limits<int64_t>::lowest()));"
      },
      {
        "filename": "include/xsimd/arch/xsimd_ssse3.hpp",
        "status": "modified",
        "additions": 14,
        "deletions": 14,
        "changes": 28,
        "patch": "@@ -27,7 +27,7 @@ namespace xsimd\n \n         // abs\n         template <class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -57,13 +57,13 @@ namespace xsimd\n         {\n \n             template <class T, class A>\n-            inline batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n+            XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const&, batch<T, A> const& other, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n             {\n                 return other;\n             }\n \n             template <class T, class A, std::size_t I, std::size_t... Is>\n-            inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (i == I)\n                 {\n@@ -75,7 +75,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, class _ = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& self, batch<T, A> const& other, std::size_t i, requires_arch<ssse3>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(0 <= i && i < size && \"index in bounds\");\n@@ -84,7 +84,7 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 2)\n             {\n@@ -107,30 +107,30 @@ namespace xsimd\n \n         // rotate_right\n         template <size_t N, class A>\n-        inline batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> rotate_right(batch<uint16_t, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             return _mm_alignr_epi8(self, self, N);\n         }\n         template <size_t N, class A>\n-        inline batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> rotate_right(batch<int16_t, A> const& self, requires_arch<ssse3>) noexcept\n         {\n             return bitwise_cast<int16_t>(rotate_right<N, A>(bitwise_cast<uint16_t>(self), ssse3 {}));\n         }\n \n         // swizzle (dynamic mask)\n         template <class A>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n         {\n             return _mm_shuffle_epi8(self, mask);\n         }\n         template <class A>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch<uint8_t, A> mask, requires_arch<ssse3>) noexcept\n         {\n             return _mm_shuffle_epi8(self, mask);\n         }\n \n         template <class A, class T, class IT>\n-        inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+        XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n         swizzle(batch<T, A> const& self, batch<IT, A> mask, requires_arch<ssse3>) noexcept\n         {\n             constexpr auto pikes = static_cast<as_unsigned_integer_t<T>>(0x0706050403020100ul);\n@@ -140,7 +140,7 @@ namespace xsimd\n \n         // swizzle (constant mask)\n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<ssse3>) noexcept\n         {\n             constexpr batch_constant<uint8_t, A, 2 * V0, 2 * V0 + 1, 2 * V1, 2 * V1 + 1, 2 * V2, 2 * V2 + 1, 2 * V3, 2 * V3 + 1,\n                                      2 * V4, 2 * V4 + 1, 2 * V5, 2 * V5 + 1, 2 * V6, 2 * V6 + 1, 2 * V7, 2 * V7 + 1>\n@@ -149,21 +149,21 @@ namespace xsimd\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<ssse3>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, ssse3 {}));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), ssse3 {});\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<ssse3>) noexcept\n         {\n             return swizzle(self, mask.as_batch(), ssse3 {});\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_sve.hpp",
        "status": "modified",
        "additions": 153,
        "deletions": 153,
        "changes": 306,
        "patch": "@@ -31,22 +31,22 @@ namespace xsimd\n             using xsimd::types::detail::sve_vector_type;\n \n             // predicate creation\n-            inline svbool_t sve_ptrue_impl(index<1>) noexcept { return svptrue_b8(); }\n-            inline svbool_t sve_ptrue_impl(index<2>) noexcept { return svptrue_b16(); }\n-            inline svbool_t sve_ptrue_impl(index<4>) noexcept { return svptrue_b32(); }\n-            inline svbool_t sve_ptrue_impl(index<8>) noexcept { return svptrue_b64(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<1>) noexcept { return svptrue_b8(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<2>) noexcept { return svptrue_b16(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<4>) noexcept { return svptrue_b32(); }\n+            XSIMD_INLINE svbool_t sve_ptrue_impl(index<8>) noexcept { return svptrue_b64(); }\n \n             template <class T>\n             svbool_t sve_ptrue() noexcept { return sve_ptrue_impl(index<sizeof(T)> {}); }\n \n             // count active lanes in a predicate\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<1>) noexcept { return svcntp_b8(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<2>) noexcept { return svcntp_b16(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<4>) noexcept { return svcntp_b32(p, p); }\n-            inline uint64_t sve_pcount_impl(svbool_t p, index<8>) noexcept { return svcntp_b64(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<1>) noexcept { return svcntp_b8(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<2>) noexcept { return svcntp_b16(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<4>) noexcept { return svcntp_b32(p, p); }\n+            XSIMD_INLINE uint64_t sve_pcount_impl(svbool_t p, index<8>) noexcept { return svcntp_b64(p, p); }\n \n             template <class T>\n-            inline uint64_t sve_pcount(svbool_t p) noexcept { return sve_pcount_impl(p, index<sizeof(T)> {}); }\n+            XSIMD_INLINE uint64_t sve_pcount(svbool_t p) noexcept { return sve_pcount_impl(p, index<sizeof(T)> {}); }\n \n             // enable for signed integers\n             template <class T>\n@@ -84,20 +84,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n         {\n             return svld1(detail::sve_ptrue<T>(), reinterpret_cast<detail::sve_fix_char_t<T> const*>(src));\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* src, convert<T>, requires_arch<sve>) noexcept\n         {\n             return load_aligned<A>(src, convert<T>(), sve {});\n         }\n \n         // load_complex\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<std::complex<T>, A> load_complex_aligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> load_complex_aligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n         {\n             const T* buf = reinterpret_cast<const T*>(mem);\n             const auto tmp = svld2(detail::sve_ptrue<T>(), buf);\n@@ -107,7 +107,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<std::complex<T>, A> load_complex_unaligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> load_complex_unaligned(std::complex<T> const* mem, convert<std::complex<T>>, requires_arch<sve>) noexcept\n         {\n             return load_complex_aligned<A>(mem, convert<std::complex<T>> {}, sve {});\n         }\n@@ -117,20 +117,20 @@ namespace xsimd\n          *********/\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline void store_aligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_aligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n         {\n             svst1(detail::sve_ptrue<T>(), reinterpret_cast<detail::sve_fix_char_t<T>*>(dst), src);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* dst, batch<T, A> const& src, requires_arch<sve>) noexcept\n         {\n             store_aligned<A>(dst, src, sve {});\n         }\n \n         // store_complex\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_complex_aligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n         {\n             using v2type = typename std::conditional<(sizeof(T) == 4), svfloat32x2_t, svfloat64x2_t>::type;\n             v2type tmp {};\n@@ -141,7 +141,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n+        XSIMD_INLINE void store_complex_unaligned(std::complex<T>* dst, batch<std::complex<T>, A> const& src, requires_arch<sve>) noexcept\n         {\n             store_complex_aligned(dst, src, sve {});\n         }\n@@ -158,14 +158,14 @@ namespace xsimd\n \n         // scatter\n         template <class A, class T, class U, detail::sve_enable_sg_t<T, U> = 0>\n-        inline void scatter(batch<T, A> const& src, T* dst, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n+        XSIMD_INLINE void scatter(batch<T, A> const& src, T* dst, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n         {\n             svst1_scatter_index(detail::sve_ptrue<T>(), dst, index.data, src.data);\n         }\n \n         // gather\n         template <class A, class T, class U, detail::sve_enable_sg_t<T, U> = 0>\n-        inline batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> gather(batch<T, A> const&, T const* src, batch<U, A> const& index, kernel::requires_arch<sve>) noexcept\n         {\n             return svld1_gather_index(detail::sve_ptrue<T>(), src, index.data);\n         }\n@@ -176,67 +176,67 @@ namespace xsimd\n \n         // broadcast\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u8(uint8_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 1> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s8(int8_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u16(uint16_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 2> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s16(int16_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u32(uint32_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 4> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s32(int32_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_u64(uint64_t(arg));\n         }\n \n         template <class A, class T, detail::enable_sized_signed_t<T, 8> = 0>\n-        inline batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_s64(int64_t(arg));\n         }\n \n         template <class A>\n-        inline batch<float, A> broadcast(float arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> broadcast(float arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_f32(arg);\n         }\n \n         template <class A>\n-        inline batch<double, A> broadcast(double arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double arg, requires_arch<sve>) noexcept\n         {\n             return svdup_n_f64(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> broadcast(T val, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<sve>) noexcept\n         {\n             return broadcast<sve>(val, sve {});\n         }\n@@ -247,128 +247,128 @@ namespace xsimd\n \n         // add\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svadd_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // sadd\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svqadd(lhs, rhs);\n         }\n \n         // sub\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svsub_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // ssub\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svqsub(lhs, rhs);\n         }\n \n         // mul\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> mul(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmul_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // div\n         template <class A, class T, typename std::enable_if<sizeof(T) >= 4, int>::type = 0>\n-        inline batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> div(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svdiv_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // max\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmax_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // min\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svmin_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // neg\n         template <class A, class T, detail::enable_sized_unsigned_t<T, 1> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u8(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s8(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 2> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u16(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s16(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 4> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u32(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s32(arg)));\n         }\n \n         template <class A, class T, detail::enable_sized_unsigned_t<T, 8> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u64(svneg_x(detail::sve_ptrue<T>(), svreinterpret_s64(arg)));\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svneg_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // abs\n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return arg;\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_or_floating_point_t<T> = 0>\n-        inline batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svabs_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // fma: x * y + z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return svmad_x(detail::sve_ptrue<T>(), x, y, z);\n         }\n \n         // fnma: z - x * y\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return svmsb_x(detail::sve_ptrue<T>(), x, y, z);\n         }\n \n         // fms: x * y - z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return -fnma(x, y, z, sve {});\n         }\n \n         // fnms: - x * y - z\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z, requires_arch<sve>) noexcept\n         {\n             return -fma(x, y, z, sve {});\n         }\n@@ -379,13 +379,13 @@ namespace xsimd\n \n         // bitwise_and\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svand_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_and(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_and(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -394,7 +394,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_and(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -403,20 +403,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svand_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_andnot\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svbic_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_andnot(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_andnot(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -425,7 +425,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_andnot(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -434,20 +434,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svbic_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_or\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svorr_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_or(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_or(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -456,7 +456,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_or(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -465,20 +465,20 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svorr_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_xor\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_xor(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_xor(batch<float, A> const& lhs, batch<float, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u32(lhs);\n             const auto rhs_bits = svreinterpret_u32(rhs);\n@@ -487,7 +487,7 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_xor(batch<double, A> const& lhs, batch<double, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto lhs_bits = svreinterpret_u64(lhs);\n             const auto rhs_bits = svreinterpret_u64(rhs);\n@@ -496,36 +496,36 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // bitwise_not\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svnot_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         template <class A>\n-        inline batch<float, A> bitwise_not(batch<float, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_not(batch<float, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto arg_bits = svreinterpret_u32(arg);\n             const auto result_bits = svnot_x(detail::sve_ptrue<float>(), arg_bits);\n             return svreinterpret_f32(result_bits);\n         }\n \n         template <class A>\n-        inline batch<double, A> bitwise_not(batch<double, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_not(batch<double, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto arg_bits = svreinterpret_u64(arg);\n             const auto result_bits = svnot_x(detail::sve_ptrue<double>(), arg_bits);\n             return svreinterpret_f64(result_bits);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svnot_z(detail::sve_ptrue<T>(), arg);\n         }\n@@ -537,76 +537,76 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<1>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<1>) noexcept\n             {\n                 return svreinterpret_u8(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<2>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<2>) noexcept\n             {\n                 return svreinterpret_u16(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<4>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<4>) noexcept\n             {\n                 return svreinterpret_u32(arg);\n             }\n \n             template <class A, class T, class U>\n-            inline batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<8>) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch_impl(batch<T, A> const& arg, index<8>) noexcept\n             {\n                 return svreinterpret_u64(arg);\n             }\n \n             template <class A, class T, class U = as_unsigned_integer_t<T>>\n-            inline batch<U, A> sve_to_unsigned_batch(batch<T, A> const& arg) noexcept\n+            XSIMD_INLINE batch<U, A> sve_to_unsigned_batch(batch<T, A> const& arg) noexcept\n             {\n                 return sve_to_unsigned_batch_impl<A, T, U>(arg, index<sizeof(T)> {});\n             }\n         } // namespace detail\n \n         // bitwise_lshift\n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svlsl_x(detail::sve_ptrue<T>(), arg, n);\n         }\n \n         template <class A, class T, detail::enable_integral_t<T> = 0>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svlsl_x(detail::sve_ptrue<T>(), lhs, detail::sve_to_unsigned_batch<A, T>(rhs));\n         }\n \n         // bitwise_rshift\n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svlsr_x(detail::sve_ptrue<T>(), arg, static_cast<T>(n));\n         }\n \n         template <class A, class T, detail::sve_enable_unsigned_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svlsr_x(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& arg, int n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = sizeof(typename batch<T, A>::value_type) * 8;\n             assert(0 <= n && static_cast<std::size_t>(n) < size && \"index in bounds\");\n             return svasr_x(detail::sve_ptrue<T>(), arg, static_cast<as_unsigned_integer_t<T>>(n));\n         }\n \n         template <class A, class T, detail::sve_enable_signed_int_t<T> = 0>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svasr_x(detail::sve_ptrue<T>(), lhs, detail::sve_to_unsigned_batch<A, T>(rhs));\n         }\n@@ -617,29 +617,29 @@ namespace xsimd\n \n         // reduce_add\n         template <class A, class T, class V = typename batch<T, A>::value_type, detail::sve_enable_all_t<T> = 0>\n-        inline V reduce_add(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE V reduce_add(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             // sve integer reduction results are promoted to 64 bits\n             return static_cast<V>(svaddv(detail::sve_ptrue<T>(), arg));\n         }\n \n         // reduce_max\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline T reduce_max(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE T reduce_max(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svmaxv(detail::sve_ptrue<T>(), arg);\n         }\n \n         // reduce_min\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline T reduce_min(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE T reduce_min(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svminv(detail::sve_ptrue<T>(), arg);\n         }\n \n         // haddp\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> haddp(const batch<T, A>* row, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> haddp(const batch<T, A>* row, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             T sums[size];\n@@ -656,55 +656,55 @@ namespace xsimd\n \n         // eq\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpeq(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             const auto neq_result = sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n             return svnot_z(detail::sve_ptrue<T>(), neq_result);\n         }\n \n         // neq\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpne(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& lhs, batch_bool<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return sveor_z(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // lt\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmplt(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // le\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmple(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // gt\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpgt(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n \n         // ge\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svcmpge(detail::sve_ptrue<T>(), lhs, rhs);\n         }\n@@ -715,22 +715,22 @@ namespace xsimd\n \n         //  rotate_right\n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> rotate_right(batch<T, A> const& a, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& a, requires_arch<sve>) noexcept\n         {\n             return svext(a, a, N);\n         }\n \n         // swizzle (dynamic)\n         template <class A, class T, class I>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch<I, A> indices, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch<I, A> indices, requires_arch<sve>) noexcept\n         {\n             return svtbl(arg, indices);\n         }\n \n         template <class A, class T, class I>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n-                                                 batch<I, A> indices,\n-                                                 requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& self,\n+                                                       batch<I, A> indices,\n+                                                       requires_arch<sve>) noexcept\n         {\n             const auto real = swizzle(self.real(), indices, sve {});\n             const auto imag = swizzle(self.imag(), indices, sve {});\n@@ -739,16 +739,16 @@ namespace xsimd\n \n         // swizzle (static)\n         template <class A, class T, class I, I... idx>\n-        inline batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...> indices, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> swizzle(batch<T, A> const& arg, batch_constant<I, A, idx...> indices, requires_arch<sve>) noexcept\n         {\n             static_assert(batch<T, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             return swizzle(arg, indices.as_batch(), sve {});\n         }\n \n         template <class A, class T, class I, I... idx>\n-        inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& arg,\n-                                                 batch_constant<I, A, idx...> indices,\n-                                                 requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& arg,\n+                                                       batch_constant<I, A, idx...> indices,\n+                                                       requires_arch<sve>) noexcept\n         {\n             static_assert(batch<std::complex<T>, A>::size == sizeof...(idx), \"invalid swizzle indices\");\n             return swizzle(arg, indices.as_batch(), sve {});\n@@ -762,14 +762,14 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T>\n-            inline batch<T, A> sve_extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair(batch<T, A> const&, batch<T, A> const& /*rhs*/, std::size_t, ::xsimd::detail::index_sequence<>) noexcept\n             {\n                 assert(false && \"extract_pair out of bounds\");\n                 return batch<T, A> {};\n             }\n \n             template <class A, class T, size_t I, size_t... Is>\n-            inline batch<T, A> sve_extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<I, Is...>) noexcept\n             {\n                 if (n == I)\n                 {\n@@ -782,7 +782,7 @@ namespace xsimd\n             }\n \n             template <class A, class T, size_t... Is>\n-            inline batch<T, A> sve_extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>) noexcept\n+            XSIMD_INLINE batch<T, A> sve_extract_pair_impl(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, ::xsimd::detail::index_sequence<0, Is...>) noexcept\n             {\n                 if (n == 0)\n                 {\n@@ -796,7 +796,7 @@ namespace xsimd\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& lhs, batch<T, A> const& rhs, std::size_t n, requires_arch<sve>) noexcept\n         {\n             constexpr std::size_t size = batch<T, A>::size;\n             assert(n < size && \"index in bounds\");\n@@ -805,27 +805,27 @@ namespace xsimd\n \n         // select\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& a, batch<T, A> const& b, requires_arch<sve>) noexcept\n         {\n             return svsel(cond, a, b);\n         }\n \n         template <class A, class T, bool... b>\n-        inline batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, b...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<sve>) noexcept\n         {\n             return select(batch_bool<T, A> { b... }, true_br, false_br, sve {});\n         }\n \n         // zip_lo\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svzip1(lhs, rhs);\n         }\n \n         // zip_hi\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& lhs, batch<T, A> const& rhs, requires_arch<sve>) noexcept\n         {\n             return svzip2(lhs, rhs);\n         }\n@@ -836,21 +836,21 @@ namespace xsimd\n \n         // rsqrt\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svrsqrte(arg);\n         }\n \n         // sqrt\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svsqrt_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // reciprocal\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> reciprocal(const batch<T, A>& arg, requires_arch<sve>) noexcept\n         {\n             return svrecpe(arg);\n         }\n@@ -863,37 +863,37 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A, class T, detail::enable_sized_integral_t<T, 4> = 0>\n-            inline batch<float, A> fast_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_f32_x(detail::sve_ptrue<T>(), arg);\n             }\n \n             template <class A, class T, detail::enable_sized_integral_t<T, 8> = 0>\n-            inline batch<double, A> fast_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_f64_x(detail::sve_ptrue<T>(), arg);\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& arg, batch<int32_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& arg, batch<int32_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_s32_x(detail::sve_ptrue<float>(), arg);\n             }\n \n             template <class A>\n-            inline batch<uint32_t, A> fast_cast(batch<float, A> const& arg, batch<uint32_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<uint32_t, A> fast_cast(batch<float, A> const& arg, batch<uint32_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_u32_x(detail::sve_ptrue<float>(), arg);\n             }\n \n             template <class A>\n-            inline batch<int64_t, A> fast_cast(batch<double, A> const& arg, batch<int64_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<int64_t, A> fast_cast(batch<double, A> const& arg, batch<int64_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_s64_x(detail::sve_ptrue<double>(), arg);\n             }\n \n             template <class A>\n-            inline batch<uint64_t, A> fast_cast(batch<double, A> const& arg, batch<uint64_t, A> const&, requires_arch<sve>) noexcept\n+            XSIMD_INLINE batch<uint64_t, A> fast_cast(batch<double, A> const& arg, batch<uint64_t, A> const&, requires_arch<sve>) noexcept\n             {\n                 return svcvt_u64_x(detail::sve_ptrue<double>(), arg);\n             }\n@@ -905,21 +905,21 @@ namespace xsimd\n \n         // set\n         template <class A, class T, class... Args>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<sve>, Args... args) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<sve>, Args... args) noexcept\n         {\n             return detail::sve_vector_type<T> { args... };\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<sve>,\n-                                             Args... args_complex) noexcept\n+        XSIMD_INLINE batch<std::complex<T>, A> set(batch<std::complex<T>, A> const&, requires_arch<sve>,\n+                                                   Args... args_complex) noexcept\n         {\n             return batch<std::complex<T>>(detail::sve_vector_type<T> { args_complex.real()... },\n                                           detail::sve_vector_type<T> { args_complex.imag()... });\n         }\n \n         template <class A, class T, class... Args>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sve>, Args... args) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<sve>, Args... args) noexcept\n         {\n             using U = as_unsigned_integer_t<T>;\n             const auto values = detail::sve_vector_type<U> { static_cast<U>(args)... };\n@@ -931,17 +931,17 @@ namespace xsimd\n         namespace detail\n         {\n             // generate index sequence (iota)\n-            inline svuint8_t sve_iota_impl(index<1>) noexcept { return svindex_u8(0, 1); }\n-            inline svuint16_t sve_iota_impl(index<2>) noexcept { return svindex_u16(0, 1); }\n-            inline svuint32_t sve_iota_impl(index<4>) noexcept { return svindex_u32(0, 1); }\n-            inline svuint64_t sve_iota_impl(index<8>) noexcept { return svindex_u64(0, 1); }\n+            XSIMD_INLINE svuint8_t sve_iota_impl(index<1>) noexcept { return svindex_u8(0, 1); }\n+            XSIMD_INLINE svuint16_t sve_iota_impl(index<2>) noexcept { return svindex_u16(0, 1); }\n+            XSIMD_INLINE svuint32_t sve_iota_impl(index<4>) noexcept { return svindex_u32(0, 1); }\n+            XSIMD_INLINE svuint64_t sve_iota_impl(index<8>) noexcept { return svindex_u64(0, 1); }\n \n             template <class T, class V = sve_vector_type<as_unsigned_integer_t<T>>>\n-            inline V sve_iota() noexcept { return sve_iota_impl(index<sizeof(T)> {}); }\n+            XSIMD_INLINE V sve_iota() noexcept { return sve_iota_impl(index<sizeof(T)> {}); }\n         } // namespace detail\n \n         template <class A, class T, size_t I, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& arg, T val, index<I>, requires_arch<sve>) noexcept\n         {\n             // create a predicate with only the I-th lane activated\n             const auto iota = detail::sve_iota<T>();\n@@ -951,89 +951,89 @@ namespace xsimd\n \n         // all\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline bool all(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_pcount<T>(arg) == batch_bool<T, A>::size;\n         }\n \n         // any\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline bool any(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svptest_any(arg, arg);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 1> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u8(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 1> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s8(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 2> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u16(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 2> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s16(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 4> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u32(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 4> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s32(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_unsigned_t<R, 8> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_u64(arg);\n         }\n \n         template <class A, class T, class R, detail::sve_enable_all_t<T> = 0, detail::enable_sized_signed_t<R, 8> = 0>\n-        inline batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<R, A> bitwise_cast(batch<T, A> const& arg, batch<R, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_s64(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<float, A> bitwise_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<float, A> bitwise_cast(batch<T, A> const& arg, batch<float, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_f32(arg);\n         }\n \n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<double, A> bitwise_cast(batch<T, A> const& arg, batch<double, A> const&, requires_arch<sve>) noexcept\n         {\n             return svreinterpret_f64(arg);\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in, detail::sve_enable_all_t<T_in> = 0>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& arg, batch_bool<T_out, A> const&, requires_arch<sve>) noexcept\n         {\n             return arg.data;\n         }\n \n         // from_bool\n         template <class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> from_bool(batch_bool<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return select(arg, batch<T, A>(1), batch<T, A>(0));\n         }\n@@ -1045,7 +1045,7 @@ namespace xsimd\n             struct sve_slider_left\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     using u8_vector = batch<uint8_t, A>;\n                     const auto left = svdup_n_u8(0);\n@@ -1059,15 +1059,15 @@ namespace xsimd\n             struct sve_slider_left<0>\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     return arg;\n                 }\n             };\n         } // namespace detail\n \n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_slider_left<N>()(arg);\n         }\n@@ -1079,7 +1079,7 @@ namespace xsimd\n             struct sve_slider_right\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const& arg) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const& arg) noexcept\n                 {\n                     using u8_vector = batch<uint8_t, A>;\n                     const auto left = bitwise_cast(arg, u8_vector {}, sve {}).data;\n@@ -1093,51 +1093,51 @@ namespace xsimd\n             struct sve_slider_right<batch<uint8_t, sve>::size>\n             {\n                 template <class A, class T>\n-                inline batch<T, A> operator()(batch<T, A> const&) noexcept\n+                XSIMD_INLINE batch<T, A> operator()(batch<T, A> const&) noexcept\n                 {\n                     return batch<T, A> {};\n                 }\n             };\n         } // namespace detail\n \n         template <size_t N, class A, class T, detail::sve_enable_all_t<T> = 0>\n-        inline batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return detail::sve_slider_right<N>()(arg);\n         }\n \n         // isnan\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> isnan(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return !(arg == arg);\n         }\n \n         // nearbyint\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& arg, requires_arch<sve>) noexcept\n         {\n             return svrintx_x(detail::sve_ptrue<T>(), arg);\n         }\n \n         // nearbyint_as_int\n         template <class A>\n-        inline batch<int32_t, A> nearbyint_as_int(batch<float, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> nearbyint_as_int(batch<float, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto nearest = svrintx_x(detail::sve_ptrue<float>(), arg);\n             return svcvt_s32_x(detail::sve_ptrue<float>(), nearest);\n         }\n \n         template <class A>\n-        inline batch<int64_t, A> nearbyint_as_int(batch<double, A> const& arg, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> nearbyint_as_int(batch<double, A> const& arg, requires_arch<sve>) noexcept\n         {\n             const auto nearest = svrintx_x(detail::sve_ptrue<double>(), arg);\n             return svcvt_s64_x(detail::sve_ptrue<double>(), nearest);\n         }\n \n         // ldexp\n         template <class A, class T, detail::sve_enable_floating_point_t<T> = 0>\n-        inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& exp, requires_arch<sve>) noexcept\n+        XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& exp, requires_arch<sve>) noexcept\n         {\n             return svscale_x(detail::sve_ptrue<T>(), x, exp);\n         }"
      },
      {
        "filename": "include/xsimd/arch/xsimd_wasm.hpp",
        "status": "modified",
        "additions": 160,
        "deletions": 160,
        "changes": 320,
        "patch": "@@ -23,7 +23,7 @@ namespace xsimd\n     struct batch_bool_constant;\n \n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept;\n \n     template <typename T, class A, T... Values>\n     struct batch_constant;\n@@ -34,15 +34,15 @@ namespace xsimd\n \n         // fwd\n         template <class A, class T, size_t I>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I>, requires_arch<generic>) noexcept;\n         template <class A, typename T, typename ITy, ITy... Indices>\n-        inline batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<ITy, A, Indices...>, requires_arch<generic>) noexcept;\n         template <class A, class T>\n-        inline batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const&, batch<T, A> const&, requires_arch<generic>) noexcept;\n \n         // abs\n         template <class A, class T, typename std::enable_if<std::is_integral<T>::value && std::is_signed<T>::value, void>::type>\n-        inline batch<T, A> abs(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> abs(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -68,20 +68,20 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> abs(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> abs(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_abs(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> abs(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> abs(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_abs(self);\n         }\n \n         // add\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> add(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -107,20 +107,20 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> add(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_add(self, other);\n         }\n \n         template <class A>\n-        inline batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> add(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_add(self, other);\n         }\n \n         // avgr\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -138,7 +138,7 @@ namespace xsimd\n \n         // avg\n         template <class A, class T, class = typename std::enable_if<std::is_unsigned<T>::value, void>::type>\n-        inline batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> avg(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -158,94 +158,94 @@ namespace xsimd\n \n         // all\n         template <class A>\n-        inline bool all(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self) == 0x0F;\n         }\n         template <class A>\n-        inline bool all(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self) == 0x03;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool all(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool all(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_bitmask(self) == 0xFFFF;\n         }\n \n         // any\n         template <class A>\n-        inline bool any(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self) != 0;\n         }\n         template <class A>\n-        inline bool any(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self) != 0;\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline bool any(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE bool any(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_bitmask(self) != 0;\n         }\n \n         // batch_bool_cast\n         template <class A, class T_out, class T_in>\n-        inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& self, batch_bool<T_out, A> const&, requires_arch<wasm>) noexcept\n         {\n             return { bitwise_cast<T_out>(batch<T_in, A>(self.data)).data };\n         }\n \n         // bitwise_and\n         template <class A, class T>\n-        inline batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_and(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_and(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_and(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_and(self, other);\n         }\n \n         // bitwise_andnot\n         template <class A, class T>\n-        inline batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_andnot(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_andnot(self, other);\n         }\n \n         // bitwise_cast\n         template <class A, class T, class Tp>\n-        inline batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<Tp, A> bitwise_cast(batch<T, A> const& self, batch<Tp, A> const&, requires_arch<wasm>) noexcept\n         {\n             return batch<Tp, A>(self.data);\n         }\n \n         // bitwise_or\n         template <class A, class T>\n-        inline batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_or(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_or(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(self, other);\n         }\n \n         // bitwise_lshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -272,7 +272,7 @@ namespace xsimd\n \n         // bitwise_rshift\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& self, int32_t other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -326,38 +326,38 @@ namespace xsimd\n \n         // bitwise_not\n         template <class A, class T>\n-        inline batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_not(self);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_not(self);\n         }\n \n         // bitwise_xor\n         template <class A, class T>\n-        inline batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> bitwise_xor(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_xor(self, other);\n         }\n \n         template <class A, class T>\n-        inline batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> bitwise_xor(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_xor(self, other);\n         }\n \n         // broadcast\n         template <class A>\n-        batch<float, A> inline broadcast(float val, requires_arch<wasm>) noexcept\n+        batch<float, A> XSIMD_INLINE broadcast(float val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_splat(val);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> broadcast(T val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> broadcast(T val, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -382,48 +382,48 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> broadcast(double val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> broadcast(double val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_splat(val);\n         }\n \n         // ceil\n         template <class A>\n-        inline batch<float, A> ceil(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> ceil(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ceil(self);\n         }\n         template <class A>\n-        inline batch<double, A> ceil(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> ceil(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ceil(self);\n         }\n \n         // div\n         template <class A>\n-        inline batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> div(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_div(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> div(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_div(self, other);\n         }\n \n         // eq\n         template <class A>\n-        inline batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_eq(self, other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> eq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_eq(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -448,7 +448,7 @@ namespace xsimd\n             }\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -473,12 +473,12 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_eq(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> eq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_eq(self, other);\n         }\n@@ -487,13 +487,13 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> fast_cast(batch<int32_t, A> const& self, batch<float, A> const&, requires_arch<wasm>) noexcept\n             {\n                 return wasm_f32x4_convert_i32x4(self);\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<uint64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to wasm\n@@ -506,7 +506,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> fast_cast(batch<int64_t, A> const& x, batch<double, A> const&, requires_arch<wasm>) noexcept\n             {\n                 // from https://stackoverflow.com/questions/41144668/how-to-efficiently-perform-double-int64-conversions-with-sse-avx\n                 // adapted to wasm\n@@ -520,7 +520,7 @@ namespace xsimd\n             }\n \n             template <class A>\n-            inline batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<int32_t, A> fast_cast(batch<float, A> const& self, batch<int32_t, A> const&, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_make(\n                     static_cast<int32_t>(wasm_f32x4_extract_lane(self, 0)),\n@@ -532,20 +532,20 @@ namespace xsimd\n \n         // floor\n         template <class A>\n-        inline batch<float, A> floor(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> floor(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_floor(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> floor(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> floor(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_floor(self);\n         }\n \n         // from_mask\n         template <class A>\n-        inline batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> from_mask(batch_bool<float, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint32_t lut[][4] = {\n                 { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },\n@@ -569,7 +569,7 @@ namespace xsimd\n             return wasm_v128_load((const v128_t*)lut[mask]);\n         }\n         template <class A>\n-        inline batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> from_mask(batch_bool<double, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut[][4] = {\n                 { 0x0000000000000000ul, 0x0000000000000000ul },\n@@ -581,7 +581,7 @@ namespace xsimd\n             return wasm_v128_load((const v128_t*)lut[mask]);\n         }\n         template <class T, class A, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> from_mask(batch_bool<T, A> const&, uint64_t mask, requires_arch<wasm>) noexcept\n         {\n             alignas(A::alignment()) static const uint64_t lut64[] = {\n                 0x0000000000000000,\n@@ -667,24 +667,24 @@ namespace xsimd\n \n         // ge\n         template <class A>\n-        inline batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> ge(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ge(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> ge(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ge(self, other);\n         }\n \n         // gt\n         template <class A>\n-        inline batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> gt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_gt(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -732,14 +732,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> gt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_gt(self, other);\n         }\n \n         // haddp\n         template <class A>\n-        inline batch<float, A> haddp(batch<float, A> const* row, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> haddp(batch<float, A> const* row, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_i32x4_shuffle(row[0], row[1], 0, 4, 1, 5);\n             v128_t tmp1 = wasm_i32x4_shuffle(row[0], row[1], 2, 6, 3, 7);\n@@ -752,20 +752,20 @@ namespace xsimd\n             return wasm_f32x4_add(tmp0, tmp2);\n         }\n         template <class A>\n-        inline batch<double, A> haddp(batch<double, A> const* row, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> haddp(batch<double, A> const* row, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_add(wasm_i64x2_shuffle(row[0], row[1], 0, 2),\n                                   wasm_i64x2_shuffle(row[0], row[1], 1, 3));\n         }\n \n         // insert\n         template <class A, size_t I>\n-        inline batch<float, A> insert(batch<float, A> const& self, float val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> insert(batch<float, A> const& self, float val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_replace_lane(self, pos, val);\n         }\n         template <class A, class T, size_t I, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> insert(batch<T, A> const& self, T val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -818,48 +818,48 @@ namespace xsimd\n         }\n \n         template <class A, size_t I>\n-        inline batch<double, A> insert(batch<double, A> const& self, double val, index<I> pos, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> insert(batch<double, A> const& self, double val, index<I> pos, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_replace_lane(self, pos, val);\n         }\n \n         // isnan\n         template <class A>\n-        inline batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> isnan(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_f32x4_ne(self, self), wasm_f32x4_ne(self, self));\n         }\n         template <class A>\n-        inline batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> isnan(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_f64x2_ne(self, self), wasm_f64x2_ne(self, self));\n         }\n \n         // le\n         template <class A>\n-        inline batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> le(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_le(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> le(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_le(self, other);\n         }\n \n         // load_aligned\n         template <class A>\n-        inline batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> load_aligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> load_aligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load((v128_t const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> load_aligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n@@ -868,42 +868,42 @@ namespace xsimd\n         namespace detail\n         {\n             template <class A>\n-            inline batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<std::complex<float>, A> load_complex(batch<float, A> const& hi, batch<float, A> const& lo, requires_arch<wasm>) noexcept\n             {\n                 return { wasm_i32x4_shuffle(hi, lo, 0, 2, 4, 6), wasm_i32x4_shuffle(hi, lo, 1, 3, 5, 7) };\n             }\n             template <class A>\n-            inline batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<std::complex<double>, A> load_complex(batch<double, A> const& hi, batch<double, A> const& lo, requires_arch<wasm>) noexcept\n             {\n                 return { wasm_i64x2_shuffle(hi, lo, 0, 2), wasm_i64x2_shuffle(hi, lo, 1, 3) };\n             }\n         }\n \n         // load_unaligned\n         template <class A>\n-        inline batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> load_unaligned(float const* mem, convert<float>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> load_unaligned(T const* mem, convert<T>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load((v128_t const*)mem);\n         }\n         template <class A>\n-        inline batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> load_unaligned(double const* mem, convert<double>, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_load(mem);\n         }\n \n         // lt\n         template <class A>\n-        inline batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> lt(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_lt(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -964,14 +964,14 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> lt(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_lt(self, other);\n         }\n \n         // mask\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline uint64_t mask(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -996,66 +996,66 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline uint64_t mask(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_bitmask(self);\n         }\n \n         template <class A>\n-        inline uint64_t mask(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE uint64_t mask(batch_bool<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_bitmask(self);\n         }\n \n         // max\n         template <class A>\n-        inline batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> max(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_pmax(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> max(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return select(self > other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> max(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_pmax(self, other);\n         }\n \n         // min\n         template <class A>\n-        inline batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> min(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_pmin(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> min(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return select(self <= other, self, other);\n         }\n         template <class A>\n-        inline batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> min(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_pmin(self, other);\n         }\n \n         // mul\n         template <class A>\n-        inline batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> mul(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_mul(self, other);\n         }\n         template <class A>\n-        inline batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> mul(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_mul(self, other);\n         }\n \n         // neg\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> neg(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> neg(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1081,67 +1081,67 @@ namespace xsimd\n         }\n \n         template <class A>\n-        inline batch<float, A> neg(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> neg(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_neg(self);\n         }\n \n         template <class A>\n-        inline batch<double, A> neg(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> neg(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_neg(self);\n         }\n \n         // neq\n         template <class A>\n-        inline batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ne(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return ~(self == other);\n         }\n         template <class A>\n-        inline batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<float, A> neq(batch_bool<float, A> const& self, batch_bool<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_ne(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch_bool<T, A> const& self, batch_bool<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return ~(self == other);\n         }\n \n         template <class A>\n-        inline batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ne(self, other);\n         }\n         template <class A>\n-        inline batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch_bool<double, A> neq(batch_bool<double, A> const& self, batch_bool<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_ne(self, other);\n         }\n \n         // reciprocal\n         template <class A>\n-        inline batch<float, A> reciprocal(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> reciprocal(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f32x4_splat(1.0f);\n             return wasm_f32x4_div(one, self);\n         }\n         template <class A>\n-        inline batch<double, A> reciprocal(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> reciprocal(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f64x2_splat(1.0);\n             return wasm_f64x2_div(one, self);\n         }\n \n         // reduce_add\n         template <class A>\n-        inline float reduce_add(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE float reduce_add(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_f32x4_add(self, wasm_i32x4_shuffle(self, self, 6, 7, 2, 3));\n             v128_t tmp1 = wasm_i32x4_shuffle(tmp0, tmp0, 1, 0, 4, 4);\n@@ -1150,7 +1150,7 @@ namespace xsimd\n             return wasm_f32x4_extract_lane(tmp3, 0);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline T reduce_add(batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE T reduce_add(batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 4)\n             {\n@@ -1172,7 +1172,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline double reduce_add(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE double reduce_add(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t tmp0 = wasm_i64x2_shuffle(self, self, 1, 3);\n             v128_t tmp1 = wasm_f64x2_add(self, tmp0);\n@@ -1182,21 +1182,21 @@ namespace xsimd\n \n         // rsqrt\n         template <class A>\n-        inline batch<float, A> rsqrt(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> rsqrt(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f32x4_splat(1.0f);\n             return wasm_f32x4_div(one, wasm_f32x4_sqrt(self));\n         }\n         template <class A>\n-        inline batch<double, A> rsqrt(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> rsqrt(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             v128_t one = wasm_f64x2_splat(1.0);\n             return wasm_f64x2_div(one, wasm_f64x2_sqrt(self));\n         }\n \n         // slide_left\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_left(batch<T, A> const& x, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(\n                 wasm_i64x2_const(0, 0), x, ((N) & 0xF0) ? 0 : 16 - ((N) & 0xF),\n@@ -1212,7 +1212,7 @@ namespace xsimd\n \n         // slide_right\n         template <size_t N, class A, class T>\n-        inline batch<T, A> slide_right(batch<T, A> const& x, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(\n                 x, wasm_i64x2_const(0, 0), ((N) & 0xF0) ? 16 : ((N) & 0xF) + 0,\n@@ -1228,7 +1228,7 @@ namespace xsimd\n \n         // sadd\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1264,102 +1264,102 @@ namespace xsimd\n \n         // select\n         template <class A>\n-        inline batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> select(batch_bool<float, A> const& cond, batch<float, A> const& true_br, batch<float, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n         template <class A, class T, bool... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const&, batch<T, A> const& true_br, batch<T, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return select(batch_bool<T, A> { Values... }, true_br, false_br, wasm {});\n         }\n         template <class A>\n-        inline batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> select(batch_bool<double, A> const& cond, batch<double, A> const& true_br, batch<double, A> const& false_br, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_or(wasm_v128_and(cond, true_br), wasm_v128_andnot(false_br, cond));\n         }\n \n         // shuffle\n         template <class A, class ITy, ITy I0, ITy I1, ITy I2, ITy I3>\n-        inline batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> shuffle(batch<float, A> const& x, batch<float, A> const& y, batch_constant<ITy, A, I0, I1, I2, I3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(x, y, I0, I1, I2, I3);\n         }\n \n         template <class A, class ITy, ITy I0, ITy I1>\n-        inline batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> shuffle(batch<double, A> const& x, batch<double, A> const& y, batch_constant<ITy, A, I0, I1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(x, y, I0, I1);\n         }\n \n         // set\n         template <class A, class... Values>\n-        inline batch<float, A> set(batch<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch<float, A> set(batch<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<float, A>::size, \"consistent init\");\n             return wasm_f32x4_make(values...);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1) noexcept\n         {\n             return wasm_i64x2_make(v0, v1);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3) noexcept\n         {\n             return wasm_i32x4_make(v0, v1, v2, v3);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7) noexcept\n         {\n             return wasm_i16x8_make(v0, v1, v2, v3, v4, v5, v6, v7);\n         }\n \n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n+        XSIMD_INLINE batch<T, A> set(batch<T, A> const&, requires_arch<wasm>, T v0, T v1, T v2, T v3, T v4, T v5, T v6, T v7, T v8, T v9, T v10, T v11, T v12, T v13, T v14, T v15) noexcept\n         {\n             return wasm_i8x16_make(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15);\n         }\n \n         template <class A, class... Values>\n-        inline batch<double, A> set(batch<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch<double, A> set(batch<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch<double, A>::size, \"consistent init\");\n             return wasm_f64x2_make(values...);\n         }\n \n         template <class A, class T, class... Values, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<T, A> set(batch_bool<T, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             return set(batch<T, A>(), A {}, static_cast<T>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<float, A> set(batch_bool<float, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<float, A>::size, \"consistent init\");\n             return set(batch<int32_t, A>(), A {}, static_cast<int32_t>(values ? -1LL : 0LL)...).data;\n         }\n \n         template <class A, class... Values>\n-        inline batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n+        XSIMD_INLINE batch_bool<double, A> set(batch_bool<double, A> const&, requires_arch<wasm>, Values... values) noexcept\n         {\n             static_assert(sizeof...(Values) == batch_bool<double, A>::size, \"consistent init\");\n             return set(batch<int64_t, A>(), A {}, static_cast<int64_t>(values ? -1LL : 0LL)...).data;\n         }\n \n         // ssub\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             if (std::is_signed<T>::value)\n             {\n@@ -1395,22 +1395,22 @@ namespace xsimd\n \n         // store_aligned\n         template <class A>\n-        inline void store_aligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A>\n-        inline void store_aligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_aligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n@@ -1420,58 +1420,58 @@ namespace xsimd\n         {\n             // complex_low\n             template <class A>\n-            inline batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_low(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_shuffle(self.real(), self.imag(), 0, 4, 1, 5);\n             }\n             // complex_high\n             template <class A>\n-            inline batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<float, A> complex_high(batch<std::complex<float>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i32x4_shuffle(self.real(), self.imag(), 2, 6, 3, 7);\n             }\n             template <class A>\n-            inline batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_low(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i64x2_shuffle(self.real(), self.imag(), 0, 2);\n             }\n             template <class A>\n-            inline batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n+            XSIMD_INLINE batch<double, A> complex_high(batch<std::complex<double>, A> const& self, requires_arch<wasm>) noexcept\n             {\n                 return wasm_i64x2_shuffle(self.real(), self.imag(), 1, 3);\n             }\n         }\n \n         // store_unaligned\n         template <class A>\n-        inline void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(float* mem, batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(T* mem, batch_bool<T, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store((v128_t*)mem, self);\n         }\n         template <class A>\n-        inline void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE void store_unaligned(double* mem, batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_v128_store(mem, self);\n         }\n \n         // sub\n         template <class A>\n-        inline batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> sub(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_sub(self, other);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> sub(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1496,106 +1496,106 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> sub(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_sub(self, other);\n         }\n \n         // sqrt\n         template <class A>\n-        inline batch<float, A> sqrt(batch<float, A> const& val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> sqrt(batch<float, A> const& val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_sqrt(val);\n         }\n         template <class A>\n-        inline batch<double, A> sqrt(batch<double, A> const& val, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> sqrt(batch<double, A> const& val, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_sqrt(val);\n         }\n \n         // swizzle\n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> swizzle(batch<float, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, self, V0, V1, V2, V3);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> swizzle(batch<double, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, self, V0, V1);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint64_t, A> swizzle(batch<uint64_t, A> const& self, batch_constant<uint64_t, A, V0, V1>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, self, V0, V1);\n         }\n \n         template <class A, uint64_t V0, uint64_t V1>\n-        inline batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int64_t, A> swizzle(batch<int64_t, A> const& self, batch_constant<uint64_t, A, V0, V1> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int64_t>(swizzle(bitwise_cast<uint64_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint32_t, A> swizzle(batch<uint32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, self, V0, V1, V2, V3);\n         }\n \n         template <class A, uint32_t V0, uint32_t V1, uint32_t V2, uint32_t V3>\n-        inline batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int32_t, A> swizzle(batch<int32_t, A> const& self, batch_constant<uint32_t, A, V0, V1, V2, V3> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int32_t>(swizzle(bitwise_cast<uint32_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint16_t, A> swizzle(batch<uint16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i16x8_shuffle(self, self, V0, V1, V2, V3, V4, V5, V6, V7);\n         }\n \n         template <class A, uint16_t V0, uint16_t V1, uint16_t V2, uint16_t V3, uint16_t V4, uint16_t V5, uint16_t V6, uint16_t V7>\n-        inline batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int16_t, A> swizzle(batch<int16_t, A> const& self, batch_constant<uint16_t, A, V0, V1, V2, V3, V4, V5, V6, V7> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int16_t>(swizzle(bitwise_cast<uint16_t>(self), mask, wasm {}));\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15>, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<uint8_t, A> swizzle(batch<uint8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15>, requires_arch<wasm>) noexcept\n         {\n             return wasm_i8x16_shuffle(self, self, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15);\n         }\n \n         template <class A, uint8_t V0, uint8_t V1, uint8_t V2, uint8_t V3, uint8_t V4, uint8_t V5, uint8_t V6, uint8_t V7,\n                   uint8_t V8, uint8_t V9, uint8_t V10, uint8_t V11, uint8_t V12, uint8_t V13, uint8_t V14, uint8_t V15>\n-        inline batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<int8_t, A> swizzle(batch<int8_t, A> const& self, batch_constant<uint8_t, A, V0, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15> mask, requires_arch<wasm>) noexcept\n         {\n             return bitwise_cast<int8_t>(swizzle(bitwise_cast<uint8_t>(self), mask, wasm {}));\n         }\n \n         // trunc\n         template <class A>\n-        inline batch<float, A> trunc(batch<float, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> trunc(batch<float, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f32x4_trunc(self);\n         }\n         template <class A>\n-        inline batch<double, A> trunc(batch<double, A> const& self, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> trunc(batch<double, A> const& self, requires_arch<wasm>) noexcept\n         {\n             return wasm_f64x2_trunc(self);\n         }\n \n         // zip_hi\n         template <class A>\n-        inline batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_hi(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, other, 2, 6, 3, 7);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1620,19 +1620,19 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_hi(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, other, 1, 3);\n         }\n \n         // zip_lo\n         template <class A>\n-        inline batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<float, A> zip_lo(batch<float, A> const& self, batch<float, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i32x4_shuffle(self, other, 0, 4, 1, 5);\n         }\n         template <class A, class T, class = typename std::enable_if<std::is_integral<T>::value, void>::type>\n-        inline batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& self, batch<T, A> const& other, requires_arch<wasm>) noexcept\n         {\n             XSIMD_IF_CONSTEXPR(sizeof(T) == 1)\n             {\n@@ -1657,7 +1657,7 @@ namespace xsimd\n             }\n         }\n         template <class A>\n-        inline batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n+        XSIMD_INLINE batch<double, A> zip_lo(batch<double, A> const& self, batch<double, A> const& other, requires_arch<wasm>) noexcept\n         {\n             return wasm_i64x2_shuffle(self, other, 0, 2);\n         }"
      },
      {
        "filename": "include/xsimd/config/xsimd_arch.hpp",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -57,13 +57,13 @@ namespace xsimd\n         };\n \n         template <typename T>\n-        inline constexpr T max_of(T value) noexcept\n+        XSIMD_INLINE constexpr T max_of(T value) noexcept\n         {\n             return value;\n         }\n \n         template <typename T, typename... Ts>\n-        inline constexpr T max_of(T head0, T head1, Ts... tail) noexcept\n+        XSIMD_INLINE constexpr T max_of(T head0, T head1, Ts... tail) noexcept\n         {\n             return max_of((head0 > head1 ? head0 : head1), tail...);\n         }\n@@ -104,7 +104,7 @@ namespace xsimd\n         }\n \n         template <class F>\n-        static inline void for_each(F&& f) noexcept\n+        static XSIMD_INLINE void for_each(F&& f) noexcept\n         {\n             (void)std::initializer_list<bool> { (f(Archs {}), true)... };\n         }\n@@ -196,14 +196,14 @@ namespace xsimd\n             F functor;\n \n             template <class Arch, class... Tys>\n-            inline auto walk_archs(arch_list<Arch>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto walk_archs(arch_list<Arch>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n             {\n                 assert(Arch::available() && \"At least one arch must be supported during dispatch\");\n                 return functor(Arch {}, std::forward<Tys>(args)...);\n             }\n \n             template <class Arch, class ArchNext, class... Archs, class... Tys>\n-            inline auto walk_archs(arch_list<Arch, ArchNext, Archs...>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto walk_archs(arch_list<Arch, ArchNext, Archs...>, Tys&&... args) noexcept -> decltype(functor(Arch {}, std::forward<Tys>(args)...))\n             {\n                 if (availables_archs.has(Arch {}))\n                     return functor(Arch {}, std::forward<Tys>(args)...);\n@@ -212,14 +212,14 @@ namespace xsimd\n             }\n \n         public:\n-            inline dispatcher(F f) noexcept\n+            XSIMD_INLINE dispatcher(F f) noexcept\n                 : availables_archs(available_architectures())\n                 , functor(f)\n             {\n             }\n \n             template <class... Tys>\n-            inline auto operator()(Tys&&... args) noexcept -> decltype(functor(default_arch {}, std::forward<Tys>(args)...))\n+            XSIMD_INLINE auto operator()(Tys&&... args) noexcept -> decltype(functor(default_arch {}, std::forward<Tys>(args)...))\n             {\n                 return walk_archs(ArchList {}, std::forward<Tys>(args)...);\n             }\n@@ -228,7 +228,7 @@ namespace xsimd\n \n     // Generic function dispatch, \u00e0 la ifunc\n     template <class ArchList = supported_architectures, class F>\n-    inline detail::dispatcher<F, ArchList> dispatch(F&& f) noexcept\n+    XSIMD_INLINE detail::dispatcher<F, ArchList> dispatch(F&& f) noexcept\n     {\n         return { std::forward<F>(f) };\n     }"
      },
      {
        "filename": "include/xsimd/config/xsimd_cpuid.hpp",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -41,7 +41,7 @@ namespace xsimd\n \n #define ARCH_FIELD_EX(arch, field_name) \\\n     unsigned field_name;                \\\n-    inline bool has(::xsimd::arch) const { return this->field_name; }\n+    XSIMD_INLINE bool has(::xsimd::arch) const { return this->field_name; }\n #define ARCH_FIELD(name) ARCH_FIELD_EX(name, name)\n \n             ARCH_FIELD(sse2)\n@@ -78,7 +78,7 @@ namespace xsimd\n \n #undef ARCH_FIELD\n \n-            inline supported_arch() noexcept\n+            XSIMD_INLINE supported_arch() noexcept\n             {\n                 memset(this, 0, sizeof(supported_arch));\n \n@@ -191,7 +191,7 @@ namespace xsimd\n         };\n     } // namespace detail\n \n-    inline detail::supported_arch available_architectures() noexcept\n+    XSIMD_INLINE detail::supported_arch available_architectures() noexcept\n     {\n         static detail::supported_arch supported;\n         return supported;"
      },
      {
        "filename": "include/xsimd/config/xsimd_inline.hpp",
        "status": "added",
        "additions": 23,
        "deletions": 0,
        "changes": 23,
        "patch": "@@ -0,0 +1,23 @@\n+/***************************************************************************\n+ * Copyright (c) Johan Mabille, Sylvain Corlay, Wolf Vollprecht and         *\n+ * Martin Renou                                                             *\n+ * Copyright (c) QuantStack                                                 *\n+ * Copyright (c) Serge Guelton                                              *\n+ *                                                                          *\n+ * Distributed under the terms of the BSD 3-Clause License.                 *\n+ *                                                                          *\n+ * The full license is in the file LICENSE, distributed with this software. *\n+ ****************************************************************************/\n+\n+#ifndef XSIMD_INLINE_HPP\n+#define XSIMD_INLINE_HPP\n+\n+#if defined(__GNUC__)\n+#define XSIMD_INLINE inline __attribute__((always_inline))\n+#elif defined(_MSC_VER)\n+#define XSIMD_INLINE inline __forceinline\n+#else\n+#define XSIMD_INLINE inline\n+#endif\n+\n+#endif"
      },
      {
        "filename": "include/xsimd/math/xsimd_rem_pio2.hpp",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -217,7 +217,7 @@ namespace xsimd\n          *\n          */\n \n-        inline int32_t __kernel_rem_pio2(double* x, double* y, int32_t e0, int32_t nx, int32_t prec, const int32_t* ipio2) noexcept\n+        XSIMD_INLINE int32_t __kernel_rem_pio2(double* x, double* y, int32_t e0, int32_t nx, int32_t prec, const int32_t* ipio2) noexcept\n         {\n             static const int32_t init_jk[] = { 2, 3, 4, 6 }; /* initial value for jk */\n \n@@ -450,7 +450,7 @@ namespace xsimd\n             return n & 7;\n         }\n \n-        inline std::int32_t __ieee754_rem_pio2(double x, double* y) noexcept\n+        XSIMD_INLINE std::int32_t __ieee754_rem_pio2(double x, double* y) noexcept\n         {\n             static const std::int32_t two_over_pi[] = {\n                 0xA2F983,"
      },
      {
        "filename": "include/xsimd/memory/xsimd_aligned_allocator.hpp",
        "status": "modified",
        "additions": 40,
        "deletions": 40,
        "changes": 80,
        "patch": "@@ -59,43 +59,43 @@ namespace xsimd\n             using other = aligned_allocator<U, Align>;\n         };\n \n-        inline aligned_allocator() noexcept;\n-        inline aligned_allocator(const aligned_allocator& rhs) noexcept;\n+        XSIMD_INLINE aligned_allocator() noexcept;\n+        XSIMD_INLINE aligned_allocator(const aligned_allocator& rhs) noexcept;\n \n         template <class U>\n-        inline aligned_allocator(const aligned_allocator<U, Align>& rhs) noexcept;\n+        XSIMD_INLINE aligned_allocator(const aligned_allocator<U, Align>& rhs) noexcept;\n \n-        inline ~aligned_allocator();\n+        XSIMD_INLINE ~aligned_allocator();\n \n-        inline pointer address(reference) noexcept;\n-        inline const_pointer address(const_reference) const noexcept;\n+        XSIMD_INLINE pointer address(reference) noexcept;\n+        XSIMD_INLINE const_pointer address(const_reference) const noexcept;\n \n-        inline pointer allocate(size_type n, const void* hint = 0);\n-        inline void deallocate(pointer p, size_type n);\n+        XSIMD_INLINE pointer allocate(size_type n, const void* hint = 0);\n+        XSIMD_INLINE void deallocate(pointer p, size_type n);\n \n-        inline size_type max_size() const noexcept;\n-        inline size_type size_max() const noexcept;\n+        XSIMD_INLINE size_type max_size() const noexcept;\n+        XSIMD_INLINE size_type size_max() const noexcept;\n \n         template <class U, class... Args>\n-        inline void construct(U* p, Args&&... args);\n+        XSIMD_INLINE void construct(U* p, Args&&... args);\n \n         template <class U>\n-        inline void destroy(U* p);\n+        XSIMD_INLINE void destroy(U* p);\n     };\n \n     template <class T1, size_t Align1, class T2, size_t Align2>\n-    inline bool operator==(const aligned_allocator<T1, Align1>& lhs,\n-                           const aligned_allocator<T2, Align2>& rhs) noexcept;\n+    XSIMD_INLINE bool operator==(const aligned_allocator<T1, Align1>& lhs,\n+                                 const aligned_allocator<T2, Align2>& rhs) noexcept;\n \n     template <class T1, size_t Align1, class T2, size_t Align2>\n-    inline bool operator!=(const aligned_allocator<T1, Align1>& lhs,\n-                           const aligned_allocator<T2, Align2>& rhs) noexcept;\n+    XSIMD_INLINE bool operator!=(const aligned_allocator<T1, Align1>& lhs,\n+                                 const aligned_allocator<T2, Align2>& rhs) noexcept;\n \n-    inline void* aligned_malloc(size_t size, size_t alignment);\n-    inline void aligned_free(void* ptr);\n+    XSIMD_INLINE void* aligned_malloc(size_t size, size_t alignment);\n+    XSIMD_INLINE void aligned_free(void* ptr);\n \n     template <class T>\n-    inline size_t get_alignment_offset(const T* p, size_t size, size_t block_size);\n+    XSIMD_INLINE size_t get_alignment_offset(const T* p, size_t size, size_t block_size);\n \n     /************************************\n      * aligned_allocator implementation *\n@@ -105,15 +105,15 @@ namespace xsimd\n      * Default constructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::aligned_allocator() noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator() noexcept\n     {\n     }\n \n     /**\n      * Copy constructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::aligned_allocator(const aligned_allocator&) noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator(const aligned_allocator&) noexcept\n     {\n     }\n \n@@ -122,15 +122,15 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U>\n-    inline aligned_allocator<T, A>::aligned_allocator(const aligned_allocator<U, A>&) noexcept\n+    XSIMD_INLINE aligned_allocator<T, A>::aligned_allocator(const aligned_allocator<U, A>&) noexcept\n     {\n     }\n \n     /**\n      * Destructor.\n      */\n     template <class T, size_t A>\n-    inline aligned_allocator<T, A>::~aligned_allocator()\n+    XSIMD_INLINE aligned_allocator<T, A>::~aligned_allocator()\n     {\n     }\n \n@@ -140,7 +140,7 @@ namespace xsimd\n      * @return the actual address of \\c r.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::address(reference r) noexcept -> pointer\n     {\n         return &r;\n@@ -152,7 +152,7 @@ namespace xsimd\n      * @return the actual address of \\c r.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::address(const_reference r) const noexcept -> const_pointer\n     {\n         return &r;\n@@ -167,7 +167,7 @@ namespace xsimd\n      * hold an array of \\c n objects of type \\c T.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::allocate(size_type n, const void*) -> pointer\n     {\n         pointer res = reinterpret_cast<pointer>(aligned_malloc(sizeof(T) * n, A));\n@@ -186,7 +186,7 @@ namespace xsimd\n      * @param n number of objects earlier passed to allocate().\n      */\n     template <class T, size_t A>\n-    inline void aligned_allocator<T, A>::deallocate(pointer p, size_type)\n+    XSIMD_INLINE void aligned_allocator<T, A>::deallocate(pointer p, size_type)\n     {\n         aligned_free(p);\n     }\n@@ -197,7 +197,7 @@ namespace xsimd\n      * @return the maximum supported allocated size.\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::max_size() const noexcept -> size_type\n     {\n         return size_type(-1) / sizeof(T);\n@@ -207,7 +207,7 @@ namespace xsimd\n      * This method is deprecated, use max_size() instead\n      */\n     template <class T, size_t A>\n-    inline auto\n+    XSIMD_INLINE auto\n     aligned_allocator<T, A>::size_max() const noexcept -> size_type\n     {\n         return size_type(-1) / sizeof(T);\n@@ -221,7 +221,7 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U, class... Args>\n-    inline void aligned_allocator<T, A>::construct(U* p, Args&&... args)\n+    XSIMD_INLINE void aligned_allocator<T, A>::construct(U* p, Args&&... args)\n     {\n         new ((void*)p) U(std::forward<Args>(args)...);\n     }\n@@ -232,7 +232,7 @@ namespace xsimd\n      */\n     template <class T, size_t A>\n     template <class U>\n-    inline void aligned_allocator<T, A>::destroy(U* p)\n+    XSIMD_INLINE void aligned_allocator<T, A>::destroy(U* p)\n     {\n         p->~U();\n     }\n@@ -250,8 +250,8 @@ namespace xsimd\n      * @return true if the allocators have the same alignment.\n      */\n     template <class T1, size_t A1, class T2, size_t A2>\n-    inline bool operator==(const aligned_allocator<T1, A1>& lhs,\n-                           const aligned_allocator<T2, A2>& rhs) noexcept\n+    XSIMD_INLINE bool operator==(const aligned_allocator<T1, A1>& lhs,\n+                                 const aligned_allocator<T2, A2>& rhs) noexcept\n     {\n         return lhs.alignment == rhs.alignment;\n     }\n@@ -265,8 +265,8 @@ namespace xsimd\n      * @return true if the allocators have different alignments.\n      */\n     template <class T1, size_t A1, class T2, size_t A2>\n-    inline bool operator!=(const aligned_allocator<T1, A1>& lhs,\n-                           const aligned_allocator<T2, A2>& rhs) noexcept\n+    XSIMD_INLINE bool operator!=(const aligned_allocator<T1, A1>& lhs,\n+                                 const aligned_allocator<T2, A2>& rhs) noexcept\n     {\n         return !(lhs == rhs);\n     }\n@@ -277,7 +277,7 @@ namespace xsimd\n \n     namespace detail\n     {\n-        inline void* xaligned_malloc(size_t size, size_t alignment)\n+        XSIMD_INLINE void* xaligned_malloc(size_t size, size_t alignment)\n         {\n             assert(((alignment & (alignment - 1)) == 0) && \"alignment must be a power of two\");\n             assert((alignment >= sizeof(void*)) && \"alignment must be at least the size of a pointer\");\n@@ -293,7 +293,7 @@ namespace xsimd\n             return res;\n         }\n \n-        inline void xaligned_free(void* ptr)\n+        XSIMD_INLINE void xaligned_free(void* ptr)\n         {\n #ifdef _WIN32\n             _aligned_free(ptr);\n@@ -303,18 +303,18 @@ namespace xsimd\n         }\n     }\n \n-    inline void* aligned_malloc(size_t size, size_t alignment)\n+    XSIMD_INLINE void* aligned_malloc(size_t size, size_t alignment)\n     {\n         return detail::xaligned_malloc(size, alignment);\n     }\n \n-    inline void aligned_free(void* ptr)\n+    XSIMD_INLINE void aligned_free(void* ptr)\n     {\n         detail::xaligned_free(ptr);\n     }\n \n     template <class T>\n-    inline size_t get_alignment_offset(const T* p, size_t size, size_t block_size)\n+    XSIMD_INLINE size_t get_alignment_offset(const T* p, size_t size, size_t block_size)\n     {\n         // size_t block_size = simd_traits<T>::size;\n         if (block_size == 1)"
      },
      {
        "filename": "include/xsimd/memory/xsimd_alignment.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -81,7 +81,7 @@ namespace xsimd\n      * @return true if the alignment requirements are met\n      */\n     template <class Arch = default_arch>\n-    inline bool is_aligned(void const* ptr)\n+    XSIMD_INLINE bool is_aligned(void const* ptr)\n     {\n         return (reinterpret_cast<uintptr_t>(ptr) % static_cast<uintptr_t>(Arch::alignment())) == 0;\n     }"
      },
      {
        "filename": "include/xsimd/types/xsimd_api.hpp",
        "status": "modified",
        "additions": 179,
        "deletions": 179,
        "changes": 358,
        "patch": "@@ -53,7 +53,7 @@ namespace xsimd\n      * @return the absolute values of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> abs(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(x, A {});\n@@ -67,7 +67,7 @@ namespace xsimd\n      * @return the absolute values of \\c z.\n      */\n     template <class T, class A>\n-    inline batch<T, A> abs(batch<std::complex<T>, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> abs(batch<std::complex<T>, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(z, A {});\n@@ -82,7 +82,7 @@ namespace xsimd\n      * @return the sum of \\c x and \\c y\n      */\n     template <class T, class A>\n-    inline auto add(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x + y)\n+    XSIMD_INLINE auto add(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x + y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x + y;\n@@ -96,7 +96,7 @@ namespace xsimd\n      * @return the arc cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> acos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> acos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::acos<A>(x, A {});\n@@ -110,7 +110,7 @@ namespace xsimd\n      * @return the inverse hyperbolic cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> acosh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> acosh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::acosh<A>(x, A {});\n@@ -124,7 +124,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> arg(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::arg<A>(z, A {});\n@@ -138,7 +138,7 @@ namespace xsimd\n      * @return the arc sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> asin(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> asin(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::asin<A>(x, A {});\n@@ -152,7 +152,7 @@ namespace xsimd\n      * @return the inverse hyperbolic sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> asinh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> asinh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::asinh<A>(x, A {});\n@@ -166,7 +166,7 @@ namespace xsimd\n      * @return the arc tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> atan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atan<A>(x, A {});\n@@ -182,7 +182,7 @@ namespace xsimd\n      * @return the arc tangent of \\c x/y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> atan2(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atan2<A>(x, y, A {});\n@@ -196,7 +196,7 @@ namespace xsimd\n      * @return the inverse hyperbolic tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> atanh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> atanh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::atanh<A>(x, A {});\n@@ -211,7 +211,7 @@ namespace xsimd\n      * @return the average of elements between \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> avg(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::avg<A>(x, y, A {});\n@@ -226,7 +226,7 @@ namespace xsimd\n      * @return the rounded average of elements between \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> avgr(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::avgr<A>(x, y, A {});\n@@ -240,7 +240,7 @@ namespace xsimd\n      * @return \\c x cast to \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T_out, A> batch_bool_cast(batch_bool<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_out, A>();\n         detail::static_check_supported_config<T_in, A>();\n@@ -256,7 +256,7 @@ namespace xsimd\n      * @return \\c x cast to \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> batch_cast(batch<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch<T_out, A> batch_cast(batch<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_out, A>();\n         detail::static_check_supported_config<T_in, A>();\n@@ -271,7 +271,7 @@ namespace xsimd\n      * @return bit of sign of \\c x\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitofsign(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> bitofsign(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitofsign<A>(x, A {});\n@@ -286,7 +286,7 @@ namespace xsimd\n      * @return the result of the bitwise and.\n      */\n     template <class T, class A>\n-    inline auto bitwise_and(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x & y)\n+    XSIMD_INLINE auto bitwise_and(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x & y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x & y;\n@@ -301,7 +301,7 @@ namespace xsimd\n      * @return the result of the bitwise and.\n      */\n     template <class T, class A>\n-    inline auto bitwise_and(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x & y)\n+    XSIMD_INLINE auto bitwise_and(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x & y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x & y;\n@@ -316,7 +316,7 @@ namespace xsimd\n      * @return the result of the bitwise and not.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_andnot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_andnot<A>(x, y, A {});\n@@ -331,7 +331,7 @@ namespace xsimd\n      * @return the result of the bitwise and not.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> bitwise_andnot(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_andnot<A>(x, y, A {});\n@@ -345,7 +345,7 @@ namespace xsimd\n      * @return \\c x reinterpreted as \\c T_out\n      */\n     template <class T_out, class T_in, class A>\n-    inline batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept\n+    XSIMD_INLINE batch<T_out, A> bitwise_cast(batch<T_in, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T_in, A>();\n         detail::static_check_supported_config<T_out, A>();\n@@ -361,13 +361,13 @@ namespace xsimd\n      * @return shifted \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_lshift(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_lshift<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> bitwise_lshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_lshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_lshift<A>(x, shift, A {});\n@@ -381,7 +381,7 @@ namespace xsimd\n      * @return the result of the bitwise not.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_not(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_not(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(x, A {});\n@@ -395,7 +395,7 @@ namespace xsimd\n      * @return the result of the bitwise not.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> bitwise_not(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> bitwise_not(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(x, A {});\n@@ -410,7 +410,7 @@ namespace xsimd\n      * @return the result of the bitwise or.\n      */\n     template <class T, class A>\n-    inline auto bitwise_or(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x | y)\n+    XSIMD_INLINE auto bitwise_or(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x | y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x | y;\n@@ -425,7 +425,7 @@ namespace xsimd\n      * @return the result of the bitwise or.\n      */\n     template <class T, class A>\n-    inline auto bitwise_or(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x | y)\n+    XSIMD_INLINE auto bitwise_or(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x | y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x | y;\n@@ -440,13 +440,13 @@ namespace xsimd\n      * @return shifted \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> bitwise_rshift(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_rshift<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> bitwise_rshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_rshift(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_rshift<A>(x, shift, A {});\n@@ -461,7 +461,7 @@ namespace xsimd\n      * @return the result of the bitwise xor.\n      */\n     template <class T, class A>\n-    inline auto bitwise_xor(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x ^ y)\n+    XSIMD_INLINE auto bitwise_xor(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x ^ y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x ^ y;\n@@ -476,7 +476,7 @@ namespace xsimd\n      * @return the result of the bitwise xor.\n      */\n     template <class T, class A>\n-    inline auto bitwise_xor(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x ^ y)\n+    XSIMD_INLINE auto bitwise_xor(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x ^ y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x ^ y;\n@@ -490,7 +490,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class T, class A = default_arch>\n-    inline batch<T, A> broadcast(T v) noexcept\n+    XSIMD_INLINE batch<T, A> broadcast(T v) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch<T, A>::broadcast(v);\n@@ -505,7 +505,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> broadcast_as(From v) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> broadcast_as(From v) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n@@ -523,7 +523,7 @@ namespace xsimd\n      * @return the cubic root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cbrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cbrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cbrt<A>(x, A {});\n@@ -538,7 +538,7 @@ namespace xsimd\n      * @return the batch of smallest integer values not less than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ceil(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> ceil(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ceil<A>(x, A {});\n@@ -554,7 +554,7 @@ namespace xsimd\n      * @return the result of the clipping.\n      */\n     template <class T, class A>\n-    inline batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) noexcept\n+    XSIMD_INLINE batch<T, A> clip(batch<T, A> const& x, batch<T, A> const& lo, batch<T, A> const& hi) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::clip(x, lo, hi, A {});\n@@ -567,7 +567,7 @@ namespace xsimd\n      * resulting vector, zeroing the remaining slots\n      */\n     template <class T, class A>\n-    inline batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> compress(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::compress<A>(x, mask, A {});\n@@ -581,7 +581,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class A, class T>\n-    inline complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> conj(batch<T, A> const& z) noexcept\n     {\n         return kernel::conj(z, A {});\n     }\n@@ -597,7 +597,7 @@ namespace xsimd\n      * matches that of \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> copysign(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::copysign<A>(x, y, A {});\n@@ -611,7 +611,7 @@ namespace xsimd\n      * @return the cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cos<A>(x, A {});\n@@ -625,7 +625,7 @@ namespace xsimd\n      * @return the hyperbolic cosine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> cosh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> cosh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::cosh<A>(x, A {});\n@@ -639,7 +639,7 @@ namespace xsimd\n      * @return the subtraction of \\c x and 1.\n      */\n     template <class T, class A>\n-    inline batch<T, A> decr(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> decr(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::decr<A>(x, A {});\n@@ -655,7 +655,7 @@ namespace xsimd\n      * @return the subtraction of \\c x and 1 when \\c mask is true.\n      */\n     template <class T, class A, class Mask>\n-    inline batch<T, A> decr_if(batch<T, A> const& x, Mask const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> decr_if(batch<T, A> const& x, Mask const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::decr_if<A>(x, mask, A {});\n@@ -670,7 +670,7 @@ namespace xsimd\n      * @return the result of the division.\n      */\n     template <class T, class A>\n-    inline auto div(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x / y)\n+    XSIMD_INLINE auto div(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x / y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x / y;\n@@ -685,7 +685,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto eq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x == y)\n+    XSIMD_INLINE auto eq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x == y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x == y;\n@@ -700,7 +700,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto eq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x == y)\n+    XSIMD_INLINE auto eq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x == y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x == y;\n@@ -714,7 +714,7 @@ namespace xsimd\n      * @return the natural exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp<A>(x, A {});\n@@ -728,7 +728,7 @@ namespace xsimd\n      * @return the base 10 exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp10(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp10(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp10<A>(x, A {});\n@@ -742,7 +742,7 @@ namespace xsimd\n      * @return the base 2 exponential of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> exp2(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> exp2(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::exp2<A>(x, A {});\n@@ -755,7 +755,7 @@ namespace xsimd\n      * mask, zeroing the other slots\n      */\n     template <class T, class A>\n-    inline batch<T, A> expand(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> expand(batch<T, A> const& x, batch_bool<T, A> const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::expand<A>(x, mask, A {});\n@@ -769,7 +769,7 @@ namespace xsimd\n      * @return the natural exponential of \\c x, minus one.\n      */\n     template <class T, class A>\n-    inline batch<T, A> expm1(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> expm1(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::expm1<A>(x, A {});\n@@ -783,7 +783,7 @@ namespace xsimd\n      * @return the error function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> erf(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> erf(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::erf<A>(x, A {});\n@@ -797,7 +797,7 @@ namespace xsimd\n      * @return the error function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> erfc(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> erfc(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::erfc<A>(x, A {});\n@@ -814,7 +814,7 @@ namespace xsimd\n      * @return.\n      */\n     template <class T, class A>\n-    inline batch<T, A> extract_pair(batch<T, A> const& x, batch<T, A> const& y, std::size_t i) noexcept\n+    XSIMD_INLINE batch<T, A> extract_pair(batch<T, A> const& x, batch<T, A> const& y, std::size_t i) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::extract_pair<A>(x, y, i, A {});\n@@ -828,7 +828,7 @@ namespace xsimd\n      * @return the absolute values of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fabs(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> fabs(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::abs<A>(x, A {});\n@@ -844,7 +844,7 @@ namespace xsimd\n      * @return the positive difference.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fdim(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fdim<A>(x, y, A {});\n@@ -859,7 +859,7 @@ namespace xsimd\n      * @return the batch of largest integer values not greater than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> floor(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> floor(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::floor<A>(x, A {});\n@@ -875,7 +875,7 @@ namespace xsimd\n      * @return the result of the fused multiply-add operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fma<A>(x, y, z, A {});\n@@ -890,7 +890,7 @@ namespace xsimd\n      * @return a batch of the larger values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmax(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::max<A>(x, y, A {});\n@@ -905,7 +905,7 @@ namespace xsimd\n      * @return a batch of the smaller values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmin(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::min<A>(x, y, A {});\n@@ -920,7 +920,7 @@ namespace xsimd\n      * @return the result of the modulo.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> fmod(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fmod<A>(x, y, A {});\n@@ -936,7 +936,7 @@ namespace xsimd\n      * @return the result of the fused multiply-sub operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fms<A>(x, y, z, A {});\n@@ -952,7 +952,7 @@ namespace xsimd\n      * @return the result of the fused negated multiply-add operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fnma(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fnma<A>(x, y, z, A {});\n@@ -968,7 +968,7 @@ namespace xsimd\n      * @return the result of the fused negated multiply-sub operation.\n      */\n     template <class T, class A>\n-    inline batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n+    XSIMD_INLINE batch<T, A> fnms(batch<T, A> const& x, batch<T, A> const& y, batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::fnms<A>(x, y, z, A {});\n@@ -983,7 +983,7 @@ namespace xsimd\n      * @return the normalized fraction of x\n      */\n     template <class T, class A>\n-    inline batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) noexcept\n+    XSIMD_INLINE batch<T, A> frexp(const batch<T, A>& x, batch<as_integer_t<T>, A>& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::frexp<A>(x, y, A {});\n@@ -999,7 +999,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x >= y;\n@@ -1015,7 +1015,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x > y;\n@@ -1031,7 +1031,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline batch<T, A> haddp(batch<T, A> const* row) noexcept\n+    XSIMD_INLINE batch<T, A> haddp(batch<T, A> const* row) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::haddp<A>(row, A {});\n@@ -1047,7 +1047,7 @@ namespace xsimd\n      * @return the square root of the sum of the squares of \\c x and \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> hypot(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::hypot<A>(x, y, A {});\n@@ -1061,7 +1061,7 @@ namespace xsimd\n      * @return the argument of \\c x.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> imag(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::imag<A>(x, A {});\n@@ -1075,7 +1075,7 @@ namespace xsimd\n      * @return the sum of \\c x and 1.\n      */\n     template <class T, class A>\n-    inline batch<T, A> incr(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> incr(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::incr<A>(x, A {});\n@@ -1091,7 +1091,7 @@ namespace xsimd\n      * @return the sum of \\c x and 1 when \\c mask is true.\n      */\n     template <class T, class A, class Mask>\n-    inline batch<T, A> incr_if(batch<T, A> const& x, Mask const& mask) noexcept\n+    XSIMD_INLINE batch<T, A> incr_if(batch<T, A> const& x, Mask const& mask) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::incr_if<A>(x, mask, A {});\n@@ -1104,7 +1104,7 @@ namespace xsimd\n      * @return a batch of positive infinity\n      */\n     template <class B>\n-    inline B infinity()\n+    XSIMD_INLINE B infinity()\n     {\n         using T = typename B::value_type;\n         using A = typename B::arch_type;\n@@ -1122,7 +1122,7 @@ namespace xsimd\n      * @return copy of \\c x with position \\c pos set to \\c val\n      */\n     template <class T, class A, size_t I>\n-    inline batch<T, A> insert(batch<T, A> const& x, T val, index<I> pos) noexcept\n+    XSIMD_INLINE batch<T, A> insert(batch<T, A> const& x, T val, index<I> pos) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::insert<A>(x, val, pos, A {});\n@@ -1136,7 +1136,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_even(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_even(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_even<A>(x, A {});\n@@ -1150,7 +1150,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_flint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_flint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_flint<A>(x, A {});\n@@ -1164,7 +1164,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> is_odd(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch_bool<T, A> is_odd(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::is_odd<A>(x, A {});\n@@ -1178,7 +1178,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isinf(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isinf<A>(x, A {});\n@@ -1192,7 +1192,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isfinite(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isfinite<A>(x, A {});\n@@ -1206,7 +1206,7 @@ namespace xsimd\n      * @return a batch of booleans.\n      */\n     template <class T, class A>\n-    inline typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE typename batch<T, A>::batch_bool_type isnan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::isnan<A>(x, A {});\n@@ -1221,7 +1221,7 @@ namespace xsimd\n      * @return a batch of floating point values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) noexcept\n+    XSIMD_INLINE batch<T, A> ldexp(const batch<T, A>& x, const batch<as_integer_t<T>, A>& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ldexp<A>(x, y, A {});\n@@ -1236,7 +1236,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x <= y;\n@@ -1250,7 +1250,7 @@ namespace xsimd\n      * @return the natural logarithm of the gamma function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> lgamma(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> lgamma(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::lgamma<A>(x, A {});\n@@ -1265,7 +1265,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> load_as(From const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> load_as(From const* ptr, aligned_mode) noexcept\n     {\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n         detail::static_check_supported_config<From, A>();\n@@ -1274,14 +1274,14 @@ namespace xsimd\n     }\n \n     template <class To, class A = default_arch>\n-    inline simd_return_type<bool, To, A> load_as(bool const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<bool, To, A> load_as(bool const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         return simd_return_type<bool, To, A>::load_aligned(ptr);\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         using batch_value_type = typename simd_return_type<std::complex<From>, To, A>::value_type;\n@@ -1290,7 +1290,7 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, aligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1307,7 +1307,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<From, To, A> load_as(From const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<From, To, A> load_as(From const* ptr, unaligned_mode) noexcept\n     {\n         using batch_value_type = typename simd_return_type<From, To, A>::value_type;\n         detail::static_check_supported_config<To, A>();\n@@ -1316,13 +1316,13 @@ namespace xsimd\n     }\n \n     template <class To, class A = default_arch>\n-    inline simd_return_type<bool, To, A> load_as(bool const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<bool, To, A> load_as(bool const* ptr, unaligned_mode) noexcept\n     {\n         return simd_return_type<bool, To, A>::load_unaligned(ptr);\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<std::complex<From>, To, A> load_as(std::complex<From> const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1332,7 +1332,7 @@ namespace xsimd\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE simd_return_type<xtl::xcomplex<From, From, i3ec>, To, A> load_as(xtl::xcomplex<From, From, i3ec> const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<To, A>();\n         detail::static_check_supported_config<From, A>();\n@@ -1349,7 +1349,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load(From const* ptr, aligned_mode = {}) noexcept\n+    XSIMD_INLINE batch<From, A> load(From const* ptr, aligned_mode = {}) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, aligned_mode {});\n@@ -1364,7 +1364,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load(From const* ptr, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<From, A> load(From const* ptr, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, unaligned_mode {});\n@@ -1379,7 +1379,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load_aligned(From const* ptr) noexcept\n+    XSIMD_INLINE batch<From, A> load_aligned(From const* ptr) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, aligned_mode {});\n@@ -1394,7 +1394,7 @@ namespace xsimd\n      * @return a new batch instance\n      */\n     template <class A = default_arch, class From>\n-    inline batch<From, A> load_unaligned(From const* ptr) noexcept\n+    XSIMD_INLINE batch<From, A> load_unaligned(From const* ptr) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         return load_as<From, A>(ptr, unaligned_mode {});\n@@ -1408,7 +1408,7 @@ namespace xsimd\n      * @return the natural logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log<A>(x, A {});\n@@ -1421,7 +1421,7 @@ namespace xsimd\n      * @return the base 2 logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log2(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log2(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log2<A>(x, A {});\n@@ -1434,7 +1434,7 @@ namespace xsimd\n      * @return the base 10 logarithm of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log10(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log10(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log10<A>(x, A {});\n@@ -1447,7 +1447,7 @@ namespace xsimd\n      * @return the natural logarithm of one plus \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> log1p(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> log1p(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::log1p<A>(x, A {});\n@@ -1462,7 +1462,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return x < y;\n@@ -1477,7 +1477,7 @@ namespace xsimd\n      * @return a batch of the larger values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> max(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::max<A>(x, y, A {});\n@@ -1492,7 +1492,7 @@ namespace xsimd\n      * @return a batch of the smaller values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> min(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::min<A>(x, y, A {});\n@@ -1505,7 +1505,7 @@ namespace xsimd\n      * @return a batch of positive infinity\n      */\n     template <class B>\n-    inline B minusinfinity() noexcept\n+    XSIMD_INLINE B minusinfinity() noexcept\n     {\n         using T = typename B::value_type;\n         using A = typename B::arch_type;\n@@ -1522,7 +1522,7 @@ namespace xsimd\n      * @return the result of the modulo.\n      */\n     template <class T, class A>\n-    inline auto mod(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x % y)\n+    XSIMD_INLINE auto mod(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x % y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x % y;\n@@ -1538,7 +1538,7 @@ namespace xsimd\n      * @return the result of the product.\n      */\n     template <class T, class A>\n-    inline auto mul(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x * y)\n+    XSIMD_INLINE auto mul(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x * y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x * y;\n@@ -1553,7 +1553,7 @@ namespace xsimd\n      * @return the batch of nearest integer values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> nearbyint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> nearbyint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::nearbyint<A>(x, A {});\n@@ -1570,7 +1570,7 @@ namespace xsimd\n      * @warning For very large values the conversion to int silently overflows.\n      */\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A>\n+    XSIMD_INLINE batch<as_integer_t<T>, A>\n     nearbyint_as_int(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -1586,7 +1586,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto neq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x != y)\n+    XSIMD_INLINE auto neq(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x != y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x != y;\n@@ -1601,7 +1601,7 @@ namespace xsimd\n      * @return a boolean batch.\n      */\n     template <class T, class A>\n-    inline auto neq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x != y)\n+    XSIMD_INLINE auto neq(batch_bool<T, A> const& x, batch_bool<T, A> const& y) noexcept -> decltype(x != y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x != y;\n@@ -1615,7 +1615,7 @@ namespace xsimd\n      * @return the opposite of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> neg(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> neg(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return -x;\n@@ -1631,7 +1631,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> nextafter(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::nextafter<A>(x, y, A {});\n@@ -1645,7 +1645,7 @@ namespace xsimd\n      * @return the norm of \\c x.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> norm(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::norm(x, A {});\n@@ -1660,7 +1660,7 @@ namespace xsimd\n      * @return \\c r exp(i * \\c theta).\n      */\n     template <class T, class A>\n-    inline complex_batch_type_t<batch<T, A>> polar(batch<T, A> const& r, batch<T, A> const& theta = batch<T, A> {}) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> polar(batch<T, A> const& r, batch<T, A> const& theta = batch<T, A> {}) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::polar<A>(r, theta, A {});\n@@ -1674,7 +1674,7 @@ namespace xsimd\n      * @return \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> pos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> pos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return +x;\n@@ -1690,7 +1690,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class A>\n-    inline batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> pow(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::pow<A>(x, y, A {});\n@@ -1706,7 +1706,7 @@ namespace xsimd\n      * @return \\c x raised to the power \\c y.\n      */\n     template <class T, class ITy, class A, class = typename std::enable_if<std::is_integral<ITy>::value, void>::type>\n-    inline batch<T, A> pow(batch<T, A> const& x, ITy y) noexcept\n+    XSIMD_INLINE batch<T, A> pow(batch<T, A> const& x, ITy y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ipow<A>(x, y, A {});\n@@ -1720,7 +1720,7 @@ namespace xsimd\n      * @return the projection of \\c z.\n      */\n     template <class T, class A>\n-    inline complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE complex_batch_type_t<batch<T, A>> proj(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::proj(z, A {});\n@@ -1734,7 +1734,7 @@ namespace xsimd\n      * @return the argument of \\c z.\n      */\n     template <class T, class A>\n-    inline real_batch_type_t<batch<T, A>> real(batch<T, A> const& z) noexcept\n+    XSIMD_INLINE real_batch_type_t<batch<T, A>> real(batch<T, A> const& z) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::real<A>(z, A {});\n@@ -1750,7 +1750,7 @@ namespace xsimd\n      * @return the reciprocal.\n      */\n     template <class T, class A, class = typename std::enable_if<std::is_floating_point<T>::value, void>::type>\n-    inline batch<T, A> reciprocal(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> reciprocal(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reciprocal(x, A {});\n@@ -1765,7 +1765,7 @@ namespace xsimd\n      * @return the result of the reduction, as a scalar.\n      */\n     template <class T, class A, class F>\n-    inline T reduce(F&& f, batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce(F&& f, batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::detail::reduce(std::forward<F>(f), x, std::integral_constant<unsigned, batch<T, A>::size>());\n@@ -1779,7 +1779,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_add(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_add(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_add<A>(x, A {});\n@@ -1793,7 +1793,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_max(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_max(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_max<A>(x, A {});\n@@ -1807,7 +1807,7 @@ namespace xsimd\n      * @return the result of the reduction.\n      */\n     template <class T, class A>\n-    inline T reduce_min(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE T reduce_min(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::reduce_min<A>(x, A {});\n@@ -1822,7 +1822,7 @@ namespace xsimd\n      * @return the result of the addition.\n      */\n     template <class T, class A>\n-    inline batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> remainder(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::remainder<A>(x, y, A {});\n@@ -1837,7 +1837,7 @@ namespace xsimd\n      * @return the batch of rounded values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rint(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rint(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return nearbyint(x);\n@@ -1855,7 +1855,7 @@ namespace xsimd\n      * @return rotated batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> rotate_left(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rotate_left(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotate_left<N, A>(x, A {});\n@@ -1873,7 +1873,7 @@ namespace xsimd\n      * @return rotated batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> rotate_right(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rotate_right(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotate_right<N, A>(x, A {});\n@@ -1889,13 +1889,13 @@ namespace xsimd\n      * @return rotated \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rotl(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotl<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> rotl(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotl(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotl<A>(x, shift, A {});\n@@ -1911,13 +1911,13 @@ namespace xsimd\n      * @return rotated \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rotr(batch<T, A> const& x, int shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& x, int shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotr<A>(x, shift, A {});\n     }\n     template <class T, class A>\n-    inline batch<T, A> rotr(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n+    XSIMD_INLINE batch<T, A> rotr(batch<T, A> const& x, batch<T, A> const& shift) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rotr<A>(x, shift, A {});\n@@ -1933,7 +1933,7 @@ namespace xsimd\n      * @return the batch of nearest integer values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> round(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> round(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::round<A>(x, A {});\n@@ -1951,7 +1951,7 @@ namespace xsimd\n      * @return the inverse square root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> rsqrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> rsqrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::rsqrt<A>(x, A {});\n@@ -1968,7 +1968,7 @@ namespace xsimd\n      * @return the result of the saturated addition.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sadd(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> sadd(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sadd<A>(x, y, A {});\n@@ -1989,7 +1989,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A>\n-    inline batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<T, A> select(batch_bool<T, A> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2010,7 +2010,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> select(batch_bool<T, A> const& cond, batch<std::complex<T>, A> const& true_br, batch<std::complex<T>, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2031,7 +2031,7 @@ namespace xsimd\n      * @return the result of the selection.\n      */\n     template <class T, class A, bool... Values>\n-    inline batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n+    XSIMD_INLINE batch<T, A> select(batch_bool_constant<T, A, Values...> const& cond, batch<T, A> const& true_br, batch<T, A> const& false_br) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::select<A>(cond, true_br, false_br, A {});\n@@ -2054,7 +2054,7 @@ namespace xsimd\n      * @return combined batch\n      */\n     template <class T, class A, class Vt, Vt... Values>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     shuffle(batch<T, A> const& x, batch<T, A> const& y, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n@@ -2070,7 +2070,7 @@ namespace xsimd\n      * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n      */\n     template <class T, class A>\n-    inline batch<T, A> sign(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sign(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sign<A>(x, A {});\n@@ -2084,7 +2084,7 @@ namespace xsimd\n      * @return -1 for each negative element, -1 or +1 for each null element and +1 for each element\n      */\n     template <class T, class A>\n-    inline batch<T, A> signnz(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> signnz(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::signnz<A>(x, A {});\n@@ -2098,7 +2098,7 @@ namespace xsimd\n      * @return the sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sin(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sin(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sin<A>(x, A {});\n@@ -2113,7 +2113,7 @@ namespace xsimd\n      * @return a pair containing the sine then the cosine of  batch \\c x\n      */\n     template <class T, class A>\n-    inline std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE std::pair<batch<T, A>, batch<T, A>> sincos(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sincos<A>(x, A {});\n@@ -2127,7 +2127,7 @@ namespace xsimd\n      * @return the hyperbolic sine of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sinh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sinh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sinh<A>(x, A {});\n@@ -2144,7 +2144,7 @@ namespace xsimd\n      * @return slided batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> slide_left(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> slide_left(batch<T, A> const& x) noexcept\n     {\n         static_assert(std::is_integral<T>::value, \"can only slide batch of integers\");\n         detail::static_check_supported_config<T, A>();\n@@ -2162,7 +2162,7 @@ namespace xsimd\n      * @return slided batch.\n      */\n     template <size_t N, class T, class A>\n-    inline batch<T, A> slide_right(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> slide_right(batch<T, A> const& x) noexcept\n     {\n         static_assert(std::is_integral<T>::value, \"can only slide batch of integers\");\n         detail::static_check_supported_config<T, A>();\n@@ -2177,7 +2177,7 @@ namespace xsimd\n      * @return the square root of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> sqrt(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> sqrt(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::sqrt<A>(x, A {});\n@@ -2193,7 +2193,7 @@ namespace xsimd\n      * @return the result of the saturated difference.\n      */\n     template <class T, class A>\n-    inline batch<T, A> ssub(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> ssub(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::ssub<A>(x, y, A {});\n@@ -2208,29 +2208,29 @@ namespace xsimd\n      * @param src the batch to copy\n      */\n     template <class To, class A = default_arch, class From>\n-    inline void store_as(To* dst, batch<From, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(To* dst, batch<From, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store_aligned<A>(dst, src, A {});\n     }\n \n     template <class A = default_arch, class From>\n-    inline void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(bool* dst, batch_bool<From, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store<A>(src, dst, A {});\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         kernel::store_complex_aligned<A>(dst, src, A {});\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n+    XSIMD_INLINE void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, aligned_mode) noexcept\n     {\n         store_as(reinterpret_cast<std::complex<To>*>(dst), src, aligned_mode());\n     }\n@@ -2245,29 +2245,29 @@ namespace xsimd\n      * @param src the batch to copy\n      */\n     template <class To, class A = default_arch, class From>\n-    inline void store_as(To* dst, batch<From, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(To* dst, batch<From, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store_unaligned<A>(dst, src, A {});\n     }\n \n     template <class A = default_arch, class From>\n-    inline void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(bool* dst, batch_bool<From, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<From, A>();\n         kernel::store<A>(src, dst, A {});\n     }\n \n     template <class To, class A = default_arch, class From>\n-    inline void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(std::complex<To>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         kernel::store_complex_unaligned<A>(dst, src, A {});\n     }\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n     template <class To, class A = default_arch, class From, bool i3ec>\n-    inline void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n+    XSIMD_INLINE void store_as(xtl::xcomplex<To, To, i3ec>* dst, batch<std::complex<From>, A> const& src, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<std::complex<From>, A>();\n         store_as(reinterpret_cast<std::complex<To>*>(dst), src, unaligned_mode());\n@@ -2283,7 +2283,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store(T* mem, batch<T, A> const& val, aligned_mode = {}) noexcept\n+    XSIMD_INLINE void store(T* mem, batch<T, A> const& val, aligned_mode = {}) noexcept\n     {\n         store_as<T, A>(mem, val, aligned_mode {});\n     }\n@@ -2297,7 +2297,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store(T* mem, batch<T, A> const& val, unaligned_mode) noexcept\n+    XSIMD_INLINE void store(T* mem, batch<T, A> const& val, unaligned_mode) noexcept\n     {\n         store_as<T, A>(mem, val, unaligned_mode {});\n     }\n@@ -2311,7 +2311,7 @@ namespace xsimd\n      * @param val the batch to copy from\n      */\n     template <class A, class T>\n-    inline void store_aligned(T* mem, batch<T, A> const& val) noexcept\n+    XSIMD_INLINE void store_aligned(T* mem, batch<T, A> const& val) noexcept\n     {\n         store_as<T, A>(mem, val, aligned_mode {});\n     }\n@@ -2325,7 +2325,7 @@ namespace xsimd\n      * @param val the batch to copy\n      */\n     template <class A, class T>\n-    inline void store_unaligned(T* mem, batch<T, A> const& val) noexcept\n+    XSIMD_INLINE void store_unaligned(T* mem, batch<T, A> const& val) noexcept\n     {\n         store_as<T, A>(mem, val, unaligned_mode {});\n     }\n@@ -2340,7 +2340,7 @@ namespace xsimd\n      * @return the difference between \\c x and \\c y\n      */\n     template <class T, class A>\n-    inline auto sub(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x - y)\n+    XSIMD_INLINE auto sub(batch<T, A> const& x, batch<T, A> const& y) noexcept -> decltype(x - y)\n     {\n         detail::static_check_supported_config<T, A>();\n         return x - y;\n@@ -2356,15 +2356,15 @@ namespace xsimd\n      * @return swizzled batch\n      */\n     template <class T, class A, class Vt, Vt... Values>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     swizzle(batch<T, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n         return kernel::swizzle<A>(x, mask, A {});\n     }\n     template <class T, class A, class Vt, Vt... Values>\n-    inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch_constant<Vt, A, Values...> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n@@ -2381,7 +2381,7 @@ namespace xsimd\n      * @return swizzled batch\n      */\n     template <class T, class A, class Vt>\n-    inline typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n+    XSIMD_INLINE typename std::enable_if<std::is_arithmetic<T>::value, batch<T, A>>::type\n     swizzle(batch<T, A> const& x, batch<Vt, A> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n@@ -2390,7 +2390,7 @@ namespace xsimd\n     }\n \n     template <class T, class A, class Vt>\n-    inline batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch<Vt, A> mask) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> swizzle(batch<std::complex<T>, A> const& x, batch<Vt, A> mask) noexcept\n     {\n         static_assert(sizeof(T) == sizeof(Vt), \"consistent mask\");\n         detail::static_check_supported_config<T, A>();\n@@ -2405,7 +2405,7 @@ namespace xsimd\n      * @return the tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tan(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tan(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tan<A>(x, A {});\n@@ -2419,7 +2419,7 @@ namespace xsimd\n      * @return the hyperbolic tangent of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tanh(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tanh(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tanh<A>(x, A {});\n@@ -2433,7 +2433,7 @@ namespace xsimd\n      * @return the gamma function of \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> tgamma(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> tgamma(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::tgamma<A>(x, A {});\n@@ -2448,7 +2448,7 @@ namespace xsimd\n      * @return \\c i converted to a value of an floating point type of the same size as \\c T\n      */\n     template <class T, class A>\n-    inline batch<as_float_t<T>, A> to_float(batch<T, A> const& i) noexcept\n+    XSIMD_INLINE batch<as_float_t<T>, A> to_float(batch<T, A> const& i) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch_cast<as_float_t<T>>(i);\n@@ -2463,7 +2463,7 @@ namespace xsimd\n      * @return \\c x converted to a value of an integer type of the same size as \\c T\n      */\n     template <class T, class A>\n-    inline batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<as_integer_t<T>, A> to_int(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch_cast<as_integer_t<T>>(x);\n@@ -2478,7 +2478,7 @@ namespace xsimd\n      * @return the batch of nearest integer values not greater in magnitude than \\c x.\n      */\n     template <class T, class A>\n-    inline batch<T, A> trunc(batch<T, A> const& x) noexcept\n+    XSIMD_INLINE batch<T, A> trunc(batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::trunc<A>(x, A {});\n@@ -2494,7 +2494,7 @@ namespace xsimd\n      * @return a batch of the high part of shuffled values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> zip_hi(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::zip_hi<A>(x, y, A {});\n@@ -2510,7 +2510,7 @@ namespace xsimd\n      * @return a batch of the low part of shuffled values.\n      */\n     template <class T, class A>\n-    inline batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) noexcept\n+    XSIMD_INLINE batch<T, A> zip_lo(batch<T, A> const& x, batch<T, A> const& y) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::zip_lo<A>(x, y, A {});\n@@ -2527,15 +2527,15 @@ namespace xsimd\n      * @return \\c self cast to a \\c batch of \\c T\n      */\n     template <class T, class A, typename std::enable_if<std::is_integral<T>::value, int>::type = 3>\n-    inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n     {\n         T z(0);\n         detail::static_check_supported_config<T, A>();\n         return select(self, batch<T, A>(T(~z)), batch<T, A>(z));\n     }\n \n     template <class T, class A, typename std::enable_if<std::is_floating_point<T>::value, int>::type = 3>\n-    inline batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n+    XSIMD_INLINE batch<T, A> bitwise_cast(batch_bool<T, A> const& self) noexcept\n     {\n         T z0(0), z1(0);\n         using int_type = as_unsigned_integer_t<T>;\n@@ -2554,7 +2554,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool all(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool all(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::all<A>(x, A {});\n@@ -2569,7 +2569,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool any(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool any(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::any<A>(x, A {});\n@@ -2584,7 +2584,7 @@ namespace xsimd\n      * @return a boolean scalar.\n      */\n     template <class T, class A>\n-    inline bool none(batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE bool none(batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return !xsimd::any(x);\n@@ -2599,7 +2599,7 @@ namespace xsimd\n      * @return a reference to \\c o\n      */\n     template <class T, class A>\n-    inline std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) noexcept\n+    XSIMD_INLINE std::ostream& operator<<(std::ostream& o, batch<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         constexpr auto size = batch<T, A>::size;\n@@ -2620,7 +2620,7 @@ namespace xsimd\n      * @return a reference to \\c o\n      */\n     template <class T, class A>\n-    inline std::ostream& operator<<(std::ostream& o, batch_bool<T, A> const& x) noexcept\n+    XSIMD_INLINE std::ostream& operator<<(std::ostream& o, batch_bool<T, A> const& x) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         constexpr auto size = batch_bool<T, A>::size;"
      },
      {
        "filename": "include/xsimd/types/xsimd_batch.hpp",
        "status": "modified",
        "additions": 245,
        "deletions": 245,
        "changes": 490,
        "patch": "@@ -29,38 +29,38 @@ namespace xsimd\n         template <class T, class A>\n         struct integral_only_operators\n         {\n-            inline batch<T, A>& operator%=(batch<T, A> const& other) noexcept;\n-            inline batch<T, A>& operator>>=(int32_t other) noexcept;\n-            inline batch<T, A>& operator>>=(batch<T, A> const& other) noexcept;\n-            inline batch<T, A>& operator<<=(int32_t other) noexcept;\n-            inline batch<T, A>& operator<<=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator%=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator>>=(int32_t other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator>>=(batch<T, A> const& other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator<<=(int32_t other) noexcept;\n+            XSIMD_INLINE batch<T, A>& operator<<=(batch<T, A> const& other) noexcept;\n \n             /** Shorthand for xsimd::mod() */\n-            friend inline batch<T, A> operator%(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator%(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) %= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_rshift() */\n-            friend inline batch<T, A> operator>>(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator>>(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) >>= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_lshift() */\n-            friend inline batch<T, A> operator<<(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator<<(batch<T, A> const& self, batch<T, A> const& other) noexcept\n             {\n                 return batch<T, A>(self) <<= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_rshift() */\n-            friend inline batch<T, A> operator>>(batch<T, A> const& self, int32_t other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator>>(batch<T, A> const& self, int32_t other) noexcept\n             {\n                 return batch<T, A>(self) >>= other;\n             }\n \n             /** Shorthand for xsimd::bitwise_lshift() */\n-            friend inline batch<T, A> operator<<(batch<T, A> const& self, int32_t other) noexcept\n+            friend XSIMD_INLINE batch<T, A> operator<<(batch<T, A> const& self, int32_t other) noexcept\n             {\n                 return batch<T, A>(self) <<= other;\n             }\n@@ -82,22 +82,22 @@ namespace xsimd\n         // with batch<T, A>. Their implementation must appear only once the\n         // kernel implementations have been included.\n         template <class T, class A>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n \n         template <class T, class A>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept;\n     }\n \n     /**\n@@ -123,152 +123,152 @@ namespace xsimd\n         using batch_bool_type = batch_bool<T, A>; ///< Associated batch type used to represented logical operations on this batch.\n \n         // constructors\n-        inline batch() = default; ///< Create a batch initialized with undefined values.\n-        inline batch(T val) noexcept;\n+        XSIMD_INLINE batch() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch(T val) noexcept;\n         template <class... Ts>\n-        inline batch(T val0, T val1, Ts... vals) noexcept;\n-        inline explicit batch(batch_bool_type const& b) noexcept;\n-        inline batch(register_type reg) noexcept;\n+        XSIMD_INLINE batch(T val0, T val1, Ts... vals) noexcept;\n+        XSIMD_INLINE explicit batch(batch_bool_type const& b) noexcept;\n+        XSIMD_INLINE batch(register_type reg) noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch broadcast(U val) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch broadcast(U val) noexcept;\n \n         // memory operators\n         template <class U>\n-        inline void store_aligned(U* mem) const noexcept;\n+        XSIMD_INLINE void store_aligned(U* mem) const noexcept;\n         template <class U>\n-        inline void store_unaligned(U* mem) const noexcept;\n+        XSIMD_INLINE void store_unaligned(U* mem) const noexcept;\n         template <class U>\n-        inline void store(U* mem, aligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, aligned_mode) const noexcept;\n         template <class U>\n-        inline void store(U* mem, unaligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, unaligned_mode) const noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load_aligned(U const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(U const* mem) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(U const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(U const* mem) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, aligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, aligned_mode) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, unaligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, unaligned_mode) noexcept;\n \n         template <class U, class V>\n-        XSIMD_NO_DISCARD static inline batch gather(U const* src, batch<V, arch_type> const& index) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch gather(U const* src, batch<V, arch_type> const& index) noexcept;\n         template <class U, class V>\n-        inline void scatter(U* dst, batch<V, arch_type> const& index) const noexcept;\n+        XSIMD_INLINE void scatter(U* dst, batch<V, arch_type> const& index) const noexcept;\n \n-        inline T get(std::size_t i) const noexcept;\n+        XSIMD_INLINE T get(std::size_t i) const noexcept;\n \n         // comparison operators. Defined as friend to enable automatic\n         // conversion of parameters from scalar to batch, at the cost of using a\n         // proxy implementation from details::.\n-        friend inline batch_bool<T, A> operator==(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator==(batch const& self, batch const& other) noexcept\n         {\n             return details::eq<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator!=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator!=(batch const& self, batch const& other) noexcept\n         {\n             return details::neq<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator>=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator>=(batch const& self, batch const& other) noexcept\n         {\n             return details::ge<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator<=(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator<=(batch const& self, batch const& other) noexcept\n         {\n             return details::le<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator>(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator>(batch const& self, batch const& other) noexcept\n         {\n             return details::gt<T, A>(self, other);\n         }\n-        friend inline batch_bool<T, A> operator<(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch_bool<T, A> operator<(batch const& self, batch const& other) noexcept\n         {\n             return details::lt<T, A>(self, other);\n         }\n \n         // Update operators\n-        inline batch& operator+=(batch const& other) noexcept;\n-        inline batch& operator-=(batch const& other) noexcept;\n-        inline batch& operator*=(batch const& other) noexcept;\n-        inline batch& operator/=(batch const& other) noexcept;\n-        inline batch& operator&=(batch const& other) noexcept;\n-        inline batch& operator|=(batch const& other) noexcept;\n-        inline batch& operator^=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator+=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator-=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator*=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator/=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator&=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator|=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator^=(batch const& other) noexcept;\n \n         // incr/decr operators\n-        inline batch& operator++() noexcept;\n-        inline batch& operator--() noexcept;\n-        inline batch operator++(int) noexcept;\n-        inline batch operator--(int) noexcept;\n+        XSIMD_INLINE batch& operator++() noexcept;\n+        XSIMD_INLINE batch& operator--() noexcept;\n+        XSIMD_INLINE batch operator++(int) noexcept;\n+        XSIMD_INLINE batch operator--(int) noexcept;\n \n         // unary operators\n-        inline batch_bool_type operator!() const noexcept;\n-        inline batch operator~() const noexcept;\n-        inline batch operator-() const noexcept;\n-        inline batch operator+() const noexcept;\n+        XSIMD_INLINE batch_bool_type operator!() const noexcept;\n+        XSIMD_INLINE batch operator~() const noexcept;\n+        XSIMD_INLINE batch operator-() const noexcept;\n+        XSIMD_INLINE batch operator+() const noexcept;\n \n         // arithmetic operators. They are defined as friend to enable automatic\n         // conversion of parameters from scalar to batch. Inline implementation\n         // is required to avoid warnings.\n \n         /** Shorthand for xsimd::add() */\n-        friend inline batch operator+(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator+(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) += other;\n         }\n \n         /** Shorthand for xsimd::sub() */\n-        friend inline batch operator-(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator-(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) -= other;\n         }\n \n         /** Shorthand for xsimd::mul() */\n-        friend inline batch operator*(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator*(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) *= other;\n         }\n \n         /** Shorthand for xsimd::div() */\n-        friend inline batch operator/(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator/(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) /= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_and() */\n-        friend inline batch operator&(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator&(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) &= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_or() */\n-        friend inline batch operator|(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator|(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) |= other;\n         }\n \n         /** Shorthand for xsimd::bitwise_xor() */\n-        friend inline batch operator^(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator^(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) ^= other;\n         }\n \n         /** Shorthand for xsimd::logical_and() */\n-        friend inline batch operator&&(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator&&(batch const& self, batch const& other) noexcept\n         {\n             return batch(self).logical_and(other);\n         }\n \n         /** Shorthand for xsimd::logical_or() */\n-        friend inline batch operator||(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator||(batch const& self, batch const& other) noexcept\n         {\n             return batch(self).logical_or(other);\n         }\n \n     private:\n-        inline batch logical_and(batch const& other) const noexcept;\n-        inline batch logical_or(batch const& other) const noexcept;\n+        XSIMD_INLINE batch logical_and(batch const& other) const noexcept;\n+        XSIMD_INLINE batch logical_or(batch const& other) const noexcept;\n     };\n \n     template <class T, class A>\n@@ -297,51 +297,51 @@ namespace xsimd\n         using batch_type = batch<T, A>; ///< Associated batch type this batch represents logical operations for.\n \n         // constructors\n-        inline batch_bool() = default; ///< Create a batch initialized with undefined values.\n-        inline batch_bool(bool val) noexcept;\n-        inline batch_bool(register_type reg) noexcept;\n+        XSIMD_INLINE batch_bool() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch_bool(bool val) noexcept;\n+        XSIMD_INLINE batch_bool(register_type reg) noexcept;\n         template <class... Ts>\n-        inline batch_bool(bool val0, bool val1, Ts... vals) noexcept;\n+        XSIMD_INLINE batch_bool(bool val0, bool val1, Ts... vals) noexcept;\n \n         template <class Tp>\n-        inline batch_bool(Tp const*) = delete;\n+        XSIMD_INLINE batch_bool(Tp const*) = delete;\n \n         // memory operators\n-        inline void store_aligned(bool* mem) const noexcept;\n-        inline void store_unaligned(bool* mem) const noexcept;\n-        XSIMD_NO_DISCARD static inline batch_bool load_aligned(bool const* mem) noexcept;\n-        XSIMD_NO_DISCARD static inline batch_bool load_unaligned(bool const* mem) noexcept;\n+        XSIMD_INLINE void store_aligned(bool* mem) const noexcept;\n+        XSIMD_INLINE void store_unaligned(bool* mem) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch_bool load_aligned(bool const* mem) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch_bool load_unaligned(bool const* mem) noexcept;\n \n-        inline bool get(std::size_t i) const noexcept;\n+        XSIMD_INLINE bool get(std::size_t i) const noexcept;\n \n         // mask operations\n-        inline uint64_t mask() const noexcept;\n-        inline static batch_bool from_mask(uint64_t mask) noexcept;\n+        XSIMD_INLINE uint64_t mask() const noexcept;\n+        XSIMD_INLINE static batch_bool from_mask(uint64_t mask) noexcept;\n \n         // comparison operators\n-        inline batch_bool operator==(batch_bool const& other) const noexcept;\n-        inline batch_bool operator!=(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator==(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator!=(batch_bool const& other) const noexcept;\n \n         // logical operators\n-        inline batch_bool operator~() const noexcept;\n-        inline batch_bool operator!() const noexcept;\n-        inline batch_bool operator&(batch_bool const& other) const noexcept;\n-        inline batch_bool operator|(batch_bool const& other) const noexcept;\n-        inline batch_bool operator^(batch_bool const& other) const noexcept;\n-        inline batch_bool operator&&(batch_bool const& other) const noexcept;\n-        inline batch_bool operator||(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator~() const noexcept;\n+        XSIMD_INLINE batch_bool operator!() const noexcept;\n+        XSIMD_INLINE batch_bool operator&(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator|(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator^(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator&&(batch_bool const& other) const noexcept;\n+        XSIMD_INLINE batch_bool operator||(batch_bool const& other) const noexcept;\n \n         // update operators\n-        inline batch_bool& operator&=(batch_bool const& other) noexcept { return (*this) = (*this) & other; }\n-        inline batch_bool& operator|=(batch_bool const& other) noexcept { return (*this) = (*this) | other; }\n-        inline batch_bool& operator^=(batch_bool const& other) noexcept { return (*this) = (*this) ^ other; }\n+        XSIMD_INLINE batch_bool& operator&=(batch_bool const& other) noexcept { return (*this) = (*this) & other; }\n+        XSIMD_INLINE batch_bool& operator|=(batch_bool const& other) noexcept { return (*this) = (*this) | other; }\n+        XSIMD_INLINE batch_bool& operator^=(batch_bool const& other) noexcept { return (*this) = (*this) ^ other; }\n \n     private:\n         template <class U, class... V, size_t I, size_t... Is>\n-        static inline register_type make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept;\n+        static XSIMD_INLINE register_type make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept;\n \n         template <class... V>\n-        static inline register_type make_register(detail::index_sequence<>, V... v) noexcept;\n+        static XSIMD_INLINE register_type make_register(detail::index_sequence<>, V... v) noexcept;\n     };\n \n     template <class T, class A>\n@@ -367,106 +367,106 @@ namespace xsimd\n         static constexpr std::size_t size = real_batch::size; ///< Number of complex elements in this batch.\n \n         // constructors\n-        inline batch() = default; ///< Create a batch initialized with undefined values.\n-        inline batch(value_type const& val) noexcept;\n-        inline batch(real_batch const& real, real_batch const& imag) noexcept;\n+        XSIMD_INLINE batch() = default; ///< Create a batch initialized with undefined values.\n+        XSIMD_INLINE batch(value_type const& val) noexcept;\n+        XSIMD_INLINE batch(real_batch const& real, real_batch const& imag) noexcept;\n \n-        inline batch(real_batch const& real) noexcept;\n-        inline batch(T val) noexcept;\n+        XSIMD_INLINE batch(real_batch const& real) noexcept;\n+        XSIMD_INLINE batch(T val) noexcept;\n         template <class... Ts>\n-        inline batch(value_type val0, value_type val1, Ts... vals) noexcept;\n-        inline explicit batch(batch_bool_type const& b) noexcept;\n+        XSIMD_INLINE batch(value_type val0, value_type val1, Ts... vals) noexcept;\n+        XSIMD_INLINE explicit batch(batch_bool_type const& b) noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch broadcast(U val) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch broadcast(U val) noexcept;\n \n         // memory operators\n-        XSIMD_NO_DISCARD static inline batch load_aligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n-        inline void store_aligned(T* real_dst, T* imag_dst) const noexcept;\n-        inline void store_unaligned(T* real_dst, T* imag_dst) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const T* real_src, const T* imag_src = nullptr) noexcept;\n+        XSIMD_INLINE void store_aligned(T* real_dst, T* imag_dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(T* real_dst, T* imag_dst) const noexcept;\n \n-        XSIMD_NO_DISCARD static inline batch load_aligned(const value_type* src) noexcept;\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const value_type* src) noexcept;\n-        inline void store_aligned(value_type* dst) const noexcept;\n-        inline void store_unaligned(value_type* dst) const noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const value_type* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const value_type* src) noexcept;\n+        XSIMD_INLINE void store_aligned(value_type* dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(value_type* dst) const noexcept;\n \n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, aligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, aligned_mode) noexcept;\n         template <class U>\n-        XSIMD_NO_DISCARD static inline batch load(U const* mem, unaligned_mode) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load(U const* mem, unaligned_mode) noexcept;\n         template <class U>\n-        inline void store(U* mem, aligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, aligned_mode) const noexcept;\n         template <class U>\n-        inline void store(U* mem, unaligned_mode) const noexcept;\n+        XSIMD_INLINE void store(U* mem, unaligned_mode) const noexcept;\n \n-        inline real_batch real() const noexcept;\n-        inline real_batch imag() const noexcept;\n+        XSIMD_INLINE real_batch real() const noexcept;\n+        XSIMD_INLINE real_batch imag() const noexcept;\n \n-        inline value_type get(std::size_t i) const noexcept;\n+        XSIMD_INLINE value_type get(std::size_t i) const noexcept;\n \n #ifdef XSIMD_ENABLE_XTL_COMPLEX\n         // xtl-related methods\n         template <bool i3ec>\n-        inline batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept;\n+        XSIMD_INLINE batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept;\n         template <bool i3ec, class... Ts>\n-        inline batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept;\n+        XSIMD_INLINE batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept;\n \n         template <bool i3ec>\n-        XSIMD_NO_DISCARD static inline batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n         template <bool i3ec>\n-        XSIMD_NO_DISCARD static inline batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n+        XSIMD_NO_DISCARD static XSIMD_INLINE batch load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept;\n         template <bool i3ec>\n-        inline void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n+        XSIMD_INLINE void store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n         template <bool i3ec>\n-        inline void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n+        XSIMD_INLINE void store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept;\n #endif\n \n         // comparison operators\n-        inline batch_bool<T, A> operator==(batch const& other) const noexcept;\n-        inline batch_bool<T, A> operator!=(batch const& other) const noexcept;\n+        XSIMD_INLINE batch_bool<T, A> operator==(batch const& other) const noexcept;\n+        XSIMD_INLINE batch_bool<T, A> operator!=(batch const& other) const noexcept;\n \n         // Update operators\n-        inline batch& operator+=(batch const& other) noexcept;\n-        inline batch& operator-=(batch const& other) noexcept;\n-        inline batch& operator*=(batch const& other) noexcept;\n-        inline batch& operator/=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator+=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator-=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator*=(batch const& other) noexcept;\n+        XSIMD_INLINE batch& operator/=(batch const& other) noexcept;\n \n         // incr/decr operators\n-        inline batch& operator++() noexcept;\n-        inline batch& operator--() noexcept;\n-        inline batch operator++(int) noexcept;\n-        inline batch operator--(int) noexcept;\n+        XSIMD_INLINE batch& operator++() noexcept;\n+        XSIMD_INLINE batch& operator--() noexcept;\n+        XSIMD_INLINE batch operator++(int) noexcept;\n+        XSIMD_INLINE batch operator--(int) noexcept;\n \n         // unary operators\n-        inline batch_bool_type operator!() const noexcept;\n-        inline batch operator~() const noexcept;\n-        inline batch operator-() const noexcept;\n-        inline batch operator+() const noexcept;\n+        XSIMD_INLINE batch_bool_type operator!() const noexcept;\n+        XSIMD_INLINE batch operator~() const noexcept;\n+        XSIMD_INLINE batch operator-() const noexcept;\n+        XSIMD_INLINE batch operator+() const noexcept;\n \n         // arithmetic operators. They are defined as friend to enable automatic\n         // conversion of parameters from scalar to batch\n \n         /** Shorthand for xsimd::add() */\n-        friend inline batch operator+(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator+(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) += other;\n         }\n \n         /** Shorthand for xsimd::sub() */\n-        friend inline batch operator-(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator-(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) -= other;\n         }\n \n         /** Shorthand for xsimd::mul() */\n-        friend inline batch operator*(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator*(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) *= other;\n         }\n \n         /** Shorthand for xsimd::div() */\n-        friend inline batch operator/(batch const& self, batch const& other) noexcept\n+        friend XSIMD_INLINE batch operator/(batch const& self, batch const& other) noexcept\n         {\n             return batch(self) /= other;\n         }\n@@ -500,7 +500,7 @@ namespace xsimd\n      * Create a batch with all element initialized to \\c val.\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(T val) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(T val) noexcept\n         : types::simd_register<T, A>(kernel::broadcast<A>(val, A {}))\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -512,7 +512,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class... Ts>\n-    inline batch<T, A>::batch(T val0, T val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(T val0, T val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<T>(vals)...))\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -525,7 +525,7 @@ namespace xsimd\n      * (resp. `false`).\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(batch_bool<T, A> const& b) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(batch_bool<T, A> const& b) noexcept\n         : batch(kernel::from_bool(b, A {}))\n     {\n     }\n@@ -535,7 +535,7 @@ namespace xsimd\n      * becomes handy when doing architecture-specific operations.\n      */\n     template <class T, class A>\n-    inline batch<T, A>::batch(register_type reg) noexcept\n+    XSIMD_INLINE batch<T, A>::batch(register_type reg) noexcept\n         : types::simd_register<T, A>({ reg })\n     {\n         detail::static_check_supported_config<T, A>();\n@@ -546,7 +546,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    XSIMD_NO_DISCARD inline batch<T, A> batch<T, A>::broadcast(U val) noexcept\n+    XSIMD_NO_DISCARD XSIMD_INLINE batch<T, A> batch<T, A>::broadcast(U val) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return batch(static_cast<T>(val));\n@@ -562,7 +562,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store_aligned(U* mem) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store_aligned(U* mem) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         assert(((reinterpret_cast<uintptr_t>(mem) % A::alignment()) == 0)\n@@ -576,7 +576,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store_unaligned(U* mem) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store_unaligned(U* mem) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         kernel::store_unaligned<A>(mem, *this, A {});\n@@ -587,7 +587,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store(U* mem, aligned_mode) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store(U* mem, aligned_mode) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return store_aligned(mem);\n@@ -598,7 +598,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline void batch<T, A>::store(U* mem, unaligned_mode) const noexcept\n+    XSIMD_INLINE void batch<T, A>::store(U* mem, unaligned_mode) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return store_unaligned(mem);\n@@ -610,7 +610,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load_aligned(U const* mem) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load_aligned(U const* mem) noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(mem) % A::alignment()) == 0)\n                && \"loaded pointer is not properly aligned\");\n@@ -624,7 +624,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load_unaligned(U const* mem) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load_unaligned(U const* mem) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::load_unaligned<A>(mem, kernel::convert<T> {}, A {});\n@@ -635,7 +635,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load(U const* mem, aligned_mode) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load(U const* mem, aligned_mode) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return load_aligned(mem);\n@@ -646,7 +646,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U>\n-    inline batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::load(U const* mem, unaligned_mode) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return load_unaligned(mem);\n@@ -660,7 +660,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <typename U, typename V>\n-    inline batch<T, A> batch<T, A>::gather(U const* src, batch<V, A> const& index) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::gather(U const* src, batch<V, A> const& index) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         static_assert(std::is_convertible<T, U>::value, \"Can't convert from src to this batch's type!\");\n@@ -675,7 +675,7 @@ namespace xsimd\n      */\n     template <class T, class A>\n     template <class U, class V>\n-    inline void batch<T, A>::scatter(U* dst, batch<V, A> const& index) const noexcept\n+    XSIMD_INLINE void batch<T, A>::scatter(U* dst, batch<V, A> const& index) const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         static_assert(std::is_convertible<T, U>::value, \"Can't convert from this batch's type to dst!\");\n@@ -688,7 +688,7 @@ namespace xsimd\n      * \\c warning This is very inefficient and should only be used for debugging purpose.\n      */\n     template <class T, class A>\n-    inline T batch<T, A>::get(std::size_t i) const noexcept\n+    XSIMD_INLINE T batch<T, A>::get(std::size_t i) const noexcept\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -702,7 +702,7 @@ namespace xsimd\n          * Shorthand for xsimd::eq()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> eq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::eq<A>(self, other, A {});\n@@ -712,7 +712,7 @@ namespace xsimd\n          * Shorthand for xsimd::neq()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> neq(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::neq<A>(self, other, A {});\n@@ -722,7 +722,7 @@ namespace xsimd\n          * Shorthand for xsimd::ge()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> ge(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::ge<A>(self, other, A {});\n@@ -732,7 +732,7 @@ namespace xsimd\n          * Shorthand for xsimd::le()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> le(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::le<A>(self, other, A {});\n@@ -742,7 +742,7 @@ namespace xsimd\n          * Shorthand for xsimd::gt()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> gt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::gt<A>(self, other, A {});\n@@ -752,7 +752,7 @@ namespace xsimd\n          * Shorthand for xsimd::lt()\n          */\n         template <class T, class A>\n-        inline batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n+        XSIMD_INLINE batch_bool<T, A> lt(batch<T, A> const& self, batch<T, A> const& other) noexcept\n         {\n             detail::static_check_supported_config<T, A>();\n             return kernel::lt<A>(self, other, A {});\n@@ -764,84 +764,84 @@ namespace xsimd\n      **************************/\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator+=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::add<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator-=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::sub<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator*=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::mul<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator/=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::div<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& types::integral_only_operators<T, A>::operator%=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& types::integral_only_operators<T, A>::operator%=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::mod<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator&=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_and<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator|=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_or<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator^=(batch<T, A> const& other) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this = kernel::bitwise_xor<A>(*this, other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_rshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(batch<T, A> const& other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(batch<T, A> const& other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_lshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(int32_t other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator>>=(int32_t other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_rshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(int32_t other) noexcept\n+    XSIMD_INLINE batch<T, A>& kernel::integral_only_operators<T, A>::operator<<=(int32_t other) noexcept\n     {\n         ::xsimd::detail::static_check_supported_config<T, A>();\n         return *static_cast<batch<T, A>*>(this) = kernel::bitwise_lshift<A>(*static_cast<batch<T, A>*>(this), other, A {});\n@@ -852,21 +852,21 @@ namespace xsimd\n      *****************************/\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator++() noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator++() noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return operator+=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<T, A>& batch<T, A>::operator--() noexcept\n+    XSIMD_INLINE batch<T, A>& batch<T, A>::operator--() noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return operator-=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator++(int) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator++(int) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         batch<T, A> copy(*this);\n@@ -875,7 +875,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator--(int) noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator--(int) noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         batch copy(*this);\n@@ -888,28 +888,28 @@ namespace xsimd\n      *************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<T, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<T, A>::operator!() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::eq<A>(*this, batch(0), A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator~() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator~() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::bitwise_not<A>(*this, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator-() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator-() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return kernel::neg<A>(*this, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::operator+() const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::operator+() const noexcept\n     {\n         detail::static_check_supported_config<T, A>();\n         return *this;\n@@ -920,13 +920,13 @@ namespace xsimd\n      ************************/\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::logical_and(batch<T, A> const& other) const noexcept\n     {\n         return kernel::logical_and<A>(*this, other, A());\n     }\n \n     template <class T, class A>\n-    inline batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch<T, A> batch<T, A>::logical_or(batch<T, A> const& other) const noexcept\n     {\n         return kernel::logical_or<A>(*this, other, A());\n     }\n@@ -936,14 +936,14 @@ namespace xsimd\n      ***************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A>::batch_bool(register_type reg) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(register_type reg) noexcept\n         : types::get_bool_simd_register_t<T, A>({ reg })\n     {\n     }\n \n     template <class T, class A>\n     template <class... Ts>\n-    inline batch_bool<T, A>::batch_bool(bool val0, bool val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(bool val0, bool val1, Ts... vals) noexcept\n         : batch_bool(kernel::set<A>(batch_bool {}, A {}, val0, val1, static_cast<bool>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"The constructor requires as many arguments as batch elements.\");\n@@ -954,19 +954,19 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    inline void batch_bool<T, A>::store_aligned(bool* mem) const noexcept\n+    XSIMD_INLINE void batch_bool<T, A>::store_aligned(bool* mem) const noexcept\n     {\n         kernel::store(*this, mem, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch_bool<T, A>::store_unaligned(bool* mem) const noexcept\n+    XSIMD_INLINE void batch_bool<T, A>::store_unaligned(bool* mem) const noexcept\n     {\n         store_aligned(mem);\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::load_aligned(bool const* mem) noexcept\n     {\n         batch_type ref(0);\n         alignas(A::alignment()) T buffer[size];\n@@ -976,7 +976,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::load_unaligned(bool const* mem) noexcept\n     {\n         return load_aligned(mem);\n     }\n@@ -987,7 +987,7 @@ namespace xsimd\n      * @return bit mask\n      */\n     template <class T, class A>\n-    inline uint64_t batch_bool<T, A>::mask() const noexcept\n+    XSIMD_INLINE uint64_t batch_bool<T, A>::mask() const noexcept\n     {\n         return kernel::mask(*this, A {});\n     }\n@@ -998,13 +998,13 @@ namespace xsimd\n      * @return bit mask\n      */\n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::from_mask(uint64_t mask) noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::from_mask(uint64_t mask) noexcept\n     {\n         return kernel::from_mask(batch_bool<T, A>(), mask, A {});\n     }\n \n     template <class T, class A>\n-    inline bool batch_bool<T, A>::get(std::size_t i) const noexcept\n+    XSIMD_INLINE bool batch_bool<T, A>::get(std::size_t i) const noexcept\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -1014,13 +1014,13 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator==(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::eq<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator!=(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::neq<A>(*this, other, A {}).data;\n     }\n@@ -1030,43 +1030,43 @@ namespace xsimd\n      ********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator~() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator~() const noexcept\n     {\n         return kernel::bitwise_not<A>(*this, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator!() const noexcept\n     {\n         return operator==(batch_bool(false));\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator&(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_and<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator|(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_or<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator^(batch_bool<T, A> const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator^(batch_bool<T, A> const& other) const noexcept\n     {\n         return kernel::bitwise_xor<A>(*this, other, A {}).data;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator&&(batch_bool const& other) const noexcept\n     {\n         return operator&(other);\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch_bool<T, A>::operator||(batch_bool const& other) const noexcept\n     {\n         return operator|(other);\n     }\n@@ -1076,21 +1076,21 @@ namespace xsimd\n      ******************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A>::batch_bool(bool val) noexcept\n+    XSIMD_INLINE batch_bool<T, A>::batch_bool(bool val) noexcept\n         : base_type { make_register(detail::make_index_sequence<size - 1>(), val) }\n     {\n     }\n \n     template <class T, class A>\n     template <class U, class... V, size_t I, size_t... Is>\n-    inline auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept -> register_type\n+    XSIMD_INLINE auto batch_bool<T, A>::make_register(detail::index_sequence<I, Is...>, U u, V... v) noexcept -> register_type\n     {\n         return make_register(detail::index_sequence<Is...>(), u, u, v...);\n     }\n \n     template <class T, class A>\n     template <class... V>\n-    inline auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) noexcept -> register_type\n+    XSIMD_INLINE auto batch_bool<T, A>::make_register(detail::index_sequence<>, V... v) noexcept -> register_type\n     {\n         return kernel::set<A>(batch_bool<T, A>(), A {}, v...).data;\n     }\n@@ -1100,51 +1100,51 @@ namespace xsimd\n      *******************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(value_type const& val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(value_type const& val) noexcept\n         : m_real(val.real())\n         , m_imag(val.imag())\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(real_batch const& real, real_batch const& imag) noexcept\n         : m_real(real)\n         , m_imag(imag)\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(real_batch const& real) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(real_batch const& real) noexcept\n         : m_real(real)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(T val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(T val) noexcept\n         : m_real(val)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n     template <class... Ts>\n-    inline batch<std::complex<T>, A>::batch(value_type val0, value_type val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(value_type val0, value_type val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<value_type>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"as many arguments as batch elements\");\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>::batch(batch_bool_type const& b) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(batch_bool_type const& b) noexcept\n         : m_real(b)\n         , m_imag(0)\n     {\n     }\n \n     template <class T, class A>\n     template <class U>\n-    XSIMD_NO_DISCARD inline batch<std::complex<T>, A> batch<std::complex<T>, A>::broadcast(U val) noexcept\n+    XSIMD_NO_DISCARD XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::broadcast(U val) noexcept\n     {\n         return batch(static_cast<std::complex<T>>(val));\n     }\n@@ -1154,100 +1154,100 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const T* real_src, const T* imag_src) noexcept\n     {\n         return { batch<T, A>::load_aligned(real_src), imag_src ? batch<T, A>::load_aligned(imag_src) : batch<T, A>(0) };\n     }\n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const T* real_src, const T* imag_src) noexcept\n     {\n         return { batch<T, A>::load_unaligned(real_src), imag_src ? batch<T, A>::load_unaligned(imag_src) : batch<T, A>(0) };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const value_type* src) noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(src) % A::alignment()) == 0)\n                && \"loaded pointer is not properly aligned\");\n         return kernel::load_complex_aligned<A>(src, kernel::convert<value_type> {}, A {});\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const value_type* src) noexcept\n     {\n         return kernel::load_complex_unaligned<A>(src, kernel::convert<value_type> {}, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_aligned(value_type* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(value_type* dst) const noexcept\n     {\n         assert(((reinterpret_cast<uintptr_t>(dst) % A::alignment()) == 0)\n                && \"store location is not properly aligned\");\n         return kernel::store_complex_aligned(dst, *this, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(value_type* dst) const noexcept\n     {\n         return kernel::store_complex_unaligned(dst, *this, A {});\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(T* real_dst, T* imag_dst) const noexcept\n     {\n         m_real.store_aligned(real_dst);\n         m_imag.store_aligned(imag_dst);\n     }\n \n     template <class T, class A>\n-    inline void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(T* real_dst, T* imag_dst) const noexcept\n     {\n         m_real.store_unaligned(real_dst);\n         m_imag.store_unaligned(imag_dst);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, aligned_mode) noexcept\n     {\n         return load_aligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load(U const* mem, unaligned_mode) noexcept\n     {\n         return load_unaligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline void batch<std::complex<T>, A>::store(U* mem, aligned_mode) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store(U* mem, aligned_mode) const noexcept\n     {\n         return store_aligned(mem);\n     }\n \n     template <class T, class A>\n     template <class U>\n-    inline void batch<std::complex<T>, A>::store(U* mem, unaligned_mode) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store(U* mem, unaligned_mode) const noexcept\n     {\n         return store_unaligned(mem);\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::real() const noexcept -> real_batch\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::real() const noexcept -> real_batch\n     {\n         return m_real;\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::imag() const noexcept -> real_batch\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::imag() const noexcept -> real_batch\n     {\n         return m_imag;\n     }\n \n     template <class T, class A>\n-    inline auto batch<std::complex<T>, A>::get(std::size_t i) const noexcept -> value_type\n+    XSIMD_INLINE auto batch<std::complex<T>, A>::get(std::size_t i) const noexcept -> value_type\n     {\n         return kernel::get(*this, i, A {});\n     }\n@@ -1260,15 +1260,15 @@ namespace xsimd\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> const& val) noexcept\n         : m_real(val.real())\n         , m_imag(val.imag())\n     {\n     }\n \n     template <class T, class A>\n     template <bool i3ec, class... Ts>\n-    inline batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>::batch(xtl::xcomplex<T, T, i3ec> val0, xtl::xcomplex<T, T, i3ec> val1, Ts... vals) noexcept\n         : batch(kernel::set<A>(batch {}, A {}, val0, val1, static_cast<xtl::xcomplex<T, T, i3ec>>(vals)...))\n     {\n         static_assert(sizeof...(Ts) + 2 == size, \"as many arguments as batch elements\");\n@@ -1280,28 +1280,28 @@ namespace xsimd\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_aligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n     {\n         return load_aligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::load_unaligned(const xtl::xcomplex<T, T, i3ec>* src) noexcept\n     {\n         return load_unaligned(reinterpret_cast<std::complex<T> const*>(src));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_aligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n     {\n         store_aligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n \n     template <class T, class A>\n     template <bool i3ec>\n-    inline void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n+    XSIMD_INLINE void batch<std::complex<T>, A>::store_unaligned(xtl::xcomplex<T, T, i3ec>* dst) const noexcept\n     {\n         store_unaligned(reinterpret_cast<std::complex<T>*>(dst));\n     }\n@@ -1313,13 +1313,13 @@ namespace xsimd\n      ***************************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator==(batch const& other) const noexcept\n     {\n         return m_real == other.m_real && m_imag == other.m_imag;\n     }\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator!=(batch const& other) const noexcept\n     {\n         return m_real != other.m_real || m_imag != other.m_imag;\n     }\n@@ -1329,23 +1329,23 @@ namespace xsimd\n      ***********************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator+=(batch const& other) noexcept\n     {\n         m_real += other.m_real;\n         m_imag += other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator-=(batch const& other) noexcept\n     {\n         m_real -= other.m_real;\n         m_imag -= other.m_imag;\n         return *this;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator*=(batch const& other) noexcept\n     {\n         real_batch new_real = fms(real(), other.real(), imag() * other.imag());\n         real_batch new_imag = fma(real(), other.imag(), imag() * other.real());\n@@ -1355,7 +1355,7 @@ namespace xsimd\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator/=(batch const& other) noexcept\n     {\n         real_batch a = real();\n         real_batch b = imag();\n@@ -1372,27 +1372,27 @@ namespace xsimd\n      **************************************/\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++() noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator++() noexcept\n     {\n         return operator+=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--() noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A>& batch<std::complex<T>, A>::operator--() noexcept\n     {\n         return operator-=(1);\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator++(int) noexcept\n     {\n         batch copy(*this);\n         operator+=(1);\n         return copy;\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int) noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator--(int) noexcept\n     {\n         batch copy(*this);\n         operator-=(1);\n@@ -1404,25 +1404,25 @@ namespace xsimd\n      **********************************/\n \n     template <class T, class A>\n-    inline batch_bool<T, A> batch<std::complex<T>, A>::operator!() const noexcept\n+    XSIMD_INLINE batch_bool<T, A> batch<std::complex<T>, A>::operator!() const noexcept\n     {\n         return operator==(batch(0));\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator~() const noexcept\n     {\n         return { ~m_real, ~m_imag };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator-() const noexcept\n     {\n         return { -m_real, -m_imag };\n     }\n \n     template <class T, class A>\n-    inline batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const noexcept\n+    XSIMD_INLINE batch<std::complex<T>, A> batch<std::complex<T>, A>::operator+() const noexcept\n     {\n         return { +m_real, +m_imag };\n     }"
      },
      {
        "filename": "include/xsimd/types/xsimd_batch_constant.hpp",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -138,12 +138,12 @@ namespace xsimd\n         /**\n          * @brief Generate a batch of @p batch_type from this @p batch_constant\n          */\n-        inline batch_type as_batch() const noexcept { return { Values... }; }\n+        XSIMD_INLINE batch_type as_batch() const noexcept { return { Values... }; }\n \n         /**\n          * @brief Generate a batch of @p batch_type from this @p batch_constant\n          */\n-        inline operator batch_type() const noexcept { return as_batch(); }\n+        XSIMD_INLINE operator batch_type() const noexcept { return as_batch(); }\n \n         /**\n          * @brief Get the @p i th element of this @p batch_constant\n@@ -246,13 +246,13 @@ namespace xsimd\n     namespace detail\n     {\n         template <typename T, class A, class G, std::size_t... Is>\n-        inline constexpr auto make_batch_constant(detail::index_sequence<Is...>) noexcept\n+        XSIMD_INLINE constexpr auto make_batch_constant(detail::index_sequence<Is...>) noexcept\n             -> batch_constant<T, A, (T)G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n         }\n         template <typename T, class A, class G, std::size_t... Is>\n-        inline constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>) noexcept\n+        XSIMD_INLINE constexpr auto make_batch_bool_constant(detail::index_sequence<Is...>) noexcept\n             -> batch_bool_constant<T, A, G::get(Is, sizeof...(Is))...>\n         {\n             return {};\n@@ -281,13 +281,13 @@ namespace xsimd\n      * @endcode\n      */\n     template <typename T, class A, class G>\n-    inline constexpr auto make_batch_constant() noexcept -> decltype(detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>()))\n+    XSIMD_INLINE constexpr auto make_batch_constant() noexcept -> decltype(detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>()))\n     {\n         return detail::make_batch_constant<T, A, G>(detail::make_index_sequence<batch<T, A>::size>());\n     }\n \n     template <typename T, class A, class G>\n-    inline constexpr auto make_batch_bool_constant() noexcept\n+    XSIMD_INLINE constexpr auto make_batch_bool_constant() noexcept\n         -> decltype(detail::make_batch_bool_constant<T, A, G>(\n             detail::make_index_sequence<batch<T, A>::size>()))\n     {"
      },
      {
        "filename": "include/xsimd/types/xsimd_emulated_register.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -55,7 +55,7 @@ namespace xsimd\n             static_assert(N % (8 * sizeof(T)) == 0, \"bit width must be a multiple of scalar width\");\n             using register_type = std::array<T, N / (8 * sizeof(T))>;\n             register_type data;\n-            inline operator register_type() const noexcept\n+            XSIMD_INLINE operator register_type() const noexcept\n             {\n                 return data;\n             }"
      },
      {
        "filename": "include/xsimd/types/xsimd_register.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -37,7 +37,7 @@ namespace xsimd\n     {                                                              \\\n         using register_type = VECTOR_TYPE;                         \\\n         register_type data;                                        \\\n-        inline operator register_type() const noexcept             \\\n+        XSIMD_INLINE operator register_type() const noexcept       \\\n         {                                                          \\\n             return data;                                           \\\n         }                                                          \\"
      },
      {
        "filename": "include/xsimd/types/xsimd_rvv_register.hpp",
        "status": "modified",
        "additions": 14,
        "deletions": 14,
        "changes": 28,
        "patch": "@@ -88,14 +88,14 @@ namespace xsimd\n         using byte_type = XSIMD_RVV_TYPE(u, 8, vmul);                                     \\\n         using fixed_type = type __attribute__((riscv_rvv_vector_bits(width)));            \\\n         template <class U>                                                                \\\n-        static inline type bitcast(U x) noexcept                                          \\\n+        static XSIMD_INLINE type bitcast(U x) noexcept                                    \\\n         {                                                                                 \\\n             const auto words = XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, s, m, vmul)(x); \\\n             return XSIMD_RVV_JOINT5(__riscv_vreinterpret_, t, s, m, vmul)(words);         \\\n         }                                                                                 \\\n         template <>                                                                       \\\n-        inline type bitcast<type>(type x) noexcept { return x; }                          \\\n-        static inline byte_type as_bytes(type x) noexcept                                 \\\n+        XSIMD_INLINE type bitcast<type>(type x) noexcept { return x; }                    \\\n+        static XSIMD_INLINE byte_type as_bytes(type x) noexcept                           \\\n         {                                                                                 \\\n             const auto words = XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, s, m, vmul)(x); \\\n             return XSIMD_RVV_JOINT5(__riscv_vreinterpret_, u, 8, m, vmul)(words);         \\\n@@ -267,17 +267,17 @@ namespace xsimd\n             //\n             template <size_t>\n             struct rvv_bool_info;\n-#define XSIMD_RVV_MAKE_BOOL_TYPE(i)                                                       \\\n-    template <>                                                                           \\\n-    struct rvv_bool_info<i>                                                               \\\n-    {                                                                                     \\\n-        using type = XSIMD_RVV_JOINT(vbool, i, _t);                                       \\\n-        template <class T>                                                                \\\n-        static inline type bitcast(T value) noexcept                                      \\\n-        {                                                                                 \\\n-            return XSIMD_RVV_JOINT(__riscv_vreinterpret_b, i, )(value);                   \\\n-        }                                                                                 \\\n-        /*template <> static inline type bitcast(type value) noexcept { return value; }*/ \\\n+#define XSIMD_RVV_MAKE_BOOL_TYPE(i)                                                             \\\n+    template <>                                                                                 \\\n+    struct rvv_bool_info<i>                                                                     \\\n+    {                                                                                           \\\n+        using type = XSIMD_RVV_JOINT(vbool, i, _t);                                             \\\n+        template <class T>                                                                      \\\n+        static XSIMD_INLINE type bitcast(T value) noexcept                                      \\\n+        {                                                                                       \\\n+            return XSIMD_RVV_JOINT(__riscv_vreinterpret_b, i, )(value);                         \\\n+        }                                                                                       \\\n+        /*template <> static XSIMD_INLINE type bitcast(type value) noexcept { return value; }*/ \\\n     };\n             XSIMD_RVV_MAKE_BOOL_TYPE(1);\n             XSIMD_RVV_MAKE_BOOL_TYPE(2);"
      },
      {
        "filename": "include/xsimd/types/xsimd_traits.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -86,7 +86,7 @@ namespace xsimd\n \n         // consistency checker\n         template <class T, class A>\n-        inline void static_check_supported_config()\n+        XSIMD_INLINE void static_check_supported_config()\n         {\n             (void)static_check_supported_config_emitter<T, A>();\n         }"
      },
      {
        "filename": "include/xsimd/xsimd.hpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -51,6 +51,7 @@\n #endif\n \n #include \"config/xsimd_config.hpp\"\n+#include \"config/xsimd_inline.hpp\"\n \n #include \"arch/xsimd_scalar.hpp\"\n #include \"memory/xsimd_aligned_allocator.hpp\""
      }
    ],
    "lines_added": 3049,
    "lines_removed": 3020
  },
  "issues": [
    {
      "number": 644,
      "url": "https://github.com/xtensor-stack/xsimd/issues/644",
      "title": "Implement an inline macro to have maximum performance on MSVC",
      "body": ":wave:\r\n\r\nWhile debugging the still bad codegen of MSVC, I realised that on reasonably big functions, it doesn't inline any of the xsimd functions even when marked as `inline`. The real workaround is to mark all functions with `__forceinline`.\r\n\r\nWould it be possible to implement such a macro?",
      "created_at": "2021-11-18T20:39:07+00:00"
    }
  ],
  "pull_requests": [],
  "build_info": {
    "old_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
    "new_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/new -B /test_workspace/workspace/new/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
    "old_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/old/build -- -j 1",
    "new_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/new/build -- -j 1",
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": true,
    "p_value": 1.3065663740489714e-47,
    "is_pair_significant": true,
    "pair_p_value": 3.8243014989834345e-38,
    "is_binom_significant": true,
    "binom_p_value": 9.313225746154785e-10,
    "is_wilcoxon_significant": true,
    "wilcoxon_p_value": 9.313225746154785e-10,
    "is_mannwhitney_significant": false,
    "mannwhitney_p_value": 1.5070924614036282e-11,
    "relative_improvement": 0.22613952237848095,
    "absolute_improvement_ms": 1738.266666666669,
    "old_mean_ms": 7686.699999999998,
    "new_mean_ms": 5948.43333333333,
    "old_std_ms": 29.5590000297484,
    "new_std_ms": 70.31882238497202,
    "effect_size_cohens_d": 32.22752493219426,
    "old_ci95_ms": [
      7675.6624879924575,
      7697.737512007539
    ],
    "new_ci95_ms": [
      5922.175853525177,
      5974.690813141481
    ],
    "old_ci99_ms": [
      7671.824583625017,
      7701.57541637498
    ],
    "new_ci99_ms": [
      5913.045743735505,
      5983.820922931153
    ],
    "new_times_s": [
      6.218999999999992,
      5.927999999999999,
      5.928999999999998,
      5.905999999999993,
      5.921999999999992,
      5.990999999999997,
      5.890999999999993,
      5.892999999999993,
      5.940999999999995,
      5.917999999999996,
      5.933999999999999,
      5.888999999999996,
      6.288999999999996,
      5.925999999999993,
      5.959999999999997,
      5.934999999999996,
      5.955999999999998,
      5.947999999999994,
      5.934999999999996,
      5.957999999999998,
      5.961999999999999,
      5.956999999999996,
      5.917999999999996,
      5.949999999999998,
      5.914999999999999,
      6.008999999999995,
      5.980999999999995,
      5.899999999999994,
      5.9489999999999945,
      5.939,
      5.923999999999994
    ],
    "old_times_s": [
      7.719,
      7.610999999999992,
      7.652999999999995,
      7.677999999999995,
      7.665999999999998,
      7.645999999999998,
      7.68,
      7.715999999999992,
      7.709,
      7.666999999999996,
      7.660000000000003,
      7.698000000000001,
      7.671999999999996,
      7.676000000000001,
      7.700000000000001,
      7.656999999999999,
      7.6629999999999985,
      7.7529999999999974,
      7.683,
      7.736,
      7.706000000000004,
      7.673999999999998,
      7.697000000000001,
      7.695999999999998,
      7.697000000000001,
      7.697999999999999,
      7.732999999999998,
      7.705999999999998,
      7.701000000000001,
      7.7090000000000005,
      7.66
    ]
  },
  "tests": {
    "total_tests": 35,
    "significant_improvements": 22,
    "significant_improvements_tests": [
      "[complex exponential]<xsimd::batch<std::complex<float> >>",
      "[complex exponential]<xsimd::batch<std::complex<double> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
      "[complex power]<xsimd::batch<std::complex<float> >>",
      "[complex power]<xsimd::batch<std::complex<double> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
      "[error gamma]<xsimd::batch<float>>",
      "[error gamma]<xsimd::batch<double>>",
      "[exponential]<xsimd::batch<float>>",
      "[exponential]<xsimd::batch<double>>",
      "[hyperbolic]<xsimd::batch<float>>",
      "[hyperbolic]<xsimd::batch<double>>",
      "[poly evaluation]<xsimd::batch<float>>",
      "[poly evaluation]<xsimd::batch<double>>",
      "[power]<xsimd::batch<float>>",
      "[power]<xsimd::batch<double>>",
      "[select]<xsimd::batch<unsigned int>>",
      "[select]<xsimd::batch<long unsigned int>>",
      "[trigonometric]<xsimd::batch<float>>",
      "[trigonometric]<xsimd::batch<double>>"
    ],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 22,
    "significant_pair_improvements_tests": [
      "[complex exponential]<xsimd::batch<std::complex<float> >>",
      "[complex exponential]<xsimd::batch<std::complex<double> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
      "[complex power]<xsimd::batch<std::complex<float> >>",
      "[complex power]<xsimd::batch<std::complex<double> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
      "[error gamma]<xsimd::batch<float>>",
      "[error gamma]<xsimd::batch<double>>",
      "[exponential]<xsimd::batch<float>>",
      "[exponential]<xsimd::batch<double>>",
      "[hyperbolic]<xsimd::batch<float>>",
      "[hyperbolic]<xsimd::batch<double>>",
      "[poly evaluation]<xsimd::batch<float>>",
      "[poly evaluation]<xsimd::batch<double>>",
      "[power]<xsimd::batch<float>>",
      "[power]<xsimd::batch<double>>",
      "[select]<xsimd::batch<unsigned int>>",
      "[select]<xsimd::batch<long unsigned int>>",
      "[trigonometric]<xsimd::batch<float>>",
      "[trigonometric]<xsimd::batch<double>>"
    ],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 23,
    "significant_binom_improvements_tests": [
      "[complex exponential]<xsimd::batch<std::complex<float> >>",
      "[complex exponential]<xsimd::batch<std::complex<double> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
      "[complex power]<xsimd::batch<std::complex<float> >>",
      "[complex power]<xsimd::batch<std::complex<double> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
      "[error gamma]<xsimd::batch<float>>",
      "[error gamma]<xsimd::batch<double>>",
      "[exponential]<xsimd::batch<float>>",
      "[exponential]<xsimd::batch<double>>",
      "[hyperbolic]<xsimd::batch<float>>",
      "[hyperbolic]<xsimd::batch<double>>",
      "[poly evaluation]<xsimd::batch<float>>",
      "[poly evaluation]<xsimd::batch<double>>",
      "[power]<xsimd::batch<float>>",
      "[power]<xsimd::batch<double>>",
      "[select]<xsimd::batch<unsigned int>>",
      "[select]<xsimd::batch<long unsigned int>>",
      "[select]<xsimd::batch<long int>>",
      "[trigonometric]<xsimd::batch<float>>",
      "[trigonometric]<xsimd::batch<double>>"
    ],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 22,
    "significant_wilcoxon_improvements_tests": [
      "[complex exponential]<xsimd::batch<std::complex<float> >>",
      "[complex exponential]<xsimd::batch<std::complex<double> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
      "[complex power]<xsimd::batch<std::complex<float> >>",
      "[complex power]<xsimd::batch<std::complex<double> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
      "[error gamma]<xsimd::batch<float>>",
      "[error gamma]<xsimd::batch<double>>",
      "[exponential]<xsimd::batch<float>>",
      "[exponential]<xsimd::batch<double>>",
      "[hyperbolic]<xsimd::batch<float>>",
      "[hyperbolic]<xsimd::batch<double>>",
      "[poly evaluation]<xsimd::batch<float>>",
      "[poly evaluation]<xsimd::batch<double>>",
      "[power]<xsimd::batch<float>>",
      "[power]<xsimd::batch<double>>",
      "[select]<xsimd::batch<unsigned int>>",
      "[select]<xsimd::batch<long unsigned int>>",
      "[trigonometric]<xsimd::batch<float>>",
      "[trigonometric]<xsimd::batch<double>>"
    ],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 28,
    "significant_mannwhitney_improvements_tests": [
      "[complex exponential]<xsimd::batch<std::complex<float> >>",
      "[complex exponential]<xsimd::batch<std::complex<double> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
      "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
      "[complex power]<xsimd::batch<std::complex<float> >>",
      "[complex power]<xsimd::batch<std::complex<double> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
      "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
      "[error gamma]<xsimd::batch<float>>",
      "[error gamma]<xsimd::batch<double>>",
      "[exponential]<xsimd::batch<float>>",
      "[exponential]<xsimd::batch<double>>",
      "[hyperbolic]<xsimd::batch<float>>",
      "[hyperbolic]<xsimd::batch<double>>",
      "[poly evaluation]<xsimd::batch<float>>",
      "[poly evaluation]<xsimd::batch<double>>",
      "[power]<xsimd::batch<float>>",
      "[power]<xsimd::batch<double>>",
      "[select]<xsimd::batch<unsigned char>>",
      "[select]<xsimd::batch<short unsigned int>>",
      "[select]<xsimd::batch<short int>>",
      "[select]<xsimd::batch<unsigned int>>",
      "[select]<xsimd::batch<int>>",
      "[select]<xsimd::batch<long unsigned int>>",
      "[select]<xsimd::batch<long int>>",
      "[select]<xsimd::batch<double>>",
      "[trigonometric]<xsimd::batch<float>>",
      "[trigonometric]<xsimd::batch<double>>"
    ],
    "significant_mannwhitney_regressions": 2,
    "significant_mannwhitney_regressions_tests": [
      "[select]<xsimd::batch<signed char>>",
      "[xsimd api | float types functions]<xsimd::batch<double>>"
    ],
    "tests": [
      {
        "test_name": "[complex exponential]<xsimd::batch<std::complex<float> >>",
        "is_significant": true,
        "p_value": 4.0067514820789534e-62,
        "is_pair_significant": true,
        "pair_p_value": 6.693836796785668e-39,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2074416764977363e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.993993144194565e-11,
        "relative_improvement": 0.23516720604099242,
        "absolute_improvement_ms": 112.7586206896552,
        "old_mean_ms": 479.4827586206896,
        "new_mean_ms": 366.7241379310344,
        "old_std_ms": 3.6608028945181332,
        "new_std_ms": 2.4186264065748744,
        "effect_size_cohens_d": 36.34423182702197,
        "old_ci95_ms": [
          478.09026367162124,
          480.875253569758
        ],
        "new_ci95_ms": [
          365.8041416482839,
          367.6441342137849
        ],
        "old_ci99_ms": [
          477.6043092960072,
          481.36120794537203
        ],
        "new_ci99_ms": [
          365.4830803622446,
          367.96519549982423
        ],
        "new_times": [
          0.371,
          0.371,
          0.367,
          0.37,
          0.367,
          0.367,
          0.365,
          0.364,
          0.368,
          0.365,
          0.366,
          0.364,
          0.363,
          0.37,
          0.363,
          0.367,
          0.368,
          0.366,
          0.371,
          0.368,
          0.366,
          0.364,
          0.368,
          0.368,
          0.365,
          0.367,
          0.365,
          0.368,
          0.363
        ],
        "old_times": [
          0.483,
          0.479,
          0.473,
          0.476,
          0.484,
          0.482,
          0.481,
          0.48,
          0.484,
          0.482,
          0.482,
          0.481,
          0.479,
          0.474,
          0.477,
          0.48,
          0.471,
          0.479,
          0.484,
          0.481,
          0.48,
          0.479,
          0.483,
          0.472,
          0.478,
          0.483,
          0.482,
          0.481,
          0.475
        ]
      },
      {
        "test_name": "[complex exponential]<xsimd::batch<std::complex<double> >>",
        "is_significant": true,
        "p_value": 9.839444143610829e-53,
        "is_pair_significant": true,
        "pair_p_value": 1.5611448492242494e-43,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2584159023132137e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.752389029650988e-11,
        "relative_improvement": 0.3312716476991588,
        "absolute_improvement_ms": 138.51724137931032,
        "old_mean_ms": 418.13793103448273,
        "new_mean_ms": 279.6206896551724,
        "old_std_ms": 4.077150071969345,
        "new_std_ms": 1.4979460486052196,
        "effect_size_cohens_d": 45.099057265507305,
        "old_ci95_ms": [
          416.5870660888958,
          419.6887959800696
        ],
        "new_ci95_ms": [
          279.05090145111933,
          280.19047785922544
        ],
        "old_ci99_ms": [
          416.0458435826322,
          420.2300184863332
        ],
        "new_ci99_ms": [
          278.85205615468357,
          280.3893231556612
        ],
        "new_times": [
          0.281,
          0.277,
          0.281,
          0.283,
          0.28,
          0.282,
          0.279,
          0.28,
          0.277,
          0.28,
          0.279,
          0.279,
          0.278,
          0.279,
          0.28,
          0.281,
          0.28,
          0.277,
          0.279,
          0.28,
          0.278,
          0.28,
          0.279,
          0.279,
          0.28,
          0.28,
          0.282,
          0.281,
          0.278
        ],
        "old_times": [
          0.418,
          0.419,
          0.422,
          0.416,
          0.423,
          0.426,
          0.416,
          0.423,
          0.417,
          0.424,
          0.416,
          0.422,
          0.419,
          0.415,
          0.419,
          0.418,
          0.418,
          0.422,
          0.417,
          0.419,
          0.411,
          0.42,
          0.41,
          0.412,
          0.419,
          0.42,
          0.419,
          0.417,
          0.409
        ]
      },
      {
        "test_name": "[complex hyperbolic]<xsimd::batch<std::complex<float> >>",
        "is_significant": true,
        "p_value": 1.2432854743288004e-51,
        "is_pair_significant": true,
        "pair_p_value": 3.269491737224344e-39,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.260116955178722e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.959767094528807e-11,
        "relative_improvement": 0.27525047386948287,
        "absolute_improvement_ms": 140.20689655172401,
        "old_mean_ms": 509.3793103448275,
        "new_mean_ms": 369.1724137931035,
        "old_std_ms": 5.512413219941808,
        "new_std_ms": 2.564440422353962,
        "effect_size_cohens_d": 32.61372836876505,
        "old_ci95_ms": [
          507.282500495425,
          511.4761201942301
        ],
        "new_ci95_ms": [
          368.1969528250568,
          370.1478747611502
        ],
        "old_ci99_ms": [
          506.55075355312596,
          512.2078671365291
        ],
        "new_ci99_ms": [
          367.8565354138626,
          370.4882921723444
        ],
        "new_times": [
          0.371,
          0.372,
          0.371,
          0.37,
          0.368,
          0.37,
          0.368,
          0.37,
          0.369,
          0.365,
          0.365,
          0.366,
          0.369,
          0.37,
          0.368,
          0.368,
          0.37,
          0.374,
          0.367,
          0.37,
          0.367,
          0.368,
          0.37,
          0.369,
          0.367,
          0.372,
          0.371,
          0.376,
          0.365
        ],
        "old_times": [
          0.502,
          0.51,
          0.512,
          0.506,
          0.512,
          0.514,
          0.51,
          0.517,
          0.502,
          0.504,
          0.511,
          0.511,
          0.5,
          0.514,
          0.511,
          0.503,
          0.51,
          0.507,
          0.51,
          0.506,
          0.5,
          0.513,
          0.501,
          0.511,
          0.514,
          0.511,
          0.518,
          0.522,
          0.51
        ]
      },
      {
        "test_name": "[complex hyperbolic]<xsimd::batch<std::complex<double> >>",
        "is_significant": true,
        "p_value": 9.77249855413345e-53,
        "is_pair_significant": true,
        "pair_p_value": 7.587649168970664e-41,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2643779376908553e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.8015259461742245e-11,
        "relative_improvement": 0.3178896814746803,
        "absolute_improvement_ms": 155.206896551724,
        "old_mean_ms": 488.24137931034477,
        "new_mean_ms": 333.0344827586208,
        "old_std_ms": 5.089591426303251,
        "new_std_ms": 2.0785083700420794,
        "effect_size_cohens_d": 39.92537938104797,
        "old_ci95_ms": [
          486.3054022697448,
          490.17735635094476
        ],
        "new_ci95_ms": [
          332.2438604579189,
          333.82510505932265
        ],
        "old_ci99_ms": [
          485.62978293295856,
          490.85297568773103
        ],
        "new_ci99_ms": [
          331.96794824240885,
          334.10101727483277
        ],
        "new_times": [
          0.332,
          0.335,
          0.333,
          0.334,
          0.332,
          0.333,
          0.338,
          0.333,
          0.332,
          0.331,
          0.333,
          0.332,
          0.333,
          0.335,
          0.336,
          0.333,
          0.331,
          0.338,
          0.333,
          0.337,
          0.33,
          0.331,
          0.332,
          0.331,
          0.332,
          0.332,
          0.331,
          0.333,
          0.332
        ],
        "old_times": [
          0.497,
          0.489,
          0.484,
          0.49,
          0.49,
          0.488,
          0.491,
          0.482,
          0.489,
          0.489,
          0.489,
          0.492,
          0.493,
          0.49,
          0.479,
          0.481,
          0.492,
          0.492,
          0.487,
          0.49,
          0.485,
          0.478,
          0.493,
          0.488,
          0.491,
          0.488,
          0.484,
          0.479,
          0.499
        ]
      },
      {
        "test_name": "[complex power]<xsimd::batch<std::complex<float> >>",
        "is_significant": true,
        "p_value": 8.067012164371686e-24,
        "is_pair_significant": true,
        "pair_p_value": 4.000987648933004e-21,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2339640055193641e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.8263925101406593e-11,
        "relative_improvement": 0.1251164647943564,
        "absolute_improvement_ms": 32.41379310344827,
        "old_mean_ms": 259.0689655172414,
        "new_mean_ms": 226.65517241379314,
        "old_std_ms": 1.9627064856387673,
        "new_std_ms": 4.3034526177823285,
        "effect_size_cohens_d": 9.691549387264697,
        "old_ci95_ms": [
          258.3223918975995,
          259.8155391368833
        ],
        "new_ci95_ms": [
          225.01822658346978,
          228.29211824411647
        ],
        "old_ci99_ms": [
          258.06185183788466,
          260.07607919659813
        ],
        "new_ci99_ms": [
          224.4469634780482,
          228.86338134953806
        ],
        "new_times": [
          0.225,
          0.231,
          0.222,
          0.234,
          0.221,
          0.223,
          0.228,
          0.226,
          0.226,
          0.226,
          0.231,
          0.222,
          0.232,
          0.22,
          0.221,
          0.229,
          0.226,
          0.231,
          0.23,
          0.231,
          0.221,
          0.229,
          0.22,
          0.232,
          0.229,
          0.223,
          0.223,
          0.231,
          0.23
        ],
        "old_times": [
          0.258,
          0.258,
          0.261,
          0.265,
          0.259,
          0.258,
          0.259,
          0.255,
          0.257,
          0.257,
          0.256,
          0.259,
          0.26,
          0.262,
          0.258,
          0.26,
          0.258,
          0.26,
          0.26,
          0.259,
          0.258,
          0.259,
          0.258,
          0.262,
          0.26,
          0.258,
          0.259,
          0.261,
          0.259
        ]
      },
      {
        "test_name": "[complex power]<xsimd::batch<std::complex<double> >>",
        "is_significant": true,
        "p_value": 3.997601453540109e-62,
        "is_pair_significant": true,
        "pair_p_value": 3.38090651625188e-38,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2499391973796608e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.919957031992287e-11,
        "relative_improvement": 0.31189245087900724,
        "absolute_improvement_ms": 103.99999999999996,
        "old_mean_ms": 333.448275862069,
        "new_mean_ms": 229.44827586206904,
        "old_std_ms": 4.049569702860435,
        "new_std_ms": 3.2578060857900706,
        "effect_size_cohens_d": 28.2987617094515,
        "old_ci95_ms": [
          331.9079019278215,
          334.9886497963165
        ],
        "new_ci95_ms": [
          228.20907270084444,
          230.6874790232936
        ],
        "old_ci99_ms": [
          331.37034058590774,
          335.5262111382302
        ],
        "new_ci99_ms": [
          227.77661425718932,
          231.11993746694876
        ],
        "new_times": [
          0.229,
          0.228,
          0.229,
          0.226,
          0.228,
          0.23,
          0.231,
          0.23,
          0.231,
          0.233,
          0.234,
          0.226,
          0.229,
          0.229,
          0.229,
          0.229,
          0.227,
          0.227,
          0.228,
          0.227,
          0.24,
          0.229,
          0.225,
          0.225,
          0.227,
          0.229,
          0.23,
          0.234,
          0.235
        ],
        "old_times": [
          0.337,
          0.335,
          0.322,
          0.331,
          0.333,
          0.34,
          0.333,
          0.327,
          0.333,
          0.334,
          0.332,
          0.332,
          0.337,
          0.338,
          0.334,
          0.338,
          0.331,
          0.334,
          0.34,
          0.327,
          0.334,
          0.333,
          0.329,
          0.332,
          0.331,
          0.334,
          0.333,
          0.339,
          0.337
        ]
      },
      {
        "test_name": "[complex trigonometric]<xsimd::batch<std::complex<float> >>",
        "is_significant": true,
        "p_value": 1.325122565298404e-55,
        "is_pair_significant": true,
        "pair_p_value": 2.5720108554069567e-35,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2626721121617209e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 3.092583755488006e-11,
        "relative_improvement": 0.2739620403321471,
        "absolute_improvement_ms": 159.27586206896555,
        "old_mean_ms": 581.3793103448276,
        "new_mean_ms": 422.10344827586204,
        "old_std_ms": 7.232929623324195,
        "new_std_ms": 4.753531225657462,
        "effect_size_cohens_d": 26.025028163586686,
        "old_ci95_ms": [
          578.6280510561813,
          584.1305696334739
        ],
        "new_ci95_ms": [
          420.2953016990882,
          423.9115948526359
        ],
        "old_ci99_ms": [
          577.6679136491044,
          585.0907070405508
        ],
        "new_ci99_ms": [
          419.6642927741922,
          424.5426037775319
        ],
        "new_times": [
          0.421,
          0.416,
          0.422,
          0.419,
          0.422,
          0.424,
          0.42,
          0.423,
          0.435,
          0.423,
          0.424,
          0.429,
          0.421,
          0.418,
          0.42,
          0.419,
          0.42,
          0.42,
          0.421,
          0.436,
          0.419,
          0.424,
          0.421,
          0.424,
          0.428,
          0.418,
          0.417,
          0.418,
          0.419
        ],
        "old_times": [
          0.591,
          0.579,
          0.579,
          0.577,
          0.573,
          0.585,
          0.582,
          0.582,
          0.582,
          0.595,
          0.587,
          0.577,
          0.578,
          0.575,
          0.573,
          0.588,
          0.581,
          0.602,
          0.577,
          0.575,
          0.583,
          0.582,
          0.59,
          0.584,
          0.585,
          0.579,
          0.575,
          0.577,
          0.567
        ]
      },
      {
        "test_name": "[complex trigonometric]<xsimd::batch<std::complex<double> >>",
        "is_significant": true,
        "p_value": 1.683995679138475e-60,
        "is_pair_significant": true,
        "pair_p_value": 1.7083017381242932e-47,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2550195134244884e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.638365103153905e-11,
        "relative_improvement": 0.3650171484566388,
        "absolute_improvement_ms": 231.20689655172404,
        "old_mean_ms": 633.4137931034483,
        "new_mean_ms": 402.2068965517242,
        "old_std_ms": 5.053409325673899,
        "new_std_ms": 2.127696781982595,
        "effect_size_cohens_d": 59.63374942108233,
        "old_ci95_ms": [
          631.4915789978724,
          635.3360072090242
        ],
        "new_ci95_ms": [
          401.39756397976396,
          403.01622912368447
        ],
        "old_ci99_ms": [
          630.8207626648605,
          636.006823542036
        ],
        "new_ci99_ms": [
          401.11512223379276,
          403.29867086965567
        ],
        "new_times": [
          0.401,
          0.403,
          0.404,
          0.406,
          0.403,
          0.404,
          0.401,
          0.41,
          0.4,
          0.401,
          0.401,
          0.402,
          0.4,
          0.401,
          0.401,
          0.402,
          0.404,
          0.403,
          0.404,
          0.401,
          0.401,
          0.403,
          0.402,
          0.4,
          0.401,
          0.4,
          0.401,
          0.403,
          0.401
        ],
        "old_times": [
          0.63,
          0.638,
          0.644,
          0.641,
          0.631,
          0.622,
          0.634,
          0.64,
          0.633,
          0.634,
          0.623,
          0.635,
          0.634,
          0.635,
          0.629,
          0.629,
          0.632,
          0.629,
          0.641,
          0.635,
          0.641,
          0.631,
          0.634,
          0.631,
          0.632,
          0.631,
          0.631,
          0.632,
          0.637
        ]
      },
      {
        "test_name": "[error gamma]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 4.3172600891188294e-27,
        "is_pair_significant": true,
        "pair_p_value": 5.591913492988356e-23,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.260116955178722e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.756140762031952e-11,
        "relative_improvement": 0.3092770313499681,
        "absolute_improvement_ms": 83.34482758620697,
        "old_mean_ms": 269.4827586206897,
        "new_mean_ms": 186.13793103448273,
        "old_std_ms": 12.26382802523389,
        "new_std_ms": 4.172393437234068,
        "effect_size_cohens_d": 9.098803488129157,
        "old_ci95_ms": [
          264.81784792392585,
          274.14766931745356
        ],
        "new_ci95_ms": [
          184.55083745029256,
          187.7250246186729
        ],
        "old_ci99_ms": [
          263.18988240314786,
          275.77563483823155
        ],
        "new_ci99_ms": [
          183.99697183500822,
          188.27889023395724
        ],
        "new_times": [
          0.183,
          0.185,
          0.183,
          0.184,
          0.185,
          0.183,
          0.186,
          0.183,
          0.187,
          0.183,
          0.198,
          0.183,
          0.193,
          0.192,
          0.184,
          0.183,
          0.188,
          0.195,
          0.192,
          0.183,
          0.182,
          0.19,
          0.183,
          0.184,
          0.186,
          0.185,
          0.186,
          0.183,
          0.186
        ],
        "old_times": [
          0.262,
          0.264,
          0.261,
          0.268,
          0.263,
          0.262,
          0.271,
          0.262,
          0.264,
          0.268,
          0.266,
          0.264,
          0.263,
          0.262,
          0.265,
          0.264,
          0.262,
          0.281,
          0.27,
          0.265,
          0.264,
          0.262,
          0.263,
          0.295,
          0.312,
          0.27,
          0.277,
          0.267,
          0.298
        ]
      },
      {
        "test_name": "[error gamma]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 0.00012591687634075791,
        "is_pair_significant": true,
        "pair_p_value": 0.00011892867519032914,
        "is_binom_significant": true,
        "binom_p_value": 5.587935447692871e-08,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 2.16152049066396e-05,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 3.5820219034131075e-10,
        "relative_improvement": 0.29756706175920156,
        "absolute_improvement_ms": 65.79310344827593,
        "old_mean_ms": 221.10344827586212,
        "new_mean_ms": 155.3103448275862,
        "old_std_ms": 2.127117895621192,
        "new_std_ms": 70.3354733952505,
        "effect_size_cohens_d": 1.3222769944204245,
        "old_ci95_ms": [
          220.29433590049743,
          221.9125606512268
        ],
        "new_ci95_ms": [
          128.5561615881981,
          182.06452806697428
        ],
        "old_ci99_ms": [
          220.01197099896947,
          222.19492555275477
        ],
        "new_ci99_ms": [
          119.219458126647,
          191.40123152852536
        ],
        "new_times": [
          0.142,
          0.142,
          0.142,
          0.142,
          0.143,
          0.141,
          0.142,
          0.142,
          0.142,
          0.142,
          0.521,
          0.144,
          0.142,
          0.143,
          0.143,
          0.143,
          0.141,
          0.142,
          0.142,
          0.143,
          0.142,
          0.141,
          0.142,
          0.142,
          0.143,
          0.143,
          0.142,
          0.143,
          0.142
        ],
        "old_times": [
          0.221,
          0.218,
          0.223,
          0.221,
          0.223,
          0.219,
          0.227,
          0.219,
          0.223,
          0.22,
          0.223,
          0.223,
          0.22,
          0.221,
          0.22,
          0.22,
          0.219,
          0.218,
          0.224,
          0.219,
          0.22,
          0.218,
          0.222,
          0.22,
          0.224,
          0.222,
          0.221,
          0.222,
          0.222
        ]
      },
      {
        "test_name": "[exponential]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 2.4384802344786454e-34,
        "is_pair_significant": true,
        "pair_p_value": 4.474323080407393e-28,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.200885599279206e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.6473986200973763e-11,
        "relative_improvement": 0.2612667478684531,
        "absolute_improvement_ms": 44.379310344827644,
        "old_mean_ms": 169.8620689655173,
        "new_mean_ms": 125.48275862068964,
        "old_std_ms": 4.340211794791273,
        "new_std_ms": 1.8051650034429478,
        "effect_size_cohens_d": 13.351748257774437,
        "old_ci95_ms": [
          168.2111406920546,
          171.51299723897995
        ],
        "new_ci95_ms": [
          124.7961105759961,
          126.16940666538318
        ],
        "old_ci99_ms": [
          167.63499797868195,
          172.0891399523526
        ],
        "new_ci99_ms": [
          124.55648340747469,
          126.40903383390459
        ],
        "new_times": [
          0.124,
          0.125,
          0.125,
          0.123,
          0.126,
          0.125,
          0.124,
          0.125,
          0.125,
          0.125,
          0.124,
          0.126,
          0.124,
          0.123,
          0.124,
          0.126,
          0.132,
          0.125,
          0.125,
          0.124,
          0.127,
          0.129,
          0.126,
          0.126,
          0.127,
          0.127,
          0.126,
          0.126,
          0.125
        ],
        "old_times": [
          0.168,
          0.165,
          0.167,
          0.166,
          0.167,
          0.167,
          0.167,
          0.167,
          0.17,
          0.17,
          0.167,
          0.165,
          0.168,
          0.17,
          0.172,
          0.184,
          0.17,
          0.179,
          0.167,
          0.166,
          0.171,
          0.171,
          0.171,
          0.168,
          0.178,
          0.173,
          0.168,
          0.173,
          0.171
        ]
      },
      {
        "test_name": "[exponential]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 1.9097784503931284e-23,
        "is_pair_significant": true,
        "pair_p_value": 2.502592074852136e-16,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2533241740913273e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.8398652177143627e-11,
        "relative_improvement": 0.31578947368421045,
        "absolute_improvement_ms": 59.58620689655172,
        "old_mean_ms": 188.6896551724138,
        "new_mean_ms": 129.1034482758621,
        "old_std_ms": 10.313318186680002,
        "new_std_ms": 12.476677750295915,
        "effect_size_cohens_d": 5.20575297403953,
        "old_ci95_ms": [
          184.76667873885813,
          192.6126316059695
        ],
        "new_ci95_ms": [
          124.35757387370367,
          133.84932267802049
        ],
        "old_ci99_ms": [
          183.39763423022822,
          193.9816761145994
        ],
        "new_ci99_ms": [
          122.70135355248364,
          135.50554299924053
        ],
        "new_times": [
          0.138,
          0.124,
          0.129,
          0.169,
          0.125,
          0.122,
          0.124,
          0.123,
          0.122,
          0.123,
          0.124,
          0.127,
          0.126,
          0.123,
          0.139,
          0.126,
          0.124,
          0.124,
          0.124,
          0.123,
          0.122,
          0.131,
          0.122,
          0.147,
          0.169,
          0.124,
          0.123,
          0.124,
          0.123
        ],
        "old_times": [
          0.183,
          0.185,
          0.199,
          0.188,
          0.214,
          0.222,
          0.181,
          0.183,
          0.184,
          0.182,
          0.183,
          0.188,
          0.209,
          0.186,
          0.187,
          0.183,
          0.183,
          0.182,
          0.183,
          0.183,
          0.197,
          0.181,
          0.188,
          0.196,
          0.187,
          0.184,
          0.182,
          0.184,
          0.185
        ]
      },
      {
        "test_name": "[hyperbolic]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 8.224272189357055e-41,
        "is_pair_significant": true,
        "pair_p_value": 4.691311535622144e-31,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.1717479771100529e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.054164613683808e-11,
        "relative_improvement": 0.26941471191072647,
        "absolute_improvement_ms": 40.79310344827584,
        "old_mean_ms": 151.41379310344826,
        "new_mean_ms": 110.62068965517243,
        "old_std_ms": 1.239736186778415,
        "new_std_ms": 2.6242521806010104,
        "effect_size_cohens_d": 19.877039846188993,
        "old_ci95_ms": [
          150.94222267805023,
          151.88536352884628
        ],
        "new_ci95_ms": [
          109.62247751104397,
          111.61890179930089
        ],
        "old_ci99_ms": [
          150.77765352698393,
          152.04993267991262
        ],
        "new_ci99_ms": [
          109.2741203701044,
          111.96725894024046
        ],
        "new_times": [
          0.11,
          0.109,
          0.11,
          0.11,
          0.111,
          0.11,
          0.109,
          0.111,
          0.119,
          0.109,
          0.11,
          0.11,
          0.11,
          0.11,
          0.111,
          0.111,
          0.112,
          0.12,
          0.11,
          0.109,
          0.11,
          0.111,
          0.11,
          0.11,
          0.111,
          0.108,
          0.108,
          0.11,
          0.109
        ],
        "old_times": [
          0.151,
          0.152,
          0.151,
          0.151,
          0.15,
          0.154,
          0.152,
          0.153,
          0.151,
          0.151,
          0.15,
          0.153,
          0.152,
          0.151,
          0.152,
          0.152,
          0.152,
          0.15,
          0.152,
          0.151,
          0.149,
          0.151,
          0.153,
          0.151,
          0.153,
          0.151,
          0.153,
          0.15,
          0.149
        ]
      },
      {
        "test_name": "[hyperbolic]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 1.508989612923993e-67,
        "is_pair_significant": true,
        "pair_p_value": 4.394079331370688e-43,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.1007034426400322e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 1.3435098104893557e-11,
        "relative_improvement": 0.33652798954476154,
        "absolute_improvement_ms": 53.27586206896551,
        "old_mean_ms": 158.3103448275862,
        "new_mean_ms": 105.03448275862068,
        "old_std_ms": 1.4418105749335284,
        "new_std_ms": 0.9442592947721239,
        "effect_size_cohens_d": 43.71543925707493,
        "old_ci95_ms": [
          157.76190941574941,
          158.85878023942297
        ],
        "new_ci95_ms": [
          104.67530573203177,
          105.39365978520958
        ],
        "old_ci99_ms": [
          157.57051583956297,
          159.0501738156094
        ],
        "new_ci99_ms": [
          104.5499597494071,
          105.51900576783427
        ],
        "new_times": [
          0.105,
          0.105,
          0.105,
          0.105,
          0.105,
          0.105,
          0.103,
          0.105,
          0.105,
          0.105,
          0.105,
          0.105,
          0.105,
          0.105,
          0.106,
          0.105,
          0.105,
          0.108,
          0.105,
          0.105,
          0.103,
          0.104,
          0.106,
          0.105,
          0.104,
          0.104,
          0.106,
          0.106,
          0.106
        ],
        "old_times": [
          0.158,
          0.157,
          0.157,
          0.159,
          0.158,
          0.159,
          0.158,
          0.158,
          0.158,
          0.158,
          0.157,
          0.157,
          0.158,
          0.157,
          0.156,
          0.16,
          0.159,
          0.162,
          0.158,
          0.159,
          0.16,
          0.16,
          0.157,
          0.158,
          0.158,
          0.157,
          0.162,
          0.157,
          0.159
        ]
      },
      {
        "test_name": "[load store]<xsimd::batch<unsigned char>>",
        "is_significant": false,
        "p_value": 0.9992154001532585,
        "is_pair_significant": false,
        "pair_p_value": 0.9989637025189844,
        "is_binom_significant": false,
        "binom_p_value": 0.9999999441206455,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999541375314366,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.927490172418524,
        "relative_improvement": -0.04519774011299439,
        "absolute_improvement_ms": -0.2758620689655182,
        "old_mean_ms": 6.10344827586207,
        "new_mean_ms": 6.379310344827588,
        "old_std_ms": 0.30993404669460345,
        "new_std_ms": 0.8624629641509365,
        "effect_size_cohens_d": -0.4256893786112049,
        "old_ci95_ms": [
          5.985555669512563,
          6.221340882211577
        ],
        "new_ci95_ms": [
          6.051246978417687,
          6.70737371123749
        ],
        "old_ci99_ms": [
          5.944413381745983,
          6.262483169978157
        ],
        "new_ci99_ms": [
          5.936759074176305,
          6.821861615478872
        ],
        "new_times": [
          0.006,
          0.006,
          0.007,
          0.006,
          0.008,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.01,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.007,
          0.006,
          0.006,
          0.006
        ],
        "old_times": [
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006
        ]
      },
      {
        "test_name": "[poly evaluation]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 1.9918856124915297e-47,
        "is_pair_significant": true,
        "pair_p_value": 3.3615628745701726e-29,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 8.686277907775259e-07,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 9.326451626505859e-12,
        "relative_improvement": 0.20591939546599505,
        "absolute_improvement_ms": 11.275862068965525,
        "old_mean_ms": 54.758620689655174,
        "new_mean_ms": 43.48275862068965,
        "old_std_ms": 0.7394579295380173,
        "new_std_ms": 0.6336227840855441,
        "effect_size_cohens_d": 16.37560690684159,
        "old_ci95_ms": [
          54.47734593611076,
          55.03989544319958
        ],
        "new_ci95_ms": [
          43.24174140346252,
          43.723775837916776
        ],
        "old_ci99_ms": [
          54.37918637199449,
          55.138055007315856
        ],
        "new_ci99_ms": [
          43.15763095739721,
          43.807886283982086
        ],
        "new_times": [
          0.043,
          0.045,
          0.043,
          0.044,
          0.043,
          0.043,
          0.044,
          0.044,
          0.043,
          0.044,
          0.043,
          0.043,
          0.043,
          0.043,
          0.043,
          0.044,
          0.043,
          0.043,
          0.043,
          0.043,
          0.044,
          0.044,
          0.044,
          0.044,
          0.043,
          0.044,
          0.045,
          0.043,
          0.043
        ],
        "old_times": [
          0.057,
          0.055,
          0.055,
          0.056,
          0.055,
          0.055,
          0.054,
          0.055,
          0.054,
          0.054,
          0.054,
          0.054,
          0.055,
          0.054,
          0.055,
          0.055,
          0.054,
          0.055,
          0.054,
          0.054,
          0.055,
          0.054,
          0.055,
          0.055,
          0.056,
          0.055,
          0.055,
          0.055,
          0.054
        ]
      },
      {
        "test_name": "[poly evaluation]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 4.644785636231051e-31,
        "is_pair_significant": true,
        "pair_p_value": 7.859095054148906e-26,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 9.132602985950441e-07,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 7.072117768833696e-12,
        "relative_improvement": 0.2240437158469946,
        "absolute_improvement_ms": 11.310344827586215,
        "old_mean_ms": 50.482758620689665,
        "new_mean_ms": 39.172413793103445,
        "old_std_ms": 1.2989195965211235,
        "new_std_ms": 0.5391107438074949,
        "effect_size_cohens_d": 11.373551220619696,
        "old_ci95_ms": [
          49.98867603019835,
          50.976841211180975
        ],
        "new_ci95_ms": [
          38.96734703333209,
          39.37748055287481
        ],
        "old_ci99_ms": [
          49.81625055969694,
          51.149266681682384
        ],
        "new_ci99_ms": [
          38.89578261633327,
          39.44904496987362
        ],
        "new_times": [
          0.039,
          0.039,
          0.039,
          0.04,
          0.039,
          0.039,
          0.039,
          0.038,
          0.039,
          0.039,
          0.039,
          0.039,
          0.039,
          0.039,
          0.039,
          0.039,
          0.039,
          0.04,
          0.039,
          0.04,
          0.039,
          0.039,
          0.039,
          0.04,
          0.04,
          0.04,
          0.04,
          0.039,
          0.038
        ],
        "old_times": [
          0.05,
          0.05,
          0.05,
          0.05,
          0.049,
          0.051,
          0.05,
          0.049,
          0.05,
          0.05,
          0.05,
          0.051,
          0.05,
          0.051,
          0.05,
          0.05,
          0.051,
          0.051,
          0.05,
          0.05,
          0.049,
          0.052,
          0.05,
          0.051,
          0.05,
          0.056,
          0.051,
          0.05,
          0.052
        ]
      },
      {
        "test_name": "[power]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 1.1285469733712426e-62,
        "is_pair_significant": true,
        "pair_p_value": 1.3696969335421548e-41,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.1870520661196868e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.2348053171296012e-11,
        "relative_improvement": 0.33261925164238787,
        "absolute_improvement_ms": 80.31034482758625,
        "old_mean_ms": 241.44827586206895,
        "new_mean_ms": 161.1379310344827,
        "old_std_ms": 2.3541180771537937,
        "new_std_ms": 1.3555214582017223,
        "effect_size_cohens_d": 41.8098491619871,
        "old_ci95_ms": [
          240.55281723553213,
          242.3437344886058
        ],
        "new_ci95_ms": [
          160.6223182472643,
          161.6535438217011
        ],
        "old_ci99_ms": [
          240.24031912698388,
          242.65623259715406
        ],
        "new_ci99_ms": [
          160.44237914569774,
          161.83348292326767
        ],
        "new_times": [
          0.16,
          0.16,
          0.16,
          0.161,
          0.16,
          0.16,
          0.16,
          0.161,
          0.16,
          0.164,
          0.163,
          0.161,
          0.165,
          0.164,
          0.162,
          0.161,
          0.161,
          0.162,
          0.161,
          0.16,
          0.161,
          0.16,
          0.16,
          0.16,
          0.161,
          0.161,
          0.161,
          0.162,
          0.161
        ],
        "old_times": [
          0.241,
          0.247,
          0.237,
          0.237,
          0.243,
          0.241,
          0.24,
          0.237,
          0.241,
          0.242,
          0.245,
          0.242,
          0.242,
          0.242,
          0.24,
          0.243,
          0.243,
          0.239,
          0.239,
          0.241,
          0.241,
          0.244,
          0.245,
          0.241,
          0.241,
          0.24,
          0.243,
          0.243,
          0.242
        ]
      },
      {
        "test_name": "[power]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 4.729724895342505e-54,
        "is_pair_significant": true,
        "pair_p_value": 7.145639529545884e-44,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.1416313377595466e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.3965386376332555e-11,
        "relative_improvement": 0.37225019878081106,
        "absolute_improvement_ms": 96.86206896551727,
        "old_mean_ms": 260.2068965517241,
        "new_mean_ms": 163.34482758620686,
        "old_std_ms": 2.895554602006667,
        "new_std_ms": 1.1425492196014246,
        "effect_size_cohens_d": 44.00627193442789,
        "old_ci95_ms": [
          259.1054864856198,
          261.30830661782846
        ],
        "new_ci95_ms": [
          162.91022510604068,
          163.779430066373
        ],
        "old_ci99_ms": [
          258.721115223572,
          261.69267787987627
        ],
        "new_ci99_ms": [
          162.75855706801246,
          163.93109810440126
        ],
        "new_times": [
          0.162,
          0.164,
          0.164,
          0.164,
          0.163,
          0.164,
          0.166,
          0.163,
          0.163,
          0.162,
          0.165,
          0.166,
          0.165,
          0.162,
          0.163,
          0.163,
          0.163,
          0.163,
          0.163,
          0.162,
          0.162,
          0.163,
          0.162,
          0.163,
          0.164,
          0.162,
          0.164,
          0.164,
          0.163
        ],
        "old_times": [
          0.26,
          0.262,
          0.261,
          0.258,
          0.261,
          0.261,
          0.257,
          0.261,
          0.263,
          0.26,
          0.266,
          0.271,
          0.259,
          0.258,
          0.262,
          0.26,
          0.261,
          0.261,
          0.259,
          0.259,
          0.258,
          0.257,
          0.258,
          0.261,
          0.258,
          0.26,
          0.257,
          0.258,
          0.259
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<unsigned char>>",
        "is_significant": false,
        "p_value": 0.9981794002009523,
        "is_pair_significant": false,
        "pair_p_value": 0.9965713895959033,
        "is_binom_significant": false,
        "binom_p_value": 0.987940227612853,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9904099458054995,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 4.593267838311024e-06,
        "relative_improvement": 0.030103480714957487,
        "absolute_improvement_ms": 1.1034482758620623,
        "old_mean_ms": 36.6551724137931,
        "new_mean_ms": 35.55172413793104,
        "old_std_ms": 1.0445731209092595,
        "new_std_ms": 0.8274835732746634,
        "effect_size_cohens_d": 1.171014742556594,
        "old_ci95_ms": [
          36.25783804843383,
          37.052506779152374
        ],
        "new_ci95_ms": [
          35.23696622022192,
          35.866482055640155
        ],
        "old_ci99_ms": [
          36.119175876942776,
          37.19116895064343
        ],
        "new_ci99_ms": [
          35.12712166568891,
          35.976326610173174
        ],
        "new_times": [
          0.035,
          0.035,
          0.036,
          0.035,
          0.035,
          0.036,
          0.035,
          0.035,
          0.035,
          0.035,
          0.035,
          0.037,
          0.036,
          0.035,
          0.035,
          0.035,
          0.036,
          0.035,
          0.035,
          0.037,
          0.035,
          0.037,
          0.036,
          0.036,
          0.035,
          0.035,
          0.035,
          0.038,
          0.036
        ],
        "old_times": [
          0.036,
          0.036,
          0.04,
          0.037,
          0.036,
          0.037,
          0.036,
          0.038,
          0.036,
          0.038,
          0.036,
          0.036,
          0.036,
          0.036,
          0.036,
          0.037,
          0.036,
          0.037,
          0.037,
          0.036,
          0.036,
          0.037,
          0.036,
          0.039,
          0.036,
          0.036,
          0.038,
          0.036,
          0.036
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<signed char>>",
        "is_significant": false,
        "p_value": 0.9999999999999883,
        "is_pair_significant": false,
        "pair_p_value": 0.9999999999858106,
        "is_binom_significant": false,
        "binom_p_value": 0.9999999981373549,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999988023366908,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.977655591341116,
        "relative_improvement": -0.012807881773398914,
        "absolute_improvement_ms": -0.44827586206896974,
        "old_mean_ms": 35.0,
        "new_mean_ms": 35.448275862068975,
        "old_std_ms": 0.8017837257372713,
        "new_std_ms": 0.8695732490526468,
        "effect_size_cohens_d": -0.5359807612184562,
        "old_ci95_ms": [
          34.695017781480054,
          35.30498221851995
        ],
        "new_ci95_ms": [
          35.11750788792756,
          35.77904383621039
        ],
        "old_ci99_ms": [
          34.588584760898264,
          35.41141523910174
        ],
        "new_ci99_ms": [
          35.00207612678936,
          35.89447559734858
        ],
        "new_times": [
          0.035,
          0.035,
          0.036,
          0.036,
          0.036,
          0.036,
          0.034,
          0.035,
          0.035,
          0.036,
          0.034,
          0.036,
          0.037,
          0.036,
          0.034,
          0.035,
          0.035,
          0.036,
          0.037,
          0.035,
          0.035,
          0.035,
          0.035,
          0.036,
          0.036,
          0.034,
          0.037,
          0.036,
          0.035
        ],
        "old_times": [
          0.034,
          0.035,
          0.036,
          0.036,
          0.035,
          0.036,
          0.035,
          0.034,
          0.035,
          0.035,
          0.034,
          0.035,
          0.036,
          0.034,
          0.035,
          0.035,
          0.035,
          0.035,
          0.035,
          0.036,
          0.036,
          0.035,
          0.037,
          0.034,
          0.034,
          0.034,
          0.034,
          0.035,
          0.035
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<short unsigned int>>",
        "is_significant": false,
        "p_value": 0.7750012046890511,
        "is_pair_significant": false,
        "pair_p_value": 0.7994793195252584,
        "is_binom_significant": false,
        "binom_p_value": 0.9692858271300793,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8170782360369973,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 1.4737828577784422e-05,
        "relative_improvement": 0.042995839112344,
        "absolute_improvement_ms": 1.06896551724138,
        "old_mean_ms": 24.862068965517246,
        "new_mean_ms": 23.79310344827587,
        "old_std_ms": 0.7894002212817054,
        "new_std_ms": 0.9775812112712388,
        "effect_size_cohens_d": 1.203130387154837,
        "old_ci95_ms": [
          24.561797180173574,
          25.16234075086092
        ],
        "new_ci95_ms": [
          23.42125144252112,
          24.16495545403062
        ],
        "old_ci99_ms": [
          24.45700801159612,
          25.267129919438375
        ],
        "new_ci99_ms": [
          23.291482132120205,
          24.29472476443153
        ],
        "new_times": [
          0.023,
          0.026,
          0.024,
          0.023,
          0.023,
          0.023,
          0.023,
          0.024,
          0.024,
          0.023,
          0.023,
          0.026,
          0.023,
          0.023,
          0.024,
          0.024,
          0.026,
          0.024,
          0.024,
          0.023,
          0.024,
          0.023,
          0.025,
          0.024,
          0.023,
          0.025,
          0.023,
          0.023,
          0.024
        ],
        "old_times": [
          0.024,
          0.025,
          0.024,
          0.025,
          0.025,
          0.024,
          0.024,
          0.026,
          0.024,
          0.026,
          0.026,
          0.026,
          0.025,
          0.025,
          0.027,
          0.025,
          0.025,
          0.025,
          0.025,
          0.024,
          0.024,
          0.025,
          0.025,
          0.024,
          0.024,
          0.025,
          0.025,
          0.024,
          0.025
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<short int>>",
        "is_significant": false,
        "p_value": 0.21321713451607838,
        "is_pair_significant": false,
        "pair_p_value": 0.23604015439679932,
        "is_binom_significant": false,
        "binom_p_value": 0.77087084017694,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.295671244877192,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 8.87726461560227e-09,
        "relative_improvement": 0.0553129548762737,
        "absolute_improvement_ms": 1.3103448275862093,
        "old_mean_ms": 23.6896551724138,
        "new_mean_ms": 22.37931034482759,
        "old_std_ms": 0.6037648712076864,
        "new_std_ms": 0.6218516763579265,
        "effect_size_cohens_d": 2.1380295022820928,
        "old_ci95_ms": [
          23.4599952978435,
          23.9193150469841
        ],
        "new_ci95_ms": [
          22.14277061751946,
          22.615850072135718
        ],
        "old_ci99_ms": [
          23.37984834935721,
          23.999461995470387
        ],
        "new_ci99_ms": [
          22.060222730671594,
          22.698397958983584
        ],
        "new_times": [
          0.022,
          0.022,
          0.023,
          0.022,
          0.022,
          0.023,
          0.022,
          0.022,
          0.022,
          0.024,
          0.023,
          0.023,
          0.022,
          0.022,
          0.023,
          0.022,
          0.022,
          0.022,
          0.022,
          0.022,
          0.023,
          0.023,
          0.022,
          0.022,
          0.022,
          0.024,
          0.022,
          0.022,
          0.022
        ],
        "old_times": [
          0.024,
          0.024,
          0.024,
          0.024,
          0.024,
          0.023,
          0.025,
          0.023,
          0.023,
          0.024,
          0.024,
          0.023,
          0.023,
          0.023,
          0.024,
          0.023,
          0.024,
          0.024,
          0.024,
          0.024,
          0.023,
          0.024,
          0.024,
          0.024,
          0.025,
          0.023,
          0.023,
          0.024,
          0.023
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<unsigned int>>",
        "is_significant": true,
        "p_value": 1.0790164611845347e-05,
        "is_pair_significant": true,
        "pair_p_value": 1.8852101074436505e-05,
        "is_binom_significant": true,
        "binom_p_value": 7.618218660354614e-06,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 2.224376787612461e-05,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.773896586691383e-10,
        "relative_improvement": 0.09430255402750488,
        "absolute_improvement_ms": 1.6551724137931072,
        "old_mean_ms": 17.55172413793104,
        "new_mean_ms": 15.896551724137932,
        "old_std_ms": 0.6316761657092357,
        "new_std_ms": 0.6732027706741485,
        "effect_size_cohens_d": 2.535614371936957,
        "old_ci95_ms": [
          17.311447374734048,
          17.79200090112803
        ],
        "new_ci95_ms": [
          15.640479085191336,
          16.152624363084527
        ],
        "old_ci99_ms": [
          17.22759533310757,
          17.875852942754513
        ],
        "new_ci99_ms": [
          15.551114581963246,
          16.241988866312617
        ],
        "new_times": [
          0.015,
          0.016,
          0.015,
          0.016,
          0.016,
          0.016,
          0.016,
          0.016,
          0.017,
          0.016,
          0.015,
          0.015,
          0.018,
          0.016,
          0.016,
          0.015,
          0.016,
          0.015,
          0.016,
          0.016,
          0.016,
          0.016,
          0.016,
          0.015,
          0.017,
          0.016,
          0.016,
          0.016,
          0.016
        ],
        "old_times": [
          0.017,
          0.017,
          0.017,
          0.018,
          0.019,
          0.018,
          0.017,
          0.018,
          0.017,
          0.017,
          0.017,
          0.018,
          0.018,
          0.018,
          0.018,
          0.017,
          0.018,
          0.017,
          0.017,
          0.019,
          0.018,
          0.018,
          0.017,
          0.018,
          0.017,
          0.017,
          0.018,
          0.017,
          0.017
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<int>>",
        "is_significant": false,
        "p_value": 0.8105047022379661,
        "is_pair_significant": false,
        "pair_p_value": 0.8029584789697297,
        "is_binom_significant": false,
        "binom_p_value": 0.22912915982306004,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.7188226163640676,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.0003096414230364919,
        "relative_improvement": 0.027459954233409637,
        "absolute_improvement_ms": 0.4137931034482768,
        "old_mean_ms": 15.068965517241383,
        "new_mean_ms": 14.655172413793105,
        "old_std_ms": 0.8836221915162052,
        "new_std_ms": 1.876087533538224,
        "effect_size_cohens_d": 0.28218839088242,
        "old_ci95_ms": [
          14.732853611185588,
          15.405077423297177
        ],
        "new_ci95_ms": [
          13.941546881434029,
          15.368797946152181
        ],
        "old_ci99_ms": [
          14.615556918976363,
          15.5223741155064
        ],
        "new_ci99_ms": [
          13.692505080433767,
          15.617839747152443
        ],
        "new_times": [
          0.014,
          0.015,
          0.014,
          0.014,
          0.014,
          0.015,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.015,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.015,
          0.014,
          0.014,
          0.015,
          0.014,
          0.016,
          0.014,
          0.014,
          0.015,
          0.024,
          0.014,
          0.015
        ],
        "old_times": [
          0.015,
          0.015,
          0.015,
          0.015,
          0.017,
          0.015,
          0.018,
          0.015,
          0.015,
          0.016,
          0.015,
          0.015,
          0.014,
          0.014,
          0.016,
          0.014,
          0.015,
          0.015,
          0.015,
          0.015,
          0.016,
          0.015,
          0.015,
          0.015,
          0.014,
          0.014,
          0.015,
          0.015,
          0.014
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<long unsigned int>>",
        "is_significant": true,
        "p_value": 0.0001562034063415805,
        "is_pair_significant": true,
        "pair_p_value": 0.00033259579086608636,
        "is_binom_significant": true,
        "binom_p_value": 5.587935447692871e-08,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.212215719994973e-05,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 3.076307498108238e-11,
        "relative_improvement": 0.12590799031477007,
        "absolute_improvement_ms": 1.7931034482758599,
        "old_mean_ms": 14.241379310344827,
        "new_mean_ms": 12.448275862068968,
        "old_std_ms": 0.6894704186026248,
        "new_std_ms": 1.3252009792737023,
        "effect_size_cohens_d": 1.697537048687386,
        "old_ci95_ms": [
          13.979118789060303,
          14.503639831629352
        ],
        "new_ci95_ms": [
          11.944196368222848,
          12.95235535591509
        ],
        "old_ci99_ms": [
          13.887594832040362,
          14.595163788649295
        ],
        "new_ci99_ms": [
          11.768282167704063,
          13.128269556433873
        ],
        "new_times": [
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.013,
          0.012,
          0.012,
          0.013,
          0.012,
          0.013,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.013,
          0.013,
          0.013,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.019
        ],
        "old_times": [
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.015,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.016,
          0.014,
          0.014,
          0.014,
          0.017,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.014,
          0.015,
          0.014
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<long int>>",
        "is_significant": false,
        "p_value": 0.5259732992143735,
        "is_pair_significant": false,
        "pair_p_value": 0.5260795632973214,
        "is_binom_significant": true,
        "binom_p_value": 0.004065029323101044,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.14214481956592645,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 9.239749252698036e-08,
        "relative_improvement": 0.04843304843304848,
        "absolute_improvement_ms": 0.5862068965517241,
        "old_mean_ms": 12.103448275862071,
        "new_mean_ms": 11.517241379310349,
        "old_std_ms": 0.3099340466946031,
        "new_std_ms": 1.5264498881665731,
        "effect_size_cohens_d": 0.5322440594701263,
        "old_ci95_ms": [
          11.985555669512566,
          12.221340882211578
        ],
        "new_ci95_ms": [
          10.936610894544042,
          12.097871864076653
        ],
        "old_ci99_ms": [
          11.944413381745985,
          12.26248316997816
        ],
        "new_ci99_ms": [
          10.733981847396809,
          12.300500911223887
        ],
        "new_times": [
          0.011,
          0.011,
          0.011,
          0.011,
          0.011,
          0.011,
          0.011,
          0.012,
          0.013,
          0.011,
          0.011,
          0.011,
          0.011,
          0.012,
          0.019,
          0.012,
          0.011,
          0.012,
          0.011,
          0.011,
          0.011,
          0.011,
          0.011,
          0.012,
          0.011,
          0.011,
          0.011,
          0.011,
          0.011
        ],
        "old_times": [
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.013,
          0.012,
          0.012,
          0.012,
          0.013,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.013,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012,
          0.012
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<float>>",
        "is_significant": false,
        "p_value": 0.9444058017822832,
        "is_pair_significant": false,
        "pair_p_value": 0.9752142022412893,
        "is_binom_significant": false,
        "binom_p_value": 0.9999991878867149,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9991976175491043,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5094912761600245,
        "relative_improvement": -0.031473533619456394,
        "absolute_improvement_ms": -0.7586206896551713,
        "old_mean_ms": 24.103448275862075,
        "new_mean_ms": 24.862068965517246,
        "old_std_ms": 4.029753870419091,
        "new_std_ms": 5.282884344283328,
        "effect_size_cohens_d": -0.16146754044121048,
        "old_ci95_ms": [
          22.570611881160197,
          25.636284670563953
        ],
        "new_ci95_ms": [
          22.852567231090948,
          26.871570699943543
        ],
        "old_ci99_ms": [
          22.035680997853145,
          26.171215553871004
        ],
        "new_ci99_ms": [
          22.151289168070544,
          27.57284876296395
        ],
        "new_times": [
          0.021,
          0.021,
          0.021,
          0.021,
          0.021,
          0.022,
          0.023,
          0.021,
          0.021,
          0.021,
          0.022,
          0.022,
          0.024,
          0.024,
          0.028,
          0.022,
          0.025,
          0.023,
          0.032,
          0.026,
          0.023,
          0.026,
          0.024,
          0.031,
          0.035,
          0.024,
          0.031,
          0.022,
          0.044
        ],
        "old_times": [
          0.022,
          0.022,
          0.021,
          0.022,
          0.025,
          0.022,
          0.022,
          0.021,
          0.022,
          0.022,
          0.022,
          0.022,
          0.022,
          0.023,
          0.022,
          0.031,
          0.022,
          0.022,
          0.023,
          0.04,
          0.023,
          0.024,
          0.024,
          0.027,
          0.026,
          0.023,
          0.03,
          0.023,
          0.029
        ]
      },
      {
        "test_name": "[select]<xsimd::batch<double>>",
        "is_significant": false,
        "p_value": 0.4615727944104057,
        "is_pair_significant": false,
        "pair_p_value": 0.4612356793374247,
        "is_binom_significant": false,
        "binom_p_value": 0.132465448230505,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.5390985683404403,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.025090245301708024,
        "relative_improvement": 0.05479452054794525,
        "absolute_improvement_ms": 0.9655172413793114,
        "old_mean_ms": 17.62068965517242,
        "new_mean_ms": 16.655172413793107,
        "old_std_ms": 4.0568618510885655,
        "new_std_ms": 2.6762962422636773,
        "effect_size_cohens_d": 0.2809498111327627,
        "old_ci95_ms": [
          16.07754193608369,
          19.163837374261146
        ],
        "new_ci95_ms": [
          15.63716376734468,
          17.673181060241532
        ],
        "old_ci99_ms": [
          15.539012595771455,
          19.702366714573383
        ],
        "new_ci99_ms": [
          15.281898021867141,
          18.028446805719074
        ],
        "new_times": [
          0.016,
          0.015,
          0.015,
          0.016,
          0.017,
          0.015,
          0.023,
          0.015,
          0.022,
          0.017,
          0.015,
          0.015,
          0.018,
          0.015,
          0.015,
          0.023,
          0.015,
          0.015,
          0.014,
          0.015,
          0.015,
          0.015,
          0.015,
          0.019,
          0.018,
          0.018,
          0.015,
          0.022,
          0.015
        ],
        "old_times": [
          0.016,
          0.016,
          0.016,
          0.016,
          0.028,
          0.029,
          0.022,
          0.016,
          0.015,
          0.029,
          0.018,
          0.016,
          0.019,
          0.016,
          0.016,
          0.016,
          0.016,
          0.016,
          0.015,
          0.016,
          0.015,
          0.015,
          0.016,
          0.016,
          0.016,
          0.016,
          0.017,
          0.017,
          0.016
        ]
      },
      {
        "test_name": "[trigonometric]<xsimd::batch<float>>",
        "is_significant": true,
        "p_value": 1.261124076503196e-72,
        "is_pair_significant": true,
        "pair_p_value": 5.881369190353434e-42,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.1432002167873225e-06,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 2.1750616047168484e-11,
        "relative_improvement": 0.2518608274190757,
        "absolute_improvement_ms": 50.172413793103416,
        "old_mean_ms": 199.2068965517241,
        "new_mean_ms": 149.0344827586207,
        "old_std_ms": 1.081642611455493,
        "new_std_ms": 1.148998278647526,
        "effect_size_cohens_d": 44.96425812063728,
        "old_ci95_ms": [
          198.79546170626867,
          199.61833139717956
        ],
        "new_ci95_ms": [
          148.59742718758713,
          149.47153832965427
        ],
        "old_ci99_ms": [
          198.65187873416315,
          199.76191436928505
        ],
        "new_ci99_ms": [
          148.44490306728613,
          149.62406244995523
        ],
        "new_times": [
          0.148,
          0.15,
          0.149,
          0.147,
          0.149,
          0.149,
          0.148,
          0.149,
          0.149,
          0.149,
          0.148,
          0.151,
          0.15,
          0.149,
          0.148,
          0.15,
          0.151,
          0.151,
          0.148,
          0.15,
          0.151,
          0.149,
          0.148,
          0.149,
          0.149,
          0.148,
          0.147,
          0.15,
          0.148
        ],
        "old_times": [
          0.2,
          0.198,
          0.2,
          0.2,
          0.2,
          0.197,
          0.198,
          0.199,
          0.198,
          0.198,
          0.199,
          0.199,
          0.199,
          0.198,
          0.2,
          0.2,
          0.201,
          0.199,
          0.199,
          0.2,
          0.201,
          0.2,
          0.2,
          0.199,
          0.199,
          0.199,
          0.201,
          0.199,
          0.197
        ]
      },
      {
        "test_name": "[trigonometric]<xsimd::batch<double>>",
        "is_significant": true,
        "p_value": 3.9328196439180245e-48,
        "is_pair_significant": true,
        "pair_p_value": 4.231079433078428e-34,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2381514760893376e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 2.735563673125611e-11,
        "relative_improvement": 0.32185107863604734,
        "absolute_improvement_ms": 63.79310344827588,
        "old_mean_ms": 198.20689655172413,
        "new_mean_ms": 134.41379310344826,
        "old_std_ms": 3.4369765611323806,
        "new_std_ms": 1.7426835090767019,
        "effect_size_cohens_d": 23.411492608765748,
        "old_ci95_ms": [
          196.8995405865587,
          199.5142525168896
        ],
        "new_ci95_ms": [
          133.7509117481456,
          135.07667445875092
        ],
        "old_ci99_ms": [
          196.44329810454303,
          199.97049499890522
        ],
        "new_ci99_ms": [
          133.51957870428947,
          135.30800750260704
        ],
        "new_times": [
          0.135,
          0.133,
          0.132,
          0.133,
          0.132,
          0.134,
          0.136,
          0.133,
          0.136,
          0.136,
          0.135,
          0.134,
          0.134,
          0.134,
          0.133,
          0.137,
          0.135,
          0.134,
          0.133,
          0.137,
          0.134,
          0.133,
          0.136,
          0.134,
          0.135,
          0.134,
          0.14,
          0.133,
          0.133
        ],
        "old_times": [
          0.194,
          0.196,
          0.195,
          0.201,
          0.199,
          0.197,
          0.197,
          0.198,
          0.206,
          0.195,
          0.199,
          0.201,
          0.198,
          0.196,
          0.198,
          0.199,
          0.199,
          0.205,
          0.194,
          0.195,
          0.195,
          0.204,
          0.198,
          0.196,
          0.196,
          0.197,
          0.199,
          0.206,
          0.195
        ]
      },
      {
        "test_name": "[xsimd api | float types functions]<float>",
        "is_significant": false,
        "p_value": 0.8750456979554011,
        "is_pair_significant": false,
        "pair_p_value": 0.8553586116061291,
        "is_binom_significant": false,
        "binom_p_value": 0.9999991878867149,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.999287134636576,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5,
        "relative_improvement": 0.005464480874316945,
        "absolute_improvement_ms": 0.034482758620689447,
        "old_mean_ms": 6.310344827586208,
        "new_mean_ms": 6.27586206896552,
        "old_std_ms": 0.9674505626299489,
        "new_std_ms": 0.9218208581830206,
        "effect_size_cohens_d": 0.036493121531925174,
        "old_ci95_ms": [
          5.94234631448601,
          6.678343340686407
        ],
        "new_ci95_ms": [
          5.925220167188202,
          6.626503970742836
        ],
        "old_ci99_ms": [
          5.813921800070265,
          6.806767855102152
        ],
        "new_ci99_ms": [
          5.802852781542532,
          6.7488713563885065
        ],
        "new_times": [
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.01,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.009,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006
        ],
        "old_times": [
          0.006,
          0.006,
          0.006,
          0.006,
          0.008,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.009,
          0.006,
          0.006,
          0.006,
          0.01,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006
        ]
      },
      {
        "test_name": "[xsimd api | float types functions]<double>",
        "is_significant": false,
        "p_value": 0.9490861990505929,
        "is_pair_significant": false,
        "pair_p_value": 0.9305442663639741,
        "is_binom_significant": false,
        "binom_p_value": 0.9999991878867149,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9991565524712864,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.6188541345218587,
        "relative_improvement": 0.005524861878453044,
        "absolute_improvement_ms": 0.034482758620690314,
        "old_mean_ms": 6.24137931034483,
        "new_mean_ms": 6.20689655172414,
        "old_std_ms": 0.7394579295380166,
        "new_std_ms": 0.5592923044110288,
        "effect_size_cohens_d": 0.052597766558402724,
        "old_ci95_ms": [
          5.9601045568004185,
          6.522654063889242
        ],
        "new_ci95_ms": [
          5.994153136854301,
          6.419639966593978
        ],
        "old_ci99_ms": [
          5.861944992684152,
          6.620813628005507
        ],
        "new_ci99_ms": [
          5.919909712554465,
          6.493883390893814
        ],
        "new_times": [
          0.006,
          0.008,
          0.006,
          0.008,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006
        ],
        "old_times": [
          0.006,
          0.006,
          0.008,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.009,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.008,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006
        ]
      },
      {
        "test_name": "[xsimd api | float types functions]<xsimd::batch<float>>",
        "is_significant": false,
        "p_value": 0.9949318578335498,
        "is_pair_significant": false,
        "pair_p_value": 0.9926252050609714,
        "is_binom_significant": false,
        "binom_p_value": 0.9999481420964003,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9938994405367436,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.7180711909566152,
        "relative_improvement": -0.06666666666666674,
        "absolute_improvement_ms": -0.4137931034482768,
        "old_mean_ms": 6.20689655172414,
        "new_mean_ms": 6.620689655172416,
        "old_std_ms": 0.4122508203948856,
        "new_std_ms": 1.3735718064190028,
        "effect_size_cohens_d": -0.4080543622915644,
        "old_ci95_ms": [
          6.050084726097363,
          6.363708377350916
        ],
        "new_ci95_ms": [
          6.0982108826918315,
          7.143168427653001
        ],
        "old_ci99_ms": [
          5.995360367590832,
          6.418432735857447
        ],
        "new_ci99_ms": [
          5.915875682250182,
          7.3255036280946495
        ],
        "new_times": [
          0.006,
          0.006,
          0.006,
          0.011,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.008,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.01,
          0.006,
          0.006,
          0.006,
          0.006,
          0.01,
          0.006,
          0.007
        ],
        "old_times": [
          0.007,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.007,
          0.006,
          0.006,
          0.006
        ]
      },
      {
        "test_name": "[xsimd api | float types functions]<xsimd::batch<double>>",
        "is_significant": false,
        "p_value": 0.9999998234353074,
        "is_pair_significant": false,
        "pair_p_value": 0.9999998234353074,
        "is_binom_significant": false,
        "binom_p_value": 1.0,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999997886959029,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.9902258267115058,
        "relative_improvement": -0.02873563218390791,
        "absolute_improvement_ms": -0.1724137931034481,
        "old_mean_ms": 6.000000000000002,
        "new_mean_ms": 6.17241379310345,
        "old_std_ms": 1.7654289301252243e-15,
        "new_std_ms": 0.38442587221924485,
        "effect_size_cohens_d": -0.6342703292561553,
        "old_ci95_ms": [
          6.000000000000001,
          6.000000000000003
        ],
        "new_ci95_ms": [
          6.026186011717539,
          6.318641574489361
        ],
        "old_ci99_ms": [
          6.000000000000001,
          6.000000000000003
        ],
        "new_ci99_ms": [
          5.975155284282332,
          6.369672301924568
        ],
        "new_times": [
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.007,
          0.006,
          0.007,
          0.007,
          0.006,
          0.006,
          0.006,
          0.007,
          0.007,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006
        ],
        "old_times": [
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006,
          0.006
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}