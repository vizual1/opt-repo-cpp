{
    "metadata": {
        "collection_date": "2026-01-14T18:41:18.949341",
        "repository": "https://github.com/google/highway",
        "repository_name": "google/highway"
    },
    "commit_info": {
        "old_sha": "283848d12c524d27408a7da42198688def1b343a",
        "new_sha": "dde031e8d411dbd867fbe4ba5aec9cc2a8a5f7bb",
        "commit_message": [
            "also apply code-sharing to Sort16Rows. -4/3% for N=64/128\n\nPiperOrigin-RevId: 532444001"
        ],
        "commit_date": "2023-05-16T13:52:42+00:00",
        "patch": [
            "--- hwy/contrib/sort/vqsort-inl.h\n@@ -180,42 +180,6 @@ HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n   StoreU(v3, d, in_out3);\n }\n \n-#if HWY_MEM_OPS_MIGHT_FAULT\n-\n-template <size_t kVectors, size_t kLPK, size_t kLanesPerRow, class D,\n-          class Traits, typename T = TFromD<D>>\n-HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n-                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n-  constexpr size_t kMinLanes = kVectors / 2 * kLanesPerRow;\n-  // Must cap for correctness: we will load up to the last valid lane, so\n-  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n-  const CappedTag<T, kMinLanes> dmax;\n-  const size_t Nmax = Lanes(dmax);\n-  HWY_DASSERT(Nmax < num_lanes);\n-  HWY_ASSUME(Nmax <= kMinLanes);\n-\n-  // Fill with padding - last in sort order, not copied to keys.\n-  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n-\n-  // Rounding down allows aligned stores, which are typically faster.\n-  size_t i = num_lanes & ~(Nmax - 1);\n-  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n-  do {\n-    Store(kPadding, dmax, buf + i);\n-    i += Nmax;\n-    // Initialize enough for the last vector even if Lanes > lanes_per_row.\n-  } while (i < (kVectors - 1) * kLanesPerRow + Lanes(d));\n-\n-  // Ensure buf contains all we will read, and perhaps more before.\n-  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n-  do {\n-    end -= static_cast<ptrdiff_t>(Nmax);\n-    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n-  } while (end > static_cast<ptrdiff_t>(kVectors / 2 * kLanesPerRow));\n-}\n-\n-#endif  // HWY_MEM_OPS_MIGHT_FAULT\n-\n template <class Traits, typename T>\n HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n@@ -365,210 +329,194 @@ HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n-template <size_t kMaxKeys, class Traits, typename T>\n+template <class Traits, typename T>\n HWY_NOINLINE void Sort16Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                             size_t /* ceil_log2 */, T* HWY_RESTRICT buf) {\n+                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = kMaxKeys / 2 + 1;\n-  HWY_DASSERT(kMinKeys <= num_keys && num_keys <= kMaxKeys);\n-  HWY_ASSUME(num_keys >= kMinKeys);\n-  HWY_ASSUME(num_keys <= kMaxKeys);\n-  constexpr size_t kKeysPerRow = DivCeil(kMaxKeys, size_t{16});\n-  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n-\n-  const CappedTag<T, kLanesPerRow> d;\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-  using V = Vec<decltype(d)>;\n+  constexpr size_t kMinKeys = 33;  // Sort8Rows handled <= 32.\n+  constexpr size_t kMinLanes = kMinKeys * kLPK;\n+  HWY_DASSERT(num_keys >= kMinKeys);\n+  (void)num_keys;\n+  (void)kMinLanes;\n \n-  // We know the first half of vectors are valid, so load unconditionally.\n-  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n-  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n-  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n-  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n-  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n-  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n-  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n-  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  constexpr size_t kRows = 16;\n+  constexpr size_t kMaxKeysPerRow = 16;  // Limited by sorting_networks-inl.h\n+  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n+  // Verify ceil_log2 can be handled by our matrix.\n+  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n+              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n+\n+  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n+  // (all are non-constexpr).\n+  const size_t max_keys = 1ull << ceil_log2;\n+  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n+  const size_t keys_per_row = DivCeil(max_keys, kRows);\n+  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n+  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n \n+  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n+  // be big enough, but loads are only lanes_per_row apart and may overlap.\n+  const CappedTag<T, kMaxLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  const V kPadding = st.LastValue(d);  // Not copied to keys.\n+  V v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n+\n+  // The first four vectors are always valid because 3 * lanes_per_row +\n+  // kMaxLanesPerRow <= num_lanes (even if kLPK == 2, because the largest\n+  // supported vector size is 512 bits i.e. four 128-bit keys, so lanes_per_row\n+  // and kMaxLanesPerRow are only eight uint64_t, and num_lanes >= 66).\n+  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n+  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n+  V v2 = LoadU(d, keys + 0x2 * lanes_per_row);\n+  V v3 = LoadU(d, keys + 0x3 * lanes_per_row);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  CopyHalfToPaddedBuf<16, kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n-  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n-  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n-  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n-  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n-  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n-  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n-  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n-  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n-#else   // !HWY_MEM_OPS_MIGHT_FAULT\n-  (void)buf;\n+  const size_t N = Lanes(d);\n+  // Copy whole vectors from keys, starting from the first potentially unsafe\n+  // load, rounded down to enable aligned stores.\n+  size_t i = (4 * lanes_per_row) & ~(N - 1);\n+  for (; i + N <= num_lanes; i += N) {\n+    Store(LoadU(d, keys + i), d, buf + i);\n+  }\n+\n+  // Fill missing inputs with padding up to vf's load. This also overwrites the\n+  // last partial vector of inputs, which we copy again below.\n+  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n+  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n+  for (; i < required_lanes; i += N) {\n+    Store(kPadding, d, buf + i);\n+  }\n \n-  // To prevent reading past the end, only activate the lanes corresponding to\n-  // the first kLanesPerRow keys (other lanes' masks will be false).\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, kLanesPerRow),\n+  // Copy over the last partial vector, possibly overwriting already-written\n+  // keys with the same value or padding. Example: num_lanes = 33 u64, N = 8,\n+  // lanes_per_row = 4, end = 25, copy [16..31], pad [32..71], copy [25..32].\n+  HWY_DASSERT(num_lanes > N);\n+  const size_t end = num_lanes - N;\n+  StoreU(LoadU(d, keys + end), d, buf + end);\n+\n+  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n+  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n+  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n+  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n+  v8 = LoadU(d, buf + 0x8 * lanes_per_row);\n+  v9 = LoadU(d, buf + 0x9 * lanes_per_row);\n+  va = LoadU(d, buf + 0xa * lanes_per_row);\n+  vb = LoadU(d, buf + 0xb * lanes_per_row);\n+  vc = LoadU(d, buf + 0xc * lanes_per_row);\n+  vd = LoadU(d, buf + 0xd * lanes_per_row);\n+  ve = LoadU(d, buf + 0xe * lanes_per_row);\n+  vf = LoadU(d, buf + 0xf * lanes_per_row);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+#if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n+  (void)buf;\n+  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n+  // Gt checks at no extra cost, and prevents reading past num_lanes.\n+  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n                                       Set(d, static_cast<T>(num_lanes)));\n-  const V kIota = Iota(d, static_cast<T>((kMinKeys - 1) * kLPK));\n-  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n+  // First offset where not all vector are guaranteed valid.\n+  const V kIota = Iota(d, static_cast<T>(4 * lanes_per_row));\n+  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n   const V k2 = Add(k1, k1);\n   const V k4 = Add(k2, k2);\n   const V k8 = Add(k4, k4);\n \n   using M = Mask<decltype(d)>;\n-  const M m8 = Gt(vnum_lanes, kIota);\n-  const M m9 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M ma = Gt(vnum_lanes, Add(kIota, k2));\n-  const M mb = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n-  const M mc = Gt(vnum_lanes, Add(kIota, k4));\n-  const M md = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-  const M me = Gt(vnum_lanes, Add(kIota, Sub(k8, k2)));\n-  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n-\n-  // Fill with padding - last in sort order, not copied to keys.\n-  const V kPadding = st.LastValue(d);\n-\n-  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n-  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n-  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n-  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n-  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n-  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n-  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n-  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n-#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+  const M m4 = Gt(vnum_lanes, kIota);\n+  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n+  const M m7 = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n+  const M m8 = Gt(vnum_lanes, Add(kIota, k4));\n+  const M m9 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n+  const M ma = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k8));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k8, k1)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k8, k2)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Add(k8, Add(k2, k1))));\n+\n+  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n+  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n+  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n+  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n+  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * lanes_per_row);\n+  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * lanes_per_row);\n+  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * lanes_per_row);\n+  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * lanes_per_row);\n+  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * lanes_per_row);\n+  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * lanes_per_row);\n+  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * lanes_per_row);\n+  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * lanes_per_row);\n+#endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n   Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n             vf);\n   Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n             vf);\n-  if (kKeysPerRow >= 8 && kMaxKeysPerVector >= 8) {\n-    // If we do not have eight keys per vector, this code will not be reached,\n-    // but we have to expand vectors to that size for Merge16x8 to compile.\n-    const CappedTag<T, HWY_MAX(kLanesPerRow, 8 * kLPK)> dw;\n-    Vec<decltype(dw)> w0 = ResizeBitCast(dw, v0);\n-    Vec<decltype(dw)> w1 = ResizeBitCast(dw, v1);\n-    Vec<decltype(dw)> w2 = ResizeBitCast(dw, v2);\n-    Vec<decltype(dw)> w3 = ResizeBitCast(dw, v3);\n-    Vec<decltype(dw)> w4 = ResizeBitCast(dw, v4);\n-    Vec<decltype(dw)> w5 = ResizeBitCast(dw, v5);\n-    Vec<decltype(dw)> w6 = ResizeBitCast(dw, v6);\n-    Vec<decltype(dw)> w7 = ResizeBitCast(dw, v7);\n-    Vec<decltype(dw)> w8 = ResizeBitCast(dw, v8);\n-    Vec<decltype(dw)> w9 = ResizeBitCast(dw, v9);\n-    Vec<decltype(dw)> wa = ResizeBitCast(dw, va);\n-    Vec<decltype(dw)> wb = ResizeBitCast(dw, vb);\n-    Vec<decltype(dw)> wc = ResizeBitCast(dw, vc);\n-    Vec<decltype(dw)> wd = ResizeBitCast(dw, vd);\n-    Vec<decltype(dw)> we = ResizeBitCast(dw, ve);\n-    Vec<decltype(dw)> wf = ResizeBitCast(dw, vf);\n-    Merge16x8(dw, st, w0, w1, w2, w3, w4, w5, w6, w7, w8, w9, wa, wb, wc, wd,\n-              we, wf);\n-    // Truncate back.\n-    v0 = ResizeBitCast(d, w0);\n-    v1 = ResizeBitCast(d, w1);\n-    v2 = ResizeBitCast(d, w2);\n-    v3 = ResizeBitCast(d, w3);\n-    v4 = ResizeBitCast(d, w4);\n-    v5 = ResizeBitCast(d, w5);\n-    v6 = ResizeBitCast(d, w6);\n-    v7 = ResizeBitCast(d, w7);\n-    v8 = ResizeBitCast(d, w8);\n-    v9 = ResizeBitCast(d, w9);\n-    va = ResizeBitCast(d, wa);\n-    vb = ResizeBitCast(d, wb);\n-    vc = ResizeBitCast(d, wc);\n-    vd = ResizeBitCast(d, wd);\n-    ve = ResizeBitCast(d, we);\n-    vf = ResizeBitCast(d, wf);\n-  }\n+  // Only merge across columns if it is possible that vectors are that big\n+  // (reduces code size) and there are actually that many cols.\n+  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n+\n+  if (HWY_LIKELY(keys_per_row >= 8 && kMaxKeysPerVector >= 8)) {\n+    Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n+              vf);\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-  if (kKeysPerRow >= 16 && kMaxKeysPerVector >= 16) {\n-    // If we do not have 16 keys per vector, this code will not be reached,\n-    // but we have to expand vectors to that size for Merge16x16 to compile.\n-    const CappedTag<T, HWY_MAX(kLanesPerRow, 16 * kLPK)> dw;\n-    Vec<decltype(dw)> w0 = ResizeBitCast(dw, v0);\n-    Vec<decltype(dw)> w1 = ResizeBitCast(dw, v1);\n-    Vec<decltype(dw)> w2 = ResizeBitCast(dw, v2);\n-    Vec<decltype(dw)> w3 = ResizeBitCast(dw, v3);\n-    Vec<decltype(dw)> w4 = ResizeBitCast(dw, v4);\n-    Vec<decltype(dw)> w5 = ResizeBitCast(dw, v5);\n-    Vec<decltype(dw)> w6 = ResizeBitCast(dw, v6);\n-    Vec<decltype(dw)> w7 = ResizeBitCast(dw, v7);\n-    Vec<decltype(dw)> w8 = ResizeBitCast(dw, v8);\n-    Vec<decltype(dw)> w9 = ResizeBitCast(dw, v9);\n-    Vec<decltype(dw)> wa = ResizeBitCast(dw, va);\n-    Vec<decltype(dw)> wb = ResizeBitCast(dw, vb);\n-    Vec<decltype(dw)> wc = ResizeBitCast(dw, vc);\n-    Vec<decltype(dw)> wd = ResizeBitCast(dw, vd);\n-    Vec<decltype(dw)> we = ResizeBitCast(dw, ve);\n-    Vec<decltype(dw)> wf = ResizeBitCast(dw, vf);\n-    Merge16x16(dw, st, w0, w1, w2, w3, w4, w5, w6, w7, w8, w9, wa, wb, wc, wd,\n-               we, wf);\n-    // Truncate back.\n-    v0 = ResizeBitCast(d, w0);\n-    v1 = ResizeBitCast(d, w1);\n-    v2 = ResizeBitCast(d, w2);\n-    v3 = ResizeBitCast(d, w3);\n-    v4 = ResizeBitCast(d, w4);\n-    v5 = ResizeBitCast(d, w5);\n-    v6 = ResizeBitCast(d, w6);\n-    v7 = ResizeBitCast(d, w7);\n-    v8 = ResizeBitCast(d, w8);\n-    v9 = ResizeBitCast(d, w9);\n-    va = ResizeBitCast(d, wa);\n-    vb = ResizeBitCast(d, wb);\n-    vc = ResizeBitCast(d, wc);\n-    vd = ResizeBitCast(d, wd);\n-    ve = ResizeBitCast(d, we);\n-    vf = ResizeBitCast(d, wf);\n-  }\n-#endif  // !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-\n-  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n-  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n-  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n-  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n-  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n-  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n-  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n-  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+    if (HWY_LIKELY(keys_per_row >= 16 && kMaxKeysPerVector >= 16)) {\n+      Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n+                 ve, vf);\n+    }\n+#endif\n+  }\n+\n+  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n+  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n+  StoreU(v2, d, keys + 0x2 * lanes_per_row);\n+  StoreU(v3, d, keys + 0x3 * lanes_per_row);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n-  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n-  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n-  StoreU(va, d, buf + 0xa * kLanesPerRow);\n-  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n-  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n-  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n-  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n-  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n-\n-  const ScalableTag<T> dmax;\n-  const size_t Nmax = Lanes(dmax);\n-\n-  // The first eight vectors have already been stored unconditionally into\n+  // Store remaining vectors into buf and safely copy them into keys.\n+  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n+  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n+  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n+  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n+  StoreU(v8, d, buf + 0x8 * lanes_per_row);\n+  StoreU(v9, d, buf + 0x9 * lanes_per_row);\n+  StoreU(va, d, buf + 0xa * lanes_per_row);\n+  StoreU(vb, d, buf + 0xb * lanes_per_row);\n+  StoreU(vc, d, buf + 0xc * lanes_per_row);\n+  StoreU(vd, d, buf + 0xd * lanes_per_row);\n+  StoreU(ve, d, buf + 0xe * lanes_per_row);\n+  StoreU(vf, d, buf + 0xf * lanes_per_row);\n+\n+  // The first four vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  size_t i = 8 * kLanesPerRow;\n+  i = 4 * lanes_per_row;\n   HWY_UNROLL(1)\n-  for (; i + Nmax <= num_lanes; i += Nmax) {\n-    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n+  for (; i + N <= num_lanes; i += N) {\n+    StoreU(LoadU(d, buf + i), d, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, dmax, buf + i, keys + i);\n-#else   // !HWY_MEM_OPS_MIGHT_FAULT\n-  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n-  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n-  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n-  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n-  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n-  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n-  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n-  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+  SafeCopyN(remaining, d, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n+#if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n+  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n+  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n+  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n+  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n+  BlendedStore(v8, m8, d, keys + 0x8 * lanes_per_row);\n+  BlendedStore(v9, m9, d, keys + 0x9 * lanes_per_row);\n+  BlendedStore(va, ma, d, keys + 0xa * lanes_per_row);\n+  BlendedStore(vb, mb, d, keys + 0xb * lanes_per_row);\n+  BlendedStore(vc, mc, d, keys + 0xc * lanes_per_row);\n+  BlendedStore(vd, md, d, keys + 0xd * lanes_per_row);\n+  BlendedStore(ve, me, d, keys + 0xe * lanes_per_row);\n+  BlendedStore(vf, mf, d, keys + 0xf * lanes_per_row);\n+#endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n // Sorts `keys` within the range [0, num_lanes) via sorting network.\n@@ -584,8 +532,6 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n                            size_t num_lanes, T* buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   HWY_DASSERT(num_lanes <= Constants::BaseCaseNumLanes<kLPK>(Lanes(d)));\n-  // Checking kMaxKeys avoids generating unreachable HWY_ASSERT codepaths.\n-  constexpr size_t kMaxKeys = MaxLanes(d) / kLPK;\n   const size_t num_keys = num_lanes / kLPK;\n \n   // Can be zero when called through HandleSpecialCases, but also 1 (in which\n@@ -594,19 +540,23 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n \n   const size_t ceil_log2 =\n       32 - Num0BitsAboveMS1Bit_Nonzero32(static_cast<uint32_t>(num_keys - 1));\n+\n+  // Checking kMaxKeysPerVector avoids generating unreachable codepaths.\n+  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n+\n   using FuncPtr = decltype(&Sort2To2<Traits, T>);\n   const FuncPtr funcs[9] = {\n     /* <= 1 */ nullptr,  // We ensured num_keys > 1.\n     /* <= 2 */ &Sort2To2<Traits, T>,\n     /* <= 4 */ &Sort3To4<Traits, T>,\n-    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 column\n-    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 columns\n-    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 columns\n-    // 16-row is only used if there are at least 4 columns.\n-    /* <= 64 */ kMaxKeys >= 4 ? &Sort16Rows<64, Traits, T> : nullptr,\n-    /* <= 128 */ kMaxKeys >= 8 ? &Sort16Rows<128, Traits, T> : nullptr,\n+    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 key per row\n+    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 keys per row\n+    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 keys per row\n+    // 16-row is only used if there are at least 4 keys per row.\n+    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<Traits, T> : nullptr,\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    /* <= 256 */ kMaxKeys >= 16 ? &Sort16Rows<256, Traits, T> : nullptr,\n+    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<Traits, T> : nullptr,\n #endif\n   };\n   funcs[ceil_log2](st, keys, num_lanes, ceil_log2, buf);"
        ],
        "files_changed": [
            {
                "filename": "hwy/contrib/sort/vqsort-inl.h",
                "status": "modified",
                "additions": 169,
                "deletions": 219,
                "changes": 388,
                "patch": "@@ -180,42 +180,6 @@ HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n   StoreU(v3, d, in_out3);\n }\n \n-#if HWY_MEM_OPS_MIGHT_FAULT\n-\n-template <size_t kVectors, size_t kLPK, size_t kLanesPerRow, class D,\n-          class Traits, typename T = TFromD<D>>\n-HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n-                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n-  constexpr size_t kMinLanes = kVectors / 2 * kLanesPerRow;\n-  // Must cap for correctness: we will load up to the last valid lane, so\n-  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n-  const CappedTag<T, kMinLanes> dmax;\n-  const size_t Nmax = Lanes(dmax);\n-  HWY_DASSERT(Nmax < num_lanes);\n-  HWY_ASSUME(Nmax <= kMinLanes);\n-\n-  // Fill with padding - last in sort order, not copied to keys.\n-  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n-\n-  // Rounding down allows aligned stores, which are typically faster.\n-  size_t i = num_lanes & ~(Nmax - 1);\n-  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n-  do {\n-    Store(kPadding, dmax, buf + i);\n-    i += Nmax;\n-    // Initialize enough for the last vector even if Lanes > lanes_per_row.\n-  } while (i < (kVectors - 1) * kLanesPerRow + Lanes(d));\n-\n-  // Ensure buf contains all we will read, and perhaps more before.\n-  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n-  do {\n-    end -= static_cast<ptrdiff_t>(Nmax);\n-    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n-  } while (end > static_cast<ptrdiff_t>(kVectors / 2 * kLanesPerRow));\n-}\n-\n-#endif  // HWY_MEM_OPS_MIGHT_FAULT\n-\n template <class Traits, typename T>\n HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n@@ -365,210 +329,194 @@ HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n-template <size_t kMaxKeys, class Traits, typename T>\n+template <class Traits, typename T>\n HWY_NOINLINE void Sort16Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                             size_t /* ceil_log2 */, T* HWY_RESTRICT buf) {\n+                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = kMaxKeys / 2 + 1;\n-  HWY_DASSERT(kMinKeys <= num_keys && num_keys <= kMaxKeys);\n-  HWY_ASSUME(num_keys >= kMinKeys);\n-  HWY_ASSUME(num_keys <= kMaxKeys);\n-  constexpr size_t kKeysPerRow = DivCeil(kMaxKeys, size_t{16});\n-  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n-\n-  const CappedTag<T, kLanesPerRow> d;\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-  using V = Vec<decltype(d)>;\n+  constexpr size_t kMinKeys = 33;  // Sort8Rows handled <= 32.\n+  constexpr size_t kMinLanes = kMinKeys * kLPK;\n+  HWY_DASSERT(num_keys >= kMinKeys);\n+  (void)num_keys;\n+  (void)kMinLanes;\n \n-  // We know the first half of vectors are valid, so load unconditionally.\n-  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n-  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n-  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n-  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n-  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n-  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n-  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n-  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  constexpr size_t kRows = 16;\n+  constexpr size_t kMaxKeysPerRow = 16;  // Limited by sorting_networks-inl.h\n+  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n+  // Verify ceil_log2 can be handled by our matrix.\n+  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n+              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n+\n+  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n+  // (all are non-constexpr).\n+  const size_t max_keys = 1ull << ceil_log2;\n+  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n+  const size_t keys_per_row = DivCeil(max_keys, kRows);\n+  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n+  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n \n+  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n+  // be big enough, but loads are only lanes_per_row apart and may overlap.\n+  const CappedTag<T, kMaxLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  const V kPadding = st.LastValue(d);  // Not copied to keys.\n+  V v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n+\n+  // The first four vectors are always valid because 3 * lanes_per_row +\n+  // kMaxLanesPerRow <= num_lanes (even if kLPK == 2, because the largest\n+  // supported vector size is 512 bits i.e. four 128-bit keys, so lanes_per_row\n+  // and kMaxLanesPerRow are only eight uint64_t, and num_lanes >= 66).\n+  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n+  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n+  V v2 = LoadU(d, keys + 0x2 * lanes_per_row);\n+  V v3 = LoadU(d, keys + 0x3 * lanes_per_row);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  CopyHalfToPaddedBuf<16, kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n-  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n-  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n-  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n-  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n-  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n-  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n-  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n-  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n-#else   // !HWY_MEM_OPS_MIGHT_FAULT\n-  (void)buf;\n+  const size_t N = Lanes(d);\n+  // Copy whole vectors from keys, starting from the first potentially unsafe\n+  // load, rounded down to enable aligned stores.\n+  size_t i = (4 * lanes_per_row) & ~(N - 1);\n+  for (; i + N <= num_lanes; i += N) {\n+    Store(LoadU(d, keys + i), d, buf + i);\n+  }\n+\n+  // Fill missing inputs with padding up to vf's load. This also overwrites the\n+  // last partial vector of inputs, which we copy again below.\n+  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n+  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n+  for (; i < required_lanes; i += N) {\n+    Store(kPadding, d, buf + i);\n+  }\n \n-  // To prevent reading past the end, only activate the lanes corresponding to\n-  // the first kLanesPerRow keys (other lanes' masks will be false).\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, kLanesPerRow),\n+  // Copy over the last partial vector, possibly overwriting already-written\n+  // keys with the same value or padding. Example: num_lanes = 33 u64, N = 8,\n+  // lanes_per_row = 4, end = 25, copy [16..31], pad [32..71], copy [25..32].\n+  HWY_DASSERT(num_lanes > N);\n+  const size_t end = num_lanes - N;\n+  StoreU(LoadU(d, keys + end), d, buf + end);\n+\n+  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n+  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n+  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n+  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n+  v8 = LoadU(d, buf + 0x8 * lanes_per_row);\n+  v9 = LoadU(d, buf + 0x9 * lanes_per_row);\n+  va = LoadU(d, buf + 0xa * lanes_per_row);\n+  vb = LoadU(d, buf + 0xb * lanes_per_row);\n+  vc = LoadU(d, buf + 0xc * lanes_per_row);\n+  vd = LoadU(d, buf + 0xd * lanes_per_row);\n+  ve = LoadU(d, buf + 0xe * lanes_per_row);\n+  vf = LoadU(d, buf + 0xf * lanes_per_row);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+#if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n+  (void)buf;\n+  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n+  // Gt checks at no extra cost, and prevents reading past num_lanes.\n+  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n                                       Set(d, static_cast<T>(num_lanes)));\n-  const V kIota = Iota(d, static_cast<T>((kMinKeys - 1) * kLPK));\n-  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n+  // First offset where not all vector are guaranteed valid.\n+  const V kIota = Iota(d, static_cast<T>(4 * lanes_per_row));\n+  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n   const V k2 = Add(k1, k1);\n   const V k4 = Add(k2, k2);\n   const V k8 = Add(k4, k4);\n \n   using M = Mask<decltype(d)>;\n-  const M m8 = Gt(vnum_lanes, kIota);\n-  const M m9 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M ma = Gt(vnum_lanes, Add(kIota, k2));\n-  const M mb = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n-  const M mc = Gt(vnum_lanes, Add(kIota, k4));\n-  const M md = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-  const M me = Gt(vnum_lanes, Add(kIota, Sub(k8, k2)));\n-  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n-\n-  // Fill with padding - last in sort order, not copied to keys.\n-  const V kPadding = st.LastValue(d);\n-\n-  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n-  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n-  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n-  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n-  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n-  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n-  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n-  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n-#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+  const M m4 = Gt(vnum_lanes, kIota);\n+  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n+  const M m7 = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n+  const M m8 = Gt(vnum_lanes, Add(kIota, k4));\n+  const M m9 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n+  const M ma = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k8));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k8, k1)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k8, k2)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Add(k8, Add(k2, k1))));\n+\n+  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n+  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n+  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n+  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n+  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * lanes_per_row);\n+  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * lanes_per_row);\n+  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * lanes_per_row);\n+  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * lanes_per_row);\n+  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * lanes_per_row);\n+  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * lanes_per_row);\n+  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * lanes_per_row);\n+  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * lanes_per_row);\n+#endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n   Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n             vf);\n   Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n             vf);\n-  if (kKeysPerRow >= 8 && kMaxKeysPerVector >= 8) {\n-    // If we do not have eight keys per vector, this code will not be reached,\n-    // but we have to expand vectors to that size for Merge16x8 to compile.\n-    const CappedTag<T, HWY_MAX(kLanesPerRow, 8 * kLPK)> dw;\n-    Vec<decltype(dw)> w0 = ResizeBitCast(dw, v0);\n-    Vec<decltype(dw)> w1 = ResizeBitCast(dw, v1);\n-    Vec<decltype(dw)> w2 = ResizeBitCast(dw, v2);\n-    Vec<decltype(dw)> w3 = ResizeBitCast(dw, v3);\n-    Vec<decltype(dw)> w4 = ResizeBitCast(dw, v4);\n-    Vec<decltype(dw)> w5 = ResizeBitCast(dw, v5);\n-    Vec<decltype(dw)> w6 = ResizeBitCast(dw, v6);\n-    Vec<decltype(dw)> w7 = ResizeBitCast(dw, v7);\n-    Vec<decltype(dw)> w8 = ResizeBitCast(dw, v8);\n-    Vec<decltype(dw)> w9 = ResizeBitCast(dw, v9);\n-    Vec<decltype(dw)> wa = ResizeBitCast(dw, va);\n-    Vec<decltype(dw)> wb = ResizeBitCast(dw, vb);\n-    Vec<decltype(dw)> wc = ResizeBitCast(dw, vc);\n-    Vec<decltype(dw)> wd = ResizeBitCast(dw, vd);\n-    Vec<decltype(dw)> we = ResizeBitCast(dw, ve);\n-    Vec<decltype(dw)> wf = ResizeBitCast(dw, vf);\n-    Merge16x8(dw, st, w0, w1, w2, w3, w4, w5, w6, w7, w8, w9, wa, wb, wc, wd,\n-              we, wf);\n-    // Truncate back.\n-    v0 = ResizeBitCast(d, w0);\n-    v1 = ResizeBitCast(d, w1);\n-    v2 = ResizeBitCast(d, w2);\n-    v3 = ResizeBitCast(d, w3);\n-    v4 = ResizeBitCast(d, w4);\n-    v5 = ResizeBitCast(d, w5);\n-    v6 = ResizeBitCast(d, w6);\n-    v7 = ResizeBitCast(d, w7);\n-    v8 = ResizeBitCast(d, w8);\n-    v9 = ResizeBitCast(d, w9);\n-    va = ResizeBitCast(d, wa);\n-    vb = ResizeBitCast(d, wb);\n-    vc = ResizeBitCast(d, wc);\n-    vd = ResizeBitCast(d, wd);\n-    ve = ResizeBitCast(d, we);\n-    vf = ResizeBitCast(d, wf);\n-  }\n+  // Only merge across columns if it is possible that vectors are that big\n+  // (reduces code size) and there are actually that many cols.\n+  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n+\n+  if (HWY_LIKELY(keys_per_row >= 8 && kMaxKeysPerVector >= 8)) {\n+    Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n+              vf);\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-  if (kKeysPerRow >= 16 && kMaxKeysPerVector >= 16) {\n-    // If we do not have 16 keys per vector, this code will not be reached,\n-    // but we have to expand vectors to that size for Merge16x16 to compile.\n-    const CappedTag<T, HWY_MAX(kLanesPerRow, 16 * kLPK)> dw;\n-    Vec<decltype(dw)> w0 = ResizeBitCast(dw, v0);\n-    Vec<decltype(dw)> w1 = ResizeBitCast(dw, v1);\n-    Vec<decltype(dw)> w2 = ResizeBitCast(dw, v2);\n-    Vec<decltype(dw)> w3 = ResizeBitCast(dw, v3);\n-    Vec<decltype(dw)> w4 = ResizeBitCast(dw, v4);\n-    Vec<decltype(dw)> w5 = ResizeBitCast(dw, v5);\n-    Vec<decltype(dw)> w6 = ResizeBitCast(dw, v6);\n-    Vec<decltype(dw)> w7 = ResizeBitCast(dw, v7);\n-    Vec<decltype(dw)> w8 = ResizeBitCast(dw, v8);\n-    Vec<decltype(dw)> w9 = ResizeBitCast(dw, v9);\n-    Vec<decltype(dw)> wa = ResizeBitCast(dw, va);\n-    Vec<decltype(dw)> wb = ResizeBitCast(dw, vb);\n-    Vec<decltype(dw)> wc = ResizeBitCast(dw, vc);\n-    Vec<decltype(dw)> wd = ResizeBitCast(dw, vd);\n-    Vec<decltype(dw)> we = ResizeBitCast(dw, ve);\n-    Vec<decltype(dw)> wf = ResizeBitCast(dw, vf);\n-    Merge16x16(dw, st, w0, w1, w2, w3, w4, w5, w6, w7, w8, w9, wa, wb, wc, wd,\n-               we, wf);\n-    // Truncate back.\n-    v0 = ResizeBitCast(d, w0);\n-    v1 = ResizeBitCast(d, w1);\n-    v2 = ResizeBitCast(d, w2);\n-    v3 = ResizeBitCast(d, w3);\n-    v4 = ResizeBitCast(d, w4);\n-    v5 = ResizeBitCast(d, w5);\n-    v6 = ResizeBitCast(d, w6);\n-    v7 = ResizeBitCast(d, w7);\n-    v8 = ResizeBitCast(d, w8);\n-    v9 = ResizeBitCast(d, w9);\n-    va = ResizeBitCast(d, wa);\n-    vb = ResizeBitCast(d, wb);\n-    vc = ResizeBitCast(d, wc);\n-    vd = ResizeBitCast(d, wd);\n-    ve = ResizeBitCast(d, we);\n-    vf = ResizeBitCast(d, wf);\n-  }\n-#endif  // !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-\n-  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n-  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n-  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n-  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n-  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n-  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n-  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n-  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+    if (HWY_LIKELY(keys_per_row >= 16 && kMaxKeysPerVector >= 16)) {\n+      Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n+                 ve, vf);\n+    }\n+#endif\n+  }\n+\n+  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n+  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n+  StoreU(v2, d, keys + 0x2 * lanes_per_row);\n+  StoreU(v3, d, keys + 0x3 * lanes_per_row);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n-  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n-  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n-  StoreU(va, d, buf + 0xa * kLanesPerRow);\n-  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n-  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n-  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n-  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n-  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n-\n-  const ScalableTag<T> dmax;\n-  const size_t Nmax = Lanes(dmax);\n-\n-  // The first eight vectors have already been stored unconditionally into\n+  // Store remaining vectors into buf and safely copy them into keys.\n+  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n+  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n+  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n+  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n+  StoreU(v8, d, buf + 0x8 * lanes_per_row);\n+  StoreU(v9, d, buf + 0x9 * lanes_per_row);\n+  StoreU(va, d, buf + 0xa * lanes_per_row);\n+  StoreU(vb, d, buf + 0xb * lanes_per_row);\n+  StoreU(vc, d, buf + 0xc * lanes_per_row);\n+  StoreU(vd, d, buf + 0xd * lanes_per_row);\n+  StoreU(ve, d, buf + 0xe * lanes_per_row);\n+  StoreU(vf, d, buf + 0xf * lanes_per_row);\n+\n+  // The first four vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  size_t i = 8 * kLanesPerRow;\n+  i = 4 * lanes_per_row;\n   HWY_UNROLL(1)\n-  for (; i + Nmax <= num_lanes; i += Nmax) {\n-    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n+  for (; i + N <= num_lanes; i += N) {\n+    StoreU(LoadU(d, buf + i), d, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, dmax, buf + i, keys + i);\n-#else   // !HWY_MEM_OPS_MIGHT_FAULT\n-  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n-  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n-  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n-  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n-  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n-  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n-  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n-  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+  SafeCopyN(remaining, d, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n+#if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n+  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n+  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n+  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n+  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n+  BlendedStore(v8, m8, d, keys + 0x8 * lanes_per_row);\n+  BlendedStore(v9, m9, d, keys + 0x9 * lanes_per_row);\n+  BlendedStore(va, ma, d, keys + 0xa * lanes_per_row);\n+  BlendedStore(vb, mb, d, keys + 0xb * lanes_per_row);\n+  BlendedStore(vc, mc, d, keys + 0xc * lanes_per_row);\n+  BlendedStore(vd, md, d, keys + 0xd * lanes_per_row);\n+  BlendedStore(ve, me, d, keys + 0xe * lanes_per_row);\n+  BlendedStore(vf, mf, d, keys + 0xf * lanes_per_row);\n+#endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n // Sorts `keys` within the range [0, num_lanes) via sorting network.\n@@ -584,8 +532,6 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n                            size_t num_lanes, T* buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   HWY_DASSERT(num_lanes <= Constants::BaseCaseNumLanes<kLPK>(Lanes(d)));\n-  // Checking kMaxKeys avoids generating unreachable HWY_ASSERT codepaths.\n-  constexpr size_t kMaxKeys = MaxLanes(d) / kLPK;\n   const size_t num_keys = num_lanes / kLPK;\n \n   // Can be zero when called through HandleSpecialCases, but also 1 (in which\n@@ -594,19 +540,23 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n \n   const size_t ceil_log2 =\n       32 - Num0BitsAboveMS1Bit_Nonzero32(static_cast<uint32_t>(num_keys - 1));\n+\n+  // Checking kMaxKeysPerVector avoids generating unreachable codepaths.\n+  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n+\n   using FuncPtr = decltype(&Sort2To2<Traits, T>);\n   const FuncPtr funcs[9] = {\n     /* <= 1 */ nullptr,  // We ensured num_keys > 1.\n     /* <= 2 */ &Sort2To2<Traits, T>,\n     /* <= 4 */ &Sort3To4<Traits, T>,\n-    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 column\n-    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 columns\n-    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 columns\n-    // 16-row is only used if there are at least 4 columns.\n-    /* <= 64 */ kMaxKeys >= 4 ? &Sort16Rows<64, Traits, T> : nullptr,\n-    /* <= 128 */ kMaxKeys >= 8 ? &Sort16Rows<128, Traits, T> : nullptr,\n+    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 key per row\n+    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 keys per row\n+    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 keys per row\n+    // 16-row is only used if there are at least 4 keys per row.\n+    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<Traits, T> : nullptr,\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    /* <= 256 */ kMaxKeys >= 16 ? &Sort16Rows<256, Traits, T> : nullptr,\n+    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<Traits, T> : nullptr,\n #endif\n   };\n   funcs[ceil_log2](st, keys, num_lanes, ceil_log2, buf);"
            }
        ],
        "lines_added": 169,
        "lines_removed": 219
    },
    "issues": [],
    "pull_requests": [],
    "build_info": {
        "old_build_script": [
            "apt-get update",
            "cmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
            "cmake --build /test_workspace/workspace/old/build -- -j 1"
        ],
        "new_build_script": [
            "apt-get update",
            "cmake -S /test_workspace/workspace/new -B /test_workspace/workspace/new/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
            "cmake --build /test_workspace/workspace/new/build -- -j 1"
        ],
        "old_test_script": [
            "cd /test_workspace/workspace/old/build",
            "ctest --output-on-failure"
        ],
        "new_test_script": [
            "cd /test_workspace/workspace/new/build",
            "ctest --output-on-failure"
        ],
        "build_system": "cmake"
    },
    "performance_analysis": {
        "is_significant": false,
        "p_value": 1.0,
        "is_pair_significant": false,
        "pair_p_value": 0.9999999999925583,
        "is_binom_significant": false,
        "binom_p_value": 0.9999999990686774,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999989373398133,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5324152504583841,
        "relative_improvement": -0.0020445517563672296,
        "absolute_improvement_ms": -34.99999999999659,
        "old_mean_ms": 17118.666666666668,
        "new_mean_ms": 17153.666666666668,
        "old_std_ms": 300.2267342426772,
        "new_std_ms": 296.0601445574907,
        "effect_size_cohens_d": -0.11739029256209989,
        "old_ci95_ms": [
            17006.560161681613,
            17230.773171651726
        ],
        "new_ci95_ms": [
            17043.115991839408,
            17264.217341493924
        ],
        "old_ci99_ms": [
            16967.57909014967,
            17269.75424318367
        ],
        "new_ci99_ms": [
            17004.67590521,
            17302.657428123333
        ],
        "new_times_s": [
            17.12,
            17.5,
            17.11,
            16.82,
            17.05,
            17.46,
            17.46,
            17.09,
            16.89,
            17.12,
            16.94,
            16.92,
            16.97,
            16.88,
            17.06,
            17.55,
            17.53,
            17.2,
            16.81,
            17.1,
            17.33,
            16.78,
            17.47,
            17.66,
            17.12,
            17.07,
            16.86,
            16.87,
            17.13,
            17.95,
            16.91
        ],
        "old_times_s": [
            17.26,
            17.05,
            16.74,
            17.17,
            17.16,
            17.12,
            17.18,
            16.99,
            16.7,
            17.17,
            16.96,
            16.91,
            16.71,
            17.73,
            17.17,
            17.02,
            17.01,
            16.92,
            17.73,
            17.13,
            16.82,
            17.17,
            16.63,
            17.5,
            16.96,
            16.81,
            17.24,
            17.57,
            17.44,
            17.19,
            17.66
        ]
    },
    "tests": {
        "total_tests": 1,
        "significant_improvements": 0,
        "significant_improvements_tests": [],
        "significant_regressions": 0,
        "significant_regressions_tests": [],
        "significant_pair_improvements": 0,
        "significant_pair_improvements_tests": [],
        "significant_pair_regressions": 0,
        "significant_pair_regressions_tests": [],
        "significant_binom_improvements": 0,
        "significant_binom_improvements_tests": [],
        "significant_binom_regressions": 0,
        "significant_binom_regressions_tests": [],
        "significant_wilcoxon_improvements": 0,
        "significant_wilcoxon_improvements_tests": [],
        "significant_wilcoxon_regressions": 0,
        "significant_wilcoxon_regressions_tests": [],
        "significant_mannwhitney_improvements": 0,
        "significant_mannwhitney_improvements_tests": [],
        "significant_mannwhitney_regressions": 0,
        "significant_mannwhitney_regressions_tests": [],
        "tests": [
            {
                "test_name": "NanobenchmarkTest.RunAll",
                "is_significant": false,
                "p_value": 0.8550770300373479,
                "is_pair_significant": false,
                "pair_p_value": 0.8401398177757348,
                "is_binom_significant": false,
                "binom_p_value": 0.644464448094368,
                "is_wilcoxon_significant": false,
                "wilcoxon_p_value": 0.8094350821500478,
                "is_mannwhitney_significant": false,
                "mannwhitney_p_value": 0.5741035101076352,
                "relative_improvement": -0.024034334763948444,
                "absolute_improvement_ms": -9.655172413793045,
                "old_mean_ms": 401.72413793103453,
                "new_mean_ms": 411.37931034482756,
                "old_std_ms": 116.13012915067185,
                "new_std_ms": 101.44400770625829,
                "effect_size_cohens_d": -0.08855144400440715,
                "old_ci95_ms": [
                    357.55059924663175,
                    445.89767661543726
                ],
                "new_ci95_ms": [
                    372.79207349693564,
                    449.9665471927194
                ],
                "old_ci99_ms": [
                    342.1348705042036,
                    461.3134053578654
                ],
                "new_ci99_ms": [
                    359.32585834001037,
                    463.43276234964475
                ],
                "new_times": [
                    0.56,
                    0.36,
                    0.41,
                    0.2,
                    0.44,
                    0.39,
                    0.38,
                    0.32,
                    0.41,
                    0.25,
                    0.47,
                    0.42,
                    0.47,
                    0.27,
                    0.52,
                    0.46,
                    0.45,
                    0.52,
                    0.55,
                    0.42,
                    0.22,
                    0.3,
                    0.52,
                    0.46,
                    0.42,
                    0.31,
                    0.58,
                    0.48,
                    0.37
                ],
                "old_times": [
                    0.47,
                    0.4,
                    0.53,
                    0.19,
                    0.34,
                    0.46,
                    0.48,
                    0.56,
                    0.4,
                    0.35,
                    0.4,
                    0.33,
                    0.21,
                    0.47,
                    0.47,
                    0.59,
                    0.24,
                    0.17,
                    0.46,
                    0.58,
                    0.42,
                    0.49,
                    0.36,
                    0.5,
                    0.46,
                    0.29,
                    0.22,
                    0.39,
                    0.42
                ]
            }
        ]
    },
    "logs": {
        "full_log_path": "/logs/full.log",
        "config_log_path": "/logs/config.log",
        "build_log_path": "/logs/build.log",
        "test_log_path": "/logs/test.log",
        "build_success": true,
        "test_success": true
    },
    "raw_timing_data": {
        "warmup_runs": 1,
        "measurement_runs": 30,
        "min_exec_time_improvement": 0.05,
        "min_p_value": 0.05
    }
}