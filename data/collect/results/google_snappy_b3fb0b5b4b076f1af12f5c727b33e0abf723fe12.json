{
  "metadata": {
    "collection_date": "2026-01-11T17:52:33.646383",
    "repository": "https://github.com/google/snappy",
    "repository_name": "google/snappy"
  },
  "commit_info": {
    "old_sha": "b638ebe5d95ec4559921a72f8c2bbc4b1b5a2fd0",
    "new_sha": "b3fb0b5b4b076f1af12f5c727b33e0abf723fe12",
    "commit_message": [
      "Enable vector byte shuffle optimizations on ARM NEON\n\nThe SSSE3 intrinsics we use have their direct analogues in NEON, so making this optimization portable requires a very thin translation layer.\n\nPiperOrigin-RevId: 381280165"
    ],
    "commit_date": "2021-06-24T17:09:34+00:00",
    "patch": [
      "--- snappy-internal.h\n@@ -36,6 +36,56 @@\n namespace snappy {\n namespace internal {\n \n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+#if SNAPPY_HAVE_SSSE3\n+using V128 = __m128i;\n+#else\n+using V128 = uint8x16_t;\n+#endif\n+\n+// Load 128 bits of integer data. `src` must be 16-byte aligned.\n+inline V128 V128_Load(const V128* src);\n+\n+// Load 128 bits of integer data. `src` does not need to be aligned.\n+inline V128 V128_LoadU(const V128* src);\n+\n+// Store 128 bits of integer data. `dst` does not need to be aligned.\n+inline void V128_StoreU(V128* dst, V128 val);\n+\n+// Shuffle packed 8-bit integers using a shuffle mask.\n+// Each packed integer in the shuffle mask must be in [0,16).\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask);\n+\n+#if SNAPPY_HAVE_SSSE3\n+inline V128 V128_Load(const V128* src) { return _mm_load_si128(src); }\n+\n+inline V128 V128_LoadU(const V128* src) { return _mm_loadu_si128(src); }\n+\n+inline void V128_StoreU(V128* dst, V128 val) { _mm_storeu_si128(dst, val); }\n+\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n+  return _mm_shuffle_epi8(input, shuffle_mask);\n+}\n+#else\n+inline V128 V128_Load(const V128* src) {\n+  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n+}\n+\n+inline V128 V128_LoadU(const V128* src) {\n+  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n+}\n+\n+inline void V128_StoreU(V128* dst, V128 val) {\n+  vst1q_u8(reinterpret_cast<uint8_t*>(dst), val);\n+}\n+\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n+  assert(vminvq_u8(shuffle_mask) >= 0 && vmaxvq_u8(shuffle_mask) <= 15);\n+  return vqtbl1q_u8(input, shuffle_mask);\n+}\n+#endif\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+\n // Working memory performs a single allocation to hold all scratch space\n // required for compression.\n class WorkingMemory {\n--- snappy.cc\n@@ -30,17 +30,6 @@\n #include \"snappy-sinksource.h\"\n #include \"snappy.h\"\n \n-#if !defined(SNAPPY_HAVE_SSSE3)\n-// __SSSE3__ is defined by GCC and Clang. Visual Studio doesn't target SIMD\n-// support between SSE2 and AVX (so SSSE3 instructions require AVX support), and\n-// defines __AVX__ when AVX support is available.\n-#if defined(__SSSE3__) || defined(__AVX__)\n-#define SNAPPY_HAVE_SSSE3 1\n-#else\n-#define SNAPPY_HAVE_SSSE3 0\n-#endif\n-#endif  // !defined(SNAPPY_HAVE_SSSE3)\n-\n #if !defined(SNAPPY_HAVE_BMI2)\n // __BMI2__ is defined by GCC and Clang. Visual Studio doesn't target BMI2\n // specifically, but it does define __AVX2__ when AVX2 support is available.\n@@ -56,12 +45,6 @@\n #endif\n #endif  // !defined(SNAPPY_HAVE_BMI2)\n \n-#if SNAPPY_HAVE_SSSE3\n-// Please do not replace with <x86intrin.h>. or with headers that assume more\n-// advanced SSE versions without checking with all the OWNERS.\n-#include <tmmintrin.h>\n-#endif\n-\n #if SNAPPY_HAVE_BMI2\n // Please do not replace with <x86intrin.h>. or with headers that assume more\n // advanced SSE versions without checking with all the OWNERS.\n@@ -91,6 +74,13 @@ using internal::COPY_2_BYTE_OFFSET;\n using internal::COPY_4_BYTE_OFFSET;\n using internal::kMaximumTagLength;\n using internal::LITERAL;\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+using internal::V128;\n+using internal::V128_Load;\n+using internal::V128_LoadU;\n+using internal::V128_Shuffle;\n+using internal::V128_StoreU;\n+#endif\n \n // We translate the information encoded in a tag through a lookup table to a\n // format that requires fewer instructions to decode. Effectively we store\n@@ -228,7 +218,7 @@ inline char* IncrementalCopySlow(const char* src, char* op,\n   return op_limit;\n }\n \n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n // Computes the bytes for shuffle control mask (please read comments on\n // 'pattern_generation_masks' as well) for the given index_offset and\n@@ -248,19 +238,19 @@ inline constexpr std::array<char, sizeof...(indexes)> MakePatternMaskBytes(\n // Computes the shuffle control mask bytes array for given pattern-sizes and\n // returns an array.\n template <size_t... pattern_sizes_minus_one>\n-inline constexpr std::array<std::array<char, sizeof(__m128i)>,\n+inline constexpr std::array<std::array<char, sizeof(V128)>,\n                             sizeof...(pattern_sizes_minus_one)>\n MakePatternMaskBytesTable(int index_offset,\n                           index_sequence<pattern_sizes_minus_one...>) {\n-  return {MakePatternMaskBytes(\n-      index_offset, pattern_sizes_minus_one + 1,\n-      make_index_sequence</*indexes=*/sizeof(__m128i)>())...};\n+  return {\n+      MakePatternMaskBytes(index_offset, pattern_sizes_minus_one + 1,\n+                           make_index_sequence</*indexes=*/sizeof(V128)>())...};\n }\n \n // This is an array of shuffle control masks that can be used as the source\n // operand for PSHUFB to permute the contents of the destination XMM register\n // into a repeating byte pattern.\n-alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n+alignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                  16> pattern_generation_masks =\n     MakePatternMaskBytesTable(\n         /*index_offset=*/0,\n@@ -271,48 +261,48 @@ alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n // Basically, pattern_reshuffle_masks is a continuation of\n // pattern_generation_masks. It follows that, pattern_reshuffle_masks is same as\n // pattern_generation_masks for offsets 1, 2, 4, 8 and 16.\n-alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n+alignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                  16> pattern_reshuffle_masks =\n     MakePatternMaskBytesTable(\n         /*index_offset=*/16,\n         /*pattern_sizes_minus_one=*/make_index_sequence<16>());\n \n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n-static inline __m128i LoadPattern(const char* src, const size_t pattern_size) {\n-  __m128i generation_mask = _mm_load_si128(reinterpret_cast<const __m128i*>(\n+static inline V128 LoadPattern(const char* src, const size_t pattern_size) {\n+  V128 generation_mask = V128_Load(reinterpret_cast<const V128*>(\n       pattern_generation_masks[pattern_size - 1].data()));\n   // Uninitialized bytes are masked out by the shuffle mask.\n   // TODO: remove annotation and macro defs once MSan is fixed.\n   SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED(src + pattern_size, 16 - pattern_size);\n-  return _mm_shuffle_epi8(\n-      _mm_loadu_si128(reinterpret_cast<const __m128i*>(src)), generation_mask);\n+  return V128_Shuffle(V128_LoadU(reinterpret_cast<const V128*>(src)),\n+                      generation_mask);\n }\n \n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n-static inline std::pair<__m128i /* pattern */, __m128i /* reshuffle_mask */>\n+static inline std::pair<V128 /* pattern */, V128 /* reshuffle_mask */>\n LoadPatternAndReshuffleMask(const char* src, const size_t pattern_size) {\n-  __m128i pattern = LoadPattern(src, pattern_size);\n+  V128 pattern = LoadPattern(src, pattern_size);\n \n   // This mask will generate the next 16 bytes in-place. Doing so enables us to\n-  // write data by at most 4 _mm_storeu_si128.\n+  // write data by at most 4 V128_StoreU.\n   //\n   // For example, suppose pattern is:        abcdefabcdefabcd\n   // Shuffling with this mask will generate: efabcdefabcdefab\n   // Shuffling again will generate:          cdefabcdefabcdef\n-  __m128i reshuffle_mask = _mm_load_si128(reinterpret_cast<const __m128i*>(\n+  V128 reshuffle_mask = V128_Load(reinterpret_cast<const V128*>(\n       pattern_reshuffle_masks[pattern_size - 1].data()));\n   return {pattern, reshuffle_mask};\n }\n \n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n // Fallback for when we need to copy while extending the pattern, for example\n // copying 10 bytes from 3 positions back abc -> abcabcabcabca.\n //\n // REQUIRES: [dst - offset, dst + 64) is a valid address range.\n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   if (SNAPPY_PREDICT_TRUE(offset <= 16)) {\n     switch (offset) {\n       case 0:\n@@ -325,20 +315,20 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n       case 4:\n       case 8:\n       case 16: {\n-        __m128i pattern = LoadPattern(dst - offset, offset);\n+        V128 pattern = LoadPattern(dst - offset, offset);\n         for (int i = 0; i < 4; i++) {\n-          _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + 16 * i), pattern);\n+          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n         }\n         return true;\n       }\n       default: {\n         auto pattern_and_reshuffle_mask =\n             LoadPatternAndReshuffleMask(dst - offset, offset);\n-        __m128i pattern = pattern_and_reshuffle_mask.first;\n-        __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+        V128 pattern = pattern_and_reshuffle_mask.first;\n+        V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n         for (int i = 0; i < 4; i++) {\n-          _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + 16 * i), pattern);\n-          pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n+          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n+          pattern = V128_Shuffle(pattern, reshuffle_mask);\n         }\n         return true;\n       }\n@@ -361,7 +351,7 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n     }\n     return true;\n   }\n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n   // Very rare.\n   for (int i = 0; i < 4; i++) {\n@@ -375,7 +365,7 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n // region of the buffer.\n inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n                              char* const buf_limit) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   constexpr int big_pattern_size_lower_bound = 16;\n #else\n   constexpr int big_pattern_size_lower_bound = 8;\n@@ -425,14 +415,14 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n   // Handle the uncommon case where pattern is less than 16 (or 8 in non-SSE)\n   // bytes.\n   if (pattern_size < big_pattern_size_lower_bound) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n     // Load the first eight bytes into an 128-bit XMM register, then use PSHUFB\n     // to permute the register's contents in-place into a repeating sequence of\n     // the first \"pattern_size\" bytes.\n     // For example, suppose:\n     //    src       == \"abc\"\n     //    op        == op + 3\n-    // After _mm_shuffle_epi8(), \"pattern\" will have five copies of \"abc\"\n+    // After V128_Shuffle(), \"pattern\" will have five copies of \"abc\"\n     // followed by one byte of slop: abcabcabcabcabca.\n     //\n     // The non-SSE fallback implementation suffers from store-forwarding stalls\n@@ -444,35 +434,35 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n     if (SNAPPY_PREDICT_TRUE(op_limit <= buf_limit - 15)) {\n       auto pattern_and_reshuffle_mask =\n           LoadPatternAndReshuffleMask(src, pattern_size);\n-      __m128i pattern = pattern_and_reshuffle_mask.first;\n-      __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+      V128 pattern = pattern_and_reshuffle_mask.first;\n+      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n \n       // There is at least one, and at most four 16-byte blocks. Writing four\n       // conditionals instead of a loop allows FDO to layout the code with\n       // respect to the actual probabilities of each length.\n       // TODO: Replace with loop with trip count hint.\n-      _mm_storeu_si128(reinterpret_cast<__m128i*>(op), pattern);\n+      V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n \n       if (op + 16 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 16), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 16), pattern);\n       }\n       if (op + 32 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 32), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 32), pattern);\n       }\n       if (op + 48 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 48), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 48), pattern);\n       }\n       return op_limit;\n     }\n     char* const op_end = buf_limit - 15;\n     if (SNAPPY_PREDICT_TRUE(op < op_end)) {\n       auto pattern_and_reshuffle_mask =\n           LoadPatternAndReshuffleMask(src, pattern_size);\n-      __m128i pattern = pattern_and_reshuffle_mask.first;\n-      __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+      V128 pattern = pattern_and_reshuffle_mask.first;\n+      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n \n       // This code path is relatively cold however so we save code size\n       // by avoiding unrolling and vectorizing.\n@@ -483,13 +473,13 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n #pragma clang loop unroll(disable)\n #endif\n       do {\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op), pattern);\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n         op += 16;\n       } while (SNAPPY_PREDICT_TRUE(op < op_end));\n     }\n     return IncrementalCopySlow(op - pattern_size, op, op_limit);\n-#else   // !SNAPPY_HAVE_SSSE3\n+#else   // !SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n     // If plenty of buffer space remains, expand the pattern to at least 8\n     // bytes. The way the following loop is written, we need 8 bytes of buffer\n     // space if pattern_size >= 4, 11 bytes if pattern_size is 1 or 3, and 10\n@@ -506,7 +496,7 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n     } else {\n       return IncrementalCopySlow(src, op, op_limit);\n     }\n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   }\n   assert(pattern_size >= big_pattern_size_lower_bound);\n   constexpr bool use_16bytes_chunk = big_pattern_size_lower_bound == 16;"
    ],
    "files_changed": [
      {
        "filename": "snappy-internal.h",
        "status": "modified",
        "additions": 50,
        "deletions": 0,
        "changes": 50,
        "patch": "@@ -36,6 +36,56 @@\n namespace snappy {\n namespace internal {\n \n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+#if SNAPPY_HAVE_SSSE3\n+using V128 = __m128i;\n+#else\n+using V128 = uint8x16_t;\n+#endif\n+\n+// Load 128 bits of integer data. `src` must be 16-byte aligned.\n+inline V128 V128_Load(const V128* src);\n+\n+// Load 128 bits of integer data. `src` does not need to be aligned.\n+inline V128 V128_LoadU(const V128* src);\n+\n+// Store 128 bits of integer data. `dst` does not need to be aligned.\n+inline void V128_StoreU(V128* dst, V128 val);\n+\n+// Shuffle packed 8-bit integers using a shuffle mask.\n+// Each packed integer in the shuffle mask must be in [0,16).\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask);\n+\n+#if SNAPPY_HAVE_SSSE3\n+inline V128 V128_Load(const V128* src) { return _mm_load_si128(src); }\n+\n+inline V128 V128_LoadU(const V128* src) { return _mm_loadu_si128(src); }\n+\n+inline void V128_StoreU(V128* dst, V128 val) { _mm_storeu_si128(dst, val); }\n+\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n+  return _mm_shuffle_epi8(input, shuffle_mask);\n+}\n+#else\n+inline V128 V128_Load(const V128* src) {\n+  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n+}\n+\n+inline V128 V128_LoadU(const V128* src) {\n+  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n+}\n+\n+inline void V128_StoreU(V128* dst, V128 val) {\n+  vst1q_u8(reinterpret_cast<uint8_t*>(dst), val);\n+}\n+\n+inline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n+  assert(vminvq_u8(shuffle_mask) >= 0 && vmaxvq_u8(shuffle_mask) <= 15);\n+  return vqtbl1q_u8(input, shuffle_mask);\n+}\n+#endif\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+\n // Working memory performs a single allocation to hold all scratch space\n // required for compression.\n class WorkingMemory {"
      },
      {
        "filename": "snappy.cc",
        "status": "modified",
        "additions": 49,
        "deletions": 59,
        "changes": 108,
        "patch": "@@ -30,17 +30,6 @@\n #include \"snappy-sinksource.h\"\n #include \"snappy.h\"\n \n-#if !defined(SNAPPY_HAVE_SSSE3)\n-// __SSSE3__ is defined by GCC and Clang. Visual Studio doesn't target SIMD\n-// support between SSE2 and AVX (so SSSE3 instructions require AVX support), and\n-// defines __AVX__ when AVX support is available.\n-#if defined(__SSSE3__) || defined(__AVX__)\n-#define SNAPPY_HAVE_SSSE3 1\n-#else\n-#define SNAPPY_HAVE_SSSE3 0\n-#endif\n-#endif  // !defined(SNAPPY_HAVE_SSSE3)\n-\n #if !defined(SNAPPY_HAVE_BMI2)\n // __BMI2__ is defined by GCC and Clang. Visual Studio doesn't target BMI2\n // specifically, but it does define __AVX2__ when AVX2 support is available.\n@@ -56,12 +45,6 @@\n #endif\n #endif  // !defined(SNAPPY_HAVE_BMI2)\n \n-#if SNAPPY_HAVE_SSSE3\n-// Please do not replace with <x86intrin.h>. or with headers that assume more\n-// advanced SSE versions without checking with all the OWNERS.\n-#include <tmmintrin.h>\n-#endif\n-\n #if SNAPPY_HAVE_BMI2\n // Please do not replace with <x86intrin.h>. or with headers that assume more\n // advanced SSE versions without checking with all the OWNERS.\n@@ -91,6 +74,13 @@ using internal::COPY_2_BYTE_OFFSET;\n using internal::COPY_4_BYTE_OFFSET;\n using internal::kMaximumTagLength;\n using internal::LITERAL;\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n+using internal::V128;\n+using internal::V128_Load;\n+using internal::V128_LoadU;\n+using internal::V128_Shuffle;\n+using internal::V128_StoreU;\n+#endif\n \n // We translate the information encoded in a tag through a lookup table to a\n // format that requires fewer instructions to decode. Effectively we store\n@@ -228,7 +218,7 @@ inline char* IncrementalCopySlow(const char* src, char* op,\n   return op_limit;\n }\n \n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n // Computes the bytes for shuffle control mask (please read comments on\n // 'pattern_generation_masks' as well) for the given index_offset and\n@@ -248,19 +238,19 @@ inline constexpr std::array<char, sizeof...(indexes)> MakePatternMaskBytes(\n // Computes the shuffle control mask bytes array for given pattern-sizes and\n // returns an array.\n template <size_t... pattern_sizes_minus_one>\n-inline constexpr std::array<std::array<char, sizeof(__m128i)>,\n+inline constexpr std::array<std::array<char, sizeof(V128)>,\n                             sizeof...(pattern_sizes_minus_one)>\n MakePatternMaskBytesTable(int index_offset,\n                           index_sequence<pattern_sizes_minus_one...>) {\n-  return {MakePatternMaskBytes(\n-      index_offset, pattern_sizes_minus_one + 1,\n-      make_index_sequence</*indexes=*/sizeof(__m128i)>())...};\n+  return {\n+      MakePatternMaskBytes(index_offset, pattern_sizes_minus_one + 1,\n+                           make_index_sequence</*indexes=*/sizeof(V128)>())...};\n }\n \n // This is an array of shuffle control masks that can be used as the source\n // operand for PSHUFB to permute the contents of the destination XMM register\n // into a repeating byte pattern.\n-alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n+alignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                  16> pattern_generation_masks =\n     MakePatternMaskBytesTable(\n         /*index_offset=*/0,\n@@ -271,48 +261,48 @@ alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n // Basically, pattern_reshuffle_masks is a continuation of\n // pattern_generation_masks. It follows that, pattern_reshuffle_masks is same as\n // pattern_generation_masks for offsets 1, 2, 4, 8 and 16.\n-alignas(16) constexpr std::array<std::array<char, sizeof(__m128i)>,\n+alignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                  16> pattern_reshuffle_masks =\n     MakePatternMaskBytesTable(\n         /*index_offset=*/16,\n         /*pattern_sizes_minus_one=*/make_index_sequence<16>());\n \n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n-static inline __m128i LoadPattern(const char* src, const size_t pattern_size) {\n-  __m128i generation_mask = _mm_load_si128(reinterpret_cast<const __m128i*>(\n+static inline V128 LoadPattern(const char* src, const size_t pattern_size) {\n+  V128 generation_mask = V128_Load(reinterpret_cast<const V128*>(\n       pattern_generation_masks[pattern_size - 1].data()));\n   // Uninitialized bytes are masked out by the shuffle mask.\n   // TODO: remove annotation and macro defs once MSan is fixed.\n   SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED(src + pattern_size, 16 - pattern_size);\n-  return _mm_shuffle_epi8(\n-      _mm_loadu_si128(reinterpret_cast<const __m128i*>(src)), generation_mask);\n+  return V128_Shuffle(V128_LoadU(reinterpret_cast<const V128*>(src)),\n+                      generation_mask);\n }\n \n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n-static inline std::pair<__m128i /* pattern */, __m128i /* reshuffle_mask */>\n+static inline std::pair<V128 /* pattern */, V128 /* reshuffle_mask */>\n LoadPatternAndReshuffleMask(const char* src, const size_t pattern_size) {\n-  __m128i pattern = LoadPattern(src, pattern_size);\n+  V128 pattern = LoadPattern(src, pattern_size);\n \n   // This mask will generate the next 16 bytes in-place. Doing so enables us to\n-  // write data by at most 4 _mm_storeu_si128.\n+  // write data by at most 4 V128_StoreU.\n   //\n   // For example, suppose pattern is:        abcdefabcdefabcd\n   // Shuffling with this mask will generate: efabcdefabcdefab\n   // Shuffling again will generate:          cdefabcdefabcdef\n-  __m128i reshuffle_mask = _mm_load_si128(reinterpret_cast<const __m128i*>(\n+  V128 reshuffle_mask = V128_Load(reinterpret_cast<const V128*>(\n       pattern_reshuffle_masks[pattern_size - 1].data()));\n   return {pattern, reshuffle_mask};\n }\n \n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n // Fallback for when we need to copy while extending the pattern, for example\n // copying 10 bytes from 3 positions back abc -> abcabcabcabca.\n //\n // REQUIRES: [dst - offset, dst + 64) is a valid address range.\n SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   if (SNAPPY_PREDICT_TRUE(offset <= 16)) {\n     switch (offset) {\n       case 0:\n@@ -325,20 +315,20 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n       case 4:\n       case 8:\n       case 16: {\n-        __m128i pattern = LoadPattern(dst - offset, offset);\n+        V128 pattern = LoadPattern(dst - offset, offset);\n         for (int i = 0; i < 4; i++) {\n-          _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + 16 * i), pattern);\n+          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n         }\n         return true;\n       }\n       default: {\n         auto pattern_and_reshuffle_mask =\n             LoadPatternAndReshuffleMask(dst - offset, offset);\n-        __m128i pattern = pattern_and_reshuffle_mask.first;\n-        __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+        V128 pattern = pattern_and_reshuffle_mask.first;\n+        V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n         for (int i = 0; i < 4; i++) {\n-          _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + 16 * i), pattern);\n-          pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n+          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n+          pattern = V128_Shuffle(pattern, reshuffle_mask);\n         }\n         return true;\n       }\n@@ -361,7 +351,7 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n     }\n     return true;\n   }\n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n \n   // Very rare.\n   for (int i = 0; i < 4; i++) {\n@@ -375,7 +365,7 @@ static inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n // region of the buffer.\n inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n                              char* const buf_limit) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   constexpr int big_pattern_size_lower_bound = 16;\n #else\n   constexpr int big_pattern_size_lower_bound = 8;\n@@ -425,14 +415,14 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n   // Handle the uncommon case where pattern is less than 16 (or 8 in non-SSE)\n   // bytes.\n   if (pattern_size < big_pattern_size_lower_bound) {\n-#if SNAPPY_HAVE_SSSE3\n+#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n     // Load the first eight bytes into an 128-bit XMM register, then use PSHUFB\n     // to permute the register's contents in-place into a repeating sequence of\n     // the first \"pattern_size\" bytes.\n     // For example, suppose:\n     //    src       == \"abc\"\n     //    op        == op + 3\n-    // After _mm_shuffle_epi8(), \"pattern\" will have five copies of \"abc\"\n+    // After V128_Shuffle(), \"pattern\" will have five copies of \"abc\"\n     // followed by one byte of slop: abcabcabcabcabca.\n     //\n     // The non-SSE fallback implementation suffers from store-forwarding stalls\n@@ -444,35 +434,35 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n     if (SNAPPY_PREDICT_TRUE(op_limit <= buf_limit - 15)) {\n       auto pattern_and_reshuffle_mask =\n           LoadPatternAndReshuffleMask(src, pattern_size);\n-      __m128i pattern = pattern_and_reshuffle_mask.first;\n-      __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+      V128 pattern = pattern_and_reshuffle_mask.first;\n+      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n \n       // There is at least one, and at most four 16-byte blocks. Writing four\n       // conditionals instead of a loop allows FDO to layout the code with\n       // respect to the actual probabilities of each length.\n       // TODO: Replace with loop with trip count hint.\n-      _mm_storeu_si128(reinterpret_cast<__m128i*>(op), pattern);\n+      V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n \n       if (op + 16 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 16), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 16), pattern);\n       }\n       if (op + 32 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 32), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 32), pattern);\n       }\n       if (op + 48 < op_limit) {\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op + 48), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op + 48), pattern);\n       }\n       return op_limit;\n     }\n     char* const op_end = buf_limit - 15;\n     if (SNAPPY_PREDICT_TRUE(op < op_end)) {\n       auto pattern_and_reshuffle_mask =\n           LoadPatternAndReshuffleMask(src, pattern_size);\n-      __m128i pattern = pattern_and_reshuffle_mask.first;\n-      __m128i reshuffle_mask = pattern_and_reshuffle_mask.second;\n+      V128 pattern = pattern_and_reshuffle_mask.first;\n+      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n \n       // This code path is relatively cold however so we save code size\n       // by avoiding unrolling and vectorizing.\n@@ -483,13 +473,13 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n #pragma clang loop unroll(disable)\n #endif\n       do {\n-        _mm_storeu_si128(reinterpret_cast<__m128i*>(op), pattern);\n-        pattern = _mm_shuffle_epi8(pattern, reshuffle_mask);\n+        V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n+        pattern = V128_Shuffle(pattern, reshuffle_mask);\n         op += 16;\n       } while (SNAPPY_PREDICT_TRUE(op < op_end));\n     }\n     return IncrementalCopySlow(op - pattern_size, op, op_limit);\n-#else   // !SNAPPY_HAVE_SSSE3\n+#else   // !SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n     // If plenty of buffer space remains, expand the pattern to at least 8\n     // bytes. The way the following loop is written, we need 8 bytes of buffer\n     // space if pattern_size >= 4, 11 bytes if pattern_size is 1 or 3, and 10\n@@ -506,7 +496,7 @@ inline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n     } else {\n       return IncrementalCopySlow(src, op, op_limit);\n     }\n-#endif  // SNAPPY_HAVE_SSSE3\n+#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n   }\n   assert(pattern_size >= big_pattern_size_lower_bound);\n   constexpr bool use_16bytes_chunk = big_pattern_size_lower_bound == 16;"
      }
    ],
    "lines_added": 99,
    "lines_removed": 59
  },
  "issues": [],
  "pull_requests": [],
  "build_info": {
    "old_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DSNAPPY_BUILD_TESTS=ON -DBENCHMARK_ENABLE_GTEST_TESTS=ON -DBENCHMARK_ENABLE_TESTING=ON -DBENCHMARK_ENABLE_ASSEMBLY_TESTS=ON",
    "new_build_script": "#!/bin/bash\n#!/bin/bash\ncmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DSNAPPY_BUILD_TESTS=ON -DBENCHMARK_ENABLE_GTEST_TESTS=ON -DBENCHMARK_ENABLE_TESTING=ON -DBENCHMARK_ENABLE_ASSEMBLY_TESTS=ON",
    "old_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/old/build -- -j 1",
    "new_test_script": "#!/bin/bash\ncmake --build /test_workspace/workspace/old/build -- -j 1",
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": false,
    "p_value": 1.0,
    "is_pair_significant": false,
    "pair_p_value": 1.0,
    "is_binom_significant": false,
    "binom_p_value": 1.0,
    "is_wilcoxon_significant": false,
    "wilcoxon_p_value": 0.9999991602583989,
    "is_mannwhitney_significant": false,
    "mannwhitney_p_value": 0.7490440726727547,
    "relative_improvement": 0.0006479248407185012,
    "absolute_improvement_ms": 6.000000000000227,
    "old_mean_ms": 9260.333333333334,
    "new_mean_ms": 9254.333333333334,
    "old_std_ms": 54.29696970863321,
    "new_std_ms": 39.71305121470274,
    "effect_size_cohens_d": 0.12613719443996232,
    "old_ci95_ms": [
      9240.05851163676,
      9280.608155029906
    ],
    "new_ci95_ms": [
      9239.504236300376,
      9269.162430366292
    ],
    "old_ci99_ms": [
      9233.008659579053,
      9287.658007087615
    ],
    "new_ci99_ms": [
      9234.347942361206,
      9274.318724305462
    ],
    "new_times_s": [
      9.13,
      9.28,
      9.22,
      9.2,
      9.24,
      9.25,
      9.29,
      9.2,
      9.25,
      9.26,
      9.27,
      9.27,
      9.23,
      9.27,
      9.29,
      9.29,
      9.25,
      9.26,
      9.27,
      9.27,
      9.27,
      9.27,
      9.28,
      9.26,
      9.25,
      9.24,
      9.11,
      9.23,
      9.35,
      9.26,
      9.25
    ],
    "old_times_s": [
      9.24,
      9.25,
      9.24,
      9.22,
      9.21,
      9.25,
      9.4,
      9.23,
      9.23,
      9.28,
      9.27,
      9.27,
      9.26,
      9.27,
      9.18,
      9.24,
      9.27,
      9.27,
      9.27,
      9.26,
      9.47,
      9.28,
      9.25,
      9.26,
      9.28,
      9.25,
      9.24,
      9.19,
      9.24,
      9.24,
      9.24
    ]
  },
  "tests": {
    "total_tests": 1,
    "significant_improvements": 0,
    "significant_improvements_tests": [],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 0,
    "significant_pair_improvements_tests": [],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 0,
    "significant_binom_improvements_tests": [],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 0,
    "significant_wilcoxon_improvements_tests": [],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 0,
    "significant_mannwhitney_improvements_tests": [],
    "significant_mannwhitney_regressions": 0,
    "significant_mannwhitney_regressions_tests": [],
    "tests": [
      {
        "test_name": "snappy_unittest",
        "is_significant": false,
        "p_value": 1.0,
        "is_pair_significant": false,
        "pair_p_value": 1.0,
        "is_binom_significant": false,
        "binom_p_value": 1.0,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.9999987525945524,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.713721769764402,
        "relative_improvement": 0.0007820937767680144,
        "absolute_improvement_ms": 7.241379310345408,
        "old_mean_ms": 9258.965517241379,
        "new_mean_ms": 9251.724137931033,
        "old_std_ms": 55.50706696231232,
        "new_std_ms": 39.37629503695635,
        "effect_size_cohens_d": 0.1504784087930488,
        "old_ci95_ms": [
          9237.851758153622,
          9280.079276329136
        ],
        "new_ci95_ms": [
          9236.746196334097,
          9266.70207952797
        ],
        "old_ci99_ms": [
          9230.483455940948,
          9287.447578541809
        ],
        "new_ci99_ms": [
          9231.519178267286,
          9271.92909759478
        ],
        "new_times": [
          9.22,
          9.2,
          9.24,
          9.25,
          9.28,
          9.2,
          9.25,
          9.25,
          9.27,
          9.27,
          9.23,
          9.27,
          9.28,
          9.29,
          9.25,
          9.25,
          9.27,
          9.26,
          9.27,
          9.27,
          9.28,
          9.26,
          9.25,
          9.24,
          9.11,
          9.23,
          9.35,
          9.26,
          9.25
        ],
        "old_times": [
          9.24,
          9.22,
          9.21,
          9.24,
          9.4,
          9.23,
          9.23,
          9.28,
          9.26,
          9.27,
          9.26,
          9.27,
          9.18,
          9.24,
          9.27,
          9.27,
          9.26,
          9.26,
          9.47,
          9.28,
          9.25,
          9.26,
          9.28,
          9.24,
          9.24,
          9.19,
          9.23,
          9.24,
          9.24
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}