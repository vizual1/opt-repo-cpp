{
  "metadata": {
    "collection_date": "2026-02-03T20:01:58.490960",
    "repository": "https://github.com/google/highway",
    "repository_name": "google/highway"
  },
  "commit_info": {
    "old_sha": "34a72feb9256a315abc0e1c69fffc6c543a13e6e",
    "new_sha": "ff1ba650cd42b6a94cbf14a1a70e2c26f6904fbc",
    "commit_message": [
      "back to per-pow2 code, reuse is too costly\n\nPiperOrigin-RevId: 532721756"
    ],
    "commit_date": "2023-05-17T09:24:54+00:00",
    "patch": [
      "--- hwy/contrib/sort/sorting_networks-inl.h\n@@ -188,8 +188,27 @@ HWY_INLINE void Sort16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,\n // ------------------------------ Merging networks\n \n // Blacher's hybrid bitonic/odd-even networks, generated by print_network.cc.\n-\n-template <class D, class Traits, class V = Vec<D>>\n+// For acceptable performance, these must be inlined, otherwise vectors are\n+// loaded from the stack. The kKeysPerVector allows calling from generic code\n+// but skipping the functions when vectors have too few lanes for\n+// st.SortPairsDistance1 to compile. `if constexpr` in the caller would also\n+// work, but is not available in C++11.\n+\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 1)>\n+HWY_INLINE void Merge8x2(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 2)>\n+HWY_INLINE void Merge8x4(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 1)>\n+HWY_INLINE void Merge16x2(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 2)>\n+HWY_INLINE void Merge16x4(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 4)>\n+HWY_INLINE void Merge16x8(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 8)>\n+HWY_INLINE void Merge16x16(...) {}\n+\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 1)>\n HWY_INLINE void Merge8x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                          V& v5, V& v6, V& v7) {\n   v7 = st.ReverseKeys2(d, v7);\n@@ -229,7 +248,8 @@ HWY_INLINE void Merge8x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   v7 = st.SortPairsDistance1(d, v7);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 2)>\n HWY_INLINE void Merge8x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                          V& v5, V& v6, V& v7) {\n   v7 = st.ReverseKeys4(d, v7);\n@@ -278,7 +298,8 @@ HWY_INLINE void Merge8x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   v7 = st.SortPairsDistance1(d, v7);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 1)>\n HWY_INLINE void Merge16x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -368,7 +389,8 @@ HWY_INLINE void Merge16x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   vf = st.SortPairsDistance1(d, vf);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 2)>\n HWY_INLINE void Merge16x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -475,7 +497,8 @@ HWY_INLINE void Merge16x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   vf = st.SortPairsDistance1(d, vf);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 4)>\n HWY_INLINE void Merge16x8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -602,7 +625,8 @@ HWY_INLINE void Merge16x8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n // Unused on MSVC, see below\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 8)>\n HWY_INLINE void Merge16x16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                            V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                            V& vc, V& vd, V& ve, V& vf) {\n@@ -748,13 +772,7 @@ HWY_INLINE void Merge16x16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n // Reshapes `buf` into a matrix, sorts columns independently, and then merges\n // into a sorted 1D array without transposing.\n //\n-// `st` is SharedTraits<Traits*<Order*>>. This abstraction layer bridges\n-//   differences in sort order and single-lane vs 128-bit keys.\n-//\n-// References:\n-// https://drops.dagstuhl.de/opus/volltexte/2021/13775/pdf/LIPIcs-SEA-2021-3.pdf\n-// https://github.com/simd-sorting/fast-and-robust/blob/master/avx2_sort_demo/avx2sort.h\n-// M. Blacher's thesis: https://github.com/mark-blacher/masterthesis\n+// DEPRECATED, use BaseCase() instead.\n template <class Traits, class V>\n HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n                                V& v3, V& v4, V& v5, V& v6, V& v7, V& v8, V& v9,\n@@ -774,22 +792,22 @@ HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n   // Checking MaxLanes avoids generating HWY_ASSERT code for the unreachable\n   // code paths: if MaxLanes < 2, then keys <= cols < 2.\n   if (HWY_LIKELY(keys >= 2 && kMaxKeys >= 2)) {\n-    Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-              vf);\n+    Merge16x2<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                        vc, vd, ve, vf);\n \n     if (HWY_LIKELY(keys >= 4 && kMaxKeys >= 4)) {\n-      Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                ve, vf);\n+      Merge16x4<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                          vc, vd, ve, vf);\n \n       if (HWY_LIKELY(keys >= 8 && kMaxKeys >= 8)) {\n-        Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                  ve, vf);\n+        Merge16x8<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va,\n+                            vb, vc, vd, ve, vf);\n \n         // Avoids build timeout. Must match #if condition in kMaxCols.\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n         if (HWY_LIKELY(keys >= 16 && kMaxKeys >= 16)) {\n-          Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc,\n-                     vd, ve, vf);\n+          Merge16x16<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9,\n+                               va, vb, vc, vd, ve, vf);\n \n           static_assert(Constants::kMaxCols <= 16, \"Add more branches\");\n         }\n@@ -802,7 +820,7 @@ HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n // As above, but loads from/stores to `buf`. This ensures full vectors are\n // aligned, and enables loads/stores without bounds checks.\n //\n-// NOINLINE because this is large and called twice from vqsort-inl.h.\n+// DEPRECATED, use BaseCase() instead.\n template <class Traits, typename T>\n HWY_NOINLINE void SortingNetwork(Traits st, T* HWY_RESTRICT buf, size_t cols) {\n   // traits*-inl assume 'full' vectors (but still capped to kMaxCols).\n--- hwy/contrib/sort/vqsort-inl.h\n@@ -127,7 +127,7 @@ void HeapSort(Traits st, T* HWY_RESTRICT lanes, const size_t num_lanes) {\n // Special cases where `num_lanes` is in the specified range (inclusive).\n template <class Traits, typename T>\n HWY_INLINE void Sort2To2(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                         size_t /* ceil_log2 */, T* HWY_RESTRICT /* buf */) {\n+                         T* HWY_RESTRICT /* buf */) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n   HWY_DASSERT(num_keys == 2);\n@@ -148,7 +148,7 @@ HWY_INLINE void Sort2To2(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n \n template <class Traits, typename T>\n HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                         size_t /* ceil_log2 */, T* HWY_RESTRICT buf) {\n+                         T* HWY_RESTRICT buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n   HWY_DASSERT(3 <= num_keys && num_keys <= 4);\n@@ -180,342 +180,265 @@ HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n   StoreU(v3, d, in_out3);\n }\n \n-template <class Traits, typename T>\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+\n+template <size_t kRows, size_t kLanesPerRow, class D, class Traits,\n+          typename T = TFromD<D>>\n+HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n+                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  // Must cap for correctness: we will load up to the last valid lane, so\n+  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n+  const CappedTag<T, kMinLanes> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+  HWY_DASSERT(Nmax < num_lanes);\n+  HWY_ASSUME(Nmax <= kMinLanes);\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Rounding down allows aligned stores, which are typically faster.\n+  size_t i = num_lanes & ~(Nmax - 1);\n+  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n+  do {\n+    Store(kPadding, dmax, buf + i);\n+    i += Nmax;\n+    // Initialize enough for the last vector even if Nmax > kLanesPerRow.\n+  } while (i < (kRows - 1) * kLanesPerRow + Lanes(d));\n+\n+  // Ensure buf contains all we will read, and perhaps more before.\n+  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n+  do {\n+    end -= static_cast<ptrdiff_t>(Nmax);\n+    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n+  } while (end > static_cast<ptrdiff_t>(kRows / 2 * kLanesPerRow));\n+}\n+\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+\n+template <size_t kKeysPerRow, class Traits, typename T>\n HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                            size_t ceil_log2, T* HWY_RESTRICT buf) {\n+                            T* HWY_RESTRICT buf) {\n+  // kKeysPerRow <= 4 because 8 64-bit keys implies 512-bit vectors, which\n+  // are likely slower than 16x4, so 8x4 is the largest we handle here.\n+  static_assert(kKeysPerRow <= 4, \"\");\n+\n   constexpr size_t kLPK = st.LanesPerKey();\n-  const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = 5;  // Sort3To4 handled <= 4.\n-  constexpr size_t kMinLanes = kMinKeys * kLPK;\n-  HWY_DASSERT(num_keys >= kMinKeys);\n-  (void)num_keys;\n-  (void)kMinLanes;\n \n-  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  // We reshape the 1D keys into kRows x kKeysPerRow.\n   constexpr size_t kRows = 8;\n-  // Chosen for efficiency. Sorting 64 keys with 8 rows would require 8 keys per\n-  // row, which means 512-bit vectors if keys are 64-bit. That is likely slower\n-  // than 16x4, so 8x4 is the largest we handle here.\n-  constexpr size_t kMaxKeysPerRow = 4;\n-  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n-  // Verify ceil_log2 can be handled by our matrix.\n-  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n-              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n-\n-  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n-  // (all are non-constexpr).\n-  const size_t max_keys = 1ull << ceil_log2;\n-  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n-  const size_t keys_per_row = DivCeil(max_keys, kRows);\n-  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n-  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n-\n-  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n-  // be big enough, but loads are only lanes_per_row apart and may overlap.\n-  const CappedTag<T, kMaxLanesPerRow> d;\n+  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  HWY_DASSERT(kMinLanes < num_lanes && num_lanes <= kRows * kLanesPerRow);\n+\n+  const CappedTag<T, kLanesPerRow> d;\n   using V = Vec<decltype(d)>;\n   const V kPadding = st.LastValue(d);  // Not copied to keys.\n-  V v2, v3, v4, v5, v6, v7;\n-\n-  // The first two vectors are always valid because 1 * lanes_per_row +\n-  // kMaxLanesPerRow <= num_lanes. For num_lanes <= 8, lanes_per_row = 1 and\n-  // 1 + kMaxLanesPerRow (4) <= (num_lanes >= kMinLanes >= 5). The inequality\n-  // holds a forteriori for larger num_lanes because kMaxLanesPerRow is capped.\n-  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n-  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n+  V v4, v5, v6, v7;\n+\n+  // At least half the kRows are valid, otherwise a different function would\n+  // have been called to handle this num_lanes.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  const size_t N = Lanes(d);\n-  // Copy whole vectors from keys, starting from the first potentially unsafe\n-  // load, rounded down to enable aligned stores.\n-  size_t i = (2 * lanes_per_row) & ~(N - 1);\n-  for (; i + N <= num_lanes; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n-  }\n-\n-  // Fill missing inputs with padding up to v7's load. This also overwrites the\n-  // last partial vector of inputs, which we copy again below.\n-  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n-  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n-  for (; i < required_lanes; i += N) {\n-    Store(kPadding, d, buf + i);\n-  }\n-\n-  // Copy over the last partial vector, possibly overwriting already-written\n-  // keys with the same value or padding. Example: num_lanes = 7, end = 3, copy\n-  // [0..3], pad [4..11], copy [3..6].\n-  HWY_DASSERT(num_lanes > N);\n-  const size_t end = num_lanes - N;\n-  StoreU(LoadU(d, keys + end), d, buf + end);\n-\n-  v2 = LoadU(d, buf + 0x2 * lanes_per_row);\n-  v3 = LoadU(d, buf + 0x3 * lanes_per_row);\n-  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n-  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n-  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n-  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n+  CopyHalfToPaddedBuf<kRows, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  v4 = LoadU(d, buf + 0x4 * kLanesPerRow);\n+  v5 = LoadU(d, buf + 0x5 * kLanesPerRow);\n+  v6 = LoadU(d, buf + 0x6 * kLanesPerRow);\n+  v7 = LoadU(d, buf + 0x7 * kLanesPerRow);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n   (void)buf;\n-  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n-  // Gt checks at no extra cost, and prevents reading past num_lanes.\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n-                                      Set(d, static_cast<T>(num_lanes)));\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n   // First offset where not all vector are guaranteed valid.\n-  const V kIota = Iota(d, static_cast<T>(2 * lanes_per_row));\n-  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n+  const V kIota = Iota(d, static_cast<T>(kMinLanes));\n+  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n   const V k2 = Add(k1, k1);\n-  const V k4 = Add(k2, k2);\n \n   using M = Mask<decltype(d)>;\n-  const M m2 = Gt(vnum_lanes, kIota);\n-  const M m3 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M m4 = Gt(vnum_lanes, Add(kIota, k2));\n-  const M m5 = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n-  const M m6 = Gt(vnum_lanes, Add(kIota, k4));\n-  const M m7 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-\n-  v2 = MaskedLoadOr(kPadding, m2, d, keys + 0x2 * lanes_per_row);\n-  v3 = MaskedLoadOr(kPadding, m3, d, keys + 0x3 * lanes_per_row);\n-  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n-  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n-  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n-  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n+  const M m4 = Gt(vnum_lanes, kIota);\n+  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n+  const M m7 = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n+\n+  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * kLanesPerRow);\n+  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * kLanesPerRow);\n+  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * kLanesPerRow);\n+  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort8(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n \n-  // Only merge across columns if it is possible that vectors are that big\n-  // (reduces code size) and there are actually that many cols.\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-  if (HWY_LIKELY(keys_per_row >= 2 && kMaxKeysPerVector >= 2)) {\n-    Merge8x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n-    if (HWY_LIKELY(keys_per_row >= 4 && kMaxKeysPerVector >= 4)) {\n-      Merge8x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n-    }\n-  }\n+  // Merge8x2 is a no-op if kKeysPerRow < 2 etc.\n+  Merge8x2<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n+  Merge8x4<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n \n-  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n-  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n   // Store remaining vectors into buf and safely copy them into keys.\n-  StoreU(v2, d, buf + 0x2 * lanes_per_row);\n-  StoreU(v3, d, buf + 0x3 * lanes_per_row);\n-  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n-  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n-  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n-  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n-\n-  // The first two vectors have already been stored unconditionally into\n+  StoreU(v4, d, buf + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, buf + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, buf + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, buf + 0x7 * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first half of vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  i = 2 * lanes_per_row;\n+  size_t i = kMinLanes;\n   HWY_UNROLL(1)\n-  for (; i + N <= num_lanes; i += N) {\n-    StoreU(LoadU(d, buf + i), d, keys + i);\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, d, buf + i, keys + i);\n+  SafeCopyN(remaining, dmax, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n-  BlendedStore(v2, m2, d, keys + 0x2 * lanes_per_row);\n-  BlendedStore(v3, m3, d, keys + 0x3 * lanes_per_row);\n-  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n-  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n-  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n-  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n+  BlendedStore(v4, m4, d, keys + 0x4 * kLanesPerRow);\n+  BlendedStore(v5, m5, d, keys + 0x5 * kLanesPerRow);\n+  BlendedStore(v6, m6, d, keys + 0x6 * kLanesPerRow);\n+  BlendedStore(v7, m7, d, keys + 0x7 * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n-template <class Traits, typename T>\n+template <size_t kKeysPerRow, class Traits, typename T>\n HWY_NOINLINE void Sort16Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n+                             T* HWY_RESTRICT buf) {\n+  static_assert(kKeysPerRow <= SortConstants::kMaxCols, \"\");\n+\n   constexpr size_t kLPK = st.LanesPerKey();\n-  const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = 33;  // Sort8Rows handled <= 32.\n-  constexpr size_t kMinLanes = kMinKeys * kLPK;\n-  HWY_DASSERT(num_keys >= kMinKeys);\n-  (void)num_keys;\n-  (void)kMinLanes;\n \n-  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  // We reshape the 1D keys into kRows x kKeysPerRow.\n   constexpr size_t kRows = 16;\n-  constexpr size_t kMaxKeysPerRow = 16;  // Limited by sorting_networks-inl.h\n-  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n-  // Verify ceil_log2 can be handled by our matrix.\n-  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n-              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n-\n-  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n-  // (all are non-constexpr).\n-  const size_t max_keys = 1ull << ceil_log2;\n-  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n-  const size_t keys_per_row = DivCeil(max_keys, kRows);\n-  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n-  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n-\n-  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n-  // be big enough, but loads are only lanes_per_row apart and may overlap.\n-  const CappedTag<T, kMaxLanesPerRow> d;\n+  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  HWY_DASSERT(kMinLanes < num_lanes && num_lanes <= kRows * kLanesPerRow);\n+\n+  const CappedTag<T, kLanesPerRow> d;\n   using V = Vec<decltype(d)>;\n   const V kPadding = st.LastValue(d);  // Not copied to keys.\n-  V v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n-\n-  // The first four vectors are always valid because 3 * lanes_per_row +\n-  // kMaxLanesPerRow <= num_lanes (even if kLPK == 2, because the largest\n-  // supported vector size is 512 bits i.e. four 128-bit keys, so lanes_per_row\n-  // and kMaxLanesPerRow are only eight uint64_t, and num_lanes >= 66).\n-  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n-  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n-  V v2 = LoadU(d, keys + 0x2 * lanes_per_row);\n-  V v3 = LoadU(d, keys + 0x3 * lanes_per_row);\n+  V v8, v9, va, vb, vc, vd, ve, vf;\n+\n+  // At least half the kRows are valid, otherwise a different function would\n+  // have been called to handle this num_lanes.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  const size_t N = Lanes(d);\n-  // Copy whole vectors from keys, starting from the first potentially unsafe\n-  // load, rounded down to enable aligned stores.\n-  size_t i = (4 * lanes_per_row) & ~(N - 1);\n-  for (; i + N <= num_lanes; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n-  }\n-\n-  // Fill missing inputs with padding up to vf's load. This also overwrites the\n-  // last partial vector of inputs, which we copy again below.\n-  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n-  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n-  for (; i < required_lanes; i += N) {\n-    Store(kPadding, d, buf + i);\n-  }\n-\n-  // Copy over the last partial vector, possibly overwriting already-written\n-  // keys with the same value or padding. Example: num_lanes = 33 u64, N = 8,\n-  // lanes_per_row = 4, end = 25, copy [16..31], pad [32..71], copy [25..32].\n-  HWY_DASSERT(num_lanes > N);\n-  const size_t end = num_lanes - N;\n-  StoreU(LoadU(d, keys + end), d, buf + end);\n-\n-  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n-  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n-  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n-  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n-  v8 = LoadU(d, buf + 0x8 * lanes_per_row);\n-  v9 = LoadU(d, buf + 0x9 * lanes_per_row);\n-  va = LoadU(d, buf + 0xa * lanes_per_row);\n-  vb = LoadU(d, buf + 0xb * lanes_per_row);\n-  vc = LoadU(d, buf + 0xc * lanes_per_row);\n-  vd = LoadU(d, buf + 0xd * lanes_per_row);\n-  ve = LoadU(d, buf + 0xe * lanes_per_row);\n-  vf = LoadU(d, buf + 0xf * lanes_per_row);\n+  CopyHalfToPaddedBuf<kRows, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  vf = LoadU(d, buf + 0xf * kLanesPerRow);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n   (void)buf;\n-  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n-  // Gt checks at no extra cost, and prevents reading past num_lanes.\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n-                                      Set(d, static_cast<T>(num_lanes)));\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n   // First offset where not all vector are guaranteed valid.\n-  const V kIota = Iota(d, static_cast<T>(4 * lanes_per_row));\n-  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n+  const V kIota = Iota(d, static_cast<T>(kMinLanes));\n+  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n   const V k2 = Add(k1, k1);\n   const V k4 = Add(k2, k2);\n   const V k8 = Add(k4, k4);\n \n   using M = Mask<decltype(d)>;\n-  const M m4 = Gt(vnum_lanes, kIota);\n-  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n-  const M m7 = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n-  const M m8 = Gt(vnum_lanes, Add(kIota, k4));\n-  const M m9 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-  const M ma = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n-  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n-  const M mc = Gt(vnum_lanes, Add(kIota, k8));\n-  const M md = Gt(vnum_lanes, Add(kIota, Add(k8, k1)));\n-  const M me = Gt(vnum_lanes, Add(kIota, Add(k8, k2)));\n-  const M mf = Gt(vnum_lanes, Add(kIota, Add(k8, Add(k2, k1))));\n-\n-  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n-  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n-  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n-  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n-  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * lanes_per_row);\n-  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * lanes_per_row);\n-  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * lanes_per_row);\n-  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * lanes_per_row);\n-  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * lanes_per_row);\n-  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * lanes_per_row);\n-  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * lanes_per_row);\n-  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * lanes_per_row);\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k2));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k4));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n+\n+  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n \n-  Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-            vf);\n-  Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-            vf);\n-  // Only merge across columns if it is possible that vectors are that big\n-  // (reduces code size) and there are actually that many cols.\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-\n-  if (HWY_LIKELY(keys_per_row >= 8 && kMaxKeysPerVector >= 8)) {\n-    Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-              vf);\n+  // Merge16x4 is a no-op if kKeysPerRow < 4 etc.\n+  Merge16x2<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n+  Merge16x4<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n+  Merge16x8<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    if (HWY_LIKELY(keys_per_row >= 16 && kMaxKeysPerVector >= 16)) {\n-      Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                 ve, vf);\n-    }\n+  Merge16x16<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                          vc, vd, ve, vf);\n #endif\n-  }\n \n-  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n-  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n-  StoreU(v2, d, keys + 0x2 * lanes_per_row);\n-  StoreU(v3, d, keys + 0x3 * lanes_per_row);\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n   // Store remaining vectors into buf and safely copy them into keys.\n-  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n-  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n-  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n-  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n-  StoreU(v8, d, buf + 0x8 * lanes_per_row);\n-  StoreU(v9, d, buf + 0x9 * lanes_per_row);\n-  StoreU(va, d, buf + 0xa * lanes_per_row);\n-  StoreU(vb, d, buf + 0xb * lanes_per_row);\n-  StoreU(vc, d, buf + 0xc * lanes_per_row);\n-  StoreU(vd, d, buf + 0xd * lanes_per_row);\n-  StoreU(ve, d, buf + 0xe * lanes_per_row);\n-  StoreU(vf, d, buf + 0xf * lanes_per_row);\n-\n-  // The first four vectors have already been stored unconditionally into\n+  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n+  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n+  StoreU(va, d, buf + 0xa * kLanesPerRow);\n+  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n+  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n+  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n+  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n+  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first half of vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  i = 4 * lanes_per_row;\n+  size_t i = kMinLanes;\n   HWY_UNROLL(1)\n-  for (; i + N <= num_lanes; i += N) {\n-    StoreU(LoadU(d, buf + i), d, keys + i);\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, d, buf + i, keys + i);\n+  SafeCopyN(remaining, dmax, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n-  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n-  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n-  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n-  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n-  BlendedStore(v8, m8, d, keys + 0x8 * lanes_per_row);\n-  BlendedStore(v9, m9, d, keys + 0x9 * lanes_per_row);\n-  BlendedStore(va, ma, d, keys + 0xa * lanes_per_row);\n-  BlendedStore(vb, mb, d, keys + 0xb * lanes_per_row);\n-  BlendedStore(vc, mc, d, keys + 0xc * lanes_per_row);\n-  BlendedStore(vd, md, d, keys + 0xd * lanes_per_row);\n-  BlendedStore(ve, me, d, keys + 0xe * lanes_per_row);\n-  BlendedStore(vf, mf, d, keys + 0xf * lanes_per_row);\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n@@ -549,17 +472,16 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n     /* <= 1 */ nullptr,  // We ensured num_keys > 1.\n     /* <= 2 */ &Sort2To2<Traits, T>,\n     /* <= 4 */ &Sort3To4<Traits, T>,\n-    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 key per row\n-    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 keys per row\n-    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 keys per row\n-    // 16-row is only used if there are at least 4 keys per row.\n-    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<Traits, T> : nullptr,\n-    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 8 */ &Sort8Rows<1, Traits, T>,  // 1 key per row\n+    /* <= 16 */ kMaxKeysPerVector >= 2 ? &Sort8Rows<2, Traits, T> : nullptr,\n+    /* <= 32 */ kMaxKeysPerVector >= 4 ? &Sort8Rows<4, Traits, T> : nullptr,\n+    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<4, Traits, T> : nullptr,\n+    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<8, Traits, T> : nullptr,\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<16, Traits, T> : nullptr,\n #endif\n   };\n-  funcs[ceil_log2](st, keys, num_lanes, ceil_log2, buf);\n+  funcs[ceil_log2](st, keys, num_lanes, buf);\n }\n \n // ------------------------------ Partition"
    ],
    "files_changed": [
      {
        "filename": "hwy/contrib/sort/sorting_networks-inl.h",
        "status": "modified",
        "additions": 41,
        "deletions": 23,
        "changes": 64,
        "patch": "@@ -188,8 +188,27 @@ HWY_INLINE void Sort16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,\n // ------------------------------ Merging networks\n \n // Blacher's hybrid bitonic/odd-even networks, generated by print_network.cc.\n-\n-template <class D, class Traits, class V = Vec<D>>\n+// For acceptable performance, these must be inlined, otherwise vectors are\n+// loaded from the stack. The kKeysPerVector allows calling from generic code\n+// but skipping the functions when vectors have too few lanes for\n+// st.SortPairsDistance1 to compile. `if constexpr` in the caller would also\n+// work, but is not available in C++11.\n+\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 1)>\n+HWY_INLINE void Merge8x2(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 2)>\n+HWY_INLINE void Merge8x4(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 1)>\n+HWY_INLINE void Merge16x2(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 2)>\n+HWY_INLINE void Merge16x4(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 4)>\n+HWY_INLINE void Merge16x8(...) {}\n+template <size_t kKeysPerVector, HWY_IF_LANES_LE(kKeysPerVector, 8)>\n+HWY_INLINE void Merge16x16(...) {}\n+\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 1)>\n HWY_INLINE void Merge8x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                          V& v5, V& v6, V& v7) {\n   v7 = st.ReverseKeys2(d, v7);\n@@ -229,7 +248,8 @@ HWY_INLINE void Merge8x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   v7 = st.SortPairsDistance1(d, v7);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 2)>\n HWY_INLINE void Merge8x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                          V& v5, V& v6, V& v7) {\n   v7 = st.ReverseKeys4(d, v7);\n@@ -278,7 +298,8 @@ HWY_INLINE void Merge8x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   v7 = st.SortPairsDistance1(d, v7);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 1)>\n HWY_INLINE void Merge16x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -368,7 +389,8 @@ HWY_INLINE void Merge16x2(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   vf = st.SortPairsDistance1(d, vf);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 2)>\n HWY_INLINE void Merge16x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -475,7 +497,8 @@ HWY_INLINE void Merge16x4(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n   vf = st.SortPairsDistance1(d, vf);\n }\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 4)>\n HWY_INLINE void Merge16x8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                           V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                           V& vc, V& vd, V& ve, V& vf) {\n@@ -602,7 +625,8 @@ HWY_INLINE void Merge16x8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n // Unused on MSVC, see below\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n \n-template <class D, class Traits, class V = Vec<D>>\n+template <size_t kKeysPerVector, class D, class Traits, class V = Vec<D>,\n+          HWY_IF_LANES_GT(kKeysPerVector, 8)>\n HWY_INLINE void Merge16x16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n                            V& v5, V& v6, V& v7, V& v8, V& v9, V& va, V& vb,\n                            V& vc, V& vd, V& ve, V& vf) {\n@@ -748,13 +772,7 @@ HWY_INLINE void Merge16x16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4,\n // Reshapes `buf` into a matrix, sorts columns independently, and then merges\n // into a sorted 1D array without transposing.\n //\n-// `st` is SharedTraits<Traits*<Order*>>. This abstraction layer bridges\n-//   differences in sort order and single-lane vs 128-bit keys.\n-//\n-// References:\n-// https://drops.dagstuhl.de/opus/volltexte/2021/13775/pdf/LIPIcs-SEA-2021-3.pdf\n-// https://github.com/simd-sorting/fast-and-robust/blob/master/avx2_sort_demo/avx2sort.h\n-// M. Blacher's thesis: https://github.com/mark-blacher/masterthesis\n+// DEPRECATED, use BaseCase() instead.\n template <class Traits, class V>\n HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n                                V& v3, V& v4, V& v5, V& v6, V& v7, V& v8, V& v9,\n@@ -774,22 +792,22 @@ HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n   // Checking MaxLanes avoids generating HWY_ASSERT code for the unreachable\n   // code paths: if MaxLanes < 2, then keys <= cols < 2.\n   if (HWY_LIKELY(keys >= 2 && kMaxKeys >= 2)) {\n-    Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-              vf);\n+    Merge16x2<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                        vc, vd, ve, vf);\n \n     if (HWY_LIKELY(keys >= 4 && kMaxKeys >= 4)) {\n-      Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                ve, vf);\n+      Merge16x4<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                          vc, vd, ve, vf);\n \n       if (HWY_LIKELY(keys >= 8 && kMaxKeys >= 8)) {\n-        Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                  ve, vf);\n+        Merge16x8<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va,\n+                            vb, vc, vd, ve, vf);\n \n         // Avoids build timeout. Must match #if condition in kMaxCols.\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n         if (HWY_LIKELY(keys >= 16 && kMaxKeys >= 16)) {\n-          Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc,\n-                     vd, ve, vf);\n+          Merge16x16<kMaxKeys>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9,\n+                               va, vb, vc, vd, ve, vf);\n \n           static_assert(Constants::kMaxCols <= 16, \"Add more branches\");\n         }\n@@ -802,7 +820,7 @@ HWY_INLINE void SortingNetwork(Traits st, size_t cols, V& v0, V& v1, V& v2,\n // As above, but loads from/stores to `buf`. This ensures full vectors are\n // aligned, and enables loads/stores without bounds checks.\n //\n-// NOINLINE because this is large and called twice from vqsort-inl.h.\n+// DEPRECATED, use BaseCase() instead.\n template <class Traits, typename T>\n HWY_NOINLINE void SortingNetwork(Traits st, T* HWY_RESTRICT buf, size_t cols) {\n   // traits*-inl assume 'full' vectors (but still capped to kMaxCols)."
      },
      {
        "filename": "hwy/contrib/sort/vqsort-inl.h",
        "status": "modified",
        "additions": 199,
        "deletions": 277,
        "changes": 476,
        "patch": "@@ -127,7 +127,7 @@ void HeapSort(Traits st, T* HWY_RESTRICT lanes, const size_t num_lanes) {\n // Special cases where `num_lanes` is in the specified range (inclusive).\n template <class Traits, typename T>\n HWY_INLINE void Sort2To2(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                         size_t /* ceil_log2 */, T* HWY_RESTRICT /* buf */) {\n+                         T* HWY_RESTRICT /* buf */) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n   HWY_DASSERT(num_keys == 2);\n@@ -148,7 +148,7 @@ HWY_INLINE void Sort2To2(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n \n template <class Traits, typename T>\n HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                         size_t /* ceil_log2 */, T* HWY_RESTRICT buf) {\n+                         T* HWY_RESTRICT buf) {\n   constexpr size_t kLPK = st.LanesPerKey();\n   const size_t num_keys = num_lanes / kLPK;\n   HWY_DASSERT(3 <= num_keys && num_keys <= 4);\n@@ -180,342 +180,265 @@ HWY_INLINE void Sort3To4(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n   StoreU(v3, d, in_out3);\n }\n \n-template <class Traits, typename T>\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+\n+template <size_t kRows, size_t kLanesPerRow, class D, class Traits,\n+          typename T = TFromD<D>>\n+HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n+                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  // Must cap for correctness: we will load up to the last valid lane, so\n+  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n+  const CappedTag<T, kMinLanes> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+  HWY_DASSERT(Nmax < num_lanes);\n+  HWY_ASSUME(Nmax <= kMinLanes);\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Rounding down allows aligned stores, which are typically faster.\n+  size_t i = num_lanes & ~(Nmax - 1);\n+  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n+  do {\n+    Store(kPadding, dmax, buf + i);\n+    i += Nmax;\n+    // Initialize enough for the last vector even if Nmax > kLanesPerRow.\n+  } while (i < (kRows - 1) * kLanesPerRow + Lanes(d));\n+\n+  // Ensure buf contains all we will read, and perhaps more before.\n+  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n+  do {\n+    end -= static_cast<ptrdiff_t>(Nmax);\n+    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n+  } while (end > static_cast<ptrdiff_t>(kRows / 2 * kLanesPerRow));\n+}\n+\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+\n+template <size_t kKeysPerRow, class Traits, typename T>\n HWY_NOINLINE void Sort8Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                            size_t ceil_log2, T* HWY_RESTRICT buf) {\n+                            T* HWY_RESTRICT buf) {\n+  // kKeysPerRow <= 4 because 8 64-bit keys implies 512-bit vectors, which\n+  // are likely slower than 16x4, so 8x4 is the largest we handle here.\n+  static_assert(kKeysPerRow <= 4, \"\");\n+\n   constexpr size_t kLPK = st.LanesPerKey();\n-  const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = 5;  // Sort3To4 handled <= 4.\n-  constexpr size_t kMinLanes = kMinKeys * kLPK;\n-  HWY_DASSERT(num_keys >= kMinKeys);\n-  (void)num_keys;\n-  (void)kMinLanes;\n \n-  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  // We reshape the 1D keys into kRows x kKeysPerRow.\n   constexpr size_t kRows = 8;\n-  // Chosen for efficiency. Sorting 64 keys with 8 rows would require 8 keys per\n-  // row, which means 512-bit vectors if keys are 64-bit. That is likely slower\n-  // than 16x4, so 8x4 is the largest we handle here.\n-  constexpr size_t kMaxKeysPerRow = 4;\n-  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n-  // Verify ceil_log2 can be handled by our matrix.\n-  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n-              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n-\n-  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n-  // (all are non-constexpr).\n-  const size_t max_keys = 1ull << ceil_log2;\n-  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n-  const size_t keys_per_row = DivCeil(max_keys, kRows);\n-  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n-  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n-\n-  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n-  // be big enough, but loads are only lanes_per_row apart and may overlap.\n-  const CappedTag<T, kMaxLanesPerRow> d;\n+  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  HWY_DASSERT(kMinLanes < num_lanes && num_lanes <= kRows * kLanesPerRow);\n+\n+  const CappedTag<T, kLanesPerRow> d;\n   using V = Vec<decltype(d)>;\n   const V kPadding = st.LastValue(d);  // Not copied to keys.\n-  V v2, v3, v4, v5, v6, v7;\n-\n-  // The first two vectors are always valid because 1 * lanes_per_row +\n-  // kMaxLanesPerRow <= num_lanes. For num_lanes <= 8, lanes_per_row = 1 and\n-  // 1 + kMaxLanesPerRow (4) <= (num_lanes >= kMinLanes >= 5). The inequality\n-  // holds a forteriori for larger num_lanes because kMaxLanesPerRow is capped.\n-  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n-  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n+  V v4, v5, v6, v7;\n+\n+  // At least half the kRows are valid, otherwise a different function would\n+  // have been called to handle this num_lanes.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  const size_t N = Lanes(d);\n-  // Copy whole vectors from keys, starting from the first potentially unsafe\n-  // load, rounded down to enable aligned stores.\n-  size_t i = (2 * lanes_per_row) & ~(N - 1);\n-  for (; i + N <= num_lanes; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n-  }\n-\n-  // Fill missing inputs with padding up to v7's load. This also overwrites the\n-  // last partial vector of inputs, which we copy again below.\n-  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n-  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n-  for (; i < required_lanes; i += N) {\n-    Store(kPadding, d, buf + i);\n-  }\n-\n-  // Copy over the last partial vector, possibly overwriting already-written\n-  // keys with the same value or padding. Example: num_lanes = 7, end = 3, copy\n-  // [0..3], pad [4..11], copy [3..6].\n-  HWY_DASSERT(num_lanes > N);\n-  const size_t end = num_lanes - N;\n-  StoreU(LoadU(d, keys + end), d, buf + end);\n-\n-  v2 = LoadU(d, buf + 0x2 * lanes_per_row);\n-  v3 = LoadU(d, buf + 0x3 * lanes_per_row);\n-  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n-  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n-  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n-  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n+  CopyHalfToPaddedBuf<kRows, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  v4 = LoadU(d, buf + 0x4 * kLanesPerRow);\n+  v5 = LoadU(d, buf + 0x5 * kLanesPerRow);\n+  v6 = LoadU(d, buf + 0x6 * kLanesPerRow);\n+  v7 = LoadU(d, buf + 0x7 * kLanesPerRow);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n   (void)buf;\n-  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n-  // Gt checks at no extra cost, and prevents reading past num_lanes.\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n-                                      Set(d, static_cast<T>(num_lanes)));\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n   // First offset where not all vector are guaranteed valid.\n-  const V kIota = Iota(d, static_cast<T>(2 * lanes_per_row));\n-  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n+  const V kIota = Iota(d, static_cast<T>(kMinLanes));\n+  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n   const V k2 = Add(k1, k1);\n-  const V k4 = Add(k2, k2);\n \n   using M = Mask<decltype(d)>;\n-  const M m2 = Gt(vnum_lanes, kIota);\n-  const M m3 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M m4 = Gt(vnum_lanes, Add(kIota, k2));\n-  const M m5 = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n-  const M m6 = Gt(vnum_lanes, Add(kIota, k4));\n-  const M m7 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-\n-  v2 = MaskedLoadOr(kPadding, m2, d, keys + 0x2 * lanes_per_row);\n-  v3 = MaskedLoadOr(kPadding, m3, d, keys + 0x3 * lanes_per_row);\n-  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n-  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n-  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n-  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n+  const M m4 = Gt(vnum_lanes, kIota);\n+  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n+  const M m7 = Gt(vnum_lanes, Add(kIota, Add(k2, k1)));\n+\n+  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * kLanesPerRow);\n+  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * kLanesPerRow);\n+  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * kLanesPerRow);\n+  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort8(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n \n-  // Only merge across columns if it is possible that vectors are that big\n-  // (reduces code size) and there are actually that many cols.\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-  if (HWY_LIKELY(keys_per_row >= 2 && kMaxKeysPerVector >= 2)) {\n-    Merge8x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n-    if (HWY_LIKELY(keys_per_row >= 4 && kMaxKeysPerVector >= 4)) {\n-      Merge8x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n-    }\n-  }\n+  // Merge8x2 is a no-op if kKeysPerRow < 2 etc.\n+  Merge8x2<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n+  Merge8x4<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n \n-  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n-  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n   // Store remaining vectors into buf and safely copy them into keys.\n-  StoreU(v2, d, buf + 0x2 * lanes_per_row);\n-  StoreU(v3, d, buf + 0x3 * lanes_per_row);\n-  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n-  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n-  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n-  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n-\n-  // The first two vectors have already been stored unconditionally into\n+  StoreU(v4, d, buf + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, buf + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, buf + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, buf + 0x7 * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first half of vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  i = 2 * lanes_per_row;\n+  size_t i = kMinLanes;\n   HWY_UNROLL(1)\n-  for (; i + N <= num_lanes; i += N) {\n-    StoreU(LoadU(d, buf + i), d, keys + i);\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, d, buf + i, keys + i);\n+  SafeCopyN(remaining, dmax, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n-  BlendedStore(v2, m2, d, keys + 0x2 * lanes_per_row);\n-  BlendedStore(v3, m3, d, keys + 0x3 * lanes_per_row);\n-  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n-  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n-  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n-  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n+  BlendedStore(v4, m4, d, keys + 0x4 * kLanesPerRow);\n+  BlendedStore(v5, m5, d, keys + 0x5 * kLanesPerRow);\n+  BlendedStore(v6, m6, d, keys + 0x6 * kLanesPerRow);\n+  BlendedStore(v7, m7, d, keys + 0x7 * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n-template <class Traits, typename T>\n+template <size_t kKeysPerRow, class Traits, typename T>\n HWY_NOINLINE void Sort16Rows(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n-                             size_t ceil_log2, T* HWY_RESTRICT buf) {\n+                             T* HWY_RESTRICT buf) {\n+  static_assert(kKeysPerRow <= SortConstants::kMaxCols, \"\");\n+\n   constexpr size_t kLPK = st.LanesPerKey();\n-  const size_t num_keys = num_lanes / kLPK;\n-  constexpr size_t kMinKeys = 33;  // Sort8Rows handled <= 32.\n-  constexpr size_t kMinLanes = kMinKeys * kLPK;\n-  HWY_DASSERT(num_keys >= kMinKeys);\n-  (void)num_keys;\n-  (void)kMinLanes;\n \n-  // We reshape the 1D keys into kRows x {1 .. kMaxKeysPerRow}.\n+  // We reshape the 1D keys into kRows x kKeysPerRow.\n   constexpr size_t kRows = 16;\n-  constexpr size_t kMaxKeysPerRow = 16;  // Limited by sorting_networks-inl.h\n-  constexpr size_t kMaxLanesPerRow = kMaxKeysPerRow * kLPK;\n-  // Verify ceil_log2 can be handled by our matrix.\n-  HWY_DASSERT(CeilLog2(kRows) <= ceil_log2 &&\n-              ceil_log2 <= CeilLog2(kRows * kMaxKeysPerRow));\n-\n-  // Verify num_keys is in (max/2, max], where max is determined by ceil_log2\n-  // (all are non-constexpr).\n-  const size_t max_keys = 1ull << ceil_log2;\n-  HWY_DASSERT(max_keys / 2 < num_keys && num_keys <= max_keys);\n-  const size_t keys_per_row = DivCeil(max_keys, kRows);\n-  HWY_DASSERT(1 <= keys_per_row && keys_per_row <= kMaxKeysPerRow);\n-  const size_t lanes_per_row = keys_per_row * kLPK;  // Stride for loads\n-\n-  // This code handles all lanes_per_row <= kMaxLanesPerRow, so the vectors must\n-  // be big enough, but loads are only lanes_per_row apart and may overlap.\n-  const CappedTag<T, kMaxLanesPerRow> d;\n+  constexpr size_t kLanesPerRow = kKeysPerRow * kLPK;\n+  constexpr size_t kMinLanes = kRows / 2 * kLanesPerRow;\n+  HWY_DASSERT(kMinLanes < num_lanes && num_lanes <= kRows * kLanesPerRow);\n+\n+  const CappedTag<T, kLanesPerRow> d;\n   using V = Vec<decltype(d)>;\n   const V kPadding = st.LastValue(d);  // Not copied to keys.\n-  V v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n-\n-  // The first four vectors are always valid because 3 * lanes_per_row +\n-  // kMaxLanesPerRow <= num_lanes (even if kLPK == 2, because the largest\n-  // supported vector size is 512 bits i.e. four 128-bit keys, so lanes_per_row\n-  // and kMaxLanesPerRow are only eight uint64_t, and num_lanes >= 66).\n-  V v0 = LoadU(d, keys + 0x0 * lanes_per_row);\n-  V v1 = LoadU(d, keys + 0x1 * lanes_per_row);\n-  V v2 = LoadU(d, keys + 0x2 * lanes_per_row);\n-  V v3 = LoadU(d, keys + 0x3 * lanes_per_row);\n+  V v8, v9, va, vb, vc, vd, ve, vf;\n+\n+  // At least half the kRows are valid, otherwise a different function would\n+  // have been called to handle this num_lanes.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n #if HWY_MEM_OPS_MIGHT_FAULT\n-  const size_t N = Lanes(d);\n-  // Copy whole vectors from keys, starting from the first potentially unsafe\n-  // load, rounded down to enable aligned stores.\n-  size_t i = (4 * lanes_per_row) & ~(N - 1);\n-  for (; i + N <= num_lanes; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n-  }\n-\n-  // Fill missing inputs with padding up to vf's load. This also overwrites the\n-  // last partial vector of inputs, which we copy again below.\n-  const size_t required_lanes = (kRows - 1) * lanes_per_row + N;\n-  HWY_DASSERT(required_lanes >= num_lanes);  // = if == max_keys * kLPK\n-  for (; i < required_lanes; i += N) {\n-    Store(kPadding, d, buf + i);\n-  }\n-\n-  // Copy over the last partial vector, possibly overwriting already-written\n-  // keys with the same value or padding. Example: num_lanes = 33 u64, N = 8,\n-  // lanes_per_row = 4, end = 25, copy [16..31], pad [32..71], copy [25..32].\n-  HWY_DASSERT(num_lanes > N);\n-  const size_t end = num_lanes - N;\n-  StoreU(LoadU(d, keys + end), d, buf + end);\n-\n-  v4 = LoadU(d, buf + 0x4 * lanes_per_row);\n-  v5 = LoadU(d, buf + 0x5 * lanes_per_row);\n-  v6 = LoadU(d, buf + 0x6 * lanes_per_row);\n-  v7 = LoadU(d, buf + 0x7 * lanes_per_row);\n-  v8 = LoadU(d, buf + 0x8 * lanes_per_row);\n-  v9 = LoadU(d, buf + 0x9 * lanes_per_row);\n-  va = LoadU(d, buf + 0xa * lanes_per_row);\n-  vb = LoadU(d, buf + 0xb * lanes_per_row);\n-  vc = LoadU(d, buf + 0xc * lanes_per_row);\n-  vd = LoadU(d, buf + 0xd * lanes_per_row);\n-  ve = LoadU(d, buf + 0xe * lanes_per_row);\n-  vf = LoadU(d, buf + 0xf * lanes_per_row);\n+  CopyHalfToPaddedBuf<kRows, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  vf = LoadU(d, buf + 0xf * kLanesPerRow);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n   (void)buf;\n-  // Zeroing from lanes_per_row adds a `lane < lanes_per_row` condition to the\n-  // Gt checks at no extra cost, and prevents reading past num_lanes.\n-  const V vnum_lanes = IfThenElseZero(FirstN(d, lanes_per_row),\n-                                      Set(d, static_cast<T>(num_lanes)));\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n   // First offset where not all vector are guaranteed valid.\n-  const V kIota = Iota(d, static_cast<T>(4 * lanes_per_row));\n-  const V k1 = Set(d, static_cast<T>(lanes_per_row));\n+  const V kIota = Iota(d, static_cast<T>(kMinLanes));\n+  const V k1 = Set(d, static_cast<T>(kLanesPerRow));\n   const V k2 = Add(k1, k1);\n   const V k4 = Add(k2, k2);\n   const V k8 = Add(k4, k4);\n \n   using M = Mask<decltype(d)>;\n-  const M m4 = Gt(vnum_lanes, kIota);\n-  const M m5 = Gt(vnum_lanes, Add(kIota, k1));\n-  const M m6 = Gt(vnum_lanes, Add(kIota, k2));\n-  const M m7 = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n-  const M m8 = Gt(vnum_lanes, Add(kIota, k4));\n-  const M m9 = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n-  const M ma = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n-  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n-  const M mc = Gt(vnum_lanes, Add(kIota, k8));\n-  const M md = Gt(vnum_lanes, Add(kIota, Add(k8, k1)));\n-  const M me = Gt(vnum_lanes, Add(kIota, Add(k8, k2)));\n-  const M mf = Gt(vnum_lanes, Add(kIota, Add(k8, Add(k2, k1))));\n-\n-  v4 = MaskedLoadOr(kPadding, m4, d, keys + 0x4 * lanes_per_row);\n-  v5 = MaskedLoadOr(kPadding, m5, d, keys + 0x5 * lanes_per_row);\n-  v6 = MaskedLoadOr(kPadding, m6, d, keys + 0x6 * lanes_per_row);\n-  v7 = MaskedLoadOr(kPadding, m7, d, keys + 0x7 * lanes_per_row);\n-  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * lanes_per_row);\n-  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * lanes_per_row);\n-  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * lanes_per_row);\n-  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * lanes_per_row);\n-  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * lanes_per_row);\n-  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * lanes_per_row);\n-  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * lanes_per_row);\n-  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * lanes_per_row);\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k1));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k2));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Sub(k4, k1)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k4));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k4, k1)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k4, k2)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k8, k1)));\n+\n+  v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n \n   Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n \n-  Merge16x2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-            vf);\n-  Merge16x4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-            vf);\n-  // Only merge across columns if it is possible that vectors are that big\n-  // (reduces code size) and there are actually that many cols.\n-  constexpr size_t kMaxKeysPerVector = MaxLanes(d) / kLPK;\n-\n-  if (HWY_LIKELY(keys_per_row >= 8 && kMaxKeysPerVector >= 8)) {\n-    Merge16x8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n-              vf);\n+  // Merge16x4 is a no-op if kKeysPerRow < 4 etc.\n+  Merge16x2<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n+  Merge16x4<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n+  Merge16x8<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                         vc, vd, ve, vf);\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    if (HWY_LIKELY(keys_per_row >= 16 && kMaxKeysPerVector >= 16)) {\n-      Merge16x16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd,\n-                 ve, vf);\n-    }\n+  Merge16x16<kKeysPerRow>(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb,\n+                          vc, vd, ve, vf);\n #endif\n-  }\n \n-  StoreU(v0, d, keys + 0x0 * lanes_per_row);\n-  StoreU(v1, d, keys + 0x1 * lanes_per_row);\n-  StoreU(v2, d, keys + 0x2 * lanes_per_row);\n-  StoreU(v3, d, keys + 0x3 * lanes_per_row);\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n \n #if HWY_MEM_OPS_MIGHT_FAULT\n   // Store remaining vectors into buf and safely copy them into keys.\n-  StoreU(v4, d, buf + 0x4 * lanes_per_row);\n-  StoreU(v5, d, buf + 0x5 * lanes_per_row);\n-  StoreU(v6, d, buf + 0x6 * lanes_per_row);\n-  StoreU(v7, d, buf + 0x7 * lanes_per_row);\n-  StoreU(v8, d, buf + 0x8 * lanes_per_row);\n-  StoreU(v9, d, buf + 0x9 * lanes_per_row);\n-  StoreU(va, d, buf + 0xa * lanes_per_row);\n-  StoreU(vb, d, buf + 0xb * lanes_per_row);\n-  StoreU(vc, d, buf + 0xc * lanes_per_row);\n-  StoreU(vd, d, buf + 0xd * lanes_per_row);\n-  StoreU(ve, d, buf + 0xe * lanes_per_row);\n-  StoreU(vf, d, buf + 0xf * lanes_per_row);\n-\n-  // The first four vectors have already been stored unconditionally into\n+  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n+  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n+  StoreU(va, d, buf + 0xa * kLanesPerRow);\n+  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n+  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n+  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n+  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n+  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first half of vectors have already been stored unconditionally into\n   // `keys`, so we do not copy them.\n-  i = 4 * lanes_per_row;\n+  size_t i = kMinLanes;\n   HWY_UNROLL(1)\n-  for (; i + N <= num_lanes; i += N) {\n-    StoreU(LoadU(d, buf + i), d, keys + i);\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n \n   // Last iteration: copy partial vector\n   const size_t remaining = num_lanes - i;\n   HWY_ASSUME(remaining < 256);  // helps FirstN\n-  SafeCopyN(remaining, d, buf + i, keys + i);\n+  SafeCopyN(remaining, dmax, buf + i, keys + i);\n #endif  // HWY_MEM_OPS_MIGHT_FAULT\n #if !HWY_MEM_OPS_MIGHT_FAULT || HWY_IDE\n-  BlendedStore(v4, m4, d, keys + 0x4 * lanes_per_row);\n-  BlendedStore(v5, m5, d, keys + 0x5 * lanes_per_row);\n-  BlendedStore(v6, m6, d, keys + 0x6 * lanes_per_row);\n-  BlendedStore(v7, m7, d, keys + 0x7 * lanes_per_row);\n-  BlendedStore(v8, m8, d, keys + 0x8 * lanes_per_row);\n-  BlendedStore(v9, m9, d, keys + 0x9 * lanes_per_row);\n-  BlendedStore(va, ma, d, keys + 0xa * lanes_per_row);\n-  BlendedStore(vb, mb, d, keys + 0xb * lanes_per_row);\n-  BlendedStore(vc, mc, d, keys + 0xc * lanes_per_row);\n-  BlendedStore(vd, md, d, keys + 0xd * lanes_per_row);\n-  BlendedStore(ve, me, d, keys + 0xe * lanes_per_row);\n-  BlendedStore(vf, mf, d, keys + 0xf * lanes_per_row);\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n #endif  // !HWY_MEM_OPS_MIGHT_FAULT\n }\n \n@@ -549,17 +472,16 @@ HWY_NOINLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n     /* <= 1 */ nullptr,  // We ensured num_keys > 1.\n     /* <= 2 */ &Sort2To2<Traits, T>,\n     /* <= 4 */ &Sort3To4<Traits, T>,\n-    /* <= 8 */ &Sort8Rows<Traits, T>,   // 1 key per row\n-    /* <= 16 */ Sort8Rows<Traits, T>,   // 2 keys per row\n-    /* <= 32 */ &Sort8Rows<Traits, T>,  // 4 keys per row\n-    // 16-row is only used if there are at least 4 keys per row.\n-    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<Traits, T> : nullptr,\n-    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 8 */ &Sort8Rows<1, Traits, T>,  // 1 key per row\n+    /* <= 16 */ kMaxKeysPerVector >= 2 ? &Sort8Rows<2, Traits, T> : nullptr,\n+    /* <= 32 */ kMaxKeysPerVector >= 4 ? &Sort8Rows<4, Traits, T> : nullptr,\n+    /* <= 64 */ kMaxKeysPerVector >= 4 ? &Sort16Rows<4, Traits, T> : nullptr,\n+    /* <= 128 */ kMaxKeysPerVector >= 8 ? &Sort16Rows<8, Traits, T> : nullptr,\n #if !HWY_COMPILER_MSVC && !HWY_IS_DEBUG_BUILD\n-    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<Traits, T> : nullptr,\n+    /* <= 256 */ kMaxKeysPerVector >= 16 ? &Sort16Rows<16, Traits, T> : nullptr,\n #endif\n   };\n-  funcs[ceil_log2](st, keys, num_lanes, ceil_log2, buf);\n+  funcs[ceil_log2](st, keys, num_lanes, buf);\n }\n \n // ------------------------------ Partition"
      }
    ],
    "lines_added": 240,
    "lines_removed": 300
  },
  "issues": [],
  "pull_requests": [],
  "build_info": {
    "old_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
      "cmake --build /test_workspace/workspace/old/build -- -j 1"
    ],
    "new_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/new -B /test_workspace/workspace/new/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
      "cmake --build /test_workspace/workspace/new/build -- -j 1"
    ],
    "old_test_script": [
      "cd /test_workspace/workspace/old/build",
      "ctest --output-on-failure"
    ],
    "new_test_script": [
      "cd /test_workspace/workspace/new/build",
      "ctest --output-on-failure"
    ],
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": false,
    "p_value": 0.9999999999999756,
    "is_pair_significant": false,
    "pair_p_value": 0.9999999999870881,
    "is_binom_significant": false,
    "binom_p_value": 1.0,
    "is_wilcoxon_significant": false,
    "wilcoxon_p_value": 0.9999991338918834,
    "is_mannwhitney_significant": false,
    "mannwhitney_p_value": 0.06293248664519395,
    "relative_improvement": 0.007923439043763017,
    "absolute_improvement_ms": 136.33333333333653,
    "old_mean_ms": 17206.333333333336,
    "new_mean_ms": 17070.0,
    "old_std_ms": 294.63575798782006,
    "new_std_ms": 287.6300305648884,
    "effect_size_cohens_d": 0.46825167794256717,
    "old_ci95_ms": [
      17096.314533192308,
      17316.352133474364
    ],
    "new_ci95_ms": [
      16962.59718147115,
      17177.40281852885
    ],
    "old_ci99_ms": [
      17058.059387170877,
      17354.607279495795
    ],
    "new_ci99_ms": [
      16925.25165051946,
      17214.748349480542
    ],
    "new_times_s": [
      16.69,
      16.61,
      17.4,
      16.59,
      17.13,
      16.79,
      17.23,
      16.72,
      17.27,
      16.83,
      16.81,
      17.27,
      17.28,
      17.17,
      17.36,
      16.73,
      16.85,
      16.7,
      16.76,
      17.29,
      17.19,
      16.86,
      17.58,
      17.51,
      17.39,
      17.06,
      17.4,
      16.97,
      17.01,
      16.98,
      17.36
    ],
    "old_times_s": [
      16.54,
      17.4,
      17.14,
      16.85,
      16.88,
      16.56,
      17.49,
      17.14,
      17.26,
      17.1,
      17.17,
      17.52,
      16.94,
      17.01,
      16.75,
      17.18,
      17.02,
      17.49,
      17.26,
      17.63,
      17.5,
      17.09,
      16.8,
      17.06,
      17.9,
      17.17,
      17.31,
      17.34,
      17.36,
      17.23,
      17.64
    ]
  },
  "tests": {
    "total_tests": 1,
    "significant_improvements": 0,
    "significant_improvements_tests": [],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 0,
    "significant_pair_improvements_tests": [],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 0,
    "significant_binom_improvements_tests": [],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 0,
    "significant_wilcoxon_improvements_tests": [],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 0,
    "significant_mannwhitney_improvements_tests": [],
    "significant_mannwhitney_regressions": 0,
    "significant_mannwhitney_regressions_tests": [],
    "tests": [
      {
        "test_name": "NanobenchmarkTest.RunAll",
        "is_significant": false,
        "p_value": 0.5381308096941556,
        "is_pair_significant": false,
        "pair_p_value": 0.5338916928351194,
        "is_binom_significant": false,
        "binom_p_value": 0.4252770096063614,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.4277102100864533,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.3318803327051011,
        "relative_improvement": 0.044303797468354465,
        "absolute_improvement_ms": 20.000000000000018,
        "old_mean_ms": 451.42857142857144,
        "new_mean_ms": 431.42857142857144,
        "old_std_ms": 98.08213812976255,
        "new_std_ms": 106.44818895226416,
        "effect_size_cohens_d": 0.19540661875655974,
        "old_ci95_ms": [
          413.39628863338623,
          489.46085422375666
        ],
        "new_ci95_ms": [
          390.1522727827191,
          472.7048700744238
        ],
        "old_ci99_ms": [
          400.07179662934834,
          502.7853462277946
        ],
        "new_ci99_ms": [
          375.6912499136799,
          487.165892943463
        ],
        "new_times": [
          0.47,
          0.47,
          0.39,
          0.21,
          0.22,
          0.48,
          0.57,
          0.47,
          0.46,
          0.44,
          0.2,
          0.55,
          0.41,
          0.43,
          0.52,
          0.49,
          0.36,
          0.27,
          0.48,
          0.56,
          0.36,
          0.39,
          0.56,
          0.45,
          0.39,
          0.52,
          0.39,
          0.57
        ],
        "old_times": [
          0.49,
          0.52,
          0.42,
          0.54,
          0.45,
          0.5,
          0.3,
          0.55,
          0.39,
          0.42,
          0.46,
          0.47,
          0.55,
          0.44,
          0.55,
          0.31,
          0.38,
          0.54,
          0.53,
          0.35,
          0.36,
          0.54,
          0.33,
          0.59,
          0.37,
          0.22,
          0.46,
          0.61
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}