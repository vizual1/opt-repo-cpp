{
  "metadata": {
    "collection_date": "2026-02-03T20:05:10.565815",
    "repository": "https://github.com/ridiculousfish/libdivide",
    "repository_name": "ridiculousfish/libdivide"
  },
  "commit_info": {
    "old_sha": "858349bfc73d7669456edde96428df01ff6c615c",
    "new_sha": "61ae9cb48e6f17526e78202c8dc319f6f1dd2302",
    "commit_message": [
      "Mark most functions as forced-inline\n\nThis improves performance for optimization levels like O2 and Os."
    ],
    "commit_date": "2021-04-02T18:48:00+00:00",
    "patch": [
      "--- libdivide.h\n@@ -74,6 +74,17 @@\n #define LIBDIVIDE_FUNCTION __func__\n #endif\n \n+// Set up forced inlining if possible.\n+// We need both the attribute and keyword to avoid \"might not be inlineable\" warnings.\n+#ifdef __has_attribute\n+#if __has_attribute(always_inline)\n+#define LIBDIVIDE_INLINE __attribute__((always_inline)) inline\n+#endif\n+#endif\n+#ifndef LIBDIVIDE_INLINE\n+#define LIBDIVIDE_INLINE inline\n+#endif\n+\n #define LIBDIVIDE_ERROR(msg)                                                                     \\\n     do {                                                                                         \\\n         fprintf(stderr, \"libdivide.h:%d: %s(): Error: %s\\n\", __LINE__, LIBDIVIDE_FUNCTION, msg); \\\n@@ -184,60 +195,64 @@ enum {\n     LIBDIVIDE_NEGATIVE_DIVISOR = 0x80\n };\n \n-static inline struct libdivide_s32_t libdivide_s32_gen(int32_t d);\n-static inline struct libdivide_u32_t libdivide_u32_gen(uint32_t d);\n-static inline struct libdivide_s64_t libdivide_s64_gen(int64_t d);\n-static inline struct libdivide_u64_t libdivide_u64_gen(uint64_t d);\n-\n-static inline struct libdivide_s32_branchfree_t libdivide_s32_branchfree_gen(int32_t d);\n-static inline struct libdivide_u32_branchfree_t libdivide_u32_branchfree_gen(uint32_t d);\n-static inline struct libdivide_s64_branchfree_t libdivide_s64_branchfree_gen(int64_t d);\n-static inline struct libdivide_u64_branchfree_t libdivide_u64_branchfree_gen(uint64_t d);\n-\n-static inline int32_t libdivide_s32_do(int32_t numer, const struct libdivide_s32_t *denom);\n-static inline uint32_t libdivide_u32_do(uint32_t numer, const struct libdivide_u32_t *denom);\n-static inline int64_t libdivide_s64_do(int64_t numer, const struct libdivide_s64_t *denom);\n-static inline uint64_t libdivide_u64_do(uint64_t numer, const struct libdivide_u64_t *denom);\n-\n-static inline int32_t libdivide_s32_branchfree_do(\n+static LIBDIVIDE_INLINE struct libdivide_s32_t libdivide_s32_gen(int32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u32_t libdivide_u32_gen(uint32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_s64_t libdivide_s64_gen(int64_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u64_t libdivide_u64_gen(uint64_t d);\n+\n+static LIBDIVIDE_INLINE struct libdivide_s32_branchfree_t libdivide_s32_branchfree_gen(int32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u32_branchfree_t libdivide_u32_branchfree_gen(uint32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_s64_branchfree_t libdivide_s64_branchfree_gen(int64_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u64_branchfree_t libdivide_u64_branchfree_gen(uint64_t d);\n+\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_do(\n+    int32_t numer, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_do(\n+    uint32_t numer, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_do(\n+    int64_t numer, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_do(\n+    uint64_t numer, const struct libdivide_u64_t *denom);\n+\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_branchfree_do(\n     int32_t numer, const struct libdivide_s32_branchfree_t *denom);\n-static inline uint32_t libdivide_u32_branchfree_do(\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_branchfree_do(\n     uint32_t numer, const struct libdivide_u32_branchfree_t *denom);\n-static inline int64_t libdivide_s64_branchfree_do(\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_branchfree_do(\n     int64_t numer, const struct libdivide_s64_branchfree_t *denom);\n-static inline uint64_t libdivide_u64_branchfree_do(\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_branchfree_do(\n     uint64_t numer, const struct libdivide_u64_branchfree_t *denom);\n \n-static inline int32_t libdivide_s32_recover(const struct libdivide_s32_t *denom);\n-static inline uint32_t libdivide_u32_recover(const struct libdivide_u32_t *denom);\n-static inline int64_t libdivide_s64_recover(const struct libdivide_s64_t *denom);\n-static inline uint64_t libdivide_u64_recover(const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_recover(const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_recover(const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_recover(const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_recover(const struct libdivide_u64_t *denom);\n \n-static inline int32_t libdivide_s32_branchfree_recover(\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_branchfree_recover(\n     const struct libdivide_s32_branchfree_t *denom);\n-static inline uint32_t libdivide_u32_branchfree_recover(\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_branchfree_recover(\n     const struct libdivide_u32_branchfree_t *denom);\n-static inline int64_t libdivide_s64_branchfree_recover(\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_branchfree_recover(\n     const struct libdivide_s64_branchfree_t *denom);\n-static inline uint64_t libdivide_u64_branchfree_recover(\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_branchfree_recover(\n     const struct libdivide_u64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n-static inline uint32_t libdivide_mullhi_u32(uint32_t x, uint32_t y) {\n+static LIBDIVIDE_INLINE uint32_t libdivide_mullhi_u32(uint32_t x, uint32_t y) {\n     uint64_t xl = x, yl = y;\n     uint64_t rl = xl * yl;\n     return (uint32_t)(rl >> 32);\n }\n \n-static inline int32_t libdivide_mullhi_s32(int32_t x, int32_t y) {\n+static LIBDIVIDE_INLINE int32_t libdivide_mullhi_s32(int32_t x, int32_t y) {\n     int64_t xl = x, yl = y;\n     int64_t rl = xl * yl;\n     // needs to be arithmetic shift\n     return (int32_t)(rl >> 32);\n }\n \n-static inline uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n+static LIBDIVIDE_INLINE uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n #if defined(LIBDIVIDE_VC) && defined(LIBDIVIDE_X86_64)\n     return __umulh(x, y);\n #elif defined(HAS_INT128_T)\n@@ -263,7 +278,7 @@ static inline uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n #endif\n }\n \n-static inline int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n+static LIBDIVIDE_INLINE int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n #if defined(LIBDIVIDE_VC) && defined(LIBDIVIDE_X86_64)\n     return __mulh(x, y);\n #elif defined(HAS_INT128_T)\n@@ -285,7 +300,7 @@ static inline int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n #endif\n }\n \n-static inline int32_t libdivide_count_leading_zeros32(uint32_t val) {\n+static LIBDIVIDE_INLINE int32_t libdivide_count_leading_zeros32(uint32_t val) {\n #if defined(__GNUC__) || __has_builtin(__builtin_clz)\n     // Fast way to count leading zeros\n     return __builtin_clz(val);\n@@ -311,7 +326,7 @@ static inline int32_t libdivide_count_leading_zeros32(uint32_t val) {\n #endif\n }\n \n-static inline int32_t libdivide_count_leading_zeros64(uint64_t val) {\n+static LIBDIVIDE_INLINE int32_t libdivide_count_leading_zeros64(uint64_t val) {\n #if defined(__GNUC__) || __has_builtin(__builtin_clzll)\n     // Fast way to count leading zeros\n     return __builtin_clzll(val);\n@@ -332,7 +347,7 @@ static inline int32_t libdivide_count_leading_zeros64(uint64_t val) {\n // libdivide_64_div_32_to_32: divides a 64-bit uint {u1, u0} by a 32-bit\n // uint {v}. The result must fit in 32 bits.\n // Returns the quotient directly and the remainder in *r\n-static inline uint32_t libdivide_64_div_32_to_32(\n+static LIBDIVIDE_INLINE uint32_t libdivide_64_div_32_to_32(\n     uint32_t u1, uint32_t u0, uint32_t v, uint32_t *r) {\n #if (defined(LIBDIVIDE_i386) || defined(LIBDIVIDE_X86_64)) && defined(LIBDIVIDE_GCC_STYLE_ASM)\n     uint32_t result;\n@@ -353,7 +368,7 @@ static uint64_t libdivide_128_div_64_to_64(uint64_t u1, uint64_t u0, uint64_t v,\n     // N.B. resist the temptation to use __uint128_t here.\n     // In LLVM compiler-rt, it performs a 128/128 -> 128 division which is many times slower than\n     // necessary. In gcc it's better but still slower than the divlu implementation, perhaps because\n-    // it's not inlined.\n+    // it's not LIBDIVIDE_INLINEd.\n #if defined(LIBDIVIDE_X86_64) && defined(LIBDIVIDE_GCC_STYLE_ASM)\n     uint64_t result;\n     __asm__(\"divq %[v]\" : \"=a\"(result), \"=d\"(*r) : [v] \"r\"(v), \"a\"(u0), \"d\"(u1));\n@@ -432,7 +447,8 @@ static uint64_t libdivide_128_div_64_to_64(uint64_t u1, uint64_t u0, uint64_t v,\n }\n \n // Bitshift a u128 in place, left (signed_shift > 0) or right (signed_shift < 0)\n-static inline void libdivide_u128_shift(uint64_t *u1, uint64_t *u0, int32_t signed_shift) {\n+static LIBDIVIDE_INLINE void libdivide_u128_shift(\n+    uint64_t *u1, uint64_t *u0, int32_t signed_shift) {\n     if (signed_shift > 0) {\n         uint32_t shift = signed_shift;\n         *u1 <<= shift;\n@@ -546,7 +562,8 @@ static uint64_t libdivide_128_div_128_to_64(\n \n ////////// UINT32\n \n-static inline struct libdivide_u32_t libdivide_internal_u32_gen(uint32_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_u32_t libdivide_internal_u32_gen(\n+    uint32_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -703,7 +720,8 @@ uint32_t libdivide_u32_branchfree_recover(const struct libdivide_u32_branchfree_\n \n /////////// UINT64\n \n-static inline struct libdivide_u64_t libdivide_internal_u64_gen(uint64_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_u64_t libdivide_internal_u64_gen(\n+    uint64_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -874,7 +892,8 @@ uint64_t libdivide_u64_branchfree_recover(const struct libdivide_u64_branchfree_\n \n /////////// SINT32\n \n-static inline struct libdivide_s32_t libdivide_internal_s32_gen(int32_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_s32_t libdivide_internal_s32_gen(\n+    int32_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -1042,7 +1061,8 @@ int32_t libdivide_s32_branchfree_recover(const struct libdivide_s32_branchfree_t\n \n ///////////// SINT64\n \n-static inline struct libdivide_s64_t libdivide_internal_s64_gen(int64_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_s64_t libdivide_internal_s64_gen(\n+    int64_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -1202,65 +1222,65 @@ int64_t libdivide_s64_branchfree_recover(const struct libdivide_s64_branchfree_t\n \n #if defined(LIBDIVIDE_NEON)\n \n-static inline uint32x4_t libdivide_u32_do_vec128(\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_do_vec128(\n     uint32x4_t numers, const struct libdivide_u32_t *denom);\n-static inline int32x4_t libdivide_s32_do_vec128(\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_do_vec128(\n     int32x4_t numers, const struct libdivide_s32_t *denom);\n-static inline uint64x2_t libdivide_u64_do_vec128(\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_do_vec128(\n     uint64x2_t numers, const struct libdivide_u64_t *denom);\n-static inline int64x2_t libdivide_s64_do_vec128(\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_do_vec128(\n     int64x2_t numers, const struct libdivide_s64_t *denom);\n \n-static inline uint32x4_t libdivide_u32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_branchfree_do_vec128(\n     uint32x4_t numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline int32x4_t libdivide_s32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_branchfree_do_vec128(\n     int32x4_t numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline uint64x2_t libdivide_u64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_branchfree_do_vec128(\n     uint64x2_t numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline int64x2_t libdivide_s64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_branchfree_do_vec128(\n     int64x2_t numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Logical right shift by runtime value.\n // NEON implements right shift as left shits by negative values.\n-static inline uint32x4_t libdivide_u32_neon_srl(uint32x4_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_neon_srl(uint32x4_t v, uint8_t amt) {\n     int32_t wamt = static_cast<int32_t>(amt);\n     return vshlq_u32(v, vdupq_n_s32(-wamt));\n }\n \n-static inline uint64x2_t libdivide_u64_neon_srl(uint64x2_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_neon_srl(uint64x2_t v, uint8_t amt) {\n     int64_t wamt = static_cast<int64_t>(amt);\n     return vshlq_u64(v, vdupq_n_s64(-wamt));\n }\n \n // Arithmetic right shift by runtime value.\n-static inline int32x4_t libdivide_s32_neon_sra(int32x4_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_neon_sra(int32x4_t v, uint8_t amt) {\n     int32_t wamt = static_cast<int32_t>(amt);\n     return vshlq_s32(v, vdupq_n_s32(-wamt));\n }\n \n-static inline int64x2_t libdivide_s64_neon_sra(int64x2_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_neon_sra(int64x2_t v, uint8_t amt) {\n     int64_t wamt = static_cast<int64_t>(amt);\n     return vshlq_s64(v, vdupq_n_s64(-wamt));\n }\n \n-static inline int64x2_t libdivide_s64_signbits(int64x2_t v) { return vshrq_n_s64(v, 63); }\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_signbits(int64x2_t v) { return vshrq_n_s64(v, 63); }\n \n-static inline uint32x4_t libdivide_mullhi_u32_vec128(uint32x4_t a, uint32_t b) {\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_mullhi_u32_vec128(uint32x4_t a, uint32_t b) {\n     // Desire is [x0, x1, x2, x3]\n     uint32x4_t w1 = vreinterpretq_u32_u64(vmull_n_u32(vget_low_u32(a), b));  // [_, x0, _, x1]\n     uint32x4_t w2 = vreinterpretq_u32_u64(vmull_high_n_u32(a, b));           //[_, x2, _, x3]\n     return vuzp2q_u32(w1, w2);                                               // [x0, x1, x2, x3]\n }\n \n-static inline int32x4_t libdivide_mullhi_s32_vec128(int32x4_t a, int32_t b) {\n+static LIBDIVIDE_INLINE int32x4_t libdivide_mullhi_s32_vec128(int32x4_t a, int32_t b) {\n     int32x4_t w1 = vreinterpretq_s32_s64(vmull_n_s32(vget_low_s32(a), b));  // [_, x0, _, x1]\n     int32x4_t w2 = vreinterpretq_s32_s64(vmull_high_n_s32(a, b));           //[_, x2, _, x3]\n     return vuzp2q_s32(w1, w2);                                              // [x0, x1, x2, x3]\n }\n \n-static inline uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy) {\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy) {\n     // full 128 bits product is:\n     // x0*y0 + (x0*y1 << 32) + (x1*y0 << 32) + (x1*y1 << 64)\n     // Note x0,y0,x1,y1 are all conceptually uint32, products are 32x32->64.\n@@ -1291,7 +1311,7 @@ static inline uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy)\n     return result;\n }\n \n-static inline int64x2_t libdivide_mullhi_s64_vec128(int64x2_t x, int64_t sy) {\n+static LIBDIVIDE_INLINE int64x2_t libdivide_mullhi_s64_vec128(int64x2_t x, int64_t sy) {\n     int64x2_t p = vreinterpretq_s64_u64(\n         libdivide_mullhi_u64_vec128(vreinterpretq_u64_s64(x), static_cast<uint64_t>(sy)));\n     int64x2_t y = vdupq_n_s64(sy);\n@@ -1472,33 +1492,37 @@ int64x2_t libdivide_s64_branchfree_do_vec128(\n \n #if defined(LIBDIVIDE_AVX512)\n \n-static inline __m512i libdivide_u32_do_vec512(__m512i numers, const struct libdivide_u32_t *denom);\n-static inline __m512i libdivide_s32_do_vec512(__m512i numers, const struct libdivide_s32_t *denom);\n-static inline __m512i libdivide_u64_do_vec512(__m512i numers, const struct libdivide_u64_t *denom);\n-static inline __m512i libdivide_s64_do_vec512(__m512i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_u32_do_vec512(\n+    __m512i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_s32_do_vec512(\n+    __m512i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_u64_do_vec512(\n+    __m512i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_do_vec512(\n+    __m512i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m512i libdivide_u32_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_u32_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m512i libdivide_s32_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_s32_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m512i libdivide_u64_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_u64_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m512i libdivide_s64_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n-static inline __m512i libdivide_s64_signbits(__m512i v) {\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_signbits(__m512i v) {\n     ;\n     return _mm512_srai_epi64(v, 63);\n }\n \n-static inline __m512i libdivide_s64_shift_right_vec512(__m512i v, int amt) {\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_shift_right_vec512(__m512i v, int amt) {\n     return _mm512_srai_epi64(v, amt);\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n     __m512i hi_product_0Z2Z = _mm512_srli_epi64(_mm512_mul_epu32(a, b), 32);\n     __m512i a1X3X = _mm512_srli_epi64(a, 32);\n     __m512i mask = _mm512_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1507,7 +1531,7 @@ static inline __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n }\n \n // b is one 32-bit value repeated.\n-static inline __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n     __m512i hi_product_0Z2Z = _mm512_srli_epi64(_mm512_mul_epi32(a, b), 32);\n     __m512i a1X3X = _mm512_srli_epi64(a, 32);\n     __m512i mask = _mm512_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1516,7 +1540,7 @@ static inline __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n     // see m128i variant for comments.\n     __m512i x0y0 = _mm512_mul_epu32(x, y);\n     __m512i x0y0_hi = _mm512_srli_epi64(x0y0, 32);\n@@ -1539,7 +1563,7 @@ static inline __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m512i libdivide_mullhi_s64_vec512(__m512i x, __m512i y) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_s64_vec512(__m512i x, __m512i y) {\n     __m512i p = libdivide_mullhi_u64_vec512(x, y);\n     __m512i t1 = _mm512_and_si512(libdivide_s64_signbits(x), y);\n     __m512i t2 = _mm512_and_si512(libdivide_s64_signbits(y), x);\n@@ -1715,31 +1739,35 @@ __m512i libdivide_s64_branchfree_do_vec512(\n \n #if defined(LIBDIVIDE_AVX2)\n \n-static inline __m256i libdivide_u32_do_vec256(__m256i numers, const struct libdivide_u32_t *denom);\n-static inline __m256i libdivide_s32_do_vec256(__m256i numers, const struct libdivide_s32_t *denom);\n-static inline __m256i libdivide_u64_do_vec256(__m256i numers, const struct libdivide_u64_t *denom);\n-static inline __m256i libdivide_s64_do_vec256(__m256i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_u32_do_vec256(\n+    __m256i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_s32_do_vec256(\n+    __m256i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_u64_do_vec256(\n+    __m256i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_do_vec256(\n+    __m256i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m256i libdivide_u32_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_u32_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m256i libdivide_s32_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_s32_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m256i libdivide_u64_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_u64_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m256i libdivide_s64_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Implementation of _mm256_srai_epi64(v, 63) (from AVX512).\n-static inline __m256i libdivide_s64_signbits(__m256i v) {\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_signbits(__m256i v) {\n     __m256i hiBitsDuped = _mm256_shuffle_epi32(v, _MM_SHUFFLE(3, 3, 1, 1));\n     __m256i signBits = _mm256_srai_epi32(hiBitsDuped, 31);\n     return signBits;\n }\n \n // Implementation of _mm256_srai_epi64 (from AVX512).\n-static inline __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n     const int b = 64 - amt;\n     __m256i m = _mm256_set1_epi64x(1ULL << (b - 1));\n     __m256i x = _mm256_srli_epi64(v, amt);\n@@ -1748,7 +1776,7 @@ static inline __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n     __m256i hi_product_0Z2Z = _mm256_srli_epi64(_mm256_mul_epu32(a, b), 32);\n     __m256i a1X3X = _mm256_srli_epi64(a, 32);\n     __m256i mask = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1757,7 +1785,7 @@ static inline __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n }\n \n // b is one 32-bit value repeated.\n-static inline __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n     __m256i hi_product_0Z2Z = _mm256_srli_epi64(_mm256_mul_epi32(a, b), 32);\n     __m256i a1X3X = _mm256_srli_epi64(a, 32);\n     __m256i mask = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1766,7 +1794,7 @@ static inline __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n     // see m128i variant for comments.\n     __m256i x0y0 = _mm256_mul_epu32(x, y);\n     __m256i x0y0_hi = _mm256_srli_epi64(x0y0, 32);\n@@ -1789,7 +1817,7 @@ static inline __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m256i libdivide_mullhi_s64_vec256(__m256i x, __m256i y) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_s64_vec256(__m256i x, __m256i y) {\n     __m256i p = libdivide_mullhi_u64_vec256(x, y);\n     __m256i t1 = _mm256_and_si256(libdivide_s64_signbits(x), y);\n     __m256i t2 = _mm256_and_si256(libdivide_s64_signbits(y), x);\n@@ -1965,31 +1993,35 @@ __m256i libdivide_s64_branchfree_do_vec256(\n \n #if defined(LIBDIVIDE_SSE2)\n \n-static inline __m128i libdivide_u32_do_vec128(__m128i numers, const struct libdivide_u32_t *denom);\n-static inline __m128i libdivide_s32_do_vec128(__m128i numers, const struct libdivide_s32_t *denom);\n-static inline __m128i libdivide_u64_do_vec128(__m128i numers, const struct libdivide_u64_t *denom);\n-static inline __m128i libdivide_s64_do_vec128(__m128i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_u32_do_vec128(\n+    __m128i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_s32_do_vec128(\n+    __m128i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_u64_do_vec128(\n+    __m128i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_do_vec128(\n+    __m128i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m128i libdivide_u32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_u32_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m128i libdivide_s32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_s32_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m128i libdivide_u64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_u64_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m128i libdivide_s64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Implementation of _mm_srai_epi64(v, 63) (from AVX512).\n-static inline __m128i libdivide_s64_signbits(__m128i v) {\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_signbits(__m128i v) {\n     __m128i hiBitsDuped = _mm_shuffle_epi32(v, _MM_SHUFFLE(3, 3, 1, 1));\n     __m128i signBits = _mm_srai_epi32(hiBitsDuped, 31);\n     return signBits;\n }\n \n // Implementation of _mm_srai_epi64 (from AVX512).\n-static inline __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n     const int b = 64 - amt;\n     __m128i m = _mm_set1_epi64x(1ULL << (b - 1));\n     __m128i x = _mm_srli_epi64(v, amt);\n@@ -1998,7 +2030,7 @@ static inline __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n     __m128i hi_product_0Z2Z = _mm_srli_epi64(_mm_mul_epu32(a, b), 32);\n     __m128i a1X3X = _mm_srli_epi64(a, 32);\n     __m128i mask = _mm_set_epi32(-1, 0, -1, 0);\n@@ -2009,7 +2041,7 @@ static inline __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n // SSE2 does not have a signed multiplication instruction, but we can convert\n // unsigned to signed pretty efficiently. Again, b is just a 32 bit value\n // repeated four times.\n-static inline __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n     __m128i p = libdivide_mullhi_u32_vec128(a, b);\n     // t1 = (a >> 31) & y, arithmetic shift\n     __m128i t1 = _mm_and_si128(_mm_srai_epi32(a, 31), b);\n@@ -2020,7 +2052,7 @@ static inline __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n     // full 128 bits product is:\n     // x0*y0 + (x0*y1 << 32) + (x1*y0 << 32) + (x1*y1 << 64)\n     // Note x0,y0,x1,y1 are all conceptually uint32, products are 32x32->64.\n@@ -2053,7 +2085,7 @@ static inline __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m128i libdivide_mullhi_s64_vec128(__m128i x, __m128i y) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_s64_vec128(__m128i x, __m128i y) {\n     __m128i p = libdivide_mullhi_u64_vec128(x, y);\n     __m128i t1 = _mm_and_si128(libdivide_s64_signbits(x), y);\n     __m128i t2 = _mm_and_si128(libdivide_s64_signbits(y), x);\n@@ -2264,45 +2296,52 @@ struct NeonVecFor<int64_t> {\n \n // Versions of our algorithms for SIMD.\n #if defined(LIBDIVIDE_NEON)\n-#define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)                                                 \\\n-    typename NeonVecFor<INT_TYPE>::type divide(typename NeonVecFor<INT_TYPE>::type n) const { \\\n-        return libdivide_##ALGO##_do_vec128(n, &denom);                                       \\\n+#define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)                    \\\n+    LIBDIVIDE_INLINE typename NeonVecFor<INT_TYPE>::type divide( \\\n+        typename NeonVecFor<INT_TYPE>::type n) const {           \\\n+        return libdivide_##ALGO##_do_vec128(n, &denom);          \\\n     }\n #else\n #define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)\n #endif\n #if defined(LIBDIVIDE_SSE2)\n-#define LIBDIVIDE_DIVIDE_SSE2(ALGO) \\\n-    __m128i divide(__m128i n) const { return libdivide_##ALGO##_do_vec128(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_SSE2(ALGO)                     \\\n+    LIBDIVIDE_INLINE __m128i divide(__m128i n) const {  \\\n+        return libdivide_##ALGO##_do_vec128(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_SSE2(ALGO)\n #endif\n \n #if defined(LIBDIVIDE_AVX2)\n-#define LIBDIVIDE_DIVIDE_AVX2(ALGO) \\\n-    __m256i divide(__m256i n) const { return libdivide_##ALGO##_do_vec256(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_AVX2(ALGO)                     \\\n+    LIBDIVIDE_INLINE __m256i divide(__m256i n) const {  \\\n+        return libdivide_##ALGO##_do_vec256(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_AVX2(ALGO)\n #endif\n \n #if defined(LIBDIVIDE_AVX512)\n-#define LIBDIVIDE_DIVIDE_AVX512(ALGO) \\\n-    __m512i divide(__m512i n) const { return libdivide_##ALGO##_do_vec512(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_AVX512(ALGO)                   \\\n+    LIBDIVIDE_INLINE __m512i divide(__m512i n) const {  \\\n+        return libdivide_##ALGO##_do_vec512(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_AVX512(ALGO)\n #endif\n \n // The DISPATCHER_GEN() macro generates C++ methods (for the given integer\n // and algorithm types) that redirect to libdivide's C API.\n-#define DISPATCHER_GEN(T, ALGO)                                      \\\n-    libdivide_##ALGO##_t denom;                                      \\\n-    dispatcher() {}                                                  \\\n-    dispatcher(T d) : denom(libdivide_##ALGO##_gen(d)) {}            \\\n-    T divide(T n) const { return libdivide_##ALGO##_do(n, &denom); } \\\n-    T recover() const { return libdivide_##ALGO##_recover(&denom); } \\\n-    LIBDIVIDE_DIVIDE_NEON(ALGO, T)                                   \\\n-    LIBDIVIDE_DIVIDE_SSE2(ALGO)                                      \\\n-    LIBDIVIDE_DIVIDE_AVX2(ALGO)                                      \\\n+#define DISPATCHER_GEN(T, ALGO)                                                       \\\n+    libdivide_##ALGO##_t denom;                                                       \\\n+    LIBDIVIDE_INLINE dispatcher() {}                                                  \\\n+    LIBDIVIDE_INLINE dispatcher(T d) : denom(libdivide_##ALGO##_gen(d)) {}            \\\n+    LIBDIVIDE_INLINE T divide(T n) const { return libdivide_##ALGO##_do(n, &denom); } \\\n+    LIBDIVIDE_INLINE T recover() const { return libdivide_##ALGO##_recover(&denom); } \\\n+    LIBDIVIDE_DIVIDE_NEON(ALGO, T)                                                    \\\n+    LIBDIVIDE_DIVIDE_SSE2(ALGO)                                                       \\\n+    LIBDIVIDE_DIVIDE_AVX2(ALGO)                                                       \\\n     LIBDIVIDE_DIVIDE_AVX512(ALGO)\n \n // The dispatcher selects a specific division algorithm for a given\n@@ -2355,10 +2394,10 @@ class divider {\n     divider() {}\n \n     // Constructor that takes the divisor as a parameter\n-    divider(T d) : div(d) {}\n+    LIBDIVIDE_INLINE divider(T d) : div(d) {}\n \n     // Divides n by the divisor\n-    T divide(T n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE T divide(T n) const { return div.divide(n); }\n \n     // Recovers the divisor, returns the value that was\n     // used to initialize this divider object.\n@@ -2374,16 +2413,16 @@ class divider {\n     // (e.g. s32, u32, s64, u64) and divides each of them by the divider, returning the packed\n     // quotients.\n #if defined(LIBDIVIDE_SSE2)\n-    __m128i divide(__m128i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m128i divide(__m128i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_AVX2)\n-    __m256i divide(__m256i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m256i divide(__m256i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_AVX512)\n-    __m512i divide(__m512i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m512i divide(__m512i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_NEON)\n-    typename NeonVecFor<T>::type divide(typename NeonVecFor<T>::type n) const {\n+    LIBDIVIDE_INLINE typename NeonVecFor<T>::type divide(typename NeonVecFor<T>::type n) const {\n         return div.divide(n);\n     }\n #endif\n@@ -2395,96 +2434,96 @@ class divider {\n \n // Overload of operator / for scalar division\n template <typename T, Branching ALGO>\n-T operator/(T n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE T operator/(T n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n // Overload of operator /= for scalar division\n template <typename T, Branching ALGO>\n-T &operator/=(T &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE T &operator/=(T &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n // Overloads for vector types.\n #if defined(LIBDIVIDE_SSE2)\n template <typename T, Branching ALGO>\n-__m128i operator/(__m128i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m128i operator/(__m128i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m128i operator/=(__m128i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m128i operator/=(__m128i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n #if defined(LIBDIVIDE_AVX2)\n template <typename T, Branching ALGO>\n-__m256i operator/(__m256i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m256i operator/(__m256i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m256i operator/=(__m256i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m256i operator/=(__m256i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n #if defined(LIBDIVIDE_AVX512)\n template <typename T, Branching ALGO>\n-__m512i operator/(__m512i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m512i operator/(__m512i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m512i operator/=(__m512i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m512i operator/=(__m512i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n \n #if defined(LIBDIVIDE_NEON)\n template <Branching ALGO>\n-uint32x4_t operator/(uint32x4_t n, const divider<uint32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint32x4_t operator/(uint32x4_t n, const divider<uint32_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-int32x4_t operator/(int32x4_t n, const divider<int32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int32x4_t operator/(int32x4_t n, const divider<int32_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-uint64x2_t operator/(uint64x2_t n, const divider<uint64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint64x2_t operator/(uint64x2_t n, const divider<uint64_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-int64x2_t operator/(int64x2_t n, const divider<int64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int64x2_t operator/(int64x2_t n, const divider<int64_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-uint32x4_t operator/=(uint32x4_t &n, const divider<uint32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint32x4_t operator/=(uint32x4_t &n, const divider<uint32_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-int32x4_t operator/=(int32x4_t &n, const divider<int32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int32x4_t operator/=(int32x4_t &n, const divider<int32_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-uint64x2_t operator/=(uint64x2_t &n, const divider<uint64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint64x2_t operator/=(uint64x2_t &n, const divider<uint64_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-int64x2_t operator/=(int64x2_t &n, const divider<int64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int64x2_t operator/=(int64x2_t &n, const divider<int64_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }"
    ],
    "files_changed": [
      {
        "filename": "libdivide.h",
        "status": "modified",
        "additions": 178,
        "deletions": 139,
        "changes": 317,
        "patch": "@@ -74,6 +74,17 @@\n #define LIBDIVIDE_FUNCTION __func__\n #endif\n \n+// Set up forced inlining if possible.\n+// We need both the attribute and keyword to avoid \"might not be inlineable\" warnings.\n+#ifdef __has_attribute\n+#if __has_attribute(always_inline)\n+#define LIBDIVIDE_INLINE __attribute__((always_inline)) inline\n+#endif\n+#endif\n+#ifndef LIBDIVIDE_INLINE\n+#define LIBDIVIDE_INLINE inline\n+#endif\n+\n #define LIBDIVIDE_ERROR(msg)                                                                     \\\n     do {                                                                                         \\\n         fprintf(stderr, \"libdivide.h:%d: %s(): Error: %s\\n\", __LINE__, LIBDIVIDE_FUNCTION, msg); \\\n@@ -184,60 +195,64 @@ enum {\n     LIBDIVIDE_NEGATIVE_DIVISOR = 0x80\n };\n \n-static inline struct libdivide_s32_t libdivide_s32_gen(int32_t d);\n-static inline struct libdivide_u32_t libdivide_u32_gen(uint32_t d);\n-static inline struct libdivide_s64_t libdivide_s64_gen(int64_t d);\n-static inline struct libdivide_u64_t libdivide_u64_gen(uint64_t d);\n-\n-static inline struct libdivide_s32_branchfree_t libdivide_s32_branchfree_gen(int32_t d);\n-static inline struct libdivide_u32_branchfree_t libdivide_u32_branchfree_gen(uint32_t d);\n-static inline struct libdivide_s64_branchfree_t libdivide_s64_branchfree_gen(int64_t d);\n-static inline struct libdivide_u64_branchfree_t libdivide_u64_branchfree_gen(uint64_t d);\n-\n-static inline int32_t libdivide_s32_do(int32_t numer, const struct libdivide_s32_t *denom);\n-static inline uint32_t libdivide_u32_do(uint32_t numer, const struct libdivide_u32_t *denom);\n-static inline int64_t libdivide_s64_do(int64_t numer, const struct libdivide_s64_t *denom);\n-static inline uint64_t libdivide_u64_do(uint64_t numer, const struct libdivide_u64_t *denom);\n-\n-static inline int32_t libdivide_s32_branchfree_do(\n+static LIBDIVIDE_INLINE struct libdivide_s32_t libdivide_s32_gen(int32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u32_t libdivide_u32_gen(uint32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_s64_t libdivide_s64_gen(int64_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u64_t libdivide_u64_gen(uint64_t d);\n+\n+static LIBDIVIDE_INLINE struct libdivide_s32_branchfree_t libdivide_s32_branchfree_gen(int32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u32_branchfree_t libdivide_u32_branchfree_gen(uint32_t d);\n+static LIBDIVIDE_INLINE struct libdivide_s64_branchfree_t libdivide_s64_branchfree_gen(int64_t d);\n+static LIBDIVIDE_INLINE struct libdivide_u64_branchfree_t libdivide_u64_branchfree_gen(uint64_t d);\n+\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_do(\n+    int32_t numer, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_do(\n+    uint32_t numer, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_do(\n+    int64_t numer, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_do(\n+    uint64_t numer, const struct libdivide_u64_t *denom);\n+\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_branchfree_do(\n     int32_t numer, const struct libdivide_s32_branchfree_t *denom);\n-static inline uint32_t libdivide_u32_branchfree_do(\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_branchfree_do(\n     uint32_t numer, const struct libdivide_u32_branchfree_t *denom);\n-static inline int64_t libdivide_s64_branchfree_do(\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_branchfree_do(\n     int64_t numer, const struct libdivide_s64_branchfree_t *denom);\n-static inline uint64_t libdivide_u64_branchfree_do(\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_branchfree_do(\n     uint64_t numer, const struct libdivide_u64_branchfree_t *denom);\n \n-static inline int32_t libdivide_s32_recover(const struct libdivide_s32_t *denom);\n-static inline uint32_t libdivide_u32_recover(const struct libdivide_u32_t *denom);\n-static inline int64_t libdivide_s64_recover(const struct libdivide_s64_t *denom);\n-static inline uint64_t libdivide_u64_recover(const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_recover(const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_recover(const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_recover(const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_recover(const struct libdivide_u64_t *denom);\n \n-static inline int32_t libdivide_s32_branchfree_recover(\n+static LIBDIVIDE_INLINE int32_t libdivide_s32_branchfree_recover(\n     const struct libdivide_s32_branchfree_t *denom);\n-static inline uint32_t libdivide_u32_branchfree_recover(\n+static LIBDIVIDE_INLINE uint32_t libdivide_u32_branchfree_recover(\n     const struct libdivide_u32_branchfree_t *denom);\n-static inline int64_t libdivide_s64_branchfree_recover(\n+static LIBDIVIDE_INLINE int64_t libdivide_s64_branchfree_recover(\n     const struct libdivide_s64_branchfree_t *denom);\n-static inline uint64_t libdivide_u64_branchfree_recover(\n+static LIBDIVIDE_INLINE uint64_t libdivide_u64_branchfree_recover(\n     const struct libdivide_u64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n-static inline uint32_t libdivide_mullhi_u32(uint32_t x, uint32_t y) {\n+static LIBDIVIDE_INLINE uint32_t libdivide_mullhi_u32(uint32_t x, uint32_t y) {\n     uint64_t xl = x, yl = y;\n     uint64_t rl = xl * yl;\n     return (uint32_t)(rl >> 32);\n }\n \n-static inline int32_t libdivide_mullhi_s32(int32_t x, int32_t y) {\n+static LIBDIVIDE_INLINE int32_t libdivide_mullhi_s32(int32_t x, int32_t y) {\n     int64_t xl = x, yl = y;\n     int64_t rl = xl * yl;\n     // needs to be arithmetic shift\n     return (int32_t)(rl >> 32);\n }\n \n-static inline uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n+static LIBDIVIDE_INLINE uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n #if defined(LIBDIVIDE_VC) && defined(LIBDIVIDE_X86_64)\n     return __umulh(x, y);\n #elif defined(HAS_INT128_T)\n@@ -263,7 +278,7 @@ static inline uint64_t libdivide_mullhi_u64(uint64_t x, uint64_t y) {\n #endif\n }\n \n-static inline int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n+static LIBDIVIDE_INLINE int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n #if defined(LIBDIVIDE_VC) && defined(LIBDIVIDE_X86_64)\n     return __mulh(x, y);\n #elif defined(HAS_INT128_T)\n@@ -285,7 +300,7 @@ static inline int64_t libdivide_mullhi_s64(int64_t x, int64_t y) {\n #endif\n }\n \n-static inline int32_t libdivide_count_leading_zeros32(uint32_t val) {\n+static LIBDIVIDE_INLINE int32_t libdivide_count_leading_zeros32(uint32_t val) {\n #if defined(__GNUC__) || __has_builtin(__builtin_clz)\n     // Fast way to count leading zeros\n     return __builtin_clz(val);\n@@ -311,7 +326,7 @@ static inline int32_t libdivide_count_leading_zeros32(uint32_t val) {\n #endif\n }\n \n-static inline int32_t libdivide_count_leading_zeros64(uint64_t val) {\n+static LIBDIVIDE_INLINE int32_t libdivide_count_leading_zeros64(uint64_t val) {\n #if defined(__GNUC__) || __has_builtin(__builtin_clzll)\n     // Fast way to count leading zeros\n     return __builtin_clzll(val);\n@@ -332,7 +347,7 @@ static inline int32_t libdivide_count_leading_zeros64(uint64_t val) {\n // libdivide_64_div_32_to_32: divides a 64-bit uint {u1, u0} by a 32-bit\n // uint {v}. The result must fit in 32 bits.\n // Returns the quotient directly and the remainder in *r\n-static inline uint32_t libdivide_64_div_32_to_32(\n+static LIBDIVIDE_INLINE uint32_t libdivide_64_div_32_to_32(\n     uint32_t u1, uint32_t u0, uint32_t v, uint32_t *r) {\n #if (defined(LIBDIVIDE_i386) || defined(LIBDIVIDE_X86_64)) && defined(LIBDIVIDE_GCC_STYLE_ASM)\n     uint32_t result;\n@@ -353,7 +368,7 @@ static uint64_t libdivide_128_div_64_to_64(uint64_t u1, uint64_t u0, uint64_t v,\n     // N.B. resist the temptation to use __uint128_t here.\n     // In LLVM compiler-rt, it performs a 128/128 -> 128 division which is many times slower than\n     // necessary. In gcc it's better but still slower than the divlu implementation, perhaps because\n-    // it's not inlined.\n+    // it's not LIBDIVIDE_INLINEd.\n #if defined(LIBDIVIDE_X86_64) && defined(LIBDIVIDE_GCC_STYLE_ASM)\n     uint64_t result;\n     __asm__(\"divq %[v]\" : \"=a\"(result), \"=d\"(*r) : [v] \"r\"(v), \"a\"(u0), \"d\"(u1));\n@@ -432,7 +447,8 @@ static uint64_t libdivide_128_div_64_to_64(uint64_t u1, uint64_t u0, uint64_t v,\n }\n \n // Bitshift a u128 in place, left (signed_shift > 0) or right (signed_shift < 0)\n-static inline void libdivide_u128_shift(uint64_t *u1, uint64_t *u0, int32_t signed_shift) {\n+static LIBDIVIDE_INLINE void libdivide_u128_shift(\n+    uint64_t *u1, uint64_t *u0, int32_t signed_shift) {\n     if (signed_shift > 0) {\n         uint32_t shift = signed_shift;\n         *u1 <<= shift;\n@@ -546,7 +562,8 @@ static uint64_t libdivide_128_div_128_to_64(\n \n ////////// UINT32\n \n-static inline struct libdivide_u32_t libdivide_internal_u32_gen(uint32_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_u32_t libdivide_internal_u32_gen(\n+    uint32_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -703,7 +720,8 @@ uint32_t libdivide_u32_branchfree_recover(const struct libdivide_u32_branchfree_\n \n /////////// UINT64\n \n-static inline struct libdivide_u64_t libdivide_internal_u64_gen(uint64_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_u64_t libdivide_internal_u64_gen(\n+    uint64_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -874,7 +892,8 @@ uint64_t libdivide_u64_branchfree_recover(const struct libdivide_u64_branchfree_\n \n /////////// SINT32\n \n-static inline struct libdivide_s32_t libdivide_internal_s32_gen(int32_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_s32_t libdivide_internal_s32_gen(\n+    int32_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -1042,7 +1061,8 @@ int32_t libdivide_s32_branchfree_recover(const struct libdivide_s32_branchfree_t\n \n ///////////// SINT64\n \n-static inline struct libdivide_s64_t libdivide_internal_s64_gen(int64_t d, int branchfree) {\n+static LIBDIVIDE_INLINE struct libdivide_s64_t libdivide_internal_s64_gen(\n+    int64_t d, int branchfree) {\n     if (d == 0) {\n         LIBDIVIDE_ERROR(\"divider must be != 0\");\n     }\n@@ -1202,65 +1222,65 @@ int64_t libdivide_s64_branchfree_recover(const struct libdivide_s64_branchfree_t\n \n #if defined(LIBDIVIDE_NEON)\n \n-static inline uint32x4_t libdivide_u32_do_vec128(\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_do_vec128(\n     uint32x4_t numers, const struct libdivide_u32_t *denom);\n-static inline int32x4_t libdivide_s32_do_vec128(\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_do_vec128(\n     int32x4_t numers, const struct libdivide_s32_t *denom);\n-static inline uint64x2_t libdivide_u64_do_vec128(\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_do_vec128(\n     uint64x2_t numers, const struct libdivide_u64_t *denom);\n-static inline int64x2_t libdivide_s64_do_vec128(\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_do_vec128(\n     int64x2_t numers, const struct libdivide_s64_t *denom);\n \n-static inline uint32x4_t libdivide_u32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_branchfree_do_vec128(\n     uint32x4_t numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline int32x4_t libdivide_s32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_branchfree_do_vec128(\n     int32x4_t numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline uint64x2_t libdivide_u64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_branchfree_do_vec128(\n     uint64x2_t numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline int64x2_t libdivide_s64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_branchfree_do_vec128(\n     int64x2_t numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Logical right shift by runtime value.\n // NEON implements right shift as left shits by negative values.\n-static inline uint32x4_t libdivide_u32_neon_srl(uint32x4_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_u32_neon_srl(uint32x4_t v, uint8_t amt) {\n     int32_t wamt = static_cast<int32_t>(amt);\n     return vshlq_u32(v, vdupq_n_s32(-wamt));\n }\n \n-static inline uint64x2_t libdivide_u64_neon_srl(uint64x2_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_u64_neon_srl(uint64x2_t v, uint8_t amt) {\n     int64_t wamt = static_cast<int64_t>(amt);\n     return vshlq_u64(v, vdupq_n_s64(-wamt));\n }\n \n // Arithmetic right shift by runtime value.\n-static inline int32x4_t libdivide_s32_neon_sra(int32x4_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE int32x4_t libdivide_s32_neon_sra(int32x4_t v, uint8_t amt) {\n     int32_t wamt = static_cast<int32_t>(amt);\n     return vshlq_s32(v, vdupq_n_s32(-wamt));\n }\n \n-static inline int64x2_t libdivide_s64_neon_sra(int64x2_t v, uint8_t amt) {\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_neon_sra(int64x2_t v, uint8_t amt) {\n     int64_t wamt = static_cast<int64_t>(amt);\n     return vshlq_s64(v, vdupq_n_s64(-wamt));\n }\n \n-static inline int64x2_t libdivide_s64_signbits(int64x2_t v) { return vshrq_n_s64(v, 63); }\n+static LIBDIVIDE_INLINE int64x2_t libdivide_s64_signbits(int64x2_t v) { return vshrq_n_s64(v, 63); }\n \n-static inline uint32x4_t libdivide_mullhi_u32_vec128(uint32x4_t a, uint32_t b) {\n+static LIBDIVIDE_INLINE uint32x4_t libdivide_mullhi_u32_vec128(uint32x4_t a, uint32_t b) {\n     // Desire is [x0, x1, x2, x3]\n     uint32x4_t w1 = vreinterpretq_u32_u64(vmull_n_u32(vget_low_u32(a), b));  // [_, x0, _, x1]\n     uint32x4_t w2 = vreinterpretq_u32_u64(vmull_high_n_u32(a, b));           //[_, x2, _, x3]\n     return vuzp2q_u32(w1, w2);                                               // [x0, x1, x2, x3]\n }\n \n-static inline int32x4_t libdivide_mullhi_s32_vec128(int32x4_t a, int32_t b) {\n+static LIBDIVIDE_INLINE int32x4_t libdivide_mullhi_s32_vec128(int32x4_t a, int32_t b) {\n     int32x4_t w1 = vreinterpretq_s32_s64(vmull_n_s32(vget_low_s32(a), b));  // [_, x0, _, x1]\n     int32x4_t w2 = vreinterpretq_s32_s64(vmull_high_n_s32(a, b));           //[_, x2, _, x3]\n     return vuzp2q_s32(w1, w2);                                              // [x0, x1, x2, x3]\n }\n \n-static inline uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy) {\n+static LIBDIVIDE_INLINE uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy) {\n     // full 128 bits product is:\n     // x0*y0 + (x0*y1 << 32) + (x1*y0 << 32) + (x1*y1 << 64)\n     // Note x0,y0,x1,y1 are all conceptually uint32, products are 32x32->64.\n@@ -1291,7 +1311,7 @@ static inline uint64x2_t libdivide_mullhi_u64_vec128(uint64x2_t x, uint64_t sy)\n     return result;\n }\n \n-static inline int64x2_t libdivide_mullhi_s64_vec128(int64x2_t x, int64_t sy) {\n+static LIBDIVIDE_INLINE int64x2_t libdivide_mullhi_s64_vec128(int64x2_t x, int64_t sy) {\n     int64x2_t p = vreinterpretq_s64_u64(\n         libdivide_mullhi_u64_vec128(vreinterpretq_u64_s64(x), static_cast<uint64_t>(sy)));\n     int64x2_t y = vdupq_n_s64(sy);\n@@ -1472,33 +1492,37 @@ int64x2_t libdivide_s64_branchfree_do_vec128(\n \n #if defined(LIBDIVIDE_AVX512)\n \n-static inline __m512i libdivide_u32_do_vec512(__m512i numers, const struct libdivide_u32_t *denom);\n-static inline __m512i libdivide_s32_do_vec512(__m512i numers, const struct libdivide_s32_t *denom);\n-static inline __m512i libdivide_u64_do_vec512(__m512i numers, const struct libdivide_u64_t *denom);\n-static inline __m512i libdivide_s64_do_vec512(__m512i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_u32_do_vec512(\n+    __m512i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_s32_do_vec512(\n+    __m512i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_u64_do_vec512(\n+    __m512i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_do_vec512(\n+    __m512i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m512i libdivide_u32_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_u32_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m512i libdivide_s32_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_s32_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m512i libdivide_u64_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_u64_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m512i libdivide_s64_branchfree_do_vec512(\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_branchfree_do_vec512(\n     __m512i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n-static inline __m512i libdivide_s64_signbits(__m512i v) {\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_signbits(__m512i v) {\n     ;\n     return _mm512_srai_epi64(v, 63);\n }\n \n-static inline __m512i libdivide_s64_shift_right_vec512(__m512i v, int amt) {\n+static LIBDIVIDE_INLINE __m512i libdivide_s64_shift_right_vec512(__m512i v, int amt) {\n     return _mm512_srai_epi64(v, amt);\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n     __m512i hi_product_0Z2Z = _mm512_srli_epi64(_mm512_mul_epu32(a, b), 32);\n     __m512i a1X3X = _mm512_srli_epi64(a, 32);\n     __m512i mask = _mm512_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1507,7 +1531,7 @@ static inline __m512i libdivide_mullhi_u32_vec512(__m512i a, __m512i b) {\n }\n \n // b is one 32-bit value repeated.\n-static inline __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n     __m512i hi_product_0Z2Z = _mm512_srli_epi64(_mm512_mul_epi32(a, b), 32);\n     __m512i a1X3X = _mm512_srli_epi64(a, 32);\n     __m512i mask = _mm512_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1516,7 +1540,7 @@ static inline __m512i libdivide_mullhi_s32_vec512(__m512i a, __m512i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n     // see m128i variant for comments.\n     __m512i x0y0 = _mm512_mul_epu32(x, y);\n     __m512i x0y0_hi = _mm512_srli_epi64(x0y0, 32);\n@@ -1539,7 +1563,7 @@ static inline __m512i libdivide_mullhi_u64_vec512(__m512i x, __m512i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m512i libdivide_mullhi_s64_vec512(__m512i x, __m512i y) {\n+static LIBDIVIDE_INLINE __m512i libdivide_mullhi_s64_vec512(__m512i x, __m512i y) {\n     __m512i p = libdivide_mullhi_u64_vec512(x, y);\n     __m512i t1 = _mm512_and_si512(libdivide_s64_signbits(x), y);\n     __m512i t2 = _mm512_and_si512(libdivide_s64_signbits(y), x);\n@@ -1715,31 +1739,35 @@ __m512i libdivide_s64_branchfree_do_vec512(\n \n #if defined(LIBDIVIDE_AVX2)\n \n-static inline __m256i libdivide_u32_do_vec256(__m256i numers, const struct libdivide_u32_t *denom);\n-static inline __m256i libdivide_s32_do_vec256(__m256i numers, const struct libdivide_s32_t *denom);\n-static inline __m256i libdivide_u64_do_vec256(__m256i numers, const struct libdivide_u64_t *denom);\n-static inline __m256i libdivide_s64_do_vec256(__m256i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_u32_do_vec256(\n+    __m256i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_s32_do_vec256(\n+    __m256i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_u64_do_vec256(\n+    __m256i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_do_vec256(\n+    __m256i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m256i libdivide_u32_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_u32_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m256i libdivide_s32_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_s32_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m256i libdivide_u64_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_u64_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m256i libdivide_s64_branchfree_do_vec256(\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_branchfree_do_vec256(\n     __m256i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Implementation of _mm256_srai_epi64(v, 63) (from AVX512).\n-static inline __m256i libdivide_s64_signbits(__m256i v) {\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_signbits(__m256i v) {\n     __m256i hiBitsDuped = _mm256_shuffle_epi32(v, _MM_SHUFFLE(3, 3, 1, 1));\n     __m256i signBits = _mm256_srai_epi32(hiBitsDuped, 31);\n     return signBits;\n }\n \n // Implementation of _mm256_srai_epi64 (from AVX512).\n-static inline __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n+static LIBDIVIDE_INLINE __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n     const int b = 64 - amt;\n     __m256i m = _mm256_set1_epi64x(1ULL << (b - 1));\n     __m256i x = _mm256_srli_epi64(v, amt);\n@@ -1748,7 +1776,7 @@ static inline __m256i libdivide_s64_shift_right_vec256(__m256i v, int amt) {\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n     __m256i hi_product_0Z2Z = _mm256_srli_epi64(_mm256_mul_epu32(a, b), 32);\n     __m256i a1X3X = _mm256_srli_epi64(a, 32);\n     __m256i mask = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1757,7 +1785,7 @@ static inline __m256i libdivide_mullhi_u32_vec256(__m256i a, __m256i b) {\n }\n \n // b is one 32-bit value repeated.\n-static inline __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n     __m256i hi_product_0Z2Z = _mm256_srli_epi64(_mm256_mul_epi32(a, b), 32);\n     __m256i a1X3X = _mm256_srli_epi64(a, 32);\n     __m256i mask = _mm256_set_epi32(-1, 0, -1, 0, -1, 0, -1, 0);\n@@ -1766,7 +1794,7 @@ static inline __m256i libdivide_mullhi_s32_vec256(__m256i a, __m256i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n     // see m128i variant for comments.\n     __m256i x0y0 = _mm256_mul_epu32(x, y);\n     __m256i x0y0_hi = _mm256_srli_epi64(x0y0, 32);\n@@ -1789,7 +1817,7 @@ static inline __m256i libdivide_mullhi_u64_vec256(__m256i x, __m256i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m256i libdivide_mullhi_s64_vec256(__m256i x, __m256i y) {\n+static LIBDIVIDE_INLINE __m256i libdivide_mullhi_s64_vec256(__m256i x, __m256i y) {\n     __m256i p = libdivide_mullhi_u64_vec256(x, y);\n     __m256i t1 = _mm256_and_si256(libdivide_s64_signbits(x), y);\n     __m256i t2 = _mm256_and_si256(libdivide_s64_signbits(y), x);\n@@ -1965,31 +1993,35 @@ __m256i libdivide_s64_branchfree_do_vec256(\n \n #if defined(LIBDIVIDE_SSE2)\n \n-static inline __m128i libdivide_u32_do_vec128(__m128i numers, const struct libdivide_u32_t *denom);\n-static inline __m128i libdivide_s32_do_vec128(__m128i numers, const struct libdivide_s32_t *denom);\n-static inline __m128i libdivide_u64_do_vec128(__m128i numers, const struct libdivide_u64_t *denom);\n-static inline __m128i libdivide_s64_do_vec128(__m128i numers, const struct libdivide_s64_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_u32_do_vec128(\n+    __m128i numers, const struct libdivide_u32_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_s32_do_vec128(\n+    __m128i numers, const struct libdivide_s32_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_u64_do_vec128(\n+    __m128i numers, const struct libdivide_u64_t *denom);\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_do_vec128(\n+    __m128i numers, const struct libdivide_s64_t *denom);\n \n-static inline __m128i libdivide_u32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_u32_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_u32_branchfree_t *denom);\n-static inline __m128i libdivide_s32_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_s32_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_s32_branchfree_t *denom);\n-static inline __m128i libdivide_u64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_u64_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_u64_branchfree_t *denom);\n-static inline __m128i libdivide_s64_branchfree_do_vec128(\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_branchfree_do_vec128(\n     __m128i numers, const struct libdivide_s64_branchfree_t *denom);\n \n //////// Internal Utility Functions\n \n // Implementation of _mm_srai_epi64(v, 63) (from AVX512).\n-static inline __m128i libdivide_s64_signbits(__m128i v) {\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_signbits(__m128i v) {\n     __m128i hiBitsDuped = _mm_shuffle_epi32(v, _MM_SHUFFLE(3, 3, 1, 1));\n     __m128i signBits = _mm_srai_epi32(hiBitsDuped, 31);\n     return signBits;\n }\n \n // Implementation of _mm_srai_epi64 (from AVX512).\n-static inline __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n+static LIBDIVIDE_INLINE __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n     const int b = 64 - amt;\n     __m128i m = _mm_set1_epi64x(1ULL << (b - 1));\n     __m128i x = _mm_srli_epi64(v, amt);\n@@ -1998,7 +2030,7 @@ static inline __m128i libdivide_s64_shift_right_vec128(__m128i v, int amt) {\n }\n \n // Here, b is assumed to contain one 32-bit value repeated.\n-static inline __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n     __m128i hi_product_0Z2Z = _mm_srli_epi64(_mm_mul_epu32(a, b), 32);\n     __m128i a1X3X = _mm_srli_epi64(a, 32);\n     __m128i mask = _mm_set_epi32(-1, 0, -1, 0);\n@@ -2009,7 +2041,7 @@ static inline __m128i libdivide_mullhi_u32_vec128(__m128i a, __m128i b) {\n // SSE2 does not have a signed multiplication instruction, but we can convert\n // unsigned to signed pretty efficiently. Again, b is just a 32 bit value\n // repeated four times.\n-static inline __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n     __m128i p = libdivide_mullhi_u32_vec128(a, b);\n     // t1 = (a >> 31) & y, arithmetic shift\n     __m128i t1 = _mm_and_si128(_mm_srai_epi32(a, 31), b);\n@@ -2020,7 +2052,7 @@ static inline __m128i libdivide_mullhi_s32_vec128(__m128i a, __m128i b) {\n }\n \n // Here, y is assumed to contain one 64-bit value repeated.\n-static inline __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n     // full 128 bits product is:\n     // x0*y0 + (x0*y1 << 32) + (x1*y0 << 32) + (x1*y1 << 64)\n     // Note x0,y0,x1,y1 are all conceptually uint32, products are 32x32->64.\n@@ -2053,7 +2085,7 @@ static inline __m128i libdivide_mullhi_u64_vec128(__m128i x, __m128i y) {\n }\n \n // y is one 64-bit value repeated.\n-static inline __m128i libdivide_mullhi_s64_vec128(__m128i x, __m128i y) {\n+static LIBDIVIDE_INLINE __m128i libdivide_mullhi_s64_vec128(__m128i x, __m128i y) {\n     __m128i p = libdivide_mullhi_u64_vec128(x, y);\n     __m128i t1 = _mm_and_si128(libdivide_s64_signbits(x), y);\n     __m128i t2 = _mm_and_si128(libdivide_s64_signbits(y), x);\n@@ -2264,45 +2296,52 @@ struct NeonVecFor<int64_t> {\n \n // Versions of our algorithms for SIMD.\n #if defined(LIBDIVIDE_NEON)\n-#define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)                                                 \\\n-    typename NeonVecFor<INT_TYPE>::type divide(typename NeonVecFor<INT_TYPE>::type n) const { \\\n-        return libdivide_##ALGO##_do_vec128(n, &denom);                                       \\\n+#define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)                    \\\n+    LIBDIVIDE_INLINE typename NeonVecFor<INT_TYPE>::type divide( \\\n+        typename NeonVecFor<INT_TYPE>::type n) const {           \\\n+        return libdivide_##ALGO##_do_vec128(n, &denom);          \\\n     }\n #else\n #define LIBDIVIDE_DIVIDE_NEON(ALGO, INT_TYPE)\n #endif\n #if defined(LIBDIVIDE_SSE2)\n-#define LIBDIVIDE_DIVIDE_SSE2(ALGO) \\\n-    __m128i divide(__m128i n) const { return libdivide_##ALGO##_do_vec128(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_SSE2(ALGO)                     \\\n+    LIBDIVIDE_INLINE __m128i divide(__m128i n) const {  \\\n+        return libdivide_##ALGO##_do_vec128(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_SSE2(ALGO)\n #endif\n \n #if defined(LIBDIVIDE_AVX2)\n-#define LIBDIVIDE_DIVIDE_AVX2(ALGO) \\\n-    __m256i divide(__m256i n) const { return libdivide_##ALGO##_do_vec256(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_AVX2(ALGO)                     \\\n+    LIBDIVIDE_INLINE __m256i divide(__m256i n) const {  \\\n+        return libdivide_##ALGO##_do_vec256(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_AVX2(ALGO)\n #endif\n \n #if defined(LIBDIVIDE_AVX512)\n-#define LIBDIVIDE_DIVIDE_AVX512(ALGO) \\\n-    __m512i divide(__m512i n) const { return libdivide_##ALGO##_do_vec512(n, &denom); }\n+#define LIBDIVIDE_DIVIDE_AVX512(ALGO)                   \\\n+    LIBDIVIDE_INLINE __m512i divide(__m512i n) const {  \\\n+        return libdivide_##ALGO##_do_vec512(n, &denom); \\\n+    }\n #else\n #define LIBDIVIDE_DIVIDE_AVX512(ALGO)\n #endif\n \n // The DISPATCHER_GEN() macro generates C++ methods (for the given integer\n // and algorithm types) that redirect to libdivide's C API.\n-#define DISPATCHER_GEN(T, ALGO)                                      \\\n-    libdivide_##ALGO##_t denom;                                      \\\n-    dispatcher() {}                                                  \\\n-    dispatcher(T d) : denom(libdivide_##ALGO##_gen(d)) {}            \\\n-    T divide(T n) const { return libdivide_##ALGO##_do(n, &denom); } \\\n-    T recover() const { return libdivide_##ALGO##_recover(&denom); } \\\n-    LIBDIVIDE_DIVIDE_NEON(ALGO, T)                                   \\\n-    LIBDIVIDE_DIVIDE_SSE2(ALGO)                                      \\\n-    LIBDIVIDE_DIVIDE_AVX2(ALGO)                                      \\\n+#define DISPATCHER_GEN(T, ALGO)                                                       \\\n+    libdivide_##ALGO##_t denom;                                                       \\\n+    LIBDIVIDE_INLINE dispatcher() {}                                                  \\\n+    LIBDIVIDE_INLINE dispatcher(T d) : denom(libdivide_##ALGO##_gen(d)) {}            \\\n+    LIBDIVIDE_INLINE T divide(T n) const { return libdivide_##ALGO##_do(n, &denom); } \\\n+    LIBDIVIDE_INLINE T recover() const { return libdivide_##ALGO##_recover(&denom); } \\\n+    LIBDIVIDE_DIVIDE_NEON(ALGO, T)                                                    \\\n+    LIBDIVIDE_DIVIDE_SSE2(ALGO)                                                       \\\n+    LIBDIVIDE_DIVIDE_AVX2(ALGO)                                                       \\\n     LIBDIVIDE_DIVIDE_AVX512(ALGO)\n \n // The dispatcher selects a specific division algorithm for a given\n@@ -2355,10 +2394,10 @@ class divider {\n     divider() {}\n \n     // Constructor that takes the divisor as a parameter\n-    divider(T d) : div(d) {}\n+    LIBDIVIDE_INLINE divider(T d) : div(d) {}\n \n     // Divides n by the divisor\n-    T divide(T n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE T divide(T n) const { return div.divide(n); }\n \n     // Recovers the divisor, returns the value that was\n     // used to initialize this divider object.\n@@ -2374,16 +2413,16 @@ class divider {\n     // (e.g. s32, u32, s64, u64) and divides each of them by the divider, returning the packed\n     // quotients.\n #if defined(LIBDIVIDE_SSE2)\n-    __m128i divide(__m128i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m128i divide(__m128i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_AVX2)\n-    __m256i divide(__m256i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m256i divide(__m256i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_AVX512)\n-    __m512i divide(__m512i n) const { return div.divide(n); }\n+    LIBDIVIDE_INLINE __m512i divide(__m512i n) const { return div.divide(n); }\n #endif\n #if defined(LIBDIVIDE_NEON)\n-    typename NeonVecFor<T>::type divide(typename NeonVecFor<T>::type n) const {\n+    LIBDIVIDE_INLINE typename NeonVecFor<T>::type divide(typename NeonVecFor<T>::type n) const {\n         return div.divide(n);\n     }\n #endif\n@@ -2395,96 +2434,96 @@ class divider {\n \n // Overload of operator / for scalar division\n template <typename T, Branching ALGO>\n-T operator/(T n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE T operator/(T n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n // Overload of operator /= for scalar division\n template <typename T, Branching ALGO>\n-T &operator/=(T &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE T &operator/=(T &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n // Overloads for vector types.\n #if defined(LIBDIVIDE_SSE2)\n template <typename T, Branching ALGO>\n-__m128i operator/(__m128i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m128i operator/(__m128i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m128i operator/=(__m128i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m128i operator/=(__m128i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n #if defined(LIBDIVIDE_AVX2)\n template <typename T, Branching ALGO>\n-__m256i operator/(__m256i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m256i operator/(__m256i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m256i operator/=(__m256i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m256i operator/=(__m256i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n #if defined(LIBDIVIDE_AVX512)\n template <typename T, Branching ALGO>\n-__m512i operator/(__m512i n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m512i operator/(__m512i n, const divider<T, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <typename T, Branching ALGO>\n-__m512i operator/=(__m512i &n, const divider<T, ALGO> &div) {\n+LIBDIVIDE_INLINE __m512i operator/=(__m512i &n, const divider<T, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n #endif\n \n #if defined(LIBDIVIDE_NEON)\n template <Branching ALGO>\n-uint32x4_t operator/(uint32x4_t n, const divider<uint32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint32x4_t operator/(uint32x4_t n, const divider<uint32_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-int32x4_t operator/(int32x4_t n, const divider<int32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int32x4_t operator/(int32x4_t n, const divider<int32_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-uint64x2_t operator/(uint64x2_t n, const divider<uint64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint64x2_t operator/(uint64x2_t n, const divider<uint64_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-int64x2_t operator/(int64x2_t n, const divider<int64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int64x2_t operator/(int64x2_t n, const divider<int64_t, ALGO> &div) {\n     return div.divide(n);\n }\n \n template <Branching ALGO>\n-uint32x4_t operator/=(uint32x4_t &n, const divider<uint32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint32x4_t operator/=(uint32x4_t &n, const divider<uint32_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-int32x4_t operator/=(int32x4_t &n, const divider<int32_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int32x4_t operator/=(int32x4_t &n, const divider<int32_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-uint64x2_t operator/=(uint64x2_t &n, const divider<uint64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE uint64x2_t operator/=(uint64x2_t &n, const divider<uint64_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }\n \n template <Branching ALGO>\n-int64x2_t operator/=(int64x2_t &n, const divider<int64_t, ALGO> &div) {\n+LIBDIVIDE_INLINE int64x2_t operator/=(int64x2_t &n, const divider<int64_t, ALGO> &div) {\n     n = div.divide(n);\n     return n;\n }"
      }
    ],
    "lines_added": 178,
    "lines_removed": 139
  },
  "issues": [],
  "pull_requests": [],
  "build_info": {
    "old_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
      "cmake --build /test_workspace/workspace/old/build -- -j 1"
    ],
    "new_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/new -B /test_workspace/workspace/new/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTS=ON",
      "cmake --build /test_workspace/workspace/new/build -- -j 1"
    ],
    "old_test_script": [
      "cd /test_workspace/workspace/old/build",
      "ctest --output-on-failure"
    ],
    "new_test_script": [
      "cd /test_workspace/workspace/new/build",
      "ctest --output-on-failure"
    ],
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": true,
    "p_value": 6.841579970895504e-38,
    "is_pair_significant": true,
    "pair_p_value": 1.1069088735019703e-27,
    "is_binom_significant": true,
    "binom_p_value": 9.313225746154785e-10,
    "is_wilcoxon_significant": true,
    "wilcoxon_p_value": 9.313225746154785e-10,
    "is_mannwhitney_significant": true,
    "mannwhitney_p_value": 1.5089833992452233e-11,
    "relative_improvement": 0.25488932885697985,
    "absolute_improvement_ms": 63040.333333333336,
    "old_mean_ms": 247324.3333333333,
    "new_mean_ms": 184284.0,
    "old_std_ms": 5935.22429831126,
    "new_std_ms": 2920.0869141528824,
    "effect_size_cohens_d": 13.478005665255793,
    "old_ci95_ms": [
      245108.08415730813,
      249540.58250935853
    ],
    "new_ci95_ms": [
      183193.62162638828,
      185374.37837361172
    ],
    "old_ci99_ms": [
      244337.461902384,
      250311.20476428265
    ],
    "new_ci99_ms": [
      182814.48111724202,
      185753.51888275795
    ],
    "new_times_s": [
      190.74,
      181.79,
      180.15,
      188.03,
      187.45,
      180.61,
      183.69,
      186.28,
      180.85,
      184.76,
      182.45,
      183.96,
      185.33,
      185.13,
      185.81,
      181.64,
      180.01,
      182.45,
      179.79,
      186.26,
      185.62,
      181.83,
      191.23,
      183.83,
      187.8,
      185.92,
      180.75,
      184.24,
      188.05,
      185.23,
      187.58
    ],
    "old_times_s": [
      256.58,
      242.66,
      241.72,
      243.19,
      250.8,
      241.66,
      244.29,
      253.79,
      250.19,
      254.84,
      256.38,
      242.69,
      241.65,
      242.45,
      251.02,
      256.76,
      242.58,
      241.68,
      255.46,
      242.1,
      253.59,
      242.17,
      242.14,
      256.81,
      254.14,
      242.19,
      251.33,
      242.61,
      253.77,
      242.16,
      242.91
    ]
  },
  "tests": {
    "total_tests": 3,
    "significant_improvements": 2,
    "significant_improvements_tests": [
      "tester",
      "benchmark_branchfree"
    ],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 2,
    "significant_pair_improvements_tests": [
      "tester",
      "benchmark_branchfree"
    ],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 3,
    "significant_binom_improvements_tests": [
      "build_tests",
      "tester",
      "benchmark_branchfree"
    ],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 2,
    "significant_wilcoxon_improvements_tests": [
      "tester",
      "benchmark_branchfree"
    ],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 3,
    "significant_mannwhitney_improvements_tests": [
      "build_tests",
      "tester",
      "benchmark_branchfree"
    ],
    "significant_mannwhitney_regressions": 0,
    "significant_mannwhitney_regressions_tests": [],
    "tests": [
      {
        "test_name": "build_tests",
        "is_significant": false,
        "p_value": 0.15991407920830442,
        "is_pair_significant": false,
        "pair_p_value": 0.15244108150301358,
        "is_binom_significant": true,
        "binom_p_value": 0.03071417286992073,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.5673045570490469,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 4.249922246153897e-08,
        "relative_improvement": 0.05763688760806909,
        "absolute_improvement_ms": 6.896551724137959,
        "old_mean_ms": 119.65517241379311,
        "new_mean_ms": 112.75862068965516,
        "old_std_ms": 1.8569533817705175,
        "new_std_ms": 4.548588261473418,
        "effect_size_cohens_d": 1.9851666679418698,
        "old_ci95_ms": [
          118.9488251235189,
          120.36151970406733
        ],
        "new_ci95_ms": [
          111.02843024728575,
          114.48881113202455
        ],
        "old_ci99_ms": [
          118.70232329122034,
          120.6080215363659
        ],
        "new_ci99_ms": [
          110.42462653749317,
          115.09261484181714
        ],
        "new_times": [
          0.11,
          0.11,
          0.11,
          0.11,
          0.12,
          0.11,
          0.11,
          0.11,
          0.11,
          0.11,
          0.12,
          0.11,
          0.12,
          0.11,
          0.11,
          0.11,
          0.11,
          0.11,
          0.12,
          0.12,
          0.12,
          0.12,
          0.11,
          0.11,
          0.11,
          0.12,
          0.11,
          0.11,
          0.11
        ],
        "old_times": [
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.12,
          0.11,
          0.12,
          0.12
        ]
      },
      {
        "test_name": "tester",
        "is_significant": true,
        "p_value": 3.687266315391947e-96,
        "is_pair_significant": true,
        "pair_p_value": 1.3301914130140043e-53,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.2798166053566659e-06,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 3.213490034477925e-11,
        "relative_improvement": 0.29240916214336865,
        "absolute_improvement_ms": 19430.68965517241,
        "old_mean_ms": 66450.3448275862,
        "new_mean_ms": 47019.6551724138,
        "old_std_ms": 180.52468684802125,
        "new_std_ms": 150.67663480401328,
        "effect_size_cohens_d": 116.86101782460182,
        "old_ci95_ms": [
          66381.67690913737,
          66519.01274603505
        ],
        "new_ci95_ms": [
          46962.340845761195,
          47076.9694990664
        ],
        "old_ci99_ms": [
          66357.71310556536,
          66542.97654960706
        ],
        "new_ci99_ms": [
          46942.33923078885,
          47096.97111403875
        ],
        "new_times": [
          46.97,
          47.1,
          46.94,
          46.63,
          47.15,
          47.06,
          47.0,
          47.13,
          47.08,
          46.82,
          46.98,
          46.87,
          47.05,
          47.08,
          46.68,
          47.02,
          46.93,
          47.08,
          47.15,
          47.15,
          47.19,
          47.09,
          46.96,
          47.04,
          47.25,
          46.96,
          46.82,
          47.26,
          47.13
        ],
        "old_times": [
          66.14,
          66.85,
          66.61,
          66.42,
          66.47,
          66.34,
          66.32,
          66.46,
          66.44,
          66.58,
          66.31,
          66.82,
          66.21,
          66.53,
          66.64,
          66.31,
          66.21,
          66.58,
          66.26,
          66.72,
          66.4,
          66.41,
          66.33,
          66.46,
          66.45,
          66.21,
          66.42,
          66.66,
          66.5
        ]
      },
      {
        "test_name": "benchmark_branchfree",
        "is_significant": true,
        "p_value": 1.718113975135137e-29,
        "is_pair_significant": true,
        "pair_p_value": 5.6258646952630525e-22,
        "is_binom_significant": true,
        "binom_p_value": 1.862645149230957e-09,
        "is_wilcoxon_significant": true,
        "wilcoxon_p_value": 1.862645149230957e-09,
        "is_mannwhitney_significant": true,
        "mannwhitney_p_value": 3.2547373213890265e-11,
        "relative_improvement": 0.24145476308922728,
        "absolute_improvement_ms": 43682.41379310348,
        "old_mean_ms": 180913.4482758621,
        "new_mean_ms": 137231.0344827586,
        "old_std_ms": 6027.468833753678,
        "new_std_ms": 2896.1407138609056,
        "effect_size_cohens_d": 9.238051612955642,
        "old_ci95_ms": [
          178620.72174878317,
          183206.17480294104
        ],
        "new_ci95_ms": [
          136129.40147162828,
          138332.66749388893
        ],
        "old_ci99_ms": [
          177820.6035949143,
          184006.2929568099
        ],
        "new_ci99_ms": [
          135744.95240598696,
          138717.11655953026
        ],
        "new_times": [
          133.06,
          140.81,
          140.4,
          133.86,
          136.42,
          139.09,
          133.73,
          137.5,
          135.25,
          137.02,
          138.23,
          138.15,
          138.64,
          134.44,
          133.22,
          135.31,
          132.74,
          139.07,
          138.35,
          134.56,
          143.92,
          136.62,
          140.72,
          138.76,
          133.38,
          137.16,
          141.11,
          137.85,
          140.33
        ],
        "old_times": [
          175.45,
          176.21,
          184.06,
          175.11,
          177.71,
          187.33,
          183.75,
          188.25,
          189.81,
          175.98,
          175.22,
          175.51,
          184.69,
          190.11,
          175.83,
          175.25,
          189.13,
          175.4,
          187.22,
          175.33,
          175.62,
          190.27,
          187.69,
          175.61,
          184.76,
          176.28,
          187.23,
          175.38,
          176.3
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}