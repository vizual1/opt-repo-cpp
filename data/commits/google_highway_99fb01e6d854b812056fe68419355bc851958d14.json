{
  "metadata": {
    "collection_date": "2026-02-03T20:01:44.713002",
    "repository": "https://github.com/google/highway",
    "repository_name": "google/highway"
  },
  "commit_info": {
    "old_sha": "63f1e2e3e8ee4bd23b62d4315355777c74b83fae",
    "new_sha": "99fb01e6d854b812056fe68419355bc851958d14",
    "commit_message": [
      "faster <=256 element sorts: pow2 ranges, masked loads\n\nPiperOrigin-RevId: 530593280"
    ],
    "commit_date": "2023-05-09T13:34:25+00:00",
    "patch": [
      "--- hwy/contrib/sort/result-inl.h\n@@ -86,7 +86,7 @@ struct Result {\n     const double bytes = static_cast<double>(num_keys) *\n                          static_cast<double>(num_threads) *\n                          static_cast<double>(sizeof_key);\n-    printf(\"%10s: %12s: %7s: %9s: %.2E %4.0f MB/s (%2zu threads)\\n\",\n+    printf(\"%10s: %12s: %7s: %9s: %03g %4.0f MB/s (%2zu threads)\\n\",\n            hwy::TargetName(target), AlgoName(algo), key_name.c_str(),\n            DistName(dist), static_cast<double>(num_keys), bytes * 1E-6 / sec,\n            num_threads);\n@@ -115,12 +115,13 @@ bool VerifySort(Traits st, const InputStats<LaneType>& input_stats,\n     if (N1 == 2) output_stats.Notify(out[i + 1]);\n     // Reverse order instead of checking !Compare1 so we accept equal keys.\n     if (st.Compare1(out + i + N1, out + i)) {\n-      printf(\"%s: i=%d of %d lanes: N1=%d %5.0f %5.0f vs. %5.0f %5.0f\\n\\n\",\n-             caller, static_cast<int>(i), static_cast<int>(num_lanes),\n-             static_cast<int>(N1), static_cast<double>(out[i + 1]),\n-             static_cast<double>(out[i + 0]),\n-             static_cast<double>(out[i + N1 + 1]),\n-             static_cast<double>(out[i + N1]));\n+      fprintf(stderr, \"%s: i=%d of %d lanes: N1=%d\", caller,\n+              static_cast<int>(i), static_cast<int>(num_lanes),\n+              static_cast<int>(N1));\n+      fprintf(stderr, \"%5.0f %5.0f vs. %5.0f %5.0f\\n\\n\",\n+              static_cast<double>(out[i + 1]), static_cast<double>(out[i + 0]),\n+              static_cast<double>(out[i + N1 + 1]),\n+              static_cast<double>(out[i + N1]));\n       HWY_ABORT(\"%d-bit sort is incorrect\\n\",\n                 static_cast<int>(sizeof(LaneType) * 8 * N1));\n     }\n--- hwy/contrib/sort/sorting_networks-inl.h\n@@ -72,6 +72,37 @@ struct SharedTraits : public Base {\n \n // ------------------------------ Sorting network\n \n+// Sorting network for independent columns in 8 vectors.\n+// https://bertdobbelaere.github.io/sorting_networks.html\n+template <class D, class Traits, class V = Vec<D>>\n+HWY_INLINE void Sort8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,\n+                      V& v6, V& v7) {\n+  st.Sort2(d, v0, v2);\n+  st.Sort2(d, v1, v3);\n+  st.Sort2(d, v4, v6);\n+  st.Sort2(d, v5, v7);\n+\n+  st.Sort2(d, v0, v4);\n+  st.Sort2(d, v1, v5);\n+  st.Sort2(d, v2, v6);\n+  st.Sort2(d, v3, v7);\n+\n+  st.Sort2(d, v0, v1);\n+  st.Sort2(d, v2, v3);\n+  st.Sort2(d, v4, v5);\n+  st.Sort2(d, v6, v7);\n+\n+  st.Sort2(d, v2, v4);\n+  st.Sort2(d, v3, v5);\n+\n+  st.Sort2(d, v1, v4);\n+  st.Sort2(d, v3, v6);\n+\n+  st.Sort2(d, v1, v2);\n+  st.Sort2(d, v3, v4);\n+  st.Sort2(d, v5, v6);\n+}\n+\n // (Green's irregular) sorting network for independent columns in 16 vectors.\n template <class D, class Traits, class V = Vec<D>>\n HWY_INLINE void Sort16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,\n--- hwy/contrib/sort/traits128-inl.h\n@@ -413,29 +413,43 @@ struct OrderDescendingKV128 : public KeyValue128 {\n   }\n };\n \n-// Shared code that depends on Order.\n-template <class Base>\n-class Traits128 : public Base {\n-  // We want to swap 2 u128, i.e. 4 u64 lanes, based on the 0 or FF..FF mask in\n-  // the most-significant of those lanes (the result of CompareTop), so\n-  // replicate it 4x. Only called for >= 256-bit vectors.\n-  template <class V>\n-  HWY_INLINE V ReplicateTop4x(V v) const {\n+// We want to swap 2 u128, i.e. 4 u64 lanes, based on the 0 or FF..FF mask in\n+// the most-significant of those lanes (the result of CompareTop), so\n+// replicate it 4x. Only called for >= 256-bit vectors.\n+\n+#if HWY_TARGET <= HWY_AVX3\n+template <class V, HWY_IF_V_SIZE_V(V, 64)>\n+HWY_INLINE V ReplicateTop4x(V v) {\n+  return V{_mm512_permutex_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+}\n+#endif  // HWY_TARGET <= HWY_AVX3\n+\n+#if HWY_TARGET <= HWY_AVX2\n+\n+template <class V, HWY_IF_V_SIZE_V(V, 32)>\n+HWY_INLINE V ReplicateTop4x(V v) {\n+  return V{_mm256_permute4x64_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+}\n+\n+#else  // HWY_TARGET > HWY_AVX2\n+\n+template <class V>\n+HWY_INLINE V ReplicateTop4x(V v) {\n #if HWY_TARGET == HWY_SVE_256\n-    return svdup_lane_u64(v, 3);\n-#elif HWY_TARGET <= HWY_AVX3\n-    return V{_mm512_permutex_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n-#elif HWY_TARGET == HWY_AVX2\n-    return V{_mm256_permute4x64_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+  return svdup_lane_u64(v, 3);\n #else\n     alignas(64) static constexpr uint64_t kIndices[8] = {3, 3, 3, 3,\n                                                          7, 7, 7, 7};\n     const ScalableTag<uint64_t> d;\n     return TableLookupLanes(v, SetTableIndices(d, kIndices));\n #endif\n-  }\n+}\n \n- public:\n+#endif  // HWY_TARGET <= HWY_AVX2\n+\n+// Shared code that depends on Order.\n+template <class Base>\n+struct Traits128 : public Base {\n   template <class D>\n   HWY_INLINE Vec<D> FirstOfLanes(D d, Vec<D> v,\n                                  TFromD<D>* HWY_RESTRICT buf) const {\n--- hwy/contrib/sort/vqsort-inl.h\n@@ -123,61 +123,581 @@ void HeapSort(Traits st, T* HWY_RESTRICT lanes, const size_t num_lanes) {\n \n // ------------------------------ BaseCase\n \n-// Sorts `keys` within the range [0, num) via sorting network.\n-template <class D, class Traits, typename T>\n-HWY_INLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n-                         T* HWY_RESTRICT keys_end, size_t num,\n+// Special cases where `num_lanes` is in the specified range (inclusive).\n+template <class Traits, typename T>\n+HWY_INLINE void Sort0To8(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n                          T* HWY_RESTRICT buf) {\n-  const size_t N = Lanes(d);\n-  using V = decltype(Zero(d));\n-\n-  // _Nonzero32 requires num - 1 != 0.\n-  if (HWY_UNLIKELY(num <= 1)) return;\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(num_keys <= 8);\n+  HWY_ASSUME(num_keys <= 8);  // reduces branches\n+\n+  // Copy into buffer so we only require a single masked load. Prevent using\n+  // unnecessarily large vectors by capping - this sorting network only handles\n+  // eight keys, so it does not make sense to load more.\n+  const CappedTagIfFixed<T, 8 * kLPK> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // Unless num_keys == 8, we pad the input because the sorting network is\n+  // fixed-size.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Values to sort, and where to store the sorted output. This may point to\n+  // buf, in which case it must be copied back to `keys`.\n+  T* HWY_RESTRICT in_out = keys;\n+  // Valid lanes in the last vector (only used if num_keys != 8).\n+  Mask<decltype(dmax)> mask;\n+\n+  // Not all of the sorting network will be used. May require padding.\n+  if (HWY_UNLIKELY(num_keys != 8)) {\n+    // Done if there are zero or one keys to sort.\n+    if (HWY_UNLIKELY(num_keys <= 1)) return;\n+\n+    in_out = buf;\n+    // Copy whole vectors(s)\n+    size_t i = 0;\n+    for (; i + Nmax <= num_lanes; i += Nmax) {\n+      Store(LoadU(dmax, keys + i), dmax, buf + i);\n+    }\n \n-  // Reshape into a matrix with kMaxRows rows, and columns limited by the\n-  // 1D `num`, which is upper-bounded by the vector width (see BaseCaseNum).\n-  const size_t num_pow2 = size_t{1}\n-                          << (32 - Num0BitsAboveMS1Bit_Nonzero32(\n-                                       static_cast<uint32_t>(num - 1)));\n-  HWY_DASSERT(num <= num_pow2 && num_pow2 <= Constants::BaseCaseNum(N));\n-  const size_t cols =\n-      HWY_MAX(st.LanesPerKey(), num_pow2 >> Constants::kMaxRowsLog2);\n-  HWY_DASSERT(cols <= N);\n+    // Last iteration: copy partial vector.\n+    const size_t remaining = num_lanes - i;\n+    HWY_ASSUME(remaining < 256);  // helps FirstN\n+    mask = FirstN(dmax, remaining);\n+    Store(MaskedLoadOr(kPadding, mask, dmax, keys + i), dmax, buf + i);\n+\n+    // If `d` is for exactly one key, we initialize 8 * kLPK lanes, and not just\n+    // for MSAN: padding prevents invalid values from influencing the sort.\n+    // But for 16-bit keys, loading a single lane may be inefficient, so we\n+    // allow larger vectors than necessary. Only the first lane is used; the\n+    // next lane is usually loaded from the next valid keys, but we require one\n+    // more padding for the last vector loaded.\n+    constexpr size_t extra = sizeof(T) == 2 ? 1 : 0;\n+    for (i += Nmax; i < 8 * kLPK + extra; i += Nmax) {\n+      Store(kPadding, dmax, buf + i);\n+    }\n+  }\n \n-  // We can avoid padding and load/store directly to `keys` after checking the\n-  // original input array has enough space. Except at the right border, it's OK\n-  // to sort more than the current sub-array. Even if we sort across a previous\n-  // partition point, we know that keys will not migrate across it. However, we\n-  // must use the maximum size of the sorting network, because the StoreU of its\n-  // last vector would otherwise write invalid data starting at kMaxRows * cols.\n-  const size_t N_sn = Lanes(CappedTag<T, Constants::kMaxCols>());\n-  if (HWY_LIKELY(keys + N_sn * Constants::kMaxRows <= keys_end)) {\n-    SortingNetwork(st, keys, N_sn);\n-    return;\n+  // One key per vector (32.. 2x64 bit), or two for 16-bit (see above).\n+  const CappedTag<T, HWY_MAX(kLPK, 4 / sizeof(T))> d;\n+  using V = Vec<decltype(d)>;\n+  using M = Mask<decltype(d)>;\n+\n+  V v0 = LoadU(d, in_out + 0x0 * kLPK);\n+  V v1 = LoadU(d, in_out + 0x1 * kLPK);\n+  V v2 = LoadU(d, in_out + 0x2 * kLPK);\n+  V v3 = LoadU(d, in_out + 0x3 * kLPK);\n+  V v4 = LoadU(d, in_out + 0x4 * kLPK);\n+  V v5 = LoadU(d, in_out + 0x5 * kLPK);\n+  V v6 = LoadU(d, in_out + 0x6 * kLPK);\n+  V v7 = LoadU(d, in_out + 0x7 * kLPK);\n+\n+  Sort8(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n+\n+  StoreU(v0, d, in_out + 0x0 * kLPK);\n+  StoreU(v1, d, in_out + 0x1 * kLPK);\n+  StoreU(v2, d, in_out + 0x2 * kLPK);\n+  StoreU(v3, d, in_out + 0x3 * kLPK);\n+  StoreU(v4, d, in_out + 0x4 * kLPK);\n+  StoreU(v5, d, in_out + 0x5 * kLPK);\n+  StoreU(v6, d, in_out + 0x6 * kLPK);\n+  StoreU(v7, d, in_out + 0x7 * kLPK);\n+\n+  // in_out == buf, so copy back to `keys`.\n+  if (HWY_UNLIKELY(num_keys != 8)) {\n+    size_t i = 0;\n+    // Copy whole vectors(s).\n+    for (; i + Nmax <= num_lanes; i += Nmax) {\n+      StoreU(Load(dmax, buf + i), dmax, keys + i);\n+    }\n+    // Copy partial vector.\n+    BlendedStore(Load(dmax, buf + i), mask, dmax, keys + i);\n   }\n+}\n+\n+template <size_t kLPK, size_t kLanesPerRow, class D, class Traits,\n+          typename T = TFromD<D>>\n+HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n+                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n+  constexpr size_t kMinLanes = 8 * kLanesPerRow;\n+  // Must cap for correctness: we will load up to the last valid lane, so\n+  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n+  const CappedTag<T, kMinLanes> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+  HWY_DASSERT(Nmax < num_lanes);\n+  HWY_ASSUME(Nmax <= kMinLanes);\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Rounding down allows aligned stores, which are typically faster.\n+  size_t i = num_lanes & ~(Nmax - 1);\n+  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n+  do {\n+    Store(kPadding, dmax, buf + i);\n+    i += Nmax;\n+    // Initialize enough for the last vector even if Lanes > lanes_per_row.\n+  } while (i < 0xf * kLanesPerRow + Lanes(d));\n+\n+  // Ensure buf contains all we will read, and perhaps more before.\n+  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n+  do {\n+    end -= Nmax;\n+    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n+  } while (end > static_cast<ptrdiff_t>(8 * kLanesPerRow));\n+}\n \n-  // Copy `keys` to `buf`.\n-  size_t i;\n-  for (i = 0; i + N <= num; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n+// Stores all vectors into buf and copies them into keys without exceeding\n+// `num_lanes`.\n+template <size_t kLanesPerRow, class D, typename T = TFromD<D>>\n+HWY_INLINE void SafeStore(D d, Vec<D> v8, Vec<D> v9, Vec<D> va, Vec<D> vb,\n+                          Vec<D> vc, Vec<D> vd, Vec<D> ve, Vec<D> vf,\n+                          T* HWY_RESTRICT keys, size_t num_lanes,\n+                          T* HWY_RESTRICT buf) {\n+  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n+  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n+  StoreU(va, d, buf + 0xa * kLanesPerRow);\n+  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n+  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n+  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n+  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n+  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first eight vectors have already been stored unconditionally into\n+  // `keys`, so we do not copy them.\n+  size_t i = 8 * kLanesPerRow;\n+#pragma unroll(1)\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n-  SafeCopyN(num - i, d, keys + i, buf + i);\n-  i = num;\n+\n+  // Last iteration: copy partial vector\n+  const size_t remaining = num_lanes - i;\n+  HWY_ASSUME(remaining < 256);  // helps FirstN\n+  Mask<decltype(dmax)> mask = FirstN(dmax, remaining);\n+  BlendedStore(LoadU(dmax, buf + i), mask, dmax, keys + i);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort9To16(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                          T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(9 <= num_keys && num_keys <= 16);\n+  HWY_ASSUME(num_keys <= 16);\n+  // 16 keys divided by 16 rows equals 1 column.\n+  constexpr size_t kLanesPerRow = 1 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort17To32(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                           T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(17 <= num_keys && num_keys <= 32);\n+  HWY_ASSUME(num_keys <= 32);\n+  (void)num_keys;\n+  // 32 keys divided by 16 rows equals 2 columns.\n+  constexpr size_t kLanesPerRow = 2 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort33To64(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                           T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(33 * kLPK <= num_lanes && num_lanes <= 64 * kLPK);\n+  // 64 keys divided by 16 rows equals 4 columns.\n+  constexpr size_t kLanesPerRow = 4 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 32 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else   // !HWY_MEM_OPS_MIGHT_FAULT\n+  // To prevent reading past the end, only activate the lanes corresponding to\n+  // the first four keys (other lanes' masks will be false).\n+  const V vnum_lanes = IfThenElseZero(FirstN(d, kLanesPerRow),\n+                                      Set(d, static_cast<T>(num_lanes)));\n+  const V kIota = Iota(d, T{32 * kLPK});\n+  const V k4 = Set(d, T{kLanesPerRow});\n+  const V k8 = Add(k4, k4);\n+  const V k16 = Add(k8, k8);\n+  const V k32 = Add(k16, k16);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k4));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k8));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k8, k4)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k16));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k16, k4)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k16, k8)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k32, k4)));\n \n   // Fill with padding - last in sort order, not copied to keys.\n   const V kPadding = st.LastValue(d);\n-  // Initialize an extra vector because SortingNetwork loads full vectors,\n-  // which may exceed cols*kMaxRows.\n-  for (; i < (cols * Constants::kMaxRows + N); i += N) {\n-    StoreU(kPadding, d, buf + i);\n-  }\n \n-  SortingNetwork(st, buf, cols);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort65To128(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                            T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(65 * kLPK <= num_lanes && num_lanes <= 128 * kLPK);\n+  // 128 keys divided by 16 rows equals 8 columns.\n+  constexpr size_t kLanesPerRow = 8 * kLPK;\n+\n+  // Eight keys per vector (128..512 bit).\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 64 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else\n+  // All lanes are now valid, so no need for FirstN.\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n+  const V kIota = Iota(d, T{64 * kLPK});\n+  const V k8 = Set(d, T{kLanesPerRow});\n+  const V k16 = Add(k8, k8);\n+  const V k32 = Add(k16, k16);\n+  const V k64 = Add(k32, k32);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k8));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k16));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k16, k8)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k32));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k32, k8)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Sub(k64, k16)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k64, k8)));\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const V kPadding = st.LastValue(d);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  //  HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+#if !HWY_COMPILER_MSVC\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort129To256(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                             T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(129 * kLPK <= num_lanes && num_lanes <= 256 * kLPK);\n+  // 256 keys divided by 16 rows equals 16 columns.\n+  constexpr size_t kLanesPerRow = 16 * kLPK;\n+\n+  // 16 keys per vector (256..512 bit).\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 64 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else\n+  // All lanes are now valid, so no need for FirstN.\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n+  const V kIota = Iota(d, T{128 * kLPK});\n+  const V k16 = Set(d, T{kLanesPerRow});\n+  const V k32 = Add(k16, k16);\n+  const V k64 = Add(k32, k32);\n+  const V k128 = Add(k64, k64);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k16));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k32));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k32, k16)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k64));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k64, k16)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Sub(k128, k32)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k128, k16)));\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const V kPadding = st.LastValue(d);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  //  HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n+          vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+#endif  // HWY_COMPILER_MSVC\n \n-  for (i = 0; i + N <= num; i += N) {\n-    StoreU(Load(d, buf + i), d, keys + i);\n+// Sorts `keys` within the range [0, num_lanes) via sorting network. We know\n+// that num_lanes <= kMaxRows * kMaxCols.\n+template <class D, class Traits, typename T>\n+HWY_INLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n+                         T* HWY_RESTRICT keys_end, size_t num_lanes,\n+                         T* HWY_RESTRICT buf) {\n+  constexpr size_t kMaxLanes = MaxLanes(d);\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_ASSERT(num_lanes * sizeof(T) <= 16 * HWY_MAX_BYTES);\n+\n+  // TODO(janwas): jump table using Num0BitsAboveMS1Bit_Nonzero32?\n+  if (num_lanes <= 8 * 1 * kLPK) {\n+    return Sort0To8(st, keys, num_lanes, buf);\n+  }\n+  // Do not check kMaxLanes - also works for single-key vectors\n+  if (num_lanes <= 16 * 1 * kLPK) {\n+    return Sort9To16(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 2 * kLPK && kMaxLanes >= 2) {\n+    return Sort17To32(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 4 * kLPK && kMaxLanes >= 4) {\n+    return Sort33To64(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 8 * kLPK && kMaxLanes >= 8) {\n+    return Sort65To128(st, keys, num_lanes, buf);\n+  }\n+\n+#if !HWY_COMPILER_MSVC\n+  if (kMaxLanes >= 16) {\n+    Sort129To256(st, keys, num_lanes, buf);\n   }\n-  SafeCopyN(num - i, d, buf + i, keys + i);\n+#endif\n+\n+  // TODO(janwas): avoid masking if in-bounds\n+  // We can avoid padding and load/store directly to `keys` after checking the\n+  // original input array has enough space. Except at the right border, it's OK\n+  // to sort more than the current sub-array. Even if we sort across a previous\n+  // partition point, we know that keys will not migrate across it. However, we\n+  // must use the maximum size of the sorting network, because the StoreU of its\n+  // last vector would otherwise write invalid data starting at kMaxRows * cols.\n+  // const size_t N_sn = Lanes(CappedTag<T, Constants::kMaxCols>());\n+  // if (HWY_LIKELY(keys + N_sn * Constants::kMaxRows <= keys_end)) {\n }\n \n // ------------------------------ Partition\n--- hwy/ops/shared-inl.h\n@@ -297,6 +297,18 @@ using ScalableTag = typename detail::ScalableTagChecker<T, kPow2>::type;\n template <typename T, size_t kLimit, int kPow2 = 0>\n using CappedTag = typename detail::CappedTagChecker<T, kLimit, kPow2>::type;\n \n+#if !HWY_HAVE_SCALABLE\n+// If the vector size is known, and the app knows it does not want more than\n+// kLimit lanes, then capping can be beneficial. For example, AVX-512 has lower\n+// IPC and potentially higher costs for unaligned load/store vs. 256-bit AVX2.\n+template <typename T, size_t kLimit, int kPow2 = 0>\n+using CappedTagIfFixed = CappedTag<T, kLimit, kPow2>;\n+#else  // HWY_HAVE_SCALABLE\n+// .. whereas on RVV/SVE, the cost of clamping Lanes() may exceed the benefit.\n+template <typename T, size_t kLimit, int kPow2 = 0>\n+using CappedTagIfFixed = ScalableTag<T, kPow2>;\n+#endif\n+\n // Alias for a tag describing a vector with *exactly* kNumLanes active lanes,\n // even on targets with scalable vectors. Requires `kNumLanes` to be a power of\n // two not exceeding `HWY_LANES(T)`."
    ],
    "files_changed": [
      {
        "filename": "hwy/contrib/sort/result-inl.h",
        "status": "modified",
        "additions": 8,
        "deletions": 7,
        "changes": 15,
        "patch": "@@ -86,7 +86,7 @@ struct Result {\n     const double bytes = static_cast<double>(num_keys) *\n                          static_cast<double>(num_threads) *\n                          static_cast<double>(sizeof_key);\n-    printf(\"%10s: %12s: %7s: %9s: %.2E %4.0f MB/s (%2zu threads)\\n\",\n+    printf(\"%10s: %12s: %7s: %9s: %03g %4.0f MB/s (%2zu threads)\\n\",\n            hwy::TargetName(target), AlgoName(algo), key_name.c_str(),\n            DistName(dist), static_cast<double>(num_keys), bytes * 1E-6 / sec,\n            num_threads);\n@@ -115,12 +115,13 @@ bool VerifySort(Traits st, const InputStats<LaneType>& input_stats,\n     if (N1 == 2) output_stats.Notify(out[i + 1]);\n     // Reverse order instead of checking !Compare1 so we accept equal keys.\n     if (st.Compare1(out + i + N1, out + i)) {\n-      printf(\"%s: i=%d of %d lanes: N1=%d %5.0f %5.0f vs. %5.0f %5.0f\\n\\n\",\n-             caller, static_cast<int>(i), static_cast<int>(num_lanes),\n-             static_cast<int>(N1), static_cast<double>(out[i + 1]),\n-             static_cast<double>(out[i + 0]),\n-             static_cast<double>(out[i + N1 + 1]),\n-             static_cast<double>(out[i + N1]));\n+      fprintf(stderr, \"%s: i=%d of %d lanes: N1=%d\", caller,\n+              static_cast<int>(i), static_cast<int>(num_lanes),\n+              static_cast<int>(N1));\n+      fprintf(stderr, \"%5.0f %5.0f vs. %5.0f %5.0f\\n\\n\",\n+              static_cast<double>(out[i + 1]), static_cast<double>(out[i + 0]),\n+              static_cast<double>(out[i + N1 + 1]),\n+              static_cast<double>(out[i + N1]));\n       HWY_ABORT(\"%d-bit sort is incorrect\\n\",\n                 static_cast<int>(sizeof(LaneType) * 8 * N1));\n     }"
      },
      {
        "filename": "hwy/contrib/sort/sorting_networks-inl.h",
        "status": "modified",
        "additions": 31,
        "deletions": 0,
        "changes": 31,
        "patch": "@@ -72,6 +72,37 @@ struct SharedTraits : public Base {\n \n // ------------------------------ Sorting network\n \n+// Sorting network for independent columns in 8 vectors.\n+// https://bertdobbelaere.github.io/sorting_networks.html\n+template <class D, class Traits, class V = Vec<D>>\n+HWY_INLINE void Sort8(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,\n+                      V& v6, V& v7) {\n+  st.Sort2(d, v0, v2);\n+  st.Sort2(d, v1, v3);\n+  st.Sort2(d, v4, v6);\n+  st.Sort2(d, v5, v7);\n+\n+  st.Sort2(d, v0, v4);\n+  st.Sort2(d, v1, v5);\n+  st.Sort2(d, v2, v6);\n+  st.Sort2(d, v3, v7);\n+\n+  st.Sort2(d, v0, v1);\n+  st.Sort2(d, v2, v3);\n+  st.Sort2(d, v4, v5);\n+  st.Sort2(d, v6, v7);\n+\n+  st.Sort2(d, v2, v4);\n+  st.Sort2(d, v3, v5);\n+\n+  st.Sort2(d, v1, v4);\n+  st.Sort2(d, v3, v6);\n+\n+  st.Sort2(d, v1, v2);\n+  st.Sort2(d, v3, v4);\n+  st.Sort2(d, v5, v6);\n+}\n+\n // (Green's irregular) sorting network for independent columns in 16 vectors.\n template <class D, class Traits, class V = Vec<D>>\n HWY_INLINE void Sort16(D d, Traits st, V& v0, V& v1, V& v2, V& v3, V& v4, V& v5,"
      },
      {
        "filename": "hwy/contrib/sort/traits128-inl.h",
        "status": "modified",
        "additions": 29,
        "deletions": 15,
        "changes": 44,
        "patch": "@@ -413,29 +413,43 @@ struct OrderDescendingKV128 : public KeyValue128 {\n   }\n };\n \n-// Shared code that depends on Order.\n-template <class Base>\n-class Traits128 : public Base {\n-  // We want to swap 2 u128, i.e. 4 u64 lanes, based on the 0 or FF..FF mask in\n-  // the most-significant of those lanes (the result of CompareTop), so\n-  // replicate it 4x. Only called for >= 256-bit vectors.\n-  template <class V>\n-  HWY_INLINE V ReplicateTop4x(V v) const {\n+// We want to swap 2 u128, i.e. 4 u64 lanes, based on the 0 or FF..FF mask in\n+// the most-significant of those lanes (the result of CompareTop), so\n+// replicate it 4x. Only called for >= 256-bit vectors.\n+\n+#if HWY_TARGET <= HWY_AVX3\n+template <class V, HWY_IF_V_SIZE_V(V, 64)>\n+HWY_INLINE V ReplicateTop4x(V v) {\n+  return V{_mm512_permutex_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+}\n+#endif  // HWY_TARGET <= HWY_AVX3\n+\n+#if HWY_TARGET <= HWY_AVX2\n+\n+template <class V, HWY_IF_V_SIZE_V(V, 32)>\n+HWY_INLINE V ReplicateTop4x(V v) {\n+  return V{_mm256_permute4x64_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+}\n+\n+#else  // HWY_TARGET > HWY_AVX2\n+\n+template <class V>\n+HWY_INLINE V ReplicateTop4x(V v) {\n #if HWY_TARGET == HWY_SVE_256\n-    return svdup_lane_u64(v, 3);\n-#elif HWY_TARGET <= HWY_AVX3\n-    return V{_mm512_permutex_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n-#elif HWY_TARGET == HWY_AVX2\n-    return V{_mm256_permute4x64_epi64(v.raw, _MM_SHUFFLE(3, 3, 3, 3))};\n+  return svdup_lane_u64(v, 3);\n #else\n     alignas(64) static constexpr uint64_t kIndices[8] = {3, 3, 3, 3,\n                                                          7, 7, 7, 7};\n     const ScalableTag<uint64_t> d;\n     return TableLookupLanes(v, SetTableIndices(d, kIndices));\n #endif\n-  }\n+}\n \n- public:\n+#endif  // HWY_TARGET <= HWY_AVX2\n+\n+// Shared code that depends on Order.\n+template <class Base>\n+struct Traits128 : public Base {\n   template <class D>\n   HWY_INLINE Vec<D> FirstOfLanes(D d, Vec<D> v,\n                                  TFromD<D>* HWY_RESTRICT buf) const {"
      },
      {
        "filename": "hwy/contrib/sort/vqsort-inl.h",
        "status": "modified",
        "additions": 563,
        "deletions": 43,
        "changes": 606,
        "patch": "@@ -123,61 +123,581 @@ void HeapSort(Traits st, T* HWY_RESTRICT lanes, const size_t num_lanes) {\n \n // ------------------------------ BaseCase\n \n-// Sorts `keys` within the range [0, num) via sorting network.\n-template <class D, class Traits, typename T>\n-HWY_INLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n-                         T* HWY_RESTRICT keys_end, size_t num,\n+// Special cases where `num_lanes` is in the specified range (inclusive).\n+template <class Traits, typename T>\n+HWY_INLINE void Sort0To8(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n                          T* HWY_RESTRICT buf) {\n-  const size_t N = Lanes(d);\n-  using V = decltype(Zero(d));\n-\n-  // _Nonzero32 requires num - 1 != 0.\n-  if (HWY_UNLIKELY(num <= 1)) return;\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(num_keys <= 8);\n+  HWY_ASSUME(num_keys <= 8);  // reduces branches\n+\n+  // Copy into buffer so we only require a single masked load. Prevent using\n+  // unnecessarily large vectors by capping - this sorting network only handles\n+  // eight keys, so it does not make sense to load more.\n+  const CappedTagIfFixed<T, 8 * kLPK> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // Unless num_keys == 8, we pad the input because the sorting network is\n+  // fixed-size.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Values to sort, and where to store the sorted output. This may point to\n+  // buf, in which case it must be copied back to `keys`.\n+  T* HWY_RESTRICT in_out = keys;\n+  // Valid lanes in the last vector (only used if num_keys != 8).\n+  Mask<decltype(dmax)> mask;\n+\n+  // Not all of the sorting network will be used. May require padding.\n+  if (HWY_UNLIKELY(num_keys != 8)) {\n+    // Done if there are zero or one keys to sort.\n+    if (HWY_UNLIKELY(num_keys <= 1)) return;\n+\n+    in_out = buf;\n+    // Copy whole vectors(s)\n+    size_t i = 0;\n+    for (; i + Nmax <= num_lanes; i += Nmax) {\n+      Store(LoadU(dmax, keys + i), dmax, buf + i);\n+    }\n \n-  // Reshape into a matrix with kMaxRows rows, and columns limited by the\n-  // 1D `num`, which is upper-bounded by the vector width (see BaseCaseNum).\n-  const size_t num_pow2 = size_t{1}\n-                          << (32 - Num0BitsAboveMS1Bit_Nonzero32(\n-                                       static_cast<uint32_t>(num - 1)));\n-  HWY_DASSERT(num <= num_pow2 && num_pow2 <= Constants::BaseCaseNum(N));\n-  const size_t cols =\n-      HWY_MAX(st.LanesPerKey(), num_pow2 >> Constants::kMaxRowsLog2);\n-  HWY_DASSERT(cols <= N);\n+    // Last iteration: copy partial vector.\n+    const size_t remaining = num_lanes - i;\n+    HWY_ASSUME(remaining < 256);  // helps FirstN\n+    mask = FirstN(dmax, remaining);\n+    Store(MaskedLoadOr(kPadding, mask, dmax, keys + i), dmax, buf + i);\n+\n+    // If `d` is for exactly one key, we initialize 8 * kLPK lanes, and not just\n+    // for MSAN: padding prevents invalid values from influencing the sort.\n+    // But for 16-bit keys, loading a single lane may be inefficient, so we\n+    // allow larger vectors than necessary. Only the first lane is used; the\n+    // next lane is usually loaded from the next valid keys, but we require one\n+    // more padding for the last vector loaded.\n+    constexpr size_t extra = sizeof(T) == 2 ? 1 : 0;\n+    for (i += Nmax; i < 8 * kLPK + extra; i += Nmax) {\n+      Store(kPadding, dmax, buf + i);\n+    }\n+  }\n \n-  // We can avoid padding and load/store directly to `keys` after checking the\n-  // original input array has enough space. Except at the right border, it's OK\n-  // to sort more than the current sub-array. Even if we sort across a previous\n-  // partition point, we know that keys will not migrate across it. However, we\n-  // must use the maximum size of the sorting network, because the StoreU of its\n-  // last vector would otherwise write invalid data starting at kMaxRows * cols.\n-  const size_t N_sn = Lanes(CappedTag<T, Constants::kMaxCols>());\n-  if (HWY_LIKELY(keys + N_sn * Constants::kMaxRows <= keys_end)) {\n-    SortingNetwork(st, keys, N_sn);\n-    return;\n+  // One key per vector (32.. 2x64 bit), or two for 16-bit (see above).\n+  const CappedTag<T, HWY_MAX(kLPK, 4 / sizeof(T))> d;\n+  using V = Vec<decltype(d)>;\n+  using M = Mask<decltype(d)>;\n+\n+  V v0 = LoadU(d, in_out + 0x0 * kLPK);\n+  V v1 = LoadU(d, in_out + 0x1 * kLPK);\n+  V v2 = LoadU(d, in_out + 0x2 * kLPK);\n+  V v3 = LoadU(d, in_out + 0x3 * kLPK);\n+  V v4 = LoadU(d, in_out + 0x4 * kLPK);\n+  V v5 = LoadU(d, in_out + 0x5 * kLPK);\n+  V v6 = LoadU(d, in_out + 0x6 * kLPK);\n+  V v7 = LoadU(d, in_out + 0x7 * kLPK);\n+\n+  Sort8(d, st, v0, v1, v2, v3, v4, v5, v6, v7);\n+\n+  StoreU(v0, d, in_out + 0x0 * kLPK);\n+  StoreU(v1, d, in_out + 0x1 * kLPK);\n+  StoreU(v2, d, in_out + 0x2 * kLPK);\n+  StoreU(v3, d, in_out + 0x3 * kLPK);\n+  StoreU(v4, d, in_out + 0x4 * kLPK);\n+  StoreU(v5, d, in_out + 0x5 * kLPK);\n+  StoreU(v6, d, in_out + 0x6 * kLPK);\n+  StoreU(v7, d, in_out + 0x7 * kLPK);\n+\n+  // in_out == buf, so copy back to `keys`.\n+  if (HWY_UNLIKELY(num_keys != 8)) {\n+    size_t i = 0;\n+    // Copy whole vectors(s).\n+    for (; i + Nmax <= num_lanes; i += Nmax) {\n+      StoreU(Load(dmax, buf + i), dmax, keys + i);\n+    }\n+    // Copy partial vector.\n+    BlendedStore(Load(dmax, buf + i), mask, dmax, keys + i);\n   }\n+}\n+\n+template <size_t kLPK, size_t kLanesPerRow, class D, class Traits,\n+          typename T = TFromD<D>>\n+HWY_INLINE void CopyHalfToPaddedBuf(D d, Traits st, T* HWY_RESTRICT keys,\n+                                    size_t num_lanes, T* HWY_RESTRICT buf) {\n+  constexpr size_t kMinLanes = 8 * kLanesPerRow;\n+  // Must cap for correctness: we will load up to the last valid lane, so\n+  // Lanes(dmax) must not exceed `num_lanes` (known to be at least kMinLanes).\n+  const CappedTag<T, kMinLanes> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+  HWY_DASSERT(Nmax < num_lanes);\n+  HWY_ASSUME(Nmax <= kMinLanes);\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const Vec<decltype(dmax)> kPadding = st.LastValue(dmax);\n+\n+  // Rounding down allows aligned stores, which are typically faster.\n+  size_t i = num_lanes & ~(Nmax - 1);\n+  HWY_ASSUME(i != 0);  // because Nmax <= num_lanes; avoids branch\n+  do {\n+    Store(kPadding, dmax, buf + i);\n+    i += Nmax;\n+    // Initialize enough for the last vector even if Lanes > lanes_per_row.\n+  } while (i < 0xf * kLanesPerRow + Lanes(d));\n+\n+  // Ensure buf contains all we will read, and perhaps more before.\n+  ptrdiff_t end = static_cast<ptrdiff_t>(num_lanes);\n+  do {\n+    end -= Nmax;\n+    StoreU(LoadU(dmax, keys + end), dmax, buf + end);\n+  } while (end > static_cast<ptrdiff_t>(8 * kLanesPerRow));\n+}\n \n-  // Copy `keys` to `buf`.\n-  size_t i;\n-  for (i = 0; i + N <= num; i += N) {\n-    Store(LoadU(d, keys + i), d, buf + i);\n+// Stores all vectors into buf and copies them into keys without exceeding\n+// `num_lanes`.\n+template <size_t kLanesPerRow, class D, typename T = TFromD<D>>\n+HWY_INLINE void SafeStore(D d, Vec<D> v8, Vec<D> v9, Vec<D> va, Vec<D> vb,\n+                          Vec<D> vc, Vec<D> vd, Vec<D> ve, Vec<D> vf,\n+                          T* HWY_RESTRICT keys, size_t num_lanes,\n+                          T* HWY_RESTRICT buf) {\n+  StoreU(v8, d, buf + 0x8 * kLanesPerRow);\n+  StoreU(v9, d, buf + 0x9 * kLanesPerRow);\n+  StoreU(va, d, buf + 0xa * kLanesPerRow);\n+  StoreU(vb, d, buf + 0xb * kLanesPerRow);\n+  StoreU(vc, d, buf + 0xc * kLanesPerRow);\n+  StoreU(vd, d, buf + 0xd * kLanesPerRow);\n+  StoreU(ve, d, buf + 0xe * kLanesPerRow);\n+  StoreU(vf, d, buf + 0xf * kLanesPerRow);\n+\n+  const ScalableTag<T> dmax;\n+  const size_t Nmax = Lanes(dmax);\n+\n+  // The first eight vectors have already been stored unconditionally into\n+  // `keys`, so we do not copy them.\n+  size_t i = 8 * kLanesPerRow;\n+#pragma unroll(1)\n+  for (; i + Nmax <= num_lanes; i += Nmax) {\n+    StoreU(LoadU(dmax, buf + i), dmax, keys + i);\n   }\n-  SafeCopyN(num - i, d, keys + i, buf + i);\n-  i = num;\n+\n+  // Last iteration: copy partial vector\n+  const size_t remaining = num_lanes - i;\n+  HWY_ASSUME(remaining < 256);  // helps FirstN\n+  Mask<decltype(dmax)> mask = FirstN(dmax, remaining);\n+  BlendedStore(LoadU(dmax, buf + i), mask, dmax, keys + i);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort9To16(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                          T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(9 <= num_keys && num_keys <= 16);\n+  HWY_ASSUME(num_keys <= 16);\n+  // 16 keys divided by 16 rows equals 1 column.\n+  constexpr size_t kLanesPerRow = 1 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort17To32(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                           T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  const size_t num_keys = num_lanes / kLPK;\n+  HWY_DASSERT(17 <= num_keys && num_keys <= 32);\n+  HWY_ASSUME(num_keys <= 32);\n+  (void)num_keys;\n+  // 32 keys divided by 16 rows equals 2 columns.\n+  constexpr size_t kLanesPerRow = 2 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort33To64(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                           T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(33 * kLPK <= num_lanes && num_lanes <= 64 * kLPK);\n+  // 64 keys divided by 16 rows equals 4 columns.\n+  constexpr size_t kLanesPerRow = 4 * kLPK;\n+\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 32 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else   // !HWY_MEM_OPS_MIGHT_FAULT\n+  // To prevent reading past the end, only activate the lanes corresponding to\n+  // the first four keys (other lanes' masks will be false).\n+  const V vnum_lanes = IfThenElseZero(FirstN(d, kLanesPerRow),\n+                                      Set(d, static_cast<T>(num_lanes)));\n+  const V kIota = Iota(d, T{32 * kLPK});\n+  const V k4 = Set(d, T{kLanesPerRow});\n+  const V k8 = Add(k4, k4);\n+  const V k16 = Add(k8, k8);\n+  const V k32 = Add(k16, k16);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k4));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k8));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k8, k4)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k16));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k16, k4)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Add(k16, k8)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k32, k4)));\n \n   // Fill with padding - last in sort order, not copied to keys.\n   const V kPadding = st.LastValue(d);\n-  // Initialize an extra vector because SortingNetwork loads full vectors,\n-  // which may exceed cols*kMaxRows.\n-  for (; i < (cols * Constants::kMaxRows + N); i += N) {\n-    StoreU(kPadding, d, buf + i);\n-  }\n \n-  SortingNetwork(st, buf, cols);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort65To128(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                            T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(65 * kLPK <= num_lanes && num_lanes <= 128 * kLPK);\n+  // 128 keys divided by 16 rows equals 8 columns.\n+  constexpr size_t kLanesPerRow = 8 * kLPK;\n+\n+  // Eight keys per vector (128..512 bit).\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 64 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else\n+  // All lanes are now valid, so no need for FirstN.\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n+  const V kIota = Iota(d, T{64 * kLPK});\n+  const V k8 = Set(d, T{kLanesPerRow});\n+  const V k16 = Add(k8, k8);\n+  const V k32 = Add(k16, k16);\n+  const V k64 = Add(k32, k32);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k8));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k16));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k16, k8)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k32));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k32, k8)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Sub(k64, k16)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k64, k8)));\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const V kPadding = st.LastValue(d);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  //  HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+#if !HWY_COMPILER_MSVC\n+\n+template <class Traits, typename T>\n+HWY_INLINE void Sort129To256(Traits st, T* HWY_RESTRICT keys, size_t num_lanes,\n+                             T* HWY_RESTRICT buf) {\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_DASSERT(129 * kLPK <= num_lanes && num_lanes <= 256 * kLPK);\n+  // 256 keys divided by 16 rows equals 16 columns.\n+  constexpr size_t kLanesPerRow = 16 * kLPK;\n+\n+  // 16 keys per vector (256..512 bit).\n+  const CappedTag<T, kLanesPerRow> d;\n+  using V = Vec<decltype(d)>;\n+\n+  // We know there are at least 64 keys (8 vectors), so load unconditionally.\n+  V v0 = LoadU(d, keys + 0x0 * kLanesPerRow);\n+  V v1 = LoadU(d, keys + 0x1 * kLanesPerRow);\n+  V v2 = LoadU(d, keys + 0x2 * kLanesPerRow);\n+  V v3 = LoadU(d, keys + 0x3 * kLanesPerRow);\n+  V v4 = LoadU(d, keys + 0x4 * kLanesPerRow);\n+  V v5 = LoadU(d, keys + 0x5 * kLanesPerRow);\n+  V v6 = LoadU(d, keys + 0x6 * kLanesPerRow);\n+  V v7 = LoadU(d, keys + 0x7 * kLanesPerRow);\n+\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  CopyHalfToPaddedBuf<kLPK, kLanesPerRow>(d, st, keys, num_lanes, buf);\n+  V v8 = LoadU(d, buf + 0x8 * kLanesPerRow);\n+  V v9 = LoadU(d, buf + 0x9 * kLanesPerRow);\n+  V va = LoadU(d, buf + 0xa * kLanesPerRow);\n+  V vb = LoadU(d, buf + 0xb * kLanesPerRow);\n+  V vc = LoadU(d, buf + 0xc * kLanesPerRow);\n+  V vd = LoadU(d, buf + 0xd * kLanesPerRow);\n+  V ve = LoadU(d, buf + 0xe * kLanesPerRow);\n+  V vf = LoadU(d, buf + 0xf * kLanesPerRow);\n+#else\n+  // All lanes are now valid, so no need for FirstN.\n+  const V vnum_lanes = Set(d, static_cast<T>(num_lanes));\n+  const V kIota = Iota(d, T{128 * kLPK});\n+  const V k16 = Set(d, T{kLanesPerRow});\n+  const V k32 = Add(k16, k16);\n+  const V k64 = Add(k32, k32);\n+  const V k128 = Add(k64, k64);\n+\n+  using M = Mask<decltype(d)>;\n+  const M m8 = Gt(vnum_lanes, kIota);\n+  const M m9 = Gt(vnum_lanes, Add(kIota, k16));\n+  const M ma = Gt(vnum_lanes, Add(kIota, k32));\n+  const M mb = Gt(vnum_lanes, Add(kIota, Add(k32, k16)));\n+  const M mc = Gt(vnum_lanes, Add(kIota, k64));\n+  const M md = Gt(vnum_lanes, Add(kIota, Add(k64, k16)));\n+  const M me = Gt(vnum_lanes, Add(kIota, Sub(k128, k32)));\n+  const M mf = Gt(vnum_lanes, Add(kIota, Sub(k128, k16)));\n+\n+  // Fill with padding - last in sort order, not copied to keys.\n+  const V kPadding = st.LastValue(d);\n+  V v8 = MaskedLoadOr(kPadding, m8, d, keys + 0x8 * kLanesPerRow);\n+  V v9 = MaskedLoadOr(kPadding, m9, d, keys + 0x9 * kLanesPerRow);\n+  V va = MaskedLoadOr(kPadding, ma, d, keys + 0xa * kLanesPerRow);\n+  V vb = MaskedLoadOr(kPadding, mb, d, keys + 0xb * kLanesPerRow);\n+  V vc = MaskedLoadOr(kPadding, mc, d, keys + 0xc * kLanesPerRow);\n+  V vd = MaskedLoadOr(kPadding, md, d, keys + 0xd * kLanesPerRow);\n+  V ve = MaskedLoadOr(kPadding, me, d, keys + 0xe * kLanesPerRow);\n+  V vf = MaskedLoadOr(kPadding, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  //  HWY_MEM_OPS_MIGHT_FAULT\n+\n+  Sort16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge2(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge4(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge8(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf);\n+  Merge16(d, st, v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve,\n+          vf);\n+\n+  StoreU(v0, d, keys + 0x0 * kLanesPerRow);\n+  StoreU(v1, d, keys + 0x1 * kLanesPerRow);\n+  StoreU(v2, d, keys + 0x2 * kLanesPerRow);\n+  StoreU(v3, d, keys + 0x3 * kLanesPerRow);\n+  StoreU(v4, d, keys + 0x4 * kLanesPerRow);\n+  StoreU(v5, d, keys + 0x5 * kLanesPerRow);\n+  StoreU(v6, d, keys + 0x6 * kLanesPerRow);\n+  StoreU(v7, d, keys + 0x7 * kLanesPerRow);\n+#if HWY_MEM_OPS_MIGHT_FAULT\n+  SafeStore<kLanesPerRow>(d, v8, v9, va, vb, vc, vd, ve, vf, keys, num_lanes,\n+                          buf);\n+#else\n+  BlendedStore(v8, m8, d, keys + 0x8 * kLanesPerRow);\n+  BlendedStore(v9, m9, d, keys + 0x9 * kLanesPerRow);\n+  BlendedStore(va, ma, d, keys + 0xa * kLanesPerRow);\n+  BlendedStore(vb, mb, d, keys + 0xb * kLanesPerRow);\n+  BlendedStore(vc, mc, d, keys + 0xc * kLanesPerRow);\n+  BlendedStore(vd, md, d, keys + 0xd * kLanesPerRow);\n+  BlendedStore(ve, me, d, keys + 0xe * kLanesPerRow);\n+  BlendedStore(vf, mf, d, keys + 0xf * kLanesPerRow);\n+#endif  // HWY_MEM_OPS_MIGHT_FAULT\n+}\n+\n+#endif  // HWY_COMPILER_MSVC\n \n-  for (i = 0; i + N <= num; i += N) {\n-    StoreU(Load(d, buf + i), d, keys + i);\n+// Sorts `keys` within the range [0, num_lanes) via sorting network. We know\n+// that num_lanes <= kMaxRows * kMaxCols.\n+template <class D, class Traits, typename T>\n+HWY_INLINE void BaseCase(D d, Traits st, T* HWY_RESTRICT keys,\n+                         T* HWY_RESTRICT keys_end, size_t num_lanes,\n+                         T* HWY_RESTRICT buf) {\n+  constexpr size_t kMaxLanes = MaxLanes(d);\n+  constexpr size_t kLPK = st.LanesPerKey();\n+  HWY_ASSERT(num_lanes * sizeof(T) <= 16 * HWY_MAX_BYTES);\n+\n+  // TODO(janwas): jump table using Num0BitsAboveMS1Bit_Nonzero32?\n+  if (num_lanes <= 8 * 1 * kLPK) {\n+    return Sort0To8(st, keys, num_lanes, buf);\n+  }\n+  // Do not check kMaxLanes - also works for single-key vectors\n+  if (num_lanes <= 16 * 1 * kLPK) {\n+    return Sort9To16(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 2 * kLPK && kMaxLanes >= 2) {\n+    return Sort17To32(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 4 * kLPK && kMaxLanes >= 4) {\n+    return Sort33To64(st, keys, num_lanes, buf);\n+  }\n+  if (num_lanes <= 16 * 8 * kLPK && kMaxLanes >= 8) {\n+    return Sort65To128(st, keys, num_lanes, buf);\n+  }\n+\n+#if !HWY_COMPILER_MSVC\n+  if (kMaxLanes >= 16) {\n+    Sort129To256(st, keys, num_lanes, buf);\n   }\n-  SafeCopyN(num - i, d, buf + i, keys + i);\n+#endif\n+\n+  // TODO(janwas): avoid masking if in-bounds\n+  // We can avoid padding and load/store directly to `keys` after checking the\n+  // original input array has enough space. Except at the right border, it's OK\n+  // to sort more than the current sub-array. Even if we sort across a previous\n+  // partition point, we know that keys will not migrate across it. However, we\n+  // must use the maximum size of the sorting network, because the StoreU of its\n+  // last vector would otherwise write invalid data starting at kMaxRows * cols.\n+  // const size_t N_sn = Lanes(CappedTag<T, Constants::kMaxCols>());\n+  // if (HWY_LIKELY(keys + N_sn * Constants::kMaxRows <= keys_end)) {\n }\n \n // ------------------------------ Partition"
      },
      {
        "filename": "hwy/ops/shared-inl.h",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -297,6 +297,18 @@ using ScalableTag = typename detail::ScalableTagChecker<T, kPow2>::type;\n template <typename T, size_t kLimit, int kPow2 = 0>\n using CappedTag = typename detail::CappedTagChecker<T, kLimit, kPow2>::type;\n \n+#if !HWY_HAVE_SCALABLE\n+// If the vector size is known, and the app knows it does not want more than\n+// kLimit lanes, then capping can be beneficial. For example, AVX-512 has lower\n+// IPC and potentially higher costs for unaligned load/store vs. 256-bit AVX2.\n+template <typename T, size_t kLimit, int kPow2 = 0>\n+using CappedTagIfFixed = CappedTag<T, kLimit, kPow2>;\n+#else  // HWY_HAVE_SCALABLE\n+// .. whereas on RVV/SVE, the cost of clamping Lanes() may exceed the benefit.\n+template <typename T, size_t kLimit, int kPow2 = 0>\n+using CappedTagIfFixed = ScalableTag<T, kPow2>;\n+#endif\n+\n // Alias for a tag describing a vector with *exactly* kNumLanes active lanes,\n // even on targets with scalable vectors. Requires `kNumLanes` to be a power of\n // two not exceeding `HWY_LANES(T)`."
      }
    ],
    "lines_added": 643,
    "lines_removed": 65
  },
  "issues": [],
  "pull_requests": [],
  "build_info": {
    "old_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/old -B /test_workspace/workspace/old/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
      "cmake --build /test_workspace/workspace/old/build -- -j 1"
    ],
    "new_build_script": [
      "apt-get update",
      "cmake -S /test_workspace/workspace/new -B /test_workspace/workspace/new/build -DCMAKE_BUILD_TYPE=Debug -DCMAKE_EXPORT_COMPILE_COMMANDS=ON -DBUILD_TESTING=ON -DHWY_ENABLE_TESTS=ON",
      "cmake --build /test_workspace/workspace/new/build -- -j 1"
    ],
    "old_test_script": [
      "cd /test_workspace/workspace/old/build",
      "ctest --output-on-failure"
    ],
    "new_test_script": [
      "cd /test_workspace/workspace/new/build",
      "ctest --output-on-failure"
    ],
    "build_system": "cmake"
  },
  "performance_analysis": {
    "is_significant": false,
    "p_value": 1.0,
    "is_pair_significant": false,
    "pair_p_value": 1.0,
    "is_binom_significant": false,
    "binom_p_value": 1.0,
    "is_wilcoxon_significant": false,
    "wilcoxon_p_value": 0.9999991338918834,
    "is_mannwhitney_significant": false,
    "mannwhitney_p_value": 1.502374570335007e-11,
    "relative_improvement": 0.02574453871142547,
    "absolute_improvement_ms": 2993.000000000009,
    "old_mean_ms": 116257.66666666667,
    "new_mean_ms": 113264.66666666666,
    "old_std_ms": 388.89483848760256,
    "new_std_ms": 435.12767316504073,
    "effect_size_cohens_d": 7.252957770002445,
    "old_ci95_ms": [
      116112.45094742185,
      116402.88238591148
    ],
    "new_ci95_ms": [
      113102.18732323355,
      113427.14601009976
    ],
    "old_ci99_ms": [
      116061.95731781311,
      116453.37601552022
    ],
    "new_ci99_ms": [
      113045.69087898465,
      113483.64245434866
    ],
    "new_times_s": [
      113.54,
      113.01,
      113.02,
      113.82,
      113.77,
      113.23,
      113.16,
      113.27,
      112.83,
      112.92,
      113.16,
      113.34,
      114.09,
      113.19,
      113.04,
      113.76,
      114.12,
      113.04,
      114.09,
      113.54,
      113.48,
      113.46,
      113.16,
      112.51,
      113.36,
      112.93,
      112.93,
      112.65,
      112.59,
      112.91,
      113.56
    ],
    "old_times_s": [
      116.15,
      116.8,
      115.92,
      116.81,
      115.94,
      116.06,
      116.48,
      116.34,
      116.36,
      116.87,
      116.64,
      116.12,
      116.35,
      116.46,
      116.27,
      116.45,
      116.9,
      116.47,
      116.3,
      116.2,
      116.46,
      116.38,
      116.85,
      115.62,
      115.6,
      116.01,
      115.59,
      115.87,
      115.83,
      115.79,
      115.99
    ]
  },
  "tests": {
    "total_tests": 1,
    "significant_improvements": 0,
    "significant_improvements_tests": [],
    "significant_regressions": 0,
    "significant_regressions_tests": [],
    "significant_pair_improvements": 0,
    "significant_pair_improvements_tests": [],
    "significant_pair_regressions": 0,
    "significant_pair_regressions_tests": [],
    "significant_binom_improvements": 0,
    "significant_binom_improvements_tests": [],
    "significant_binom_regressions": 0,
    "significant_binom_regressions_tests": [],
    "significant_wilcoxon_improvements": 0,
    "significant_wilcoxon_improvements_tests": [],
    "significant_wilcoxon_regressions": 0,
    "significant_wilcoxon_regressions_tests": [],
    "significant_mannwhitney_improvements": 0,
    "significant_mannwhitney_improvements_tests": [],
    "significant_mannwhitney_regressions": 0,
    "significant_mannwhitney_regressions_tests": [],
    "tests": [
      {
        "test_name": "NanobenchmarkTest.RunAll",
        "is_significant": false,
        "p_value": 0.674275610436539,
        "is_pair_significant": false,
        "pair_p_value": 0.6495266527735724,
        "is_binom_significant": false,
        "binom_p_value": 0.8275357745587826,
        "is_wilcoxon_significant": false,
        "wilcoxon_p_value": 0.8418457206527268,
        "is_mannwhitney_significant": false,
        "mannwhitney_p_value": 0.5327018134637911,
        "relative_improvement": 0.02530612244897963,
        "absolute_improvement_ms": 11.071428571428621,
        "old_mean_ms": 437.50000000000006,
        "new_mean_ms": 426.42857142857144,
        "old_std_ms": 65.35515508585605,
        "new_std_ms": 109.24706236288753,
        "effect_size_cohens_d": 0.12299227544885226,
        "old_ci95_ms": [
          412.1579164387709,
          462.84208356122923
        ],
        "new_ci95_ms": [
          384.0669829726629,
          468.79015988448
        ],
        "old_ci99_ms": [
          403.27939627221366,
          471.72060372778645
        ],
        "new_ci99_ms": [
          369.22573219371793,
          483.6314106634249
        ],
        "new_times": [
          0.24,
          0.2,
          0.22,
          0.56,
          0.51,
          0.37,
          0.42,
          0.47,
          0.46,
          0.36,
          0.5,
          0.54,
          0.55,
          0.42,
          0.52,
          0.51,
          0.62,
          0.5,
          0.43,
          0.39,
          0.37,
          0.42,
          0.46,
          0.49,
          0.38,
          0.38,
          0.45,
          0.2
        ],
        "old_times": [
          0.34,
          0.52,
          0.47,
          0.42,
          0.47,
          0.56,
          0.45,
          0.46,
          0.5,
          0.4,
          0.43,
          0.49,
          0.37,
          0.42,
          0.34,
          0.3,
          0.38,
          0.44,
          0.48,
          0.37,
          0.43,
          0.48,
          0.44,
          0.43,
          0.38,
          0.47,
          0.42,
          0.59
        ]
      }
    ]
  },
  "logs": {
    "full_log_path": "/logs/full.log",
    "config_log_path": "/logs/config.log",
    "build_log_path": "/logs/build.log",
    "test_log_path": "/logs/test.log",
    "build_success": true,
    "test_success": true
  },
  "raw_timing_data": {
    "warmup_runs": 1,
    "measurement_runs": 30,
    "min_exec_time_improvement": 0.05,
    "min_p_value": 0.05
  }
}